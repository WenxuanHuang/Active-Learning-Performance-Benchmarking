{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic.ipynb",
      "provenance": [],
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d"
    },
    "kernelspec": {
      "name": "python382jvsc74a57bd05edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9",
      "display_name": "Python 3.8.2 64-bit"
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from scipy.special import expit\n",
        "from scipy import stats\n",
        "from pylab import rcParams\n",
        "import mplcursors\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, \\\n",
        "    GradientBoostingClassifier\n",
        "from sklearn import neighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "# pd.options.display.max_rows = 20\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "max_queried = 500\n",
        "trainset_size = 1302"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        " def data_prep():\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/ML-for-COVID-19-dataset/main/all_training.csv\", sep=',')\n",
        "    # Column selection\n",
        "    df = data.iloc[:,np.r_[3:34]].copy()\n",
        "    # define row and column index\n",
        "    col = df.columns\n",
        "    row = [i for i in range(df.shape[0])]\n",
        "    # define imputer\n",
        "    imputer = IterativeImputer(estimator=linear_model.BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
        "    # fit on the dataset\n",
        "    imputer.fit(df)\n",
        "    # transform the dataset\n",
        "    df_imputed = imputer.transform(df)\n",
        "    # convert back to pandas dataframe and rename back to df_normalized\n",
        "    df = pd.DataFrame(data=df_imputed, index=row, columns=col)\n",
        "    X = df\n",
        "    y = data.target    \n",
        "    # Recursive feature elimination\n",
        "    rdmreg = RandomForestClassifier(n_estimators=100)\n",
        "    # Define the method\n",
        "    rfe = RFE(estimator=rdmreg, n_features_to_select=10)\n",
        "    # Fit the model\n",
        "    rfe = rfe.fit(X, y.values.ravel())\n",
        "    print(rfe.support_)\n",
        "    # Drop columns that failed RFE test\n",
        "    col = df.columns[rfe.support_]\n",
        "    X = X[col]\n",
        "    X = X.to_numpy()\n",
        "    print ('df:', X.shape, y.shape)\n",
        "    return (X, y)\n",
        "\n",
        "\n",
        "def split(train_size):\n",
        "    X_train_full = X[:train_size]\n",
        "    y_train_full = y[:train_size]\n",
        "    X_test = X[train_size:]\n",
        "    y_test = y[train_size:]   \n",
        "    return (X_train_full, y_train_full, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SvmModel(BaseModel):\n",
        "\n",
        "    model_type = 'Support Vector Machine with linear Kernel'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training svm...')\n",
        "        self.classifier = SVC(\n",
        "            C=1, \n",
        "            kernel='linear', \n",
        "            probability=True,\n",
        "            class_weight=c_weight\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    model_type = 'Logistic Regression' \n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training logistic regression...')\n",
        "        train_samples = X_train.shape[0]\n",
        "        self.classifier = LogisticRegression(\n",
        "            C=50. / train_samples,\n",
        "            penalty='l1',\n",
        "            solver='liblinear',\n",
        "            tol=0.1,\n",
        "            class_weight=c_weight\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class RfModel(BaseModel):\n",
        "\n",
        "    model_type = 'Random Forest'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training random forest...')\n",
        "        self.classifier = RandomForestClassifier(\n",
        "            n_estimators=500, \n",
        "            class_weight=c_weight, \n",
        "            n_jobs=-1\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
        "\n",
        "class GDBCModel(BaseModel):\n",
        "\n",
        "    model_type = 'Gradient Boost classifier'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training GDBC...')\n",
        "        self.classifier = GradientBoostingClassifier(\n",
        "            n_estimators=1200,\n",
        "            max_depth=3,\n",
        "            subsample=0.5,\n",
        "            learning_rate=0.01,\n",
        "            min_samples_leaf=1,\n",
        "            random_state=3\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
        "\n",
        "class KnnModel(BaseModel):\n",
        "\n",
        "    model_type = 'Nearest Neighbour classifier'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training KNN...')\n",
        "        self.classifier = neighbors.KNeighborsClassifier(\n",
        "            n_neighbors = 10,\n",
        "            n_jobs = -1\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    # we train normally and use the probabilities to select the most uncertain samples\n",
        "    def train(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('Train set:', X_train.shape)\n",
        "        print ('Validation set:', X_val.shape)\n",
        "        print ('Test set:', X_test.shape)\n",
        "        t0 = time.time()\n",
        "        (X_train, X_val, X_test, self.val_y_predicted,\n",
        "         self.test_y_predicted) = \\\n",
        "            self.model_object.fit_predict(X_train, y_train, X_val, X_test, c_weight)\n",
        "        self.run_time = time.time() - t0\n",
        "        return (X_train, X_val, X_test)\n",
        "\n",
        "    # we want accuracy only for the test set\n",
        "    def get_test_accuracy(self, i, y_test):\n",
        "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        print('--------------------------------')\n",
        "        print('y-test set:',y_test.shape)\n",
        "        print('Training run in %.3f s' % self.run_time,'\\n')\n",
        "        print(\"Accuracy rate is %f \" % (classif_rate))    \n",
        "        print(\"Classification report for %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
        "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
        "        print('--------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseSelectionFunction(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def select(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomSelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        random_state = check_random_state(0)\n",
        "        selection = np.random.choice(probas_val.shape[0], initial_labeled_samples, replace=False)\n",
        "        print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',initial_labeled_samples)\n",
        "        return selection\n",
        "\n",
        "\n",
        "class EntropySelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "        selection = (np.argsort(e)[::-1])[:initial_labeled_samples]\n",
        "        return selection\n",
        "\n",
        "class MinStdSelection(BaseSelectionFunction):\n",
        "\n",
        "    # select the samples where the std is smallest. There is uncertainty regarding the relevant class\n",
        "    # and then train on these \"hard\" to classify samples.\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        std = np.std(probas_val * 100, axis=1)\n",
        "        selection = std.argsort()[:initial_labeled_samples]\n",
        "        selection = selection.astype('int64')\n",
        "        print('std',std.shape,std)\n",
        "        print('selection',selection, selection.shape, std[selection])\n",
        "        return selection\n",
        "      \n",
        "      \n",
        "class MarginSamplingSelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        rev = np.sort(probas_val, axis=1)[:, ::-1]\n",
        "        values = rev[:, 0] - rev[:, 1]\n",
        "        selection = np.argsort(values)[:initial_labeled_samples]\n",
        "        return selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Normalize(object):\n",
        "    \n",
        "    def normalize(self, X_train, X_val, X_test):\n",
        "        self.scaler = RobustScaler()\n",
        "        X_train = self.scaler.fit_transform(X_train)\n",
        "        X_val   = self.scaler.transform(X_val)\n",
        "        X_test  = self.scaler.transform(X_test)\n",
        "        return (X_train, X_val, X_test) \n",
        "    \n",
        "    def inverse(self, X_train, X_val, X_test):\n",
        "        X_train = self.scaler.inverse_transform(X_train)\n",
        "        X_val   = self.scaler.inverse_transform(X_val)\n",
        "        X_test  = self.scaler.inverse_transform(X_test)\n",
        "        return (X_train, X_val, X_test) "
      ]
    },
    {
      "source": [
        "def get_k_random_samples(initial_labeled_samples, X_train_full,\n",
        "                         y_train_full):\n",
        "    random_state = check_random_state(0)\n",
        "    permutation = np.random.choice(trainset_size,\n",
        "                                   initial_labeled_samples,\n",
        "                                   replace=False)\n",
        "    print ()\n",
        "    print ('initial random chosen samples', permutation.shape),\n",
        "    X_train = X_train_full[permutation]\n",
        "    y_train = y_train_full[permutation]\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "    bin_count = np.bincount(y_train.astype('int64'))\n",
        "    unique = np.unique(y_train.astype('int64'))\n",
        "    print (\n",
        "        'initial train set:',\n",
        "        X_train.shape,\n",
        "        y_train.shape,\n",
        "        'unique(labels):',\n",
        "        bin_count,\n",
        "        unique,\n",
        "        )\n",
        "    return (permutation, X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TheAlgorithm(object):\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    def __init__(self, initial_labeled_samples, model_object, selection_function):\n",
        "        self.initial_labeled_samples = initial_labeled_samples\n",
        "        self.model_object = model_object\n",
        "        self.sample_selection_function = selection_function\n",
        "\n",
        "    def run(self, X_train_full, y_train_full, X_test, y_test):\n",
        "\n",
        "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
        "        (permutation, X_train, y_train) = \\\n",
        "            get_k_random_samples(self.initial_labeled_samples,\n",
        "                                 X_train_full, y_train_full)\n",
        "        self.queried = self.initial_labeled_samples\n",
        "        self.samplecount = [self.initial_labeled_samples]\n",
        "\n",
        "        # assign the val set the rest of the 'unlabelled' training data\n",
        "        X_val = np.array([])\n",
        "        y_val = np.array([])\n",
        "        X_val = np.copy(X_train_full)\n",
        "        X_val = np.delete(X_val, permutation, axis=0)\n",
        "        y_val = np.copy(y_train_full)\n",
        "        y_val = np.delete(y_val, permutation, axis=0)\n",
        "        print ('Val set:', X_val.shape, y_val.shape, permutation.shape)\n",
        "        print ()\n",
        "\n",
        "        # normalize data\n",
        "        normalizer = Normalize()\n",
        "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)   \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(1, y_test)\n",
        "\n",
        "        while self.queried < max_queried:\n",
        "\n",
        "            active_iteration += 1\n",
        "\n",
        "            # get validation probabilities\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
        "            print ('val predicted:',\n",
        "                   self.clf_model.val_y_predicted.shape,\n",
        "                   self.clf_model.val_y_predicted)\n",
        "            print ('probabilities:', probas_val.shape, '\\n',\n",
        "                   np.argmax(probas_val, axis=1))\n",
        "\n",
        "            # select samples using a selection function\n",
        "            uncertain_samples = \\\n",
        "                self.sample_selection_function.select(probas_val, self.initial_labeled_samples)\n",
        "\n",
        "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
        "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
        "\n",
        "            # get the uncertain samples from the validation set\n",
        "            print ('trainset before adding uncertain samples', X_train.shape, y_train.shape)\n",
        "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
        "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
        "            print ('trainset after adding uncertain samples', X_train.shape, y_train.shape)\n",
        "            self.samplecount.append(X_train.shape[0])\n",
        "\n",
        "            bin_count = np.bincount(y_train.astype('int64'))\n",
        "            unique = np.unique(y_train.astype('int64'))\n",
        "            print (\n",
        "                'updated train set:',\n",
        "                X_train.shape,\n",
        "                y_train.shape,\n",
        "                'unique(labels):',\n",
        "                bin_count,\n",
        "                unique,\n",
        "                )\n",
        "\n",
        "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
        "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
        "            print ('val set:', X_val.shape, y_val.shape)\n",
        "            print ()\n",
        "\n",
        "            # normalize again after creating the 'new' train/test sets\n",
        "            normalizer = Normalize()\n",
        "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
        "\n",
        "            self.queried += self.initial_labeled_samples\n",
        "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
        "\n",
        "        print ('final active learning accuracies',\n",
        "               self.clf_model.accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True False False False False False  True False  True  True False False\n",
            " False  True  True False False False False False False False False False\n",
            "  True  True  True False False  True False]\n",
            "df: (1736, 10) (1736,)\n",
            "train: (1302, 10) (1302,)\n",
            "test : (434, 10) (434,)\n",
            "unique classes 2\n",
            "stopping at: 500\n",
            "Count = 1, using model = LogModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 10) (250,) unique(labels): [102 148] [0 1]\n",
            "Val set: (1052, 10) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.880184 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.61      0.42      0.50       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.66      0.68       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "uniques chosen: 250 <= should be equal to: 250\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [228 272] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.88018433179722, 79.95391705069125]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-1.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 2, using model = LogModel, selection_function = RandomSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 10) (125,) unique(labels): [58 67] [0 1]\n",
            "Val set: (1177, 10) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 125 <= should be equal to: 125\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [118 132] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.70      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 125 <= should be equal to: 125\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [169 206] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.51      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0\n",
            " 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0\n",
            " 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0\n",
            " 0 0]\n",
            "uniques chosen: 125 <= should be equal to: 125\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [230 270] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "final active learning accuracies [81.5668202764977, 81.10599078341014, 81.10599078341014, 79.49308755760369]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-2.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 3, using model = LogModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 10) (50,) unique(labels): [21 29] [0 1]\n",
            "Val set: (1252, 10) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.188940 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.84       321\n",
            "           1       0.56      0.60      0.58       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.72      0.71       434\n",
            "weighted avg       0.78      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [44 56] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.267281 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.54      0.54      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [65 85] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 77.649770 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 90 110] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [117 133] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.62      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [144 156] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [171 179] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [190 210] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [206 244] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [229 271] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.880184 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.18894009216591, 76.26728110599078, 77.64976958525345, 79.26267281105991, 79.03225806451613, 78.57142857142857, 80.18433179723502, 80.64516129032258, 80.87557603686636, 77.88018433179722]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-3.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 4, using model = LogModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 10) (25,) unique(labels): [16  9] [0 1]\n",
            "Val set: (1277, 10) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 10)\n",
            "Validation set: (1277, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=2.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (25, 10) (25,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [24 26] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (75, 10) (75,)\n",
            "updated train set: (75, 10) (75,) unique(labels): [35 40] [0 1]\n",
            "val set: (1227, 10) (1227,)\n",
            "\n",
            "Train set: (75, 10)\n",
            "Validation set: (1227, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.6666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.69      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (75, 10) (75,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [48 52] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (125, 10) (125,)\n",
            "updated train set: (125, 10) (125,) unique(labels): [60 65] [0 1]\n",
            "val set: (1177, 10) (1177,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [73 77] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (175, 10) (175,)\n",
            "updated train set: (175, 10) (175,) unique(labels): [82 93] [0 1]\n",
            "val set: (1127, 10) (1127,)\n",
            "\n",
            "Train set: (175, 10)\n",
            "Validation set: (1127, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.2857142857142857, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (175, 10) (175,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 92 108] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (225, 10) (225,)\n",
            "updated train set: (225, 10) (225,) unique(labels): [107 118] [0 1]\n",
            "val set: (1077, 10) (1077,)\n",
            "\n",
            "Train set: (225, 10)\n",
            "Validation set: (1077, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.2222222222222222, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (225, 10) (225,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [119 131] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (275, 10) (275,)\n",
            "updated train set: (275, 10) (275,) unique(labels): [132 143] [0 1]\n",
            "val set: (1027, 10) (1027,)\n",
            "\n",
            "Train set: (275, 10)\n",
            "Validation set: (1027, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.18181818181818182, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (275, 10) (275,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [141 159] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.62      0.44      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.67      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (325, 10) (325,)\n",
            "updated train set: (325, 10) (325,) unique(labels): [153 172] [0 1]\n",
            "val set: (977, 10) (977,)\n",
            "\n",
            "Train set: (325, 10)\n",
            "Validation set: (977, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.15384615384615385, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (325, 10) (325,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [167 183] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [176 199] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [185 215] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (425, 10) (425,)\n",
            "updated train set: (425, 10) (425,) unique(labels): [198 227] [0 1]\n",
            "val set: (877, 10) (877,)\n",
            "\n",
            "Train set: (425, 10)\n",
            "Validation set: (877, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.11764705882352941, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.64      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (425, 10) (425,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [205 245] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.63      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0\n",
            " 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (475, 10) (475,)\n",
            "updated train set: (475, 10) (475,) unique(labels): [213 262] [0 1]\n",
            "val set: (827, 10) (827,)\n",
            "\n",
            "Train set: (475, 10)\n",
            "Validation set: (827, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.10526315789473684, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.64      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (475, 10) (475,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [230 270] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "final active learning accuracies [79.72350230414746, 79.95391705069125, 80.87557603686636, 78.57142857142857, 80.18433179723502, 80.18433179723502, 78.11059907834101, 79.95391705069125, 79.72350230414746, 79.72350230414746, 78.57142857142857, 78.57142857142857, 78.57142857142857, 79.49308755760369, 79.49308755760369, 78.80184331797236, 79.26267281105991, 79.26267281105991, 79.26267281105991, 79.03225806451613]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-4.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 5, using model = LogModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 10) (10,) unique(labels): [5 5] [0 1]\n",
            "Val set: (1292, 10) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 10)\n",
            "Validation set: (1292, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 76.728111 \n",
            "Classification report for LogisticRegression(C=5.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85       321\n",
            "           1       0.59      0.35      0.44       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.63      0.65       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 73  40]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (10, 10) (10,)\n",
            "trainset after adding uncertain samples (20, 10) (20,)\n",
            "updated train set: (20, 10) (20,) unique(labels): [ 9 11] [0 1]\n",
            "val set: (1282, 10) (1282,)\n",
            "\n",
            "Train set: (20, 10)\n",
            "Validation set: (1282, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.728111 \n",
            "Classification report for LogisticRegression(C=2.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.57      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.65      0.67       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (20, 10) (20,)\n",
            "trainset after adding uncertain samples (30, 10) (30,)\n",
            "updated train set: (30, 10) (30,) unique(labels): [15 15] [0 1]\n",
            "val set: (1272, 10) (1272,)\n",
            "\n",
            "Train set: (30, 10)\n",
            "Validation set: (1272, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=1.6666666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (30, 10) (30,)\n",
            "trainset after adding uncertain samples (40, 10) (40,)\n",
            "updated train set: (40, 10) (40,) unique(labels): [16 24] [0 1]\n",
            "val set: (1262, 10) (1262,)\n",
            "\n",
            "Train set: (40, 10)\n",
            "Validation set: (1262, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.732719 \n",
            "Classification report for LogisticRegression(C=1.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.78      0.81       321\n",
            "           1       0.50      0.63      0.55       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.68      0.70      0.68       434\n",
            "weighted avg       0.76      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[249  72]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 1 0 1]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 1 0 1]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (40, 10) (40,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [22 28] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (60, 10) (60,)\n",
            "updated train set: (60, 10) (60,) unique(labels): [27 33] [0 1]\n",
            "val set: (1242, 10) (1242,)\n",
            "\n",
            "Train set: (60, 10)\n",
            "Validation set: (1242, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.8333333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (60, 10) (60,)\n",
            "trainset after adding uncertain samples (70, 10) (70,)\n",
            "updated train set: (70, 10) (70,) unique(labels): [32 38] [0 1]\n",
            "val set: (1232, 10) (1232,)\n",
            "\n",
            "Train set: (70, 10)\n",
            "Validation set: (1232, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.7142857142857143, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.87       321\n",
            "           1       0.65      0.58      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.74      0.74       434\n",
            "weighted avg       0.80      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (70, 10) (70,)\n",
            "trainset after adding uncertain samples (80, 10) (80,)\n",
            "updated train set: (80, 10) (80,) unique(labels): [36 44] [0 1]\n",
            "val set: (1222, 10) (1222,)\n",
            "\n",
            "Train set: (80, 10)\n",
            "Validation set: (1222, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (80, 10) (80,)\n",
            "trainset after adding uncertain samples (90, 10) (90,)\n",
            "updated train set: (90, 10) (90,) unique(labels): [39 51] [0 1]\n",
            "val set: (1212, 10) (1212,)\n",
            "\n",
            "Train set: (90, 10)\n",
            "Validation set: (1212, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.5555555555555556, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (90, 10) (90,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [42 58] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88       321\n",
            "           1       0.67      0.57      0.62       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.76      0.73      0.75       434\n",
            "weighted avg       0.81      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (110, 10) (110,)\n",
            "updated train set: (110, 10) (110,) unique(labels): [49 61] [0 1]\n",
            "val set: (1192, 10) (1192,)\n",
            "\n",
            "Train set: (110, 10)\n",
            "Validation set: (1192, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.45454545454545453, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.66      0.56      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (110, 10) (110,)\n",
            "trainset after adding uncertain samples (120, 10) (120,)\n",
            "updated train set: (120, 10) (120,) unique(labels): [55 65] [0 1]\n",
            "val set: (1182, 10) (1182,)\n",
            "\n",
            "Train set: (120, 10)\n",
            "Validation set: (1182, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.4166666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (120, 10) (120,)\n",
            "trainset after adding uncertain samples (130, 10) (130,)\n",
            "updated train set: (130, 10) (130,) unique(labels): [59 71] [0 1]\n",
            "val set: (1172, 10) (1172,)\n",
            "\n",
            "Train set: (130, 10)\n",
            "Validation set: (1172, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.38461538461538464, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (130, 10) (130,)\n",
            "trainset after adding uncertain samples (140, 10) (140,)\n",
            "updated train set: (140, 10) (140,) unique(labels): [66 74] [0 1]\n",
            "val set: (1162, 10) (1162,)\n",
            "\n",
            "Train set: (140, 10)\n",
            "Validation set: (1162, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.35714285714285715, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (140, 10) (140,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [69 81] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (160, 10) (160,)\n",
            "updated train set: (160, 10) (160,) unique(labels): [73 87] [0 1]\n",
            "val set: (1142, 10) (1142,)\n",
            "\n",
            "Train set: (160, 10)\n",
            "Validation set: (1142, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.3125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (160, 10) (160,)\n",
            "trainset after adding uncertain samples (170, 10) (170,)\n",
            "updated train set: (170, 10) (170,) unique(labels): [77 93] [0 1]\n",
            "val set: (1132, 10) (1132,)\n",
            "\n",
            "Train set: (170, 10)\n",
            "Validation set: (1132, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.29411764705882354, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (170, 10) (170,)\n",
            "trainset after adding uncertain samples (180, 10) (180,)\n",
            "updated train set: (180, 10) (180,) unique(labels): [ 80 100] [0 1]\n",
            "val set: (1122, 10) (1122,)\n",
            "\n",
            "Train set: (180, 10)\n",
            "Validation set: (1122, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.2777777777777778, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (180, 10) (180,)\n",
            "trainset after adding uncertain samples (190, 10) (190,)\n",
            "updated train set: (190, 10) (190,) unique(labels): [ 82 108] [0 1]\n",
            "val set: (1112, 10) (1112,)\n",
            "\n",
            "Train set: (190, 10)\n",
            "Validation set: (1112, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.2631578947368421, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (190, 10) (190,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 87 113] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (210, 10) (210,)\n",
            "updated train set: (210, 10) (210,) unique(labels): [ 94 116] [0 1]\n",
            "val set: (1092, 10) (1092,)\n",
            "\n",
            "Train set: (210, 10)\n",
            "Validation set: (1092, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.23809523809523808, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (210, 10) (210,)\n",
            "trainset after adding uncertain samples (220, 10) (220,)\n",
            "updated train set: (220, 10) (220,) unique(labels): [ 99 121] [0 1]\n",
            "val set: (1082, 10) (1082,)\n",
            "\n",
            "Train set: (220, 10)\n",
            "Validation set: (1082, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.22727272727272727, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (220, 10) (220,)\n",
            "trainset after adding uncertain samples (230, 10) (230,)\n",
            "updated train set: (230, 10) (230,) unique(labels): [102 128] [0 1]\n",
            "val set: (1072, 10) (1072,)\n",
            "\n",
            "Train set: (230, 10)\n",
            "Validation set: (1072, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.21739130434782608, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (230, 10) (230,)\n",
            "trainset after adding uncertain samples (240, 10) (240,)\n",
            "updated train set: (240, 10) (240,) unique(labels): [108 132] [0 1]\n",
            "val set: (1062, 10) (1062,)\n",
            "\n",
            "Train set: (240, 10)\n",
            "Validation set: (1062, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.20833333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (240, 10) (240,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [112 138] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (260, 10) (260,)\n",
            "updated train set: (260, 10) (260,) unique(labels): [118 142] [0 1]\n",
            "val set: (1042, 10) (1042,)\n",
            "\n",
            "Train set: (260, 10)\n",
            "Validation set: (1042, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.19230769230769232, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (260, 10) (260,)\n",
            "trainset after adding uncertain samples (270, 10) (270,)\n",
            "updated train set: (270, 10) (270,) unique(labels): [120 150] [0 1]\n",
            "val set: (1032, 10) (1032,)\n",
            "\n",
            "Train set: (270, 10)\n",
            "Validation set: (1032, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.18518518518518517, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (270, 10) (270,)\n",
            "trainset after adding uncertain samples (280, 10) (280,)\n",
            "updated train set: (280, 10) (280,) unique(labels): [122 158] [0 1]\n",
            "val set: (1022, 10) (1022,)\n",
            "\n",
            "Train set: (280, 10)\n",
            "Validation set: (1022, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.17857142857142858, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (280, 10) (280,)\n",
            "trainset after adding uncertain samples (290, 10) (290,)\n",
            "updated train set: (290, 10) (290,) unique(labels): [128 162] [0 1]\n",
            "val set: (1012, 10) (1012,)\n",
            "\n",
            "Train set: (290, 10)\n",
            "Validation set: (1012, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.1724137931034483, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (290, 10) (290,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [131 169] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (310, 10) (310,)\n",
            "updated train set: (310, 10) (310,) unique(labels): [136 174] [0 1]\n",
            "val set: (992, 10) (992,)\n",
            "\n",
            "Train set: (310, 10)\n",
            "Validation set: (992, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.16129032258064516, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (310, 10) (310,)\n",
            "trainset after adding uncertain samples (320, 10) (320,)\n",
            "updated train set: (320, 10) (320,) unique(labels): [140 180] [0 1]\n",
            "val set: (982, 10) (982,)\n",
            "\n",
            "Train set: (320, 10)\n",
            "Validation set: (982, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.15625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (320, 10) (320,)\n",
            "trainset after adding uncertain samples (330, 10) (330,)\n",
            "updated train set: (330, 10) (330,) unique(labels): [146 184] [0 1]\n",
            "val set: (972, 10) (972,)\n",
            "\n",
            "Train set: (330, 10)\n",
            "Validation set: (972, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.15151515151515152, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (330, 10) (330,)\n",
            "trainset after adding uncertain samples (340, 10) (340,)\n",
            "updated train set: (340, 10) (340,) unique(labels): [149 191] [0 1]\n",
            "val set: (962, 10) (962,)\n",
            "\n",
            "Train set: (340, 10)\n",
            "Validation set: (962, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.14705882352941177, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (340, 10) (340,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [150 200] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.88       321\n",
            "           1       0.67      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (360, 10) (360,)\n",
            "updated train set: (360, 10) (360,) unique(labels): [153 207] [0 1]\n",
            "val set: (942, 10) (942,)\n",
            "\n",
            "Train set: (360, 10)\n",
            "Validation set: (942, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.1388888888888889, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (360, 10) (360,)\n",
            "trainset after adding uncertain samples (370, 10) (370,)\n",
            "updated train set: (370, 10) (370,) unique(labels): [156 214] [0 1]\n",
            "val set: (932, 10) (932,)\n",
            "\n",
            "Train set: (370, 10)\n",
            "Validation set: (932, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.13513513513513514, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (370, 10) (370,)\n",
            "trainset after adding uncertain samples (380, 10) (380,)\n",
            "updated train set: (380, 10) (380,) unique(labels): [160 220] [0 1]\n",
            "val set: (922, 10) (922,)\n",
            "\n",
            "Train set: (380, 10)\n",
            "Validation set: (922, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.13157894736842105, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (380, 10) (380,)\n",
            "trainset after adding uncertain samples (390, 10) (390,)\n",
            "updated train set: (390, 10) (390,) unique(labels): [166 224] [0 1]\n",
            "val set: (912, 10) (912,)\n",
            "\n",
            "Train set: (390, 10)\n",
            "Validation set: (912, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.1282051282051282, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (390, 10) (390,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [169 231] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 0 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 0 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (410, 10) (410,)\n",
            "updated train set: (410, 10) (410,) unique(labels): [175 235] [0 1]\n",
            "val set: (892, 10) (892,)\n",
            "\n",
            "Train set: (410, 10)\n",
            "Validation set: (892, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.12195121951219512, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (410, 10) (410,)\n",
            "trainset after adding uncertain samples (420, 10) (420,)\n",
            "updated train set: (420, 10) (420,) unique(labels): [180 240] [0 1]\n",
            "val set: (882, 10) (882,)\n",
            "\n",
            "Train set: (420, 10)\n",
            "Validation set: (882, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.11904761904761904, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (420, 10) (420,)\n",
            "trainset after adding uncertain samples (430, 10) (430,)\n",
            "updated train set: (430, 10) (430,) unique(labels): [183 247] [0 1]\n",
            "val set: (872, 10) (872,)\n",
            "\n",
            "Train set: (430, 10)\n",
            "Validation set: (872, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.11627906976744186, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (430, 10) (430,)\n",
            "trainset after adding uncertain samples (440, 10) (440,)\n",
            "updated train set: (440, 10) (440,) unique(labels): [189 251] [0 1]\n",
            "val set: (862, 10) (862,)\n",
            "\n",
            "Train set: (440, 10)\n",
            "Validation set: (862, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.11363636363636363, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (440, 10) (440,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [194 256] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 0\n",
            " 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (460, 10) (460,)\n",
            "updated train set: (460, 10) (460,) unique(labels): [198 262] [0 1]\n",
            "val set: (842, 10) (842,)\n",
            "\n",
            "Train set: (460, 10)\n",
            "Validation set: (842, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.10869565217391304, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (460, 10) (460,)\n",
            "trainset after adding uncertain samples (470, 10) (470,)\n",
            "updated train set: (470, 10) (470,) unique(labels): [200 270] [0 1]\n",
            "val set: (832, 10) (832,)\n",
            "\n",
            "Train set: (470, 10)\n",
            "Validation set: (832, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.10638297872340426, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (470, 10) (470,)\n",
            "trainset after adding uncertain samples (480, 10) (480,)\n",
            "updated train set: (480, 10) (480,) unique(labels): [205 275] [0 1]\n",
            "val set: (822, 10) (822,)\n",
            "\n",
            "Train set: (480, 10)\n",
            "Validation set: (822, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.10416666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (480, 10) (480,)\n",
            "trainset after adding uncertain samples (490, 10) (490,)\n",
            "updated train set: (490, 10) (490,) unique(labels): [211 279] [0 1]\n",
            "val set: (812, 10) (812,)\n",
            "\n",
            "Train set: (490, 10)\n",
            "Validation set: (812, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.10204081632653061, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (490, 10) (490,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [218 282] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.72811059907833, 76.72811059907833, 79.26267281105991, 73.73271889400922, 78.3410138248848, 79.26267281105991, 80.87557603686636, 80.4147465437788, 80.18433179723502, 81.5668202764977, 81.10599078341014, 79.49308755760369, 80.64516129032258, 79.95391705069125, 79.26267281105991, 79.72350230414746, 79.49308755760369, 79.72350230414746, 78.80184331797236, 79.03225806451613, 79.26267281105991, 79.03225806451613, 79.03225806451613, 79.26267281105991, 79.49308755760369, 79.49308755760369, 79.26267281105991, 79.72350230414746, 79.72350230414746, 79.26267281105991, 80.4147465437788, 80.18433179723502, 80.4147465437788, 80.64516129032258, 80.87557603686636, 79.72350230414746, 79.26267281105991, 79.26267281105991, 79.03225806451613, 79.26267281105991, 79.72350230414746, 80.18433179723502, 79.49308755760369, 80.18433179723502, 79.72350230414746, 79.72350230414746, 80.18433179723502, 80.64516129032258, 79.95391705069125, 80.4147465437788]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-5.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          79.26267281105991,\n",
            "          73.73271889400922,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.87557603686636,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          78.11059907834101,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 6, using model = LogModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 10) (250,) unique(labels): [121 129] [0 1]\n",
            "Val set: (1052, 10) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 1 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [225 275] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.70      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [79.72350230414746, 81.33640552995391]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-6.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 7, using model = LogModel, selection_function = MarginSamplingSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 10) (125,) unique(labels): [64 61] [0 1]\n",
            "Val set: (1177, 10) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [111 139] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 1 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [171 204] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.88       321\n",
            "           1       0.68      0.52      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 0 0]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [213 287] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "final active learning accuracies [79.72350230414746, 80.4147465437788, 81.10599078341014, 79.95391705069125]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-7.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 8, using model = LogModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 10) (50,) unique(labels): [23 27] [0 1]\n",
            "Val set: (1252, 10) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [41 59] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [65 85] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 81 119] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [ 95 155] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [115 185] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [144 206] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [175 225] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [194 256] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [209 291] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "final active learning accuracies [78.57142857142857, 80.18433179723502, 80.4147465437788, 80.18433179723502, 80.64516129032258, 80.4147465437788, 80.64516129032258, 80.87557603686636, 81.10599078341014, 80.87557603686636]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-8.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 9, using model = LogModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 10) (25,) unique(labels): [10 15] [0 1]\n",
            "Val set: (1277, 10) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 10)\n",
            "Validation set: (1277, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.728111 \n",
            "Classification report for LogisticRegression(C=2.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (25, 10) (25,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [16 34] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.502304 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.79      0.81       321\n",
            "           1       0.49      0.58      0.53       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.69      0.67       434\n",
            "weighted avg       0.75      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[253  68]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (75, 10) (75,)\n",
            "updated train set: (75, 10) (75,) unique(labels): [25 50] [0 1]\n",
            "val set: (1227, 10) (1227,)\n",
            "\n",
            "Train set: (75, 10)\n",
            "Validation set: (1227, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 73.963134 \n",
            "Classification report for LogisticRegression(C=0.6666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82       321\n",
            "           1       0.50      0.56      0.53       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.68      0.67       434\n",
            "weighted avg       0.75      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[258  63]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (75, 10) (75,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [43 57] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (125, 10) (125,)\n",
            "updated train set: (125, 10) (125,) unique(labels): [54 71] [0 1]\n",
            "val set: (1177, 10) (1177,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [66 84] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (175, 10) (175,)\n",
            "updated train set: (175, 10) (175,) unique(labels): [ 75 100] [0 1]\n",
            "val set: (1127, 10) (1127,)\n",
            "\n",
            "Train set: (175, 10)\n",
            "Validation set: (1127, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.2857142857142857, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (175, 10) (175,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 85 115] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (225, 10) (225,)\n",
            "updated train set: (225, 10) (225,) unique(labels): [ 99 126] [0 1]\n",
            "val set: (1077, 10) (1077,)\n",
            "\n",
            "Train set: (225, 10)\n",
            "Validation set: (1077, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.2222222222222222, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (225, 10) (225,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [110 140] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (275, 10) (275,)\n",
            "updated train set: (275, 10) (275,) unique(labels): [122 153] [0 1]\n",
            "val set: (1027, 10) (1027,)\n",
            "\n",
            "Train set: (275, 10)\n",
            "Validation set: (1027, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.18181818181818182, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (275, 10) (275,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [140 160] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (325, 10) (325,)\n",
            "updated train set: (325, 10) (325,) unique(labels): [150 175] [0 1]\n",
            "val set: (977, 10) (977,)\n",
            "\n",
            "Train set: (325, 10)\n",
            "Validation set: (977, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.15384615384615385, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (325, 10) (325,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [156 194] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [164 211] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [175 225] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (425, 10) (425,)\n",
            "updated train set: (425, 10) (425,) unique(labels): [182 243] [0 1]\n",
            "val set: (877, 10) (877,)\n",
            "\n",
            "Train set: (425, 10)\n",
            "Validation set: (877, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.11764705882352941, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before adding uncertain samples (425, 10) (425,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [193 257] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (475, 10) (475,)\n",
            "updated train set: (475, 10) (475,) unique(labels): [200 275] [0 1]\n",
            "val set: (827, 10) (827,)\n",
            "\n",
            "Train set: (475, 10)\n",
            "Validation set: (827, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.10526315789473684, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 0]\n",
            "trainset before adding uncertain samples (475, 10) (475,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [212 288] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.72811059907833, 73.50230414746544, 73.963133640553, 80.4147465437788, 79.72350230414746, 79.49308755760369, 80.87557603686636, 79.03225806451613, 80.64516129032258, 79.95391705069125, 79.72350230414746, 80.18433179723502, 79.49308755760369, 79.95391705069125, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-9.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 10, using model = LogModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 10) (10,) unique(labels): [6 4] [0 1]\n",
            "Val set: (1292, 10) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 10)\n",
            "Validation set: (1292, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 60.138249 \n",
            "Classification report for LogisticRegression(C=5.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.67      0.71       321\n",
            "           1       0.31      0.42      0.35       113\n",
            "\n",
            "    accuracy                           0.60       434\n",
            "   macro avg       0.53      0.54      0.53       434\n",
            "weighted avg       0.64      0.60      0.62       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[214 107]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [1 0 0 ... 1 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [1 0 0 ... 1 0 0]\n",
            "trainset before adding uncertain samples (10, 10) (10,)\n",
            "trainset after adding uncertain samples (20, 10) (20,)\n",
            "updated train set: (20, 10) (20,) unique(labels): [11  9] [0 1]\n",
            "val set: (1282, 10) (1282,)\n",
            "\n",
            "Train set: (20, 10)\n",
            "Validation set: (1282, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 56.682028 \n",
            "Classification report for LogisticRegression(C=2.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.56      0.66       321\n",
            "           1       0.32      0.59      0.42       113\n",
            "\n",
            "    accuracy                           0.57       434\n",
            "   macro avg       0.56      0.58      0.54       434\n",
            "weighted avg       0.67      0.57      0.59       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[179 142]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (20, 10) (20,)\n",
            "trainset after adding uncertain samples (30, 10) (30,)\n",
            "updated train set: (30, 10) (30,) unique(labels): [16 14] [0 1]\n",
            "val set: (1272, 10) (1272,)\n",
            "\n",
            "Train set: (30, 10)\n",
            "Validation set: (1272, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 60.599078 \n",
            "Classification report for LogisticRegression(C=1.6666666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.61      0.70       321\n",
            "           1       0.35      0.60      0.44       113\n",
            "\n",
            "    accuracy                           0.61       434\n",
            "   macro avg       0.58      0.60      0.57       434\n",
            "weighted avg       0.69      0.61      0.63       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[195 126]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [1 1 1 ... 1 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [1 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (30, 10) (30,)\n",
            "trainset after adding uncertain samples (40, 10) (40,)\n",
            "updated train set: (40, 10) (40,) unique(labels): [18 22] [0 1]\n",
            "val set: (1262, 10) (1262,)\n",
            "\n",
            "Train set: (40, 10)\n",
            "Validation set: (1262, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 68.433180 \n",
            "Classification report for LogisticRegression(C=1.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.71      0.77       321\n",
            "           1       0.43      0.62      0.51       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.63      0.66      0.64       434\n",
            "weighted avg       0.73      0.68      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[227  94]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (40, 10) (40,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [22 28] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 70.276498 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.73      0.78       321\n",
            "           1       0.45      0.64      0.53       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.75      0.70      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[233  88]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (60, 10) (60,)\n",
            "updated train set: (60, 10) (60,) unique(labels): [28 32] [0 1]\n",
            "val set: (1242, 10) (1242,)\n",
            "\n",
            "Train set: (60, 10)\n",
            "Validation set: (1242, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.811060 \n",
            "Classification report for LogisticRegression(C=0.8333333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.75      0.80       321\n",
            "           1       0.48      0.67      0.56       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.68      0.71      0.68       434\n",
            "weighted avg       0.77      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[240  81]\n",
            " [ 37  76]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [1 1 1 ... 1 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [1 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (60, 10) (60,)\n",
            "trainset after adding uncertain samples (70, 10) (70,)\n",
            "updated train set: (70, 10) (70,) unique(labels): [34 36] [0 1]\n",
            "val set: (1232, 10) (1232,)\n",
            "\n",
            "Train set: (70, 10)\n",
            "Validation set: (1232, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 71.198157 \n",
            "Classification report for LogisticRegression(C=0.7142857142857143, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.71      0.79       321\n",
            "           1       0.47      0.71      0.56       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.67      0.71      0.67       434\n",
            "weighted avg       0.77      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[229  92]\n",
            " [ 33  80]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (70, 10) (70,)\n",
            "trainset after adding uncertain samples (80, 10) (80,)\n",
            "updated train set: (80, 10) (80,) unique(labels): [40 40] [0 1]\n",
            "val set: (1222, 10) (1222,)\n",
            "\n",
            "Train set: (80, 10)\n",
            "Validation set: (1222, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.811060 \n",
            "Classification report for LogisticRegression(C=0.625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.75      0.80       321\n",
            "           1       0.48      0.66      0.56       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.71      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[241  80]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (80, 10) (80,)\n",
            "trainset after adding uncertain samples (90, 10) (90,)\n",
            "updated train set: (90, 10) (90,) unique(labels): [45 45] [0 1]\n",
            "val set: (1212, 10) (1212,)\n",
            "\n",
            "Train set: (90, 10)\n",
            "Validation set: (1212, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.119816 \n",
            "Classification report for LogisticRegression(C=0.5555555555555556, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.80       321\n",
            "           1       0.47      0.67      0.56       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.67      0.71      0.68       434\n",
            "weighted avg       0.76      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[237  84]\n",
            " [ 37  76]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (90, 10) (90,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [50 50] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 71.428571 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.79       321\n",
            "           1       0.47      0.65      0.54       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.70      0.67       434\n",
            "weighted avg       0.76      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[236  85]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (110, 10) (110,)\n",
            "updated train set: (110, 10) (110,) unique(labels): [57 53] [0 1]\n",
            "val set: (1192, 10) (1192,)\n",
            "\n",
            "Train set: (110, 10)\n",
            "Validation set: (1192, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 71.428571 \n",
            "Classification report for LogisticRegression(C=0.45454545454545453, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.79       321\n",
            "           1       0.47      0.65      0.54       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.70      0.67       434\n",
            "weighted avg       0.76      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[236  85]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (110, 10) (110,)\n",
            "trainset after adding uncertain samples (120, 10) (120,)\n",
            "updated train set: (120, 10) (120,) unique(labels): [61 59] [0 1]\n",
            "val set: (1182, 10) (1182,)\n",
            "\n",
            "Train set: (120, 10)\n",
            "Validation set: (1182, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.580645 \n",
            "Classification report for LogisticRegression(C=0.4166666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.75      0.80       321\n",
            "           1       0.48      0.66      0.56       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.71      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[240  81]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (120, 10) (120,)\n",
            "trainset after adding uncertain samples (130, 10) (130,)\n",
            "updated train set: (130, 10) (130,) unique(labels): [66 64] [0 1]\n",
            "val set: (1172, 10) (1172,)\n",
            "\n",
            "Train set: (130, 10)\n",
            "Validation set: (1172, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.580645 \n",
            "Classification report for LogisticRegression(C=0.38461538461538464, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.76      0.80       321\n",
            "           1       0.48      0.64      0.55       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[243  78]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (130, 10) (130,)\n",
            "trainset after adding uncertain samples (140, 10) (140,)\n",
            "updated train set: (140, 10) (140,) unique(labels): [73 67] [0 1]\n",
            "val set: (1162, 10) (1162,)\n",
            "\n",
            "Train set: (140, 10)\n",
            "Validation set: (1162, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.119816 \n",
            "Classification report for LogisticRegression(C=0.35714285714285715, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.76      0.80       321\n",
            "           1       0.47      0.61      0.53       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[244  77]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (140, 10) (140,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [76 74] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 70.737327 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79       321\n",
            "           1       0.45      0.62      0.52       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[237  84]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (160, 10) (160,)\n",
            "updated train set: (160, 10) (160,) unique(labels): [80 80] [0 1]\n",
            "val set: (1142, 10) (1142,)\n",
            "\n",
            "Train set: (160, 10)\n",
            "Validation set: (1142, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.580645 \n",
            "Classification report for LogisticRegression(C=0.3125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.48      0.60      0.53       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (160, 10) (160,)\n",
            "trainset after adding uncertain samples (170, 10) (170,)\n",
            "updated train set: (170, 10) (170,) unique(labels): [85 85] [0 1]\n",
            "val set: (1132, 10) (1132,)\n",
            "\n",
            "Train set: (170, 10)\n",
            "Validation set: (1132, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 71.198157 \n",
            "Classification report for LogisticRegression(C=0.29411764705882354, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79       321\n",
            "           1       0.46      0.62      0.53       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.75      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[239  82]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before adding uncertain samples (170, 10) (170,)\n",
            "trainset after adding uncertain samples (180, 10) (180,)\n",
            "updated train set: (180, 10) (180,) unique(labels): [90 90] [0 1]\n",
            "val set: (1122, 10) (1122,)\n",
            "\n",
            "Train set: (180, 10)\n",
            "Validation set: (1122, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.041475 \n",
            "Classification report for LogisticRegression(C=0.2777777777777778, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.49      0.62      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before adding uncertain samples (180, 10) (180,)\n",
            "trainset after adding uncertain samples (190, 10) (190,)\n",
            "updated train set: (190, 10) (190,) unique(labels): [95 95] [0 1]\n",
            "val set: (1112, 10) (1112,)\n",
            "\n",
            "Train set: (190, 10)\n",
            "Validation set: (1112, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.811060 \n",
            "Classification report for LogisticRegression(C=0.2631578947368421, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.48      0.61      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.67       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (190, 10) (190,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 99 101] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.580645 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.48      0.60      0.53       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (210, 10) (210,)\n",
            "updated train set: (210, 10) (210,) unique(labels): [104 106] [0 1]\n",
            "val set: (1092, 10) (1092,)\n",
            "\n",
            "Train set: (210, 10)\n",
            "Validation set: (1092, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.580645 \n",
            "Classification report for LogisticRegression(C=0.23809523809523808, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.48      0.60      0.53       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (210, 10) (210,)\n",
            "trainset after adding uncertain samples (220, 10) (220,)\n",
            "updated train set: (220, 10) (220,) unique(labels): [108 112] [0 1]\n",
            "val set: (1082, 10) (1082,)\n",
            "\n",
            "Train set: (220, 10)\n",
            "Validation set: (1082, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 72.580645 \n",
            "Classification report for LogisticRegression(C=0.22727272727272727, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.48      0.60      0.53       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (220, 10) (220,)\n",
            "trainset after adding uncertain samples (230, 10) (230,)\n",
            "updated train set: (230, 10) (230,) unique(labels): [116 114] [0 1]\n",
            "val set: (1072, 10) (1072,)\n",
            "\n",
            "Train set: (230, 10)\n",
            "Validation set: (1072, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.041475 \n",
            "Classification report for LogisticRegression(C=0.21739130434782608, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.49      0.63      0.55       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[246  75]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (230, 10) (230,)\n",
            "trainset after adding uncertain samples (240, 10) (240,)\n",
            "updated train set: (240, 10) (240,) unique(labels): [121 119] [0 1]\n",
            "val set: (1062, 10) (1062,)\n",
            "\n",
            "Train set: (240, 10)\n",
            "Validation set: (1062, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.271889 \n",
            "Classification report for LogisticRegression(C=0.20833333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.78      0.81       321\n",
            "           1       0.49      0.61      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[249  72]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (240, 10) (240,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [125 125] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 73.502304 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.78      0.81       321\n",
            "           1       0.49      0.62      0.55       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[249  72]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (260, 10) (260,)\n",
            "updated train set: (260, 10) (260,) unique(labels): [129 131] [0 1]\n",
            "val set: (1042, 10) (1042,)\n",
            "\n",
            "Train set: (260, 10)\n",
            "Validation set: (1042, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.811060 \n",
            "Classification report for LogisticRegression(C=0.19230769230769232, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.48      0.60      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[248  73]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (260, 10) (260,)\n",
            "trainset after adding uncertain samples (270, 10) (270,)\n",
            "updated train set: (270, 10) (270,) unique(labels): [133 137] [0 1]\n",
            "val set: (1032, 10) (1032,)\n",
            "\n",
            "Train set: (270, 10)\n",
            "Validation set: (1032, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.041475 \n",
            "Classification report for LogisticRegression(C=0.18518518518518517, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.78      0.81       321\n",
            "           1       0.49      0.59      0.53       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.67       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[250  71]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (270, 10) (270,)\n",
            "trainset after adding uncertain samples (280, 10) (280,)\n",
            "updated train set: (280, 10) (280,) unique(labels): [143 137] [0 1]\n",
            "val set: (1022, 10) (1022,)\n",
            "\n",
            "Train set: (280, 10)\n",
            "Validation set: (1022, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.732719 \n",
            "Classification report for LogisticRegression(C=0.17857142857142858, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.79      0.82       321\n",
            "           1       0.50      0.60      0.54       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.69      0.68       434\n",
            "weighted avg       0.76      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[252  69]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (280, 10) (280,)\n",
            "trainset after adding uncertain samples (290, 10) (290,)\n",
            "updated train set: (290, 10) (290,) unique(labels): [153 137] [0 1]\n",
            "val set: (1012, 10) (1012,)\n",
            "\n",
            "Train set: (290, 10)\n",
            "Validation set: (1012, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 74.193548 \n",
            "Classification report for LogisticRegression(C=0.1724137931034483, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82       321\n",
            "           1       0.50      0.58      0.54       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.69      0.68       434\n",
            "weighted avg       0.76      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[256  65]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (290, 10) (290,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [162 138] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (310, 10) (310,)\n",
            "updated train set: (310, 10) (310,) unique(labels): [165 145] [0 1]\n",
            "val set: (992, 10) (992,)\n",
            "\n",
            "Train set: (310, 10)\n",
            "Validation set: (992, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.16129032258064516, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (310, 10) (310,)\n",
            "trainset after adding uncertain samples (320, 10) (320,)\n",
            "updated train set: (320, 10) (320,) unique(labels): [168 152] [0 1]\n",
            "val set: (982, 10) (982,)\n",
            "\n",
            "Train set: (320, 10)\n",
            "Validation set: (982, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 77.649770 \n",
            "Classification report for LogisticRegression(C=0.15625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (320, 10) (320,)\n",
            "trainset after adding uncertain samples (330, 10) (330,)\n",
            "updated train set: (330, 10) (330,) unique(labels): [172 158] [0 1]\n",
            "val set: (972, 10) (972,)\n",
            "\n",
            "Train set: (330, 10)\n",
            "Validation set: (972, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.15151515151515152, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (330, 10) (330,)\n",
            "trainset after adding uncertain samples (340, 10) (340,)\n",
            "updated train set: (340, 10) (340,) unique(labels): [172 168] [0 1]\n",
            "val set: (962, 10) (962,)\n",
            "\n",
            "Train set: (340, 10)\n",
            "Validation set: (962, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.14705882352941177, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (340, 10) (340,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [178 172] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (360, 10) (360,)\n",
            "updated train set: (360, 10) (360,) unique(labels): [181 179] [0 1]\n",
            "val set: (942, 10) (942,)\n",
            "\n",
            "Train set: (360, 10)\n",
            "Validation set: (942, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=0.1388888888888889, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (360, 10) (360,)\n",
            "trainset after adding uncertain samples (370, 10) (370,)\n",
            "updated train set: (370, 10) (370,) unique(labels): [186 184] [0 1]\n",
            "val set: (932, 10) (932,)\n",
            "\n",
            "Train set: (370, 10)\n",
            "Validation set: (932, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.13513513513513514, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 0 1 1 1 1 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 0 1 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (370, 10) (370,)\n",
            "trainset after adding uncertain samples (380, 10) (380,)\n",
            "updated train set: (380, 10) (380,) unique(labels): [191 189] [0 1]\n",
            "val set: (922, 10) (922,)\n",
            "\n",
            "Train set: (380, 10)\n",
            "Validation set: (922, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.13157894736842105, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (380, 10) (380,)\n",
            "trainset after adding uncertain samples (390, 10) (390,)\n",
            "updated train set: (390, 10) (390,) unique(labels): [195 195] [0 1]\n",
            "val set: (912, 10) (912,)\n",
            "\n",
            "Train set: (390, 10)\n",
            "Validation set: (912, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.1282051282051282, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (390, 10) (390,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [198 202] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (410, 10) (410,)\n",
            "updated train set: (410, 10) (410,) unique(labels): [200 210] [0 1]\n",
            "val set: (892, 10) (892,)\n",
            "\n",
            "Train set: (410, 10)\n",
            "Validation set: (892, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.12195121951219512, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0]\n",
            "trainset before adding uncertain samples (410, 10) (410,)\n",
            "trainset after adding uncertain samples (420, 10) (420,)\n",
            "updated train set: (420, 10) (420,) unique(labels): [204 216] [0 1]\n",
            "val set: (882, 10) (882,)\n",
            "\n",
            "Train set: (420, 10)\n",
            "Validation set: (882, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.11904761904761904, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (420, 10) (420,)\n",
            "trainset after adding uncertain samples (430, 10) (430,)\n",
            "updated train set: (430, 10) (430,) unique(labels): [205 225] [0 1]\n",
            "val set: (872, 10) (872,)\n",
            "\n",
            "Train set: (430, 10)\n",
            "Validation set: (872, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.11627906976744186, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (430, 10) (430,)\n",
            "trainset after adding uncertain samples (440, 10) (440,)\n",
            "updated train set: (440, 10) (440,) unique(labels): [208 232] [0 1]\n",
            "val set: (862, 10) (862,)\n",
            "\n",
            "Train set: (440, 10)\n",
            "Validation set: (862, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.11363636363636363, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (440, 10) (440,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [214 236] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (460, 10) (460,)\n",
            "updated train set: (460, 10) (460,) unique(labels): [218 242] [0 1]\n",
            "val set: (842, 10) (842,)\n",
            "\n",
            "Train set: (460, 10)\n",
            "Validation set: (842, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.10869565217391304, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before adding uncertain samples (460, 10) (460,)\n",
            "trainset after adding uncertain samples (470, 10) (470,)\n",
            "updated train set: (470, 10) (470,) unique(labels): [223 247] [0 1]\n",
            "val set: (832, 10) (832,)\n",
            "\n",
            "Train set: (470, 10)\n",
            "Validation set: (832, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.10638297872340426, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0]\n",
            "trainset before adding uncertain samples (470, 10) (470,)\n",
            "trainset after adding uncertain samples (480, 10) (480,)\n",
            "updated train set: (480, 10) (480,) unique(labels): [226 254] [0 1]\n",
            "val set: (822, 10) (822,)\n",
            "\n",
            "Train set: (480, 10)\n",
            "Validation set: (822, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.10416666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 0]\n",
            "trainset before adding uncertain samples (480, 10) (480,)\n",
            "trainset after adding uncertain samples (490, 10) (490,)\n",
            "updated train set: (490, 10) (490,) unique(labels): [230 260] [0 1]\n",
            "val set: (812, 10) (812,)\n",
            "\n",
            "Train set: (490, 10)\n",
            "Validation set: (812, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.10204081632653061, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0]\n",
            "trainset before adding uncertain samples (490, 10) (490,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [236 264] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "final active learning accuracies [60.13824884792627, 56.68202764976959, 60.59907834101382, 68.4331797235023, 70.27649769585254, 72.81105990783409, 71.19815668202764, 72.81105990783409, 72.11981566820278, 71.42857142857143, 71.42857142857143, 72.58064516129032, 72.58064516129032, 72.11981566820278, 70.73732718894009, 72.58064516129032, 71.19815668202764, 73.04147465437788, 72.81105990783409, 72.58064516129032, 72.58064516129032, 72.58064516129032, 73.04147465437788, 73.27188940092167, 73.50230414746544, 72.81105990783409, 73.04147465437788, 73.73271889400922, 74.19354838709677, 79.26267281105991, 79.03225806451613, 77.64976958525345, 78.57142857142857, 78.11059907834101, 78.80184331797236, 78.3410138248848, 78.80184331797236, 79.03225806451613, 79.26267281105991, 79.03225806451613, 79.49308755760369, 79.26267281105991, 79.49308755760369, 78.80184331797236, 78.57142857142857, 78.57142857142857, 78.80184331797236, 79.26267281105991, 79.03225806451613, 78.80184331797236]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-10.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          60.13824884792627,\n",
            "          56.68202764976959,\n",
            "          60.59907834101382,\n",
            "          68.4331797235023,\n",
            "          70.27649769585254,\n",
            "          72.81105990783409,\n",
            "          71.19815668202764,\n",
            "          72.81105990783409,\n",
            "          72.11981566820278,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          72.58064516129032,\n",
            "          72.58064516129032,\n",
            "          72.11981566820278,\n",
            "          70.73732718894009,\n",
            "          72.58064516129032,\n",
            "          71.19815668202764,\n",
            "          73.04147465437788,\n",
            "          72.81105990783409,\n",
            "          72.58064516129032,\n",
            "          72.58064516129032,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          73.27188940092167,\n",
            "          73.50230414746544,\n",
            "          72.81105990783409,\n",
            "          73.04147465437788,\n",
            "          73.73271889400922,\n",
            "          74.19354838709677,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          73.50230414746544,\n",
            "          73.963133640553,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.87557603686636,\n",
            "          79.03225806451613,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          81.33640552995391\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          79.26267281105991,\n",
            "          73.73271889400922,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.87557603686636,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          78.11059907834101,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 11, using model = LogModel, selection_function = EntropySelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 10) (250,) unique(labels): [124 126] [0 1]\n",
            "Val set: (1052, 10) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [232 268] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.88       321\n",
            "           1       0.67      0.52      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "final active learning accuracies [78.57142857142857, 80.87557603686636]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-11.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 12, using model = LogModel, selection_function = EntropySelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 10) (125,) unique(labels): [58 67] [0 1]\n",
            "Val set: (1177, 10) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.419355 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.54      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [124 126] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.88       321\n",
            "           1       0.68      0.52      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [170 205] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.87       321\n",
            "           1       0.67      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [221 279] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.88       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.41935483870968, 81.10599078341014, 80.4147465437788, 80.64516129032258]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-12.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 13, using model = LogModel, selection_function = EntropySelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 10) (50,) unique(labels): [17 33] [0 1]\n",
            "Val set: (1252, 10) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.497696 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [50 50] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 73.732719 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82       321\n",
            "           1       0.50      0.56      0.53       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.68      0.67       434\n",
            "weighted avg       0.75      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[257  64]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [77 73] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.880184 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [100 100] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [126 124] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [153 147] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.88       321\n",
            "           1       0.69      0.52      0.60       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.77      0.72      0.74       434\n",
            "weighted avg       0.81      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [166 184] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.69      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [181 219] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "<ipython-input-60-82e50070648f>:24: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-60-82e50070648f>:24: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.49      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [203 247] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.006 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.69      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [215 285] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.49769585253456, 73.73271889400922, 77.88018433179722, 79.26267281105991, 79.72350230414746, 81.5668202764977, 81.10599078341014, 81.33640552995391, 80.87557603686636, 80.64516129032258]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-13.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 14, using model = LogModel, selection_function = EntropySelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 10) (25,) unique(labels): [15 10] [0 1]\n",
            "Val set: (1277, 10) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 10)\n",
            "Validation set: (1277, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.271889 \n",
            "Classification report for LogisticRegression(C=2.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.76      0.81       321\n",
            "           1       0.49      0.66      0.56       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.68      0.71      0.69       434\n",
            "weighted avg       0.77      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[243  78]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (25, 10) (25,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [31 19] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 71.658986 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.75      0.80       321\n",
            "           1       0.47      0.61      0.53       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.68      0.66       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[242  79]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (75, 10) (75,)\n",
            "updated train set: (75, 10) (75,) unique(labels): [41 34] [0 1]\n",
            "val set: (1227, 10) (1227,)\n",
            "\n",
            "Train set: (75, 10)\n",
            "Validation set: (1227, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 69.815668 \n",
            "Classification report for LogisticRegression(C=0.6666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.74      0.78       321\n",
            "           1       0.44      0.57      0.49       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.63      0.66      0.64       434\n",
            "weighted avg       0.73      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[239  82]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (75, 10) (75,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [53 47] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 74.654378 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83       321\n",
            "           1       0.51      0.49      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.67       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[269  52]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (125, 10) (125,)\n",
            "updated train set: (125, 10) (125,) unique(labels): [65 60] [0 1]\n",
            "val set: (1177, 10) (1177,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 74.423963 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83       321\n",
            "           1       0.51      0.48      0.49       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.66      0.66       434\n",
            "weighted avg       0.74      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[269  52]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [75 75] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 75.576037 \n",
            "<ipython-input-60-82e50070648f>:24: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-60-82e50070648f>:24: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-60-82e50070648f>:24: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-60-82e50070648f>:24: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.84       321\n",
            "           1       0.53      0.48      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.67      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (175, 10) (175,)\n",
            "updated train set: (175, 10) (175,) unique(labels): [86 89] [0 1]\n",
            "val set: (1127, 10) (1127,)\n",
            "\n",
            "Train set: (175, 10)\n",
            "Validation set: (1127, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.2857142857142857, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before adding uncertain samples (175, 10) (175,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 97 103] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 0 ... 1 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 0 ... 1 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (225, 10) (225,)\n",
            "updated train set: (225, 10) (225,) unique(labels): [108 117] [0 1]\n",
            "val set: (1077, 10) (1077,)\n",
            "\n",
            "Train set: (225, 10)\n",
            "Validation set: (1077, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.2222222222222222, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 0 ... 1 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 0 ... 1 0 0]\n",
            "trainset before adding uncertain samples (225, 10) (225,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [115 135] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 1 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (275, 10) (275,)\n",
            "updated train set: (275, 10) (275,) unique(labels): [123 152] [0 1]\n",
            "val set: (1027, 10) (1027,)\n",
            "\n",
            "Train set: (275, 10)\n",
            "Validation set: (1027, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.18181818181818182, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.70      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 0 ... 1 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 0 ... 1 0 0]\n",
            "trainset before adding uncertain samples (275, 10) (275,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [134 166] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 82.027650 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89       321\n",
            "           1       0.73      0.49      0.59       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.71      0.74       434\n",
            "weighted avg       0.81      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 0 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 0 ... 1 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (325, 10) (325,)\n",
            "updated train set: (325, 10) (325,) unique(labels): [143 182] [0 1]\n",
            "val set: (977, 10) (977,)\n",
            "\n",
            "Train set: (325, 10)\n",
            "Validation set: (977, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 82.258065 \n",
            "Classification report for LogisticRegression(C=0.15384615384615385, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89       321\n",
            "           1       0.75      0.48      0.58       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.71      0.74       434\n",
            "weighted avg       0.81      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (325, 10) (325,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [150 200] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.72      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [161 214] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.72      0.47      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 0]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [170 230] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (425, 10) (425,)\n",
            "updated train set: (425, 10) (425,) unique(labels): [177 248] [0 1]\n",
            "val set: (877, 10) (877,)\n",
            "\n",
            "Train set: (425, 10)\n",
            "Validation set: (877, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.11764705882352941, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (425, 10) (425,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [184 266] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (475, 10) (475,)\n",
            "updated train set: (475, 10) (475,) unique(labels): [198 277] [0 1]\n",
            "val set: (827, 10) (827,)\n",
            "\n",
            "Train set: (475, 10)\n",
            "Validation set: (827, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.10526315789473684, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (475, 10) (475,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [220 280] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [73.27188940092167, 71.6589861751152, 69.81566820276498, 74.65437788018433, 74.42396313364056, 75.57603686635944, 78.11059907834101, 79.49308755760369, 79.03225806451613, 80.18433179723502, 81.33640552995391, 82.02764976958525, 82.25806451612904, 81.5668202764977, 81.33640552995391, 80.64516129032258, 81.10599078341014, 81.10599078341014, 81.33640552995391, 80.87557603686636]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-14.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 15, using model = LogModel, selection_function = EntropySelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 10) (10,) unique(labels): [6 4] [0 1]\n",
            "Val set: (1292, 10) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 10)\n",
            "Validation set: (1292, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 76.497696 \n",
            "Classification report for LogisticRegression(C=5.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.92      0.85       321\n",
            "           1       0.59      0.33      0.42       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.62      0.64       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 76  37]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (10, 10) (10,)\n",
            "trainset after adding uncertain samples (20, 10) (20,)\n",
            "updated train set: (20, 10) (20,) unique(labels): [ 8 12] [0 1]\n",
            "val set: (1282, 10) (1282,)\n",
            "\n",
            "Train set: (20, 10)\n",
            "Validation set: (1282, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 77.419355 \n",
            "Classification report for LogisticRegression(C=2.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.60      0.40      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.65      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (20, 10) (20,)\n",
            "trainset after adding uncertain samples (30, 10) (30,)\n",
            "updated train set: (30, 10) (30,) unique(labels): [14 16] [0 1]\n",
            "val set: (1272, 10) (1272,)\n",
            "\n",
            "Train set: (30, 10)\n",
            "Validation set: (1272, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.728111 \n",
            "Classification report for LogisticRegression(C=1.6666666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.58      0.38      0.46       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.64      0.66       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (30, 10) (30,)\n",
            "trainset after adding uncertain samples (40, 10) (40,)\n",
            "updated train set: (40, 10) (40,) unique(labels): [17 23] [0 1]\n",
            "val set: (1262, 10) (1262,)\n",
            "\n",
            "Train set: (40, 10)\n",
            "Validation set: (1262, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.728111 \n",
            "Classification report for LogisticRegression(C=1.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.44      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (40, 10) (40,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [21 29] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 74.654378 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83       321\n",
            "           1       0.51      0.48      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (60, 10) (60,)\n",
            "updated train set: (60, 10) (60,) unique(labels): [23 37] [0 1]\n",
            "val set: (1242, 10) (1242,)\n",
            "\n",
            "Train set: (60, 10)\n",
            "Validation set: (1242, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.419355 \n",
            "Classification report for LogisticRegression(C=0.8333333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.85       321\n",
            "           1       0.59      0.42      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (60, 10) (60,)\n",
            "trainset after adding uncertain samples (70, 10) (70,)\n",
            "updated train set: (70, 10) (70,) unique(labels): [28 42] [0 1]\n",
            "val set: (1232, 10) (1232,)\n",
            "\n",
            "Train set: (70, 10)\n",
            "Validation set: (1232, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.7142857142857143, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.66      0.45      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (70, 10) (70,)\n",
            "trainset after adding uncertain samples (80, 10) (80,)\n",
            "updated train set: (80, 10) (80,) unique(labels): [33 47] [0 1]\n",
            "val set: (1222, 10) (1222,)\n",
            "\n",
            "Train set: (80, 10)\n",
            "Validation set: (1222, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "<ipython-input-60-82e50070648f>:24: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-60-82e50070648f>:24: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "val predicted: (1222,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (80, 10) (80,)\n",
            "trainset after adding uncertain samples (90, 10) (90,)\n",
            "updated train set: (90, 10) (90,) unique(labels): [39 51] [0 1]\n",
            "val set: (1212, 10) (1212,)\n",
            "\n",
            "Train set: (90, 10)\n",
            "Validation set: (1212, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.5555555555555556, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.48      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (90, 10) (90,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [45 55] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.880184 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.47      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (110, 10) (110,)\n",
            "updated train set: (110, 10) (110,) unique(labels): [49 61] [0 1]\n",
            "val set: (1192, 10) (1192,)\n",
            "\n",
            "Train set: (110, 10)\n",
            "Validation set: (1192, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.958525 \n",
            "Classification report for LogisticRegression(C=0.45454545454545453, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.46      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (110, 10) (110,)\n",
            "trainset after adding uncertain samples (120, 10) (120,)\n",
            "updated train set: (120, 10) (120,) unique(labels): [53 67] [0 1]\n",
            "val set: (1182, 10) (1182,)\n",
            "\n",
            "Train set: (120, 10)\n",
            "Validation set: (1182, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.419355 \n",
            "Classification report for LogisticRegression(C=0.4166666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.58      0.48      0.52       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (120, 10) (120,)\n",
            "trainset after adding uncertain samples (130, 10) (130,)\n",
            "updated train set: (130, 10) (130,) unique(labels): [57 73] [0 1]\n",
            "val set: (1172, 10) (1172,)\n",
            "\n",
            "Train set: (130, 10)\n",
            "Validation set: (1172, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.649770 \n",
            "Classification report for LogisticRegression(C=0.38461538461538464, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.47      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (130, 10) (130,)\n",
            "trainset after adding uncertain samples (140, 10) (140,)\n",
            "updated train set: (140, 10) (140,) unique(labels): [62 78] [0 1]\n",
            "val set: (1162, 10) (1162,)\n",
            "\n",
            "Train set: (140, 10)\n",
            "Validation set: (1162, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.036866 \n",
            "Classification report for LogisticRegression(C=0.35714285714285715, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (140, 10) (140,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [62 88] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.188940 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.58      0.44      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (160, 10) (160,)\n",
            "updated train set: (160, 10) (160,) unique(labels): [64 96] [0 1]\n",
            "val set: (1142, 10) (1142,)\n",
            "\n",
            "Train set: (160, 10)\n",
            "Validation set: (1142, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.419355 \n",
            "Classification report for LogisticRegression(C=0.3125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.59      0.44      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (160, 10) (160,)\n",
            "trainset after adding uncertain samples (170, 10) (170,)\n",
            "updated train set: (170, 10) (170,) unique(labels): [ 64 106] [0 1]\n",
            "val set: (1132, 10) (1132,)\n",
            "\n",
            "Train set: (170, 10)\n",
            "Validation set: (1132, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.958525 \n",
            "Classification report for LogisticRegression(C=0.29411764705882354, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.44      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.68       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (170, 10) (170,)\n",
            "trainset after adding uncertain samples (180, 10) (180,)\n",
            "updated train set: (180, 10) (180,) unique(labels): [ 66 114] [0 1]\n",
            "val set: (1122, 10) (1122,)\n",
            "\n",
            "Train set: (180, 10)\n",
            "Validation set: (1122, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.2777777777777778, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (180, 10) (180,)\n",
            "trainset after adding uncertain samples (190, 10) (190,)\n",
            "updated train set: (190, 10) (190,) unique(labels): [ 70 120] [0 1]\n",
            "val set: (1112, 10) (1112,)\n",
            "\n",
            "Train set: (190, 10)\n",
            "Validation set: (1112, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.2631578947368421, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (190, 10) (190,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 73 127] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.86       321\n",
            "           1       0.63      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (210, 10) (210,)\n",
            "updated train set: (210, 10) (210,) unique(labels): [ 78 132] [0 1]\n",
            "val set: (1092, 10) (1092,)\n",
            "\n",
            "Train set: (210, 10)\n",
            "Validation set: (1092, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.23809523809523808, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.63      0.44      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (210, 10) (210,)\n",
            "trainset after adding uncertain samples (220, 10) (220,)\n",
            "updated train set: (220, 10) (220,) unique(labels): [ 83 137] [0 1]\n",
            "val set: (1082, 10) (1082,)\n",
            "\n",
            "Train set: (220, 10)\n",
            "Validation set: (1082, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.22727272727272727, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (220, 10) (220,)\n",
            "trainset after adding uncertain samples (230, 10) (230,)\n",
            "updated train set: (230, 10) (230,) unique(labels): [ 86 144] [0 1]\n",
            "val set: (1072, 10) (1072,)\n",
            "\n",
            "Train set: (230, 10)\n",
            "Validation set: (1072, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.21739130434782608, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (230, 10) (230,)\n",
            "trainset after adding uncertain samples (240, 10) (240,)\n",
            "updated train set: (240, 10) (240,) unique(labels): [ 91 149] [0 1]\n",
            "val set: (1062, 10) (1062,)\n",
            "\n",
            "Train set: (240, 10)\n",
            "Validation set: (1062, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.20833333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (240, 10) (240,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [ 95 155] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (260, 10) (260,)\n",
            "updated train set: (260, 10) (260,) unique(labels): [ 99 161] [0 1]\n",
            "val set: (1042, 10) (1042,)\n",
            "\n",
            "Train set: (260, 10)\n",
            "Validation set: (1042, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.19230769230769232, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (260, 10) (260,)\n",
            "trainset after adding uncertain samples (270, 10) (270,)\n",
            "updated train set: (270, 10) (270,) unique(labels): [104 166] [0 1]\n",
            "val set: (1032, 10) (1032,)\n",
            "\n",
            "Train set: (270, 10)\n",
            "Validation set: (1032, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.18518518518518517, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (270, 10) (270,)\n",
            "trainset after adding uncertain samples (280, 10) (280,)\n",
            "updated train set: (280, 10) (280,) unique(labels): [108 172] [0 1]\n",
            "val set: (1022, 10) (1022,)\n",
            "\n",
            "Train set: (280, 10)\n",
            "Validation set: (1022, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.17857142857142858, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (280, 10) (280,)\n",
            "trainset after adding uncertain samples (290, 10) (290,)\n",
            "updated train set: (290, 10) (290,) unique(labels): [114 176] [0 1]\n",
            "val set: (1012, 10) (1012,)\n",
            "\n",
            "Train set: (290, 10)\n",
            "Validation set: (1012, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.1724137931034483, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (290, 10) (290,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [117 183] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (310, 10) (310,)\n",
            "updated train set: (310, 10) (310,) unique(labels): [121 189] [0 1]\n",
            "val set: (992, 10) (992,)\n",
            "\n",
            "Train set: (310, 10)\n",
            "Validation set: (992, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.16129032258064516, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (310, 10) (310,)\n",
            "trainset after adding uncertain samples (320, 10) (320,)\n",
            "updated train set: (320, 10) (320,) unique(labels): [126 194] [0 1]\n",
            "val set: (982, 10) (982,)\n",
            "\n",
            "Train set: (320, 10)\n",
            "Validation set: (982, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.15625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (320, 10) (320,)\n",
            "trainset after adding uncertain samples (330, 10) (330,)\n",
            "updated train set: (330, 10) (330,) unique(labels): [127 203] [0 1]\n",
            "val set: (972, 10) (972,)\n",
            "\n",
            "Train set: (330, 10)\n",
            "Validation set: (972, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.15151515151515152, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (330, 10) (330,)\n",
            "trainset after adding uncertain samples (340, 10) (340,)\n",
            "updated train set: (340, 10) (340,) unique(labels): [132 208] [0 1]\n",
            "val set: (962, 10) (962,)\n",
            "\n",
            "Train set: (340, 10)\n",
            "Validation set: (962, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.14705882352941177, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (340, 10) (340,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [134 216] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (360, 10) (360,)\n",
            "updated train set: (360, 10) (360,) unique(labels): [139 221] [0 1]\n",
            "val set: (942, 10) (942,)\n",
            "\n",
            "Train set: (360, 10)\n",
            "Validation set: (942, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.1388888888888889, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (360, 10) (360,)\n",
            "trainset after adding uncertain samples (370, 10) (370,)\n",
            "updated train set: (370, 10) (370,) unique(labels): [139 231] [0 1]\n",
            "val set: (932, 10) (932,)\n",
            "\n",
            "Train set: (370, 10)\n",
            "Validation set: (932, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.13513513513513514, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (370, 10) (370,)\n",
            "trainset after adding uncertain samples (380, 10) (380,)\n",
            "updated train set: (380, 10) (380,) unique(labels): [141 239] [0 1]\n",
            "val set: (922, 10) (922,)\n",
            "\n",
            "Train set: (380, 10)\n",
            "Validation set: (922, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.13157894736842105, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.47      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (380, 10) (380,)\n",
            "trainset after adding uncertain samples (390, 10) (390,)\n",
            "updated train set: (390, 10) (390,) unique(labels): [145 245] [0 1]\n",
            "val set: (912, 10) (912,)\n",
            "\n",
            "Train set: (390, 10)\n",
            "Validation set: (912, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.797235 \n",
            "Classification report for LogisticRegression(C=0.1282051282051282, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.74      0.46      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.70      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (390, 10) (390,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [148 252] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.72      0.44      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (410, 10) (410,)\n",
            "updated train set: (410, 10) (410,) unique(labels): [152 258] [0 1]\n",
            "val set: (892, 10) (892,)\n",
            "\n",
            "Train set: (410, 10)\n",
            "Validation set: (892, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.12195121951219512, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88       321\n",
            "           1       0.75      0.43      0.55       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.69      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0]\n",
            "trainset before adding uncertain samples (410, 10) (410,)\n",
            "trainset after adding uncertain samples (420, 10) (420,)\n",
            "updated train set: (420, 10) (420,) unique(labels): [155 265] [0 1]\n",
            "val set: (882, 10) (882,)\n",
            "\n",
            "Train set: (420, 10)\n",
            "Validation set: (882, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.11904761904761904, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.70      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (420, 10) (420,)\n",
            "trainset after adding uncertain samples (430, 10) (430,)\n",
            "updated train set: (430, 10) (430,) unique(labels): [158 272] [0 1]\n",
            "val set: (872, 10) (872,)\n",
            "\n",
            "Train set: (430, 10)\n",
            "Validation set: (872, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.11627906976744186, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88       321\n",
            "           1       0.75      0.44      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.69      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (430, 10) (430,)\n",
            "trainset after adding uncertain samples (440, 10) (440,)\n",
            "updated train set: (440, 10) (440,) unique(labels): [160 280] [0 1]\n",
            "val set: (862, 10) (862,)\n",
            "\n",
            "Train set: (440, 10)\n",
            "Validation set: (862, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.11363636363636363, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88       321\n",
            "           1       0.74      0.43      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (440, 10) (440,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [161 289] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (460, 10) (460,)\n",
            "updated train set: (460, 10) (460,) unique(labels): [163 297] [0 1]\n",
            "val set: (842, 10) (842,)\n",
            "\n",
            "Train set: (460, 10)\n",
            "Validation set: (842, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.10869565217391304, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (460, 10) (460,)\n",
            "trainset after adding uncertain samples (470, 10) (470,)\n",
            "updated train set: (470, 10) (470,) unique(labels): [164 306] [0 1]\n",
            "val set: (832, 10) (832,)\n",
            "\n",
            "Train set: (470, 10)\n",
            "Validation set: (832, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.10638297872340426, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (470, 10) (470,)\n",
            "trainset after adding uncertain samples (480, 10) (480,)\n",
            "updated train set: (480, 10) (480,) unique(labels): [166 314] [0 1]\n",
            "val set: (822, 10) (822,)\n",
            "\n",
            "Train set: (480, 10)\n",
            "Validation set: (822, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.10416666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88       321\n",
            "           1       0.74      0.43      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (480, 10) (480,)\n",
            "trainset after adding uncertain samples (490, 10) (490,)\n",
            "updated train set: (490, 10) (490,) unique(labels): [166 324] [0 1]\n",
            "val set: (812, 10) (812,)\n",
            "\n",
            "Train set: (490, 10)\n",
            "Validation set: (812, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.10204081632653061, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (490, 10) (490,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [170 330] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88       321\n",
            "           1       0.74      0.43      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.49769585253456, 77.41935483870968, 76.72811059907833, 76.72811059907833, 74.65437788018433, 77.41935483870968, 79.72350230414746, 78.57142857142857, 78.11059907834101, 77.88018433179722, 76.95852534562212, 77.41935483870968, 77.64976958525345, 76.036866359447, 77.18894009216591, 77.41935483870968, 76.95852534562212, 79.03225806451613, 79.26267281105991, 79.03225806451613, 78.80184331797236, 79.49308755760369, 79.49308755760369, 79.03225806451613, 80.64516129032258, 80.18433179723502, 80.18433179723502, 80.18433179723502, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.4147465437788, 80.64516129032258, 81.33640552995391, 81.10599078341014, 81.10599078341014, 80.64516129032258, 81.5668202764977, 81.79723502304147, 81.10599078341014, 81.5668202764977, 80.18433179723502, 81.5668202764977, 81.33640552995391, 80.64516129032258, 81.10599078341014, 80.87557603686636, 81.33640552995391, 81.10599078341014, 81.33640552995391]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-15.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          76.49769585253456,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          74.65437788018433,\n",
            "          77.41935483870968,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          76.036866359447,\n",
            "          77.18894009216591,\n",
            "          77.41935483870968,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          81.33640552995391,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          81.5668202764977,\n",
            "          81.79723502304147,\n",
            "          81.10599078341014,\n",
            "          81.5668202764977,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          81.33640552995391,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          81.33640552995391,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.27188940092167,\n",
            "          71.6589861751152,\n",
            "          69.81566820276498,\n",
            "          74.65437788018433,\n",
            "          74.42396313364056,\n",
            "          75.57603686635944,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502,\n",
            "          81.33640552995391,\n",
            "          82.02764976958525,\n",
            "          82.25806451612904,\n",
            "          81.5668202764977,\n",
            "          81.33640552995391,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.57142857142857,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.49769585253456,\n",
            "          73.73271889400922,\n",
            "          77.88018433179722,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          60.13824884792627,\n",
            "          56.68202764976959,\n",
            "          60.59907834101382,\n",
            "          68.4331797235023,\n",
            "          70.27649769585254,\n",
            "          72.81105990783409,\n",
            "          71.19815668202764,\n",
            "          72.81105990783409,\n",
            "          72.11981566820278,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          72.58064516129032,\n",
            "          72.58064516129032,\n",
            "          72.11981566820278,\n",
            "          70.73732718894009,\n",
            "          72.58064516129032,\n",
            "          71.19815668202764,\n",
            "          73.04147465437788,\n",
            "          72.81105990783409,\n",
            "          72.58064516129032,\n",
            "          72.58064516129032,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          73.27188940092167,\n",
            "          73.50230414746544,\n",
            "          72.81105990783409,\n",
            "          73.04147465437788,\n",
            "          73.73271889400922,\n",
            "          74.19354838709677,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          73.50230414746544,\n",
            "          73.963133640553,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.87557603686636,\n",
            "          79.03225806451613,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          81.33640552995391\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          79.26267281105991,\n",
            "          73.73271889400922,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.87557603686636,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          78.11059907834101,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 16, using model = LogModel, selection_function = MinStdSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 10) (250,) unique(labels): [113 137] [0 1]\n",
            "Val set: (1052, 10) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1052,) [33.34210192 33.73567715 26.25145668 ...  3.78956756 33.36187916\n",
            " 12.2145533 ]\n",
            "selection [ 415  272  709  694  971  149  531   57  938  769   29  280  881  559\n",
            "  112   73  145  408  191  316 1027  373   25  553  734  552  253  150\n",
            "   82  222  241  327  385  991  528  178  886  110  742  870   74 1007\n",
            "  606  954  934  375  634  821  772  851  706  753  913  787  624  845\n",
            "  247 1049  740  931  838  417  172   90  458  153  943  681 1018   36\n",
            "  276  798  163  591  613  348  972 1006  862  625  746  869  593  365\n",
            "  142  444  128  155  690  975   76  519  269  224  808  161 1015  422\n",
            " 1039  245  929  777    9  383  947  865  120   54  922  216  434  884\n",
            "  789  512 1009  433  214  364  231  674  180  855  919  354  752  343\n",
            "  554   69  843  970  498  767   43  960  521  250  775  805  442  485\n",
            "  103 1045  895   44  840  298  976  979  297  358  185  723  732  407\n",
            "  514   21  797  454 1034  223  822  733  409  928  333  563  923  562\n",
            "  134  380 1011  744  218  785  958  471   68  658  662  697  466  137\n",
            "  590  548  529  537  197  867  372  714  639 1041  270    6  645  644\n",
            "  577  904  757  168  770  304  864  246  969  656  482  303  165  660\n",
            "  585  374  472  642  463  421   13  611  244  736  508  893  966  801\n",
            "  700  735 1029  671  141  763 1051  495   22  932  926 1021  353  179\n",
            "   83  392  722  219  561  411  612  349  638  151  618  616] (250,) [ 0.02988698  0.13199273  0.21504924  0.32082602  0.43315715  0.45022685\n",
            "  0.63995555  0.74836532  0.77822156  0.88197087  0.99073363  1.00988731\n",
            "  1.01502893  1.07257799  1.10230497  1.18468771  1.22140955  1.25146678\n",
            "  1.27753781  1.33639303  1.35493708  1.5275081   1.53466911  1.54572875\n",
            "  1.54871401  1.55418742  1.55597502  1.64509125  1.72474678  1.82840232\n",
            "  1.84601757  1.9570538   2.03105945  2.03308681  2.07059241  2.12744904\n",
            "  2.25553524  2.41467054  2.64681127  2.66300154  2.77858884  2.81974241\n",
            "  2.96037054  2.98073869  3.0365449   3.12234161  3.12536453  3.18568473\n",
            "  3.27155965  3.37141086  3.46778337  3.53645001  3.55952838  3.65501767\n",
            "  3.73212189  3.74764076  3.74764076  3.78956756  3.79344318  3.80383204\n",
            "  3.88732776  3.89770346  3.89773968  3.90364526  3.92066861  3.9427759\n",
            "  3.96065233  3.98218607  4.09734307  4.30546385  4.33237383  4.37036527\n",
            "  4.38003514  4.54068592  4.59170787  4.61234293  4.62263146  4.63411979\n",
            "  4.63658177  4.70449645  5.00395591  5.17196406  5.2870764   5.30449153\n",
            "  5.35876917  5.39150459  5.41543822  5.53967542  5.59528014  5.64368286\n",
            "  5.76134063  5.8353158   5.84735864  5.85820002  5.94049713  5.9929737\n",
            "  6.00832113  6.07750872  6.17091511  6.17253817  6.22121028  6.38411974\n",
            "  6.47834925  6.68233308  6.7178347   6.72713336  6.77343442  6.81749213\n",
            "  6.87244039  6.8796236   6.91535933  6.91544258  7.14084664  7.18927501\n",
            "  7.27686703  7.29352983  7.45906687  7.53343452  7.54989772  7.6531805\n",
            "  7.67211498  7.67347126  7.67848831  7.69231106  7.7162485   7.84636874\n",
            "  7.91634275  7.91794327  7.98040888  7.98652255  7.99010983  8.06170518\n",
            "  8.10295259  8.14126552  8.19301879  8.21111783  8.25562207  8.28447287\n",
            "  8.34860529  8.36531737  8.42413123  8.47946032  8.49008817  8.51378369\n",
            "  8.55961007  8.64644595  8.64644595  8.64644595  8.64644595  8.64644595\n",
            "  8.64644595  8.64644595  8.64644595  8.64644595  8.64644595  8.64644595\n",
            "  8.64644595  8.64644595  8.64644595  8.64644595  8.64644595  8.64644595\n",
            "  8.64644595  8.64644595  8.64644595  8.64644595  8.64644595  8.64644595\n",
            "  8.64644595  8.7279065   8.72970346  8.81225062  8.85168066  8.87265142\n",
            "  8.91225273  8.97377411  8.98915399  9.16460332  9.23956459  9.3182264\n",
            "  9.39869688  9.50144136  9.58621715  9.63302666  9.64783067  9.66263129\n",
            "  9.9581281   9.96086013  9.99467295 10.01257376 10.16520403 10.18236885\n",
            " 10.18794098 10.21319988 10.40770304 10.42070497 10.44236326 10.45347538\n",
            " 10.5033613  10.57071223 10.57556789 10.63235443 10.68773745 10.77202144\n",
            " 10.85028768 10.91031916 10.91468526 10.91527962 11.06246839 11.11925843\n",
            " 11.1925956  11.1925956  11.2499737  11.25338307 11.28046843 11.30515506\n",
            " 11.32337464 11.42447614 11.44586634 11.47334214 11.51200937 11.62581127\n",
            " 11.63444947 11.67040267 11.78519464 11.89572137 11.9308473  12.09761234\n",
            " 12.10422255 12.10826601 12.2145533  12.23451924 12.33441765 12.33441765\n",
            " 12.52347604 12.60166012 12.73407145 12.77362119 12.82542997 12.83504255\n",
            " 12.85216369 12.87492428 12.88432259 12.91536253 12.91909209 12.9259896\n",
            " 13.01075624 13.01526857 13.03962857 13.04212005]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [211 289] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "final active learning accuracies [79.03225806451613, 79.49308755760369]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-16.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 17, using model = LogModel, selection_function = MinStdSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 10) (125,) unique(labels): [60 65] [0 1]\n",
            "Val set: (1177, 10) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 70.967742 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.73      0.79       321\n",
            "           1       0.46      0.65      0.54       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.69      0.66       434\n",
            "weighted avg       0.75      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[235  86]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1177,) [ 6.76343095 45.39945611 28.35318489 ...  3.96120548 39.7186706\n",
            " 17.37093451]\n",
            "selection [ 199 1173 1120  759  577  236 1067  797  596  976  900  339  679  813\n",
            "   63  243  261  587  306  352  245  888  128  311  615  661    8  641\n",
            "  856  618  774  470  732  318  216  664  430  113  284  769  578  109\n",
            "  836  162  988  799  708  130 1051  819  426  682  249  123 1012 1106\n",
            "  592  593  439  803  627  396 1115  386 1061  133  788  170  782  360\n",
            "  527   10 1025 1137 1151 1110  817   26  823  953  619 1157 1174  520\n",
            "  535 1132  694  305  537  999  825  837  772  715  419  191 1168  508\n",
            "  983  227  559  994  127  744  522  721  310  524  159 1109 1080  634\n",
            "  835 1062  831  891  862  977  532  792  203   30   50  811  171] (125,) [0.0171231  0.09733154 0.13538333 0.15072148 0.21764048 0.24167972\n",
            " 0.27253321 0.36663694 0.38510682 0.45764126 0.51677857 0.61532983\n",
            " 0.68425117 0.69843092 0.74072852 0.74371686 0.94804551 1.02897165\n",
            " 1.08990224 1.11875893 1.16090693 1.24654895 1.30100868 1.33250644\n",
            " 1.38719247 1.38869543 1.45347207 1.46283987 1.59417085 1.61126708\n",
            " 1.64393863 1.68343179 1.83923059 1.86668259 1.95495349 1.96611683\n",
            " 2.03694844 2.04773133 2.20152009 2.23855242 2.27447246 2.35880181\n",
            " 2.41602038 2.41689616 2.47621985 2.51024869 2.73231334 2.73595839\n",
            " 2.74969349 2.80535136 2.91348664 2.94788333 2.97140457 2.97408629\n",
            " 3.07533956 3.12181064 3.18075196 3.19045158 3.20920095 3.21334927\n",
            " 3.25029981 3.30623237 3.32060327 3.33573439 3.36098817 3.37492156\n",
            " 3.44016609 3.44278644 3.46155774 3.47810674 3.48599783 3.5250113\n",
            " 3.56576301 3.57006874 3.60988705 3.70156002 3.7208305  3.77659807\n",
            " 3.77800402 3.86066277 3.88194634 3.93276451 3.96120548 3.99609409\n",
            " 3.99738143 4.08797321 4.13085262 4.13285649 4.13835228 4.14973705\n",
            " 4.16879314 4.24234977 4.25967652 4.39400036 4.43377668 4.48300239\n",
            " 4.4886485  4.52498353 4.55826276 4.60704586 4.7357907  4.73882302\n",
            " 4.79637993 4.88549428 4.90908319 5.08140421 5.22064217 5.22660401\n",
            " 5.22978329 5.2460881  5.25436644 5.27082593 5.27971986 5.40544999\n",
            " 5.40677164 5.45170385 5.46262802 5.56742276 5.62172346 5.63920606\n",
            " 5.69703684 5.77055525 5.77699571 5.83728578 5.85383092]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [119 131] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.271889 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.77      0.81       321\n",
            "           1       0.49      0.64      0.55       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[246  75]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1052,) [17.68672228 39.47760428 27.44837796 ... 25.75464752 35.36699066\n",
            " 14.46365291]\n",
            "selection [ 923  384 1047   88  532  331  193   81  395  559  892  249  376  246\n",
            "  435  937   25  694  728  119  474  118  334  369  765  399   27  668\n",
            "  985  879   56  762  751  629  545  780  563  225  449   23  931 1005\n",
            "  275  323  870  826  549   14  206  153   48  793  303  722  418  420\n",
            "  141  730  729  167  468  195  526  567  224  566  789  305  337   34\n",
            " 1034  361   22  980  973  926  920  536   17  741   26  131   95  535\n",
            "  995  229  200  115  148   46  322  862  782  444  482  426 1025  865\n",
            "  345  236   73  757  602  764  282  640  390  520  593  415  548  481\n",
            "  821  366 1015  498  950  125  180  534  442  801  280  439  143] (125,) [0.09750588 0.26899877 0.51540413 0.52983017 0.76882204 0.84047422\n",
            " 0.9676504  1.03471465 1.11176569 1.55516652 1.64556552 1.64556552\n",
            " 1.81047087 1.90270255 2.22159214 2.44222563 2.51936116 2.6820497\n",
            " 2.70592784 2.95284988 2.98911753 2.990598   3.04677989 3.11787386\n",
            " 3.15013954 3.55365601 3.58574901 3.73054782 3.77429154 3.8753507\n",
            " 3.92298664 3.97066768 4.01188507 4.12215057 4.194244   4.23108283\n",
            " 4.38346083 4.45687549 4.47961065 4.4905758  4.4905758  4.53709017\n",
            " 4.69344492 4.83214576 4.86402319 4.88148069 4.98288617 4.99200174\n",
            " 5.03769257 5.41288699 5.51682574 5.57024601 5.57024601 5.57024601\n",
            " 5.57024601 5.57024601 5.57024601 5.57024601 5.57024601 5.57024601\n",
            " 5.57024601 5.57024601 5.57024601 5.57024601 5.57024601 5.57024601\n",
            " 5.57024601 5.57024601 5.57024601 5.57024601 5.57024601 5.57024601\n",
            " 5.57024601 5.57024601 5.57024601 5.57024601 5.57024601 5.57024601\n",
            " 5.63383556 5.63383556 5.63731638 5.64745981 5.66910907 5.71852531\n",
            " 5.94214508 5.98821966 6.23314368 6.27767217 6.31268415 6.41448196\n",
            " 6.45151352 6.49220156 6.51640048 6.54978508 6.59754599 6.69239819\n",
            " 6.81483273 6.82587765 6.87959606 6.88043245 7.0453105  7.10246495\n",
            " 7.24358766 7.26345003 7.28848208 7.44332426 7.62726251 7.66023397\n",
            " 7.68523691 7.69209159 7.74839809 7.75609042 7.76880341 7.78040645\n",
            " 7.80026728 7.92248247 7.96815406 8.12322505 8.23338859 8.35213137\n",
            " 8.39865714 8.40895717 8.41972247 8.59043092 8.71974374]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [188 187] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0]\n",
            "std (927,) [18.50349574 30.79195589 23.1848935   9.48246058  8.64387999 24.62381607\n",
            "  8.81348485 13.57330522 28.83951716 17.45425895 13.10818968 27.40743445\n",
            " 40.15050562 11.78508408 36.0062234  46.48506786 31.82422446 27.05943433\n",
            " 16.66080201 20.30165476 22.1416207  35.83122895 23.84344305  8.0956608\n",
            " 12.9207221  39.45756045 27.03883192 38.24312808 22.07418368 27.51736837\n",
            " 27.83778306  3.27280134 24.0403792  33.16698125 23.7030458  25.17591314\n",
            " 38.2876334  21.70876527 16.62607002  2.5132577  19.85603987 15.17834796\n",
            " 22.25349005 22.06228565 40.65006338 12.66776816 16.55191903 23.77316849\n",
            " 25.6325414  10.92514709 31.82458215 19.26563996 26.10918938 33.24555961\n",
            " 12.74394294 30.26249399 16.85990011 13.37280274 14.6032567  22.99954128\n",
            " 12.25032962 10.54240402 25.04363896 45.33099579 31.74350405 32.66093123\n",
            "  5.97742375  7.49916725 14.14344242 33.69277414 13.07603893 34.18811373\n",
            " 29.4637839  28.49519988 15.24120151 11.7951423  41.46583627 23.08220177\n",
            "  9.09657093 32.7272602  39.65254676  9.89504252 32.85954234 42.66868237\n",
            " 10.30392402 14.26133806 27.23112366  7.24845892 14.19333865 30.45753157\n",
            "  5.87044446 34.1597817  17.17702216 19.31762478 22.92979573 22.59259106\n",
            "  4.60063491 32.99186262 24.55784018 23.44660243 24.52852375 26.76355432\n",
            " 19.29503533 30.55910073 42.60573487 27.71632612 23.45929492  8.04340048\n",
            " 22.08880642 33.11788313 28.33913612 10.12742021 21.16540165 28.06743547\n",
            "  8.47572693 24.29247099 17.74262963 10.86171871 24.62728023 26.74075543\n",
            " 12.34967315 32.21149407  1.54325526 33.48009792 28.087696   12.29654053\n",
            " 26.00943396 26.81199817  3.31589601 20.29043193 38.160374   22.46143172\n",
            "  2.22379389 18.03152853 23.01723304 25.22451104 22.1593274  15.22110397\n",
            " 22.17670633 17.11144334 34.50074201 24.34620655 10.70908964 26.02064047\n",
            "  4.20921261 31.19606899 20.61364154  8.28566328 15.46255116  9.96674684\n",
            "  5.73037979 23.23235626 12.04246791 26.78558673  9.98473249 22.76449884\n",
            " 30.50911591 21.91192037 19.40665112  7.35190021 21.11309984 12.3229898\n",
            " 12.38334319 11.84764238 14.46427703 23.87836886  2.2165015  28.45927367\n",
            " 17.19103101 21.74161567 16.87103326 47.93971175 21.55153295 17.55140907\n",
            " 21.85358448 15.36574895 38.20230468 22.94656429 17.28143771  6.04563436\n",
            " 20.54272086 49.84305513 21.42792552 30.42777153 11.65092459 24.00899944\n",
            " 13.1623231  29.97169123 26.83666806 36.18498859 31.39985812  8.21844311\n",
            " 25.82639295 49.36038471 23.88989997 20.08318284 12.15304668 27.25897873\n",
            " 15.99595616 27.60878186 20.20905553 10.84070826 31.83317033 20.35071648\n",
            " 42.63034809 21.01537331 23.9260887   8.49275947 35.45614291 22.58428998\n",
            " 17.28498845  7.34583136 29.78832659  6.1811796  14.14078128 10.7643971\n",
            " 26.41816174 13.26421687 23.11338797 14.84397059 33.59541598 28.42190978\n",
            " 23.1596973  38.98156135 18.9664476  29.19527511 29.05293013 17.227688\n",
            " 29.665661   25.15343783 30.15567783 34.58284066 24.59608129 27.72893423\n",
            " 31.90899108 37.68312271  0.30146089 14.41740562 31.47227665 35.84942476\n",
            " 16.67470419 20.24391096 35.53303271 30.16058021 21.54952705  0.97605103\n",
            " 45.06584252 28.40193723 31.35758434 15.53350009 19.19111655 39.36633837\n",
            " 33.40148256 13.08089877 40.91094209 10.83845045 30.48698165  5.62256644\n",
            " 37.0286144  16.12248988 16.15357522 28.01342626 29.96458231 11.79441683\n",
            " 46.65528067  4.79224745 27.10189673 19.1881547  31.5505613  35.88258028\n",
            " 29.09259716 12.5556483  20.08262106 36.75110827 19.7637102   5.74213013\n",
            " 25.8778913  27.10459431 23.167176   27.09741398 12.54461476 44.81137002\n",
            " 17.22792869 25.08565113 13.73330888 20.76175145 19.63856323 23.82194757\n",
            " 20.58935051 31.88097167 26.18960579  9.23678826  6.81422677 32.8243342\n",
            " 36.5942578  20.90161949 17.52315949 21.06148043 30.41766417 11.76996176\n",
            "  9.33445909 24.93414726 18.34370552  9.6206856   4.72485867 11.42155822\n",
            " 25.11470557  1.36162971 14.79439749 23.84011452 19.02183633  9.72174114\n",
            " 22.77459897 23.60043968 14.93793435 42.75523902 31.80023267 23.7258788\n",
            "  9.73973434 11.43023954 23.34323131 31.92040194 32.22953514 21.04399374\n",
            " 26.6049561  18.49808203 12.1919768  14.21913365 25.48967959 21.64072964\n",
            " 27.79005988 15.1543383  41.35097668 20.97084677  4.92401381 16.72929191\n",
            " 24.82570359 25.34440115 34.38767105  9.44750551 19.20166409 33.6994988\n",
            " 37.81507497 16.91001793 15.47651764 40.22019073 19.36855733 24.44292111\n",
            " 19.54292437 15.80559941 45.2588655  25.35521799  6.8760716  16.20494052\n",
            "  6.96307612 19.8607398  16.09276655 30.5372378  49.18937205  8.89444958\n",
            " 29.37099603 33.15801046  1.17470946 20.30550215 16.53234037 29.19609156\n",
            " 18.9676658  26.28169856 29.33549578 28.30281019 29.65632843 21.84844923\n",
            " 38.20862327 20.11954566 24.13404324 14.93896851 11.21986889  0.63376005\n",
            " 27.41126407 24.93531199  6.45944199 37.07001569 14.49789418 16.77807896\n",
            " 38.66743246 36.11860915 17.31758625 27.2398326  29.4265953  15.55160134\n",
            " 21.81138009 29.01750089  8.16328489 17.02363586 15.03951371 29.51656007\n",
            " 17.6976198  28.48159296 21.5294552  19.68801579 37.4707684  16.72896144\n",
            " 25.95835309 10.36170801 24.78655158 22.13991489 18.54228482 10.152273\n",
            "  9.92816443 32.88854059 37.73801663 39.13368846 28.77230204 11.68674831\n",
            " 41.86780443 11.76829937  6.60662798 34.36429805 40.714997   23.80682032\n",
            " 41.89137689 12.5763695  35.09435221 16.98526492 25.20678525 29.40669494\n",
            " 30.18128472 20.34642311 32.88747227 18.3376381  18.56104075 19.56133936\n",
            " 11.64960585 13.69911367 13.22813959 18.83324611 11.53861013 29.67154496\n",
            " 29.02510569 37.71601023 21.44214179 33.96368188 47.89815377 32.89119224\n",
            " 16.85754312 10.63602743 21.1494585  17.20628505 21.32017186 17.77569014\n",
            " 28.38641658  6.25682322 26.5827046  30.01266958 14.73160197 40.48697132\n",
            " 16.62488601 14.50962036 28.45750855 20.48470334 16.76965862 14.57489276\n",
            " 15.40396381 17.15623148 34.14346869 12.78347437 35.06408389 14.33563102\n",
            " 39.1649555  32.11604263 20.99592101 32.51525266 14.38646934  9.5436383\n",
            " 21.66475082 18.74375838 16.81774061  8.69189843 22.50374137 18.57431216\n",
            " 31.08620441 13.73191782 45.43891954 29.99965842 36.94584971 35.65863812\n",
            "  4.51054434 34.51690717 29.6535294   8.3950744  11.65460238 41.03140892\n",
            " 18.6412541  32.11002516 12.59820771 10.36008933  4.05030488 35.0518259\n",
            " 28.54087651 12.06551102  8.43473506 18.5078559  30.02592825 27.5634187\n",
            " 37.71429816 29.02674679 23.22235318 14.21913365 28.06985929 14.02578528\n",
            " 33.59911691 39.33944531  7.35063519 49.11709864 12.26813723 12.76505947\n",
            " 25.20184275 21.62203385 15.08031806 17.88338097 24.17291386 34.80652406\n",
            " 25.48105144 42.45504186 48.45301736 16.17611078  7.39719582 20.53283131\n",
            "  4.49027715 33.91056572 11.84215177 39.72825718 15.1134302  17.18960655\n",
            " 42.89249755 24.72753351  9.21667165 13.1386337  29.2266596  15.09271875\n",
            " 15.81850875 28.26126704  8.16043636 39.21385604 25.34898191 29.89823443\n",
            " 12.07665771 18.77617215 36.21376764 15.17653839  9.91773511 16.98342006\n",
            " 14.59665694 30.38211569  7.55502763 19.71908672  9.45872333 22.03440671\n",
            " 24.97710312 43.88650187 29.42323477 39.917268   22.67472334 30.6205407\n",
            "  0.3822978  12.36589266 46.48021869 22.51661118 13.94762956 14.88264929\n",
            " 18.54131307 21.69998928 25.14742244 41.76186392 22.42839424  6.95593665\n",
            "  9.80774518 26.01824496 21.21810352  3.22567275  9.31559286 26.63824437\n",
            " 23.55937465  8.77388068 22.10128322 45.14723952 16.95280891 15.30832284\n",
            " 26.67005207 15.59229349 28.04163003 19.64086834  7.16245736 19.35553938\n",
            " 23.8712222   8.21079322 20.56827404 30.66456596 15.74720652 22.09887082\n",
            " 34.30053399 20.85600027 35.41824262 19.65456912 38.39294695 22.9261638\n",
            " 41.17867033 10.19080698 22.13441521 25.8294874  26.83066949  7.15751449\n",
            " 35.96520192 34.49247585 15.33840244 26.15558935 11.72984121 13.21483942\n",
            " 23.82194757 41.94480708 21.81423261 21.6529763  28.72015686 45.54866044\n",
            "  5.57764501 31.60890415 18.91600737 18.16738782 14.19721493 23.83010086\n",
            "  6.14308074 38.9466234  24.17267997 13.77056525 12.53364987 18.69911907\n",
            " 20.97064342 14.97123479 22.51845446  9.28187342 24.50330658 33.91547833\n",
            " 26.62812106 43.84466869  7.29679146 17.44974263 14.39418495 22.84465622\n",
            " 21.80190411  7.78465371 14.22974366  4.49579847  5.90949681 47.1250904\n",
            " 21.70556799 27.01363077 26.58701923 19.50354574  8.27006635 13.43088757\n",
            " 13.39709148 17.91789133 14.06557943 30.8648142  26.85651247  5.48201857\n",
            " 23.77367106 30.9116082  36.13195179 15.57701184 42.05908671 23.32415629\n",
            " 12.37338964 20.16040481 10.01388432 17.64832996 19.87353782 33.75296359\n",
            " 27.87225525 44.66571334 14.91093459 36.70396425 23.43134728 14.76651794\n",
            " 15.78935128 19.39280527 36.17978022 28.25066163 32.01297005 31.49967595\n",
            " 33.17962633 15.75993677 25.09680504 21.99571458 33.54712437 14.0298015\n",
            " 33.75560097 15.10493453 24.30264757 17.81466789 29.58289301 22.99438673\n",
            " 31.22929368 43.57309811 12.19471108 39.92451843 13.11332846 25.72811112\n",
            " 21.2431879  40.88014803 28.53523578 14.44967675 37.77410545  4.95227309\n",
            " 31.82601821 31.91548321 13.92916958 34.43928501 12.58676323 20.07371683\n",
            " 11.99619671 18.10988281 36.98292302 16.35727752 29.01562332 14.52555372\n",
            " 37.90535718 23.10512052  5.62461018 10.98203437  1.51566124 23.70128513\n",
            " 15.29874456  0.65932945 23.61652236 13.42664224  6.1811796  20.46897307\n",
            " 23.68790975 12.20179786 41.04597406 42.27363239 34.23009365 25.68795609\n",
            " 22.63395739 17.37004084 25.1756056  15.49519616 34.2198029  24.15606216\n",
            " 32.30350901 32.83700109 10.41339493 30.72701619  9.77796223 39.83960388\n",
            "  6.24993001  7.56718986 13.72586885  2.68089751  8.31594146 32.59593184\n",
            " 12.95613248 12.39013971  0.20320348 21.63743899 41.30860574 21.67779535\n",
            " 32.80727352 27.40769351 38.71218232 29.47441155 28.89820576 26.20305931\n",
            " 43.12625077  9.44293559 15.97141801 40.4612441   2.70385336  9.72215183\n",
            " 21.75189013 17.49791597  9.48638122 14.89192674 22.5895384  30.23260099\n",
            " 46.44464493 31.72997931 35.04781812  3.24529404 35.11607467 22.19412047\n",
            " 28.07487485 18.75052644 16.52622844 23.36613104 17.59290797 31.73562775\n",
            " 15.87361946 22.68501265 22.61148103 40.28942448 24.30411542 24.05847298\n",
            " 26.73464129 13.75902873 43.41065565  3.19220887 18.89171195  9.83089569\n",
            " 48.27782794 43.25668576 21.55625047 17.84095231  6.08470151 24.95875643\n",
            " 24.57174177 30.62147751 22.19911417 12.91569143 34.5640041  27.44609006\n",
            " 34.07788113 23.11338797 28.54087651 21.19547028 16.67585255 19.46209369\n",
            "  8.79071222 43.74239518 15.15484876  3.51269557 17.17702216 22.43595886\n",
            " 14.41245727 19.42338106 31.25606761 30.53733921 17.14698373 23.92266661\n",
            " 11.17539455 11.12833151 45.40856434 31.86009481 25.67742168 10.1342193\n",
            " 28.79423385 24.43386714  5.14859    23.87096464 23.28722169 15.4702446\n",
            "  4.1188654   5.94205309 10.74440004 33.45908329 27.72650026 42.84078589\n",
            " 23.15481231 40.54768186 29.22186118 11.84803416 17.2793     13.90643038\n",
            " 32.86484054 40.66183334 34.00176767 42.75744045 32.07778512 22.16495215\n",
            "  8.75974593 24.91528008 36.65414868 19.98878773 30.1662771  23.17570205\n",
            " 32.20594968 34.1157726   6.3998021  23.21556831 27.8848207  23.68128967\n",
            " 14.05684756 27.2715636  17.57551154 38.73317096 20.09955789  1.4978839\n",
            " 20.94656809 23.08942442  9.59030808  9.96655815 27.71929406 38.34429969\n",
            " 31.85924755 15.48029977 16.78347503  6.50133248 39.38635809 13.62906134\n",
            " 22.03005888  3.03987513 12.70477628  8.35253616 31.50332331 21.86354703\n",
            "  8.36870047 18.97898778 16.844139   15.95248884 11.64759906 10.65339742\n",
            " 41.19853005 18.09555458 14.85507669  8.80294205  0.26081623 49.99999974\n",
            "  7.20675473 35.31217022 16.53419471 28.75843491 28.94780301 28.66202231\n",
            " 19.07414175 29.34215841 10.26023087]\n",
            "selection [764 916 236 564 377 733 245 362 307 887 730 122 166 132  39 759 778 901\n",
            " 807 579 789  31 128 831 496 852 144 528 651 486  96 304 265 334 713 848\n",
            " 665 624 257 728 150 275  90 652 853  66 179 814 630 736 213 756 451 878\n",
            " 380 897 416 292 352 575 354 611 592 918  87 644 211 512 159 526  67 554\n",
            " 757 649 107  23 542 392 595 191 658 147 760 903 906 489 500 114 207   4\n",
            " 477 870 583 828 915   6 359  78 536 291 639 580 300 775 339 556   3 782\n",
            " 473 890 303 311 779 318 754 576 809  81 550 408 891 149 154 674 111] (125,) [ 0.20320348  0.26081623  0.30146089  0.3822978   0.63376005  0.65932945\n",
            "  0.97605103  1.17470946  1.36162971  1.4978839   1.51566124  1.54325526\n",
            "  2.2165015   2.22379389  2.5132577   2.68089751  2.70385336  3.03987513\n",
            "  3.19220887  3.22567275  3.24529404  3.27280134  3.31589601  3.51269557\n",
            "  4.05030488  4.1188654   4.20921261  4.49027715  4.49579847  4.51054434\n",
            "  4.60063491  4.72485867  4.79224745  4.92401381  4.95227309  5.14859\n",
            "  5.48201857  5.57764501  5.62256644  5.62461018  5.73037979  5.74213013\n",
            "  5.87044446  5.90949681  5.94205309  5.97742375  6.04563436  6.08470151\n",
            "  6.14308074  6.1811796   6.1811796   6.24993001  6.25682322  6.3998021\n",
            "  6.45944199  6.50133248  6.60662798  6.81422677  6.8760716   6.95593665\n",
            "  6.96307612  7.15751449  7.16245736  7.20675473  7.24845892  7.29679146\n",
            "  7.34583136  7.35063519  7.35190021  7.39719582  7.49916725  7.55502763\n",
            "  7.56718986  7.78465371  8.04340048  8.0956608   8.16043636  8.16328489\n",
            "  8.21079322  8.21844311  8.27006635  8.28566328  8.31594146  8.35253616\n",
            "  8.36870047  8.3950744   8.43473506  8.47572693  8.49275947  8.64387999\n",
            "  8.69189843  8.75974593  8.77388068  8.79071222  8.80294205  8.81348485\n",
            "  8.89444958  9.09657093  9.21667165  9.23678826  9.28187342  9.31559286\n",
            "  9.33445909  9.44293559  9.44750551  9.45872333  9.48246058  9.48638122\n",
            "  9.5436383   9.59030808  9.6206856   9.72174114  9.72215183  9.73973434\n",
            "  9.77796223  9.80774518  9.83089569  9.89504252  9.91773511  9.92816443\n",
            "  9.96655815  9.96674684  9.98473249 10.01388432 10.12742021]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [240 260] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "final active learning accuracies [70.96774193548387, 73.27188940092167, 79.03225806451613, 79.26267281105991]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-17.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 18, using model = LogModel, selection_function = MinStdSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 10) (50,) unique(labels): [22 28] [0 1]\n",
            "Val set: (1252, 10) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.649770 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.53      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "std (1252,) [38.91234894 39.40799322  8.85943606 ... 48.57312233 36.72859864\n",
            " 21.53520506]\n",
            "selection [ 847  870  354  698 1186  506  534  782 1245  208  259   92   98  754\n",
            "  524  430  661  248  884  894  102  142   94 1212  690  844  703  200\n",
            "  312   48  573  971  600  620  651  562  237  751  604  694  908 1200\n",
            " 1079  778  565  827  816 1018  610 1072] (50,) [0.05106354 0.08624794 0.14131368 0.23323048 0.33016121 0.35977735\n",
            " 0.39006149 0.41291509 0.49901385 0.57001402 0.58787697 0.58950693\n",
            " 0.59366492 0.80631241 0.81458499 0.84297803 0.90151897 0.96903792\n",
            " 0.97289743 1.01821848 1.07250151 1.08416652 1.10107618 1.13273192\n",
            " 1.23669926 1.36506549 1.38215749 1.45573332 1.47602294 1.682614\n",
            " 1.71509979 1.76649769 1.99729    2.04286583 2.09936562 2.10571942\n",
            " 2.13138686 2.3027754  2.31651028 2.41382077 2.42844221 2.5654191\n",
            " 2.57337173 2.60558461 2.63497255 2.78624344 2.84941447 2.86385858\n",
            " 2.87868656 2.91964417]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [52 48] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "std (1202,) [25.16934523 28.48951904 29.47568756 ... 44.2004727  19.41459231\n",
            "  1.05460832]\n",
            "selection [ 190  216  979  172 1170   56 1125  712  275  952  926  776  305  557\n",
            "  333  494  743   39  234  719  529  707 1193  133 1201  238   47  155\n",
            "  111 1048  471  472   21  520  823  586  596  634  635  925  899  893\n",
            "  822  150 1053 1178  408  338 1109  345] (50,) [0.0709823  0.14575532 0.27583051 0.39282999 0.41411163 0.42293559\n",
            " 0.54755577 0.59826821 0.61825626 0.61825626 0.65589051 0.6883113\n",
            " 0.69847724 0.70278409 0.73277529 0.75657416 0.75666682 0.8661194\n",
            " 0.91034796 0.92616197 0.98278583 0.99810564 1.01392863 1.0180894\n",
            " 1.05460832 1.16773635 1.17431443 1.21589745 1.24529583 1.24529583\n",
            " 1.24529583 1.24529583 1.24529583 1.24529583 1.24529583 1.24529583\n",
            " 1.24529583 1.24529583 1.24529583 1.24529583 1.24529583 1.24529583\n",
            " 1.24529583 1.24529583 1.24529583 1.24529583 1.24529583 1.24529583\n",
            " 1.24529583 1.24529583]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [85 65] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "std (1152,) [40.06456609 35.16190041 22.32439341 ...  3.61864367 49.63573711\n",
            " 29.18509399]\n",
            "selection [ 830  358  145  802   16  569  316  404 1010   96  803  606  290 1115\n",
            "  789  387  936  106 1100 1141  198  230  950  710  474  343  426  609\n",
            " 1060  515  816  591  767  595  751  868  577  295  961  381  201  236\n",
            " 1070  174   33 1137  361  323 1044   69] (50,) [2.36048476e-03 6.12695685e-02 1.89776418e-01 3.09333581e-01\n",
            " 3.09333581e-01 4.45805470e-01 5.77889667e-01 7.48357318e-01\n",
            " 8.40396539e-01 8.48218866e-01 9.64434769e-01 1.01760400e+00\n",
            " 1.02336396e+00 1.12381451e+00 1.24552272e+00 1.28186422e+00\n",
            " 1.31132447e+00 1.33553779e+00 1.34288395e+00 1.39357507e+00\n",
            " 1.43079590e+00 1.62173703e+00 1.62619058e+00 1.63069257e+00\n",
            " 1.63496369e+00 1.71413541e+00 1.74885022e+00 1.80144413e+00\n",
            " 1.89213756e+00 1.89770491e+00 2.06726439e+00 2.09030752e+00\n",
            " 2.15113158e+00 2.15833785e+00 2.24445565e+00 2.25051519e+00\n",
            " 2.27075801e+00 2.29939663e+00 2.62680001e+00 2.64878068e+00\n",
            " 2.67612800e+00 2.67612800e+00 2.67612800e+00 2.67612800e+00\n",
            " 2.67612800e+00 2.67612800e+00 2.67612800e+00 2.67612800e+00\n",
            " 2.70194518e+00 2.72983288e+00]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [115  85] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "std (1102,) [41.09666636 27.66368194 25.25936813 ... 12.59282791 49.50381584\n",
            " 26.07219549]\n",
            "selection [ 548  915  782   95  957  194   94  226  199  287  400  760  869   93\n",
            "   12  773   13  644  519  785  626  657  435   88  180  913 1066  450\n",
            " 1046  767  150  517   23  952  897   65  554  360  140  905  866  479\n",
            "   25   45 1079   71 1059   74  408  756] (50,) [0.05136747 0.15876023 0.20557556 0.40316434 0.45937078 0.46040265\n",
            " 0.58548764 0.62915412 0.66873107 0.73269779 0.84350069 0.86079968\n",
            " 0.8641064  1.09114289 1.12418707 1.15637994 1.17185861 1.20469637\n",
            " 1.24687948 1.32433978 1.46795688 1.49571157 1.55434863 1.71731673\n",
            " 1.93073127 1.97522064 1.9924098  2.13458605 2.19417233 2.25155824\n",
            " 2.47320449 2.50000736 2.52599405 2.67715234 2.77108587 2.79560514\n",
            " 2.8361394  2.85230567 2.93663232 3.10253414 3.10796684 3.12990071\n",
            " 3.25935545 3.28449423 3.36942422 3.39368945 3.5863195  3.60691894\n",
            " 3.63177539 3.74060563]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [138 112] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "std (1052,) [40.69863698 22.0982056  17.02534494 ... 15.03833313 49.33961985\n",
            " 27.63439026]\n",
            "selection [ 517  775  238   22  449  404  634  925   18   30  721  188  110 1022\n",
            "  528  877  856  611  336  615  628  211  963  914  736  231  496  855\n",
            "  111  579  776  699  646  949  459  947  592 1038  515  508  465  577\n",
            "  222  115 1046  555  943  644  921  635] (50,) [0.14826178 0.19033068 0.34962808 0.44043904 0.49049633 0.62241136\n",
            " 0.6931954  0.7933587  0.7933587  0.90816654 1.08379905 1.09419737\n",
            " 1.15658411 1.17799363 1.21970335 1.32182243 1.32320196 1.40892627\n",
            " 1.41458701 1.75345899 1.80434168 2.0898203  2.18126005 2.19736168\n",
            " 2.5157382  2.54231581 2.58911925 2.59209471 2.82018613 2.91710446\n",
            " 3.0595818  3.12865412 3.17781363 3.33117139 3.3346031  3.34104917\n",
            " 3.36023848 3.57213662 3.73019692 3.75568296 3.89855182 4.01857894\n",
            " 4.01969424 4.15266648 4.19610562 4.22338497 4.30273546 4.32748935\n",
            " 4.37537621 4.41846572]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [155 145] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "std (1002,) [39.98154237 21.55027075 14.95615149 ... 11.87542337 49.46548857\n",
            " 27.06795709]\n",
            "selection [261 987 653 977   3 113 904 680  96 662 593 682 848 259 962 749 487 878\n",
            " 703 442 991 922 637 979 728 625 411 136 446  90 204 597 254 854 823 360\n",
            " 810 675 651 621 293 376 972 766 401 986 420 468 998 167] (50,) [0.40166114 0.43264689 1.49437034 2.02565001 2.08623522 2.20702301\n",
            " 2.75590274 2.90767659 3.19581841 3.31896232 3.56163142 3.68946788\n",
            " 3.72622742 4.02009699 4.11508077 4.39111701 4.3918443  4.54388596\n",
            " 4.69841609 4.81314722 5.1253947  5.13799158 5.3169975  5.33883101\n",
            " 5.35028616 5.36323605 5.3702406  5.37341552 5.4179187  5.48999966\n",
            " 5.54443486 5.58400165 5.70990212 5.73945061 5.74799693 5.80406015\n",
            " 6.00083759 6.01738459 6.11285762 6.19001051 6.21513537 6.30202639\n",
            " 6.42799685 6.43742462 6.53986012 6.62374061 6.68841477 6.79286799\n",
            " 6.81826317 6.83168938]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [172 178] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0]\n",
            "std (952,) [36.7857561  18.82082806 14.61837823 18.43536984 18.38393417 23.16580809\n",
            "  8.29238549 10.76813024 12.22557331 38.75335878 44.12621091 44.39603971\n",
            " 44.19546154 28.27516527 21.08070931 19.13792628 15.6091569  11.03503487\n",
            " 33.38843706  9.45252853 41.89870766 21.6386843  10.19260052 35.06266967\n",
            " 22.62454747 35.41485269 20.07793938 25.1219712  26.06524353 31.68733931\n",
            " 19.69030238  8.28438773 26.20508575 40.78907117 13.72306374 14.16018283\n",
            " 13.73933827 14.45065942 19.9383618  20.7746317  16.32545776 34.7732753\n",
            "  5.71063815 10.27776983 20.94443913 23.62382334  9.92030406 27.41352305\n",
            " 11.92730743 44.11583275 30.0103539  12.86888767 28.38634314 12.09343295\n",
            " 17.43163708 19.80750542 15.86263981 18.66211556 42.29933247 23.35970306\n",
            " 42.11970645  7.22199955 12.99079039 29.75896296  9.30622262  7.46196018\n",
            " 29.49962311 29.05390118 25.3142963   9.43145661 11.79998292 19.16679961\n",
            " 23.80438599 29.40558588 27.90279472 12.26036115 22.04851531 49.5433628\n",
            " 16.90159458 34.97320082 31.77233983 12.76198329 16.3782861  22.12320973\n",
            " 21.62640907 26.57554742 31.99146311 19.22162143 21.78353248 19.24315362\n",
            " 34.64451972 35.25049747 12.71137338 31.91528083 24.79435653  6.05498279\n",
            " 34.60200987 13.52956412 25.40378681 49.84707861 36.61642647 39.62640546\n",
            " 32.13464611 20.03671521 28.27695654 24.93373256 24.1646011  23.87400832\n",
            " 39.646029   28.31841577 24.94234413 16.19376746 16.13347768 17.12420245\n",
            " 27.80061602 16.70084544  9.40580647 30.92225798 42.87441931 28.07768057\n",
            " 10.39408715 37.24409505  8.58592747 10.03404311 10.684193   25.53857983\n",
            " 23.00708318 15.21390531 20.89978366 49.27161246  6.56793048  8.77280663\n",
            " 24.69372557 12.48037789 17.82720414 13.44985512 12.05529939 49.98342061\n",
            " 23.12052805 28.51867923 28.61336495 19.03200066 23.36420469 24.35293076\n",
            " 30.83531918 16.62500023  8.29081797 13.40437331 27.0176344  10.35322156\n",
            " 12.72680417 18.54299043 28.74108881 17.09782086  6.66976548 20.79513619\n",
            " 27.60813826 18.63886869 14.49353597 19.58041427 10.25920309 10.31362366\n",
            " 14.17402478 24.43731852  6.26544705 26.86765558 22.1258671  17.39329935\n",
            " 17.26619087 13.84564227 47.34275877 17.81928984 19.6251923  18.64613887\n",
            " 11.03291303 25.26977663 10.31658126 34.02433433 12.40557889 24.38754626\n",
            " 15.94238828 49.55001455 21.46922798 26.7097067  17.02593657 20.74681448\n",
            " 26.09206288 30.39804002 34.47521433 42.79467486 27.99222194 12.5850226\n",
            "  7.25562158 13.870279   27.50866917 47.95878603 34.90283306 16.99444213\n",
            " 28.29660971 15.16835335 18.87357882 31.68257534 16.42642353  9.11237371\n",
            " 26.4996269  27.89406046 11.82563827 13.11457057 43.78033853 14.84824218\n",
            " 28.82305642 32.73930091 33.66194959 16.41197083  6.84516872 24.13591895\n",
            " 25.69844138 11.34692479 24.76183501 11.85016416 18.45504802 36.88846377\n",
            " 12.79849805 24.40291062 19.28535538 33.15137224 16.5725515  25.05270955\n",
            " 29.84713677 13.4913243  20.33839269 21.14149744 27.54931369 19.04554131\n",
            " 21.33149207 22.33033208 29.78435544 27.30652701 33.92851441 18.35762157\n",
            " 27.45983583  6.12611407 47.19402703 13.98651207  9.44011951 12.2318187\n",
            " 31.17941619 11.58442146 27.29694065  6.41626623 20.50444392 40.57522934\n",
            " 28.8159766  30.36850432 17.28995049 14.41303518 39.96865956 37.87520751\n",
            " 30.03544821 19.96176701 49.38430639 15.60885645 30.54779875 33.89633222\n",
            " 13.8896412  24.75038743 22.77926375 17.69050612 49.91144437 33.95100624\n",
            " 28.11933732 36.17557043 25.90545144 33.13821042 37.9837994   7.85421811\n",
            " 16.97474663 21.3995558  45.64655703 19.50263181 13.91133241 22.41886664\n",
            " 16.26312016 27.6240213  17.33123918 20.27321847 18.35240378 10.85861832\n",
            " 43.48132326 48.85031692 19.41790326 17.87582228 20.80575257 15.54457499\n",
            " 18.23345923 16.87978637 19.80631401  4.36194058 28.636705   18.62913486\n",
            " 24.73117131 22.76962266  7.1212963  22.37663351 30.08063084 24.81492887\n",
            " 23.80438599 33.86026758 12.38523939 14.04269338 25.8232302  24.0511043\n",
            " 17.28360087  9.70825379 25.30067028 25.60144895 14.91683374 10.32188952\n",
            " 10.31692244 20.46996759 29.43599412 24.76839569 30.6133489  13.35663836\n",
            "  8.71482332 26.52596751 22.97145137 10.94982783 39.46257119 19.2550188\n",
            " 27.091709   19.28066873 14.73236226  7.71911796 10.86526481 26.59573081\n",
            " 25.18464995 28.41302617 19.01064366 21.21234256 14.95819932  4.65884144\n",
            " 12.29218489 25.60909784 16.94364061  6.69086664 19.46340032 18.32491461\n",
            " 37.08643323 17.85759175 12.83873564 12.68177891 14.79788126 23.10565081\n",
            " 21.36721403  5.7397049  28.29928492  8.58553357 16.2427177  28.1101705\n",
            " 31.65210269 30.60482605 13.00749431 13.23162566 34.96868148 19.03567674\n",
            " 20.02357247 25.30178795 16.84193511 49.63855435 21.06219334  8.14527683\n",
            " 17.47808206 17.56182134 14.22536589 10.79751593 29.72109148 48.0196888\n",
            " 25.57243571 36.29720282 26.59918569 28.91502305 18.06577369 21.43111817\n",
            " 26.32828498  7.68890368 24.57965822 24.19254236 27.35253139 26.93234469\n",
            " 23.55021118 29.33184179 18.20940378 19.27666947 13.29551238 11.09629727\n",
            "  9.03561225 22.80382132 22.17241719 33.50225653 14.81425573 15.63837727\n",
            " 46.64858023 31.20504022  5.53536753 28.51088199 28.05984507 21.06882232\n",
            " 19.43822254 24.16537186  9.56106806  9.49719528 36.82241624 14.69705625\n",
            " 22.43502327 22.94804584 19.9459953  15.71292533 34.66511503 32.85212476\n",
            " 23.79277401 21.0522156  32.93530271 11.26361646 12.34370451 17.07565362\n",
            " 45.7488285  34.84089375 35.03791234 20.0591309  38.55865976 35.47516099\n",
            " 10.67142924  9.56106806 28.84111153 24.27295719 11.68262829 36.98641969\n",
            " 18.18879713 33.66875248 31.78452012 11.21010128 22.66462086 34.943739\n",
            " 23.66701909 20.78520387 27.2792549   4.57049319  9.17334333 12.84463739\n",
            " 29.61904477 46.08528317  9.46485673 15.78407007  9.80286736 27.51615995\n",
            " 22.9812242  31.313718   18.50459307 41.5852467  16.3951973  14.11583845\n",
            " 12.78291134 11.22033001  8.22676761 24.11630713 14.71017841 28.17610771\n",
            " 20.32181119 47.60752814  6.95613463 42.99789705  9.88846218 31.30680201\n",
            " 16.84732282 14.59729006 11.30940314 17.95872134 15.10361177 13.16711907\n",
            "  6.29298639 37.99919068  9.38639544 35.3391962  21.93618754 28.19579107\n",
            " 22.15067889 27.83099863 13.76158584 14.15671996 23.49559377 11.07892752\n",
            " 17.64384314 16.00088834 16.89196263 14.85881124 22.63610175 16.91507966\n",
            " 25.9131713  31.56366478 28.16578231  4.32301672 10.56526185 20.43395351\n",
            " 41.46048208 33.52288096 33.39173572 31.76390371 27.78778155 23.96373029\n",
            "  9.92708468 17.18746692 36.83205233 31.14576105 18.99302818 13.90852434\n",
            "  6.75365188 40.13338485 23.54206183 17.70307189  8.28738433 21.15422996\n",
            " 27.64648306 30.98958755 34.06282634 21.59854974 36.1033772  26.23544371\n",
            " 12.29218489 20.78155244 11.31850276 28.96156536 23.57834461 35.50040233\n",
            " 48.9761233   7.98311068 22.97145214 16.91507966  7.58341555 12.49823362\n",
            " 14.48348129 32.80439826 23.86682898 40.09642789 49.96526827 21.75751427\n",
            " 12.07964485 12.66089281 34.3432877  32.34425536 10.81921692 13.21971881\n",
            " 36.9135241  11.41193387 13.57772736 39.64066232 17.79719786 19.7990919\n",
            " 10.35922288 36.35553498 26.23956573 20.55487229  6.95167943 12.700065\n",
            " 34.88772539  6.78518162 35.14991318 23.31634068 28.61177079 20.71813105\n",
            " 31.60198082 20.18207236 12.08660258 15.02995167 20.4894297  26.09497825\n",
            "  7.51571914 16.59841351 23.46506482 41.6134009  27.44010003 30.06635993\n",
            " 20.90988205 29.9306262  10.7019357  49.97067149 32.57882358  9.93606654\n",
            "  9.63133238 29.90912217 19.2603036  19.65888354 37.78465746 18.34987907\n",
            " 20.54436218 12.03840556 18.90958893 30.74416355 20.43555836 16.47593501\n",
            " 46.88429695 16.296665   38.25665248 21.6795817  11.51707759 26.87646013\n",
            " 21.06209559 19.8385681   7.91427066 14.87846112 24.93197945  9.18689549\n",
            " 21.29255215 28.59965606 10.13631957 13.52572283 31.11918756 15.60664719\n",
            " 30.58867742 15.62141107 35.3642803  20.5809593  32.62166804 11.05852867\n",
            " 13.69792451 31.66194055 18.763595   15.61104137 43.52159396 21.91795467\n",
            "  7.89382132 45.47287732 31.88617661 12.90027777 31.05259133  6.43248409\n",
            " 19.80631401 49.25713949 21.95296221 21.15595954 25.98695156 42.2379985\n",
            "  8.06990445 27.40528766 29.16478729 37.65936529 18.94159279 42.22425481\n",
            " 20.7176452  20.66914587 35.23623454 45.15837272 27.47880564 17.92421826\n",
            " 13.76413245 29.09655103 17.4903814  28.98823896  5.55364174 34.56207598\n",
            " 30.67427739 34.74930285 40.66086995 43.75127219 15.06395043 13.033522\n",
            " 25.08633445 23.93765422 18.53159117 20.58656216 13.29284088 12.6729652\n",
            " 41.47252386 18.13415052 19.96648946 32.68990561 18.36855295  7.57673659\n",
            "  8.28318032 15.61029404 21.17278318 27.36370029 23.04685747 14.1438611\n",
            " 33.77112651  6.74540715 31.83643007 34.51215138 31.83311197  7.24684748\n",
            " 15.23837128 39.49762147 20.43413454  6.47017979 15.85150275 10.11507199\n",
            " 13.13824844 49.89790787 43.16584959 32.4294695  25.95039951 45.92997429\n",
            "  6.4819222  30.80845133 20.08990034 19.48256489 20.25214249 17.89406926\n",
            " 34.3098702  28.52460213 28.23436985 24.33919412 14.01757577 22.79303723\n",
            "  8.27596404 20.89503363 22.64825832 20.26337376 18.25532814 13.36867127\n",
            " 29.03042423  8.82081234 16.84933559  9.86826841 18.73325511 19.6552327\n",
            " 31.04619184 17.50157449 29.22566228 49.6333514  10.13639126 23.77088235\n",
            " 24.35764126 19.65436584 41.53437397 18.07667946 15.65889933 39.15805927\n",
            " 29.7194866  41.54437514 30.12176931 34.95048244 11.12731913 14.07006976\n",
            "  8.7015353  14.1353223  21.63244618 32.86503142 21.13469261 33.48793175\n",
            " 23.04092222 23.38747414 47.68242511 19.0694466   9.89958459 12.55668021\n",
            " 28.64992925  9.68300763 23.32487897 20.69335346  8.28145617 22.66230577\n",
            " 18.07721773 35.14783315 14.18276649 23.49559377 39.14632426 37.63267471\n",
            " 28.61517571 27.26127149 19.30578968 13.29248075 28.31906049 10.06125733\n",
            " 35.32256857 21.52373299 29.7556155  31.7015556  29.51888111 45.01322037\n",
            "  8.59329919 27.4676768   8.33153605 31.11031816 12.4984198   5.08103913\n",
            "  6.04719766 10.96973227 47.84094343 17.08111764 11.41121436  9.25711216\n",
            " 25.28287242 41.96870209 36.16718478 24.08436884 23.8646985  22.76638161\n",
            " 39.72943311  9.91687054 10.14449835 47.6596611  10.20688657 16.48590961\n",
            " 40.22484382 17.42610363 29.0146272  22.78689031  5.41305583 25.69844138\n",
            " 13.96055832 39.91264619 16.66362706 45.58321679 29.89921882 35.0846493\n",
            " 27.72014618  5.85154559 36.27674938 20.7321079  47.63091036 19.19065408\n",
            " 10.65466057 22.63774079 27.8029428  14.97757464 15.07151945 18.39919763\n",
            " 25.34642715 37.06723157 20.916821   25.34164664 40.41093555 36.16042052\n",
            " 25.7778019  26.32505574 16.64457692 13.14733329 46.92529846 22.38998188\n",
            " 22.54310672 26.15687127 20.07829909 18.53159117  7.99418994 31.00866809\n",
            " 25.66752918 30.75232161  7.43344604 26.46789987 18.45504802 23.54206183\n",
            " 16.06646913 18.51006469 24.13800403  6.14639317  7.03847328 40.48659845\n",
            " 22.11049979 12.21544055 12.76198329 23.52295777 10.29281719 18.99649668\n",
            " 31.27918163 27.56221912 13.7063028  24.91928449 11.6609599   9.80118709\n",
            " 40.24034095 28.36194045 22.37832775 14.58722012 18.42768838 22.42515727\n",
            " 20.38724255 23.44375062 11.54119299 11.50773519  7.54851227 14.50510489\n",
            " 28.35096206  8.89258101 26.00966983 39.42550307 18.34884603 35.05996027\n",
            " 25.80217048 23.90666757 20.44115869 31.81867122 28.09092224 35.4069129\n",
            " 31.89685672  7.42344436 39.20982085 25.98570745 26.89426778 10.87575988\n",
            " 30.2793136  34.54106596  9.50414833 28.50064163 18.06615295 48.05542547\n",
            "  9.39875984 17.77208415 28.5452856  33.32003703  5.27392605 17.88130094\n",
            " 24.72365793 19.19758343 11.15573233 27.19710177 26.37595557 23.38594171\n",
            " 47.70623128 15.02005151 25.57882289 16.87019723 29.54710593  9.66844469\n",
            " 35.84526386  5.54882353 48.16140857 12.70930179 25.02984613 44.17376101\n",
            " 19.33187868  9.50136774 15.85026635 20.59583361 23.27014571 17.8474358\n",
            " 22.73091097 16.2575435  15.54094699 15.97555802 31.2301418  12.10322419\n",
            "  9.3823177  49.99999705 38.99994402 15.97274735 25.38496777 41.04485452\n",
            " 15.20576463 11.10397343 49.18606758 24.69646655]\n",
            "selection [501 297 447 341 785 910 808 404 925 658  42 355 817 786  95 241 855 164\n",
            " 480 249 635 693 702 130 154 345 685 516 565 214 562 470 856 302  61 689\n",
            " 192 895 848  65 576 880 677 538 385 333 275 630 608 535] (50,) [4.32301672 4.36194058 4.57049319 4.65884144 5.08103913 5.27392605\n",
            " 5.41305583 5.53536753 5.54882353 5.55364174 5.71063815 5.7397049\n",
            " 5.85154559 6.04719766 6.05498279 6.12611407 6.14639317 6.26544705\n",
            " 6.29298639 6.41626623 6.43248409 6.47017979 6.4819222  6.56793048\n",
            " 6.66976548 6.69086664 6.74540715 6.75365188 6.78518162 6.84516872\n",
            " 6.95167943 6.95613463 7.03847328 7.1212963  7.22199955 7.24684748\n",
            " 7.25562158 7.42344436 7.43344604 7.46196018 7.51571914 7.54851227\n",
            " 7.57673659 7.58341555 7.68890368 7.71911796 7.85421811 7.89382132\n",
            " 7.91427066 7.98311068]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [185 215] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.64      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 0 0 0]\n",
            "std (902,) [39.04427689 18.12008139 18.11415849 14.66251312 19.90357518 23.4057112\n",
            "  7.51832551 10.12534682 14.1061147  28.45999736 45.73276667 45.54923064\n",
            " 42.2559748  27.58679306 18.95517658 20.72011541 16.85789042 13.56403971\n",
            " 37.21029761 11.54663925 43.72816462 19.31183963 11.82964987 33.97833326\n",
            " 21.51706504 33.21438995 18.46660787 24.11166077 27.87637546 33.60501589\n",
            " 18.546422    8.3776909  26.77971393 41.978359   10.48037399 13.12523596\n",
            " 16.17875717 16.11102399 20.08746473 22.66727509 14.16046921 31.21979013\n",
            "  9.8903197  19.75384929 21.37453555  9.15498583 27.02272019  9.35028184\n",
            " 45.98936752 28.87495908 13.05075964 25.23062644 12.39886549 16.56615109\n",
            " 18.79701686 15.06759663 16.53568846 40.89238228 22.08453895 43.82947745\n",
            " 13.24685543 28.77177282 10.92115524 28.75893048 30.52521403 23.0254101\n",
            "  7.17145238 11.06453665 20.91382019 25.61320278 27.55742003 22.28696805\n",
            " 12.22230521 22.15698542 49.81815035 14.92108056 37.16688613 29.9265439\n",
            " 14.19784489 15.16653421 25.71518514 19.58244072 25.17309054 28.90232255\n",
            " 18.24893619 19.80903076 21.01551184 35.81492365 35.3179727  12.7589148\n",
            " 32.31290748 28.72058601 35.68588954 11.46895477 24.43517912 49.95125768\n",
            " 37.64381065 37.27891444 29.81332895 19.72249935 27.23254513 19.227794\n",
            " 24.49809002 22.88117309 37.01963124 29.34284124 27.1968495  15.52554906\n",
            " 18.2476402  16.03820484 29.01312775 16.10102676  7.46997182 28.01812493\n",
            " 44.12972598 24.2149656  10.90289802 39.62646439  3.59935729  9.81569168\n",
            " 13.15811932 27.56149617 22.10138352 16.33065126 26.00239771 49.72994285\n",
            "  7.90397139 27.73063118 13.98144253 17.90657638 12.62290738 11.1822649\n",
            " 49.99588538 20.10249968 30.75603517 29.08145856 19.77425568 25.1703146\n",
            " 23.28290434 31.77000254 16.75933876  7.64222181 13.8955155  26.76289352\n",
            "  8.4814678  14.41090719 18.79042155 28.99695043 17.66173632 19.22606935\n",
            " 32.31013806 18.09263274 15.66730428 18.1704388   9.30054769 10.40462815\n",
            " 12.99563755 26.46889052 28.44228047 22.25402377 16.25414746 14.1467711\n",
            " 13.80894652 45.7740682  17.33364717 21.22072051 21.37063063  8.89670151\n",
            " 26.60719287 13.63161437 35.77339255 11.42711153 25.40222664 18.41615714\n",
            " 49.31512776 19.59474078 25.94196172 18.8390319  20.71777663 26.44094022\n",
            " 30.78181835 37.8878193  44.80791084 25.93579798 14.59719068 13.47232318\n",
            " 28.598248   48.24733333 35.84909013 15.66178623 30.08310074 14.41236459\n",
            " 17.15030915 33.64440499 18.47995447  9.01353063 26.3289115  34.48042695\n",
            " 12.83650077 12.17061682 43.5153188  13.29758403 28.75765267 30.41263657\n",
            " 36.68503059 14.68531538 23.53612212 30.18475517 10.65376834 22.64737186\n",
            " 10.74597616 17.6702352  35.9049747  11.24674308 25.70118048 18.23402589\n",
            " 33.17537084 13.4170254  23.85287172 30.47096764 14.06715846 17.80543269\n",
            " 19.92977224 25.90635252 16.36155504 20.71566069 21.47761958 30.46553101\n",
            " 26.04449882 32.39431531 24.46172101 26.60581966 48.4436686  14.18298543\n",
            "  8.18175572 12.6913079  29.63749138 12.05257822 25.9753366  17.75357945\n",
            " 38.07378028 30.45010833 30.88365765 13.31688728 15.6995209  41.76028507\n",
            " 34.69147083 28.91210396 23.43267536 49.67328101 15.71470465 29.19802581\n",
            " 27.66536778 13.33283755 24.36655454 23.31398079 19.84836854 49.97148979\n",
            " 39.00161932 27.16794243 40.86118848 25.33327636 31.25862799 40.07622898\n",
            "  6.0053563  25.09246401 46.91281342 17.66156204 14.74829376 22.21242894\n",
            " 18.62290203 27.87035888 18.71974918 19.48591617 20.03979679 10.35768966\n",
            " 42.30845551 49.43877304 20.15240571 16.64849082 21.91067438 15.82972308\n",
            " 15.22194528 17.22430651 19.74058682 31.89207429 17.53251604 20.3407396\n",
            " 21.62158709 28.88080299 27.4229512  21.8080883  25.61320278 31.77636068\n",
            " 13.11024078 14.10597964 27.30524681 22.80746438 19.42067756  9.46366439\n",
            " 25.40695727 24.57434787 19.00180818  7.95935001  9.89960596 20.18652303\n",
            " 31.52563786 26.14210719 29.09431441  9.62999276  6.18586438 27.0489505\n",
            " 19.93708713  7.43117608 38.20135471 18.2487276  29.73419508 18.5365942\n",
            " 16.94452865  9.90852364 27.26159476 23.61449785 27.29850875 18.28043497\n",
            " 21.73398888 15.3011807  11.73938476 27.91450121 16.863088   15.83173651\n",
            " 20.35332379 36.27125872 18.50201291 14.69880984 17.20218234 14.88634136\n",
            " 21.72537346 21.17905825 24.76777797  7.40121077 15.35997281 27.1096036\n",
            " 28.68535269 26.61866119 10.51622732 14.91784098 34.56219703 17.08185788\n",
            " 19.71815188 24.09607589 17.76470688 49.84441003 21.06252107  6.8964429\n",
            " 18.70714839 20.32690267 15.19337602  7.60844347 29.33209097 47.59031394\n",
            " 24.71682214 40.10976862 24.3516926  28.11131604 17.31374174 20.91413129\n",
            " 25.64023815 25.36984753 23.37260699 25.17012893 25.33366905 24.70073951\n",
            " 25.07499715 17.21196687 15.94730075 11.80344042 13.54561093  9.03758263\n",
            " 22.45208238 21.24454169 31.4652081  13.82578472 13.90823122 47.61518552\n",
            " 30.54005432 27.35356119 24.58631593 24.00719226 18.6412424  24.29809001\n",
            "  9.54171819  6.76230897 38.50742729 10.60945159 24.41625687 19.50979332\n",
            " 20.88249611 14.99397982 32.6629261  32.90401509 20.97954637 20.22921346\n",
            " 36.60426013 12.14764039  9.805046   18.23860626 47.44307911 33.27858658\n",
            " 36.20900212 18.14347922 36.67357352 39.79647389 10.87138771  9.54171819\n",
            " 26.97730182 21.70182763 12.09919247 35.95519085 18.09446299 28.16419674\n",
            " 29.23489317  9.64472884 22.63146988 37.27148257 23.29427932 21.53410544\n",
            " 26.42567141 11.29325482 12.17622093 31.92411286 47.61433121  9.32673143\n",
            " 16.27491604 10.0627767  26.17888    23.16860927 31.53184431 17.22133331\n",
            " 43.67490403 15.49203586 12.33854612  8.85194891 13.34985869  6.52454649\n",
            " 25.54315114 15.21887695 29.88017603 18.15606045 48.5944957  42.57801613\n",
            " 10.05901183 29.61583428 16.96091779 13.09825494 16.25770347 20.78383398\n",
            " 17.94882541 12.05552303 40.13602403  9.74956138 34.96963239 21.2262423\n",
            " 31.43024065 19.21058258 26.64968752 14.37688285 12.57084102 22.72156739\n",
            "  9.62193946 17.78570397 15.61340922 15.31399143 16.4894263  20.10487809\n",
            " 16.30428814 27.22387468 33.59522526 26.2705949   9.16862609 20.96744892\n",
            " 40.89035441 31.86782264 30.89298852 30.14741341 28.3723036  19.93131641\n",
            " 10.19022076 17.15953865 35.60470503 33.07615485 20.82655207 15.77858726\n",
            " 39.73142274 23.31498309 19.06215741  7.89374442 23.18590109 25.87995169\n",
            " 30.72330978 31.74336253 20.41333886 38.32120574 21.29062335 11.73938476\n",
            " 19.64211595 12.49566715 24.60555066 24.1538245  34.01319622 47.82791402\n",
            " 21.96968476 16.30428814 13.5159313  12.69344979 30.10521641 22.14100821\n",
            " 37.41308356 49.98821728 21.80669039  9.77354769  9.62291726 39.21464429\n",
            " 30.31800594 10.72294123 12.66511031 34.61294417  9.66123081 13.56363277\n",
            " 37.25545604 18.70034542 18.73937874 10.95162804 37.37987419 28.50134387\n",
            " 16.31827221 13.68809181 36.23293162 33.72950174 22.01061735 27.82695974\n",
            " 23.88215648 30.15489635 21.59144882 12.20144895 14.25371752 19.90418461\n",
            " 25.03743098 16.17356222 21.7747763  39.86703933 29.25540182 35.21433845\n",
            " 19.42038285 31.37777214 10.35542385 49.99129994 34.90175735 11.80169748\n",
            "  9.47949856 32.30606035 18.01190039 18.18769936 36.42994782 18.32261486\n",
            " 20.22704888 13.20458277 18.29405817 36.07577943 19.04870862 14.05038582\n",
            " 47.47316223 14.49427363 40.34559594 21.24170663  3.83004084 25.28026439\n",
            " 20.91087027 22.82412395 15.97109744 27.36663632  8.28248563 19.20391616\n",
            " 29.01402296 10.14859065 12.32845424 28.39982004 14.92519984 29.66662124\n",
            " 15.72319041 33.76022484 18.35320528 34.3799061  13.50213476 14.28458432\n",
            " 29.58711873 16.81929899 15.11579382 41.74618414 22.0524993  46.93350438\n",
            " 29.87481549 14.12495235 29.17510708 19.74058682 49.67217309 23.0278223\n",
            " 18.76560111 23.98480932 41.08592102  5.50358489 27.09287164 33.25199359\n",
            " 35.46691295 17.79777131 44.7025717  19.60211049 17.04416509 33.79737217\n",
            " 47.48231819 26.81603495 17.28969427 12.49994867 30.47297382 16.23451996\n",
            " 32.11323171 37.66715965 27.52302085 36.47219288 39.16833487 45.82394742\n",
            " 11.08105326 12.09056994 25.62047223 24.13149796 18.98259381 19.97993409\n",
            " 14.74605517 13.01547592 43.97818293 17.77164162 17.39541554 31.40238597\n",
            " 17.31894559  5.09866703 14.90532338 21.98051253 29.79999034 22.73506412\n",
            " 11.75453611 35.60594009 33.19004764 35.45236002 30.09643874 12.95361526\n",
            " 37.56847135 20.33566367 15.03295317 11.00245803  9.84384075 49.96542248\n",
            " 46.09866693 33.48977002 24.07068695 47.33262831 29.38702247 19.42944167\n",
            " 19.65508314 19.43162283 17.08880538 34.18771001 28.31487471 26.70382474\n",
            " 24.81943145 11.77219912 23.20299927 10.01935855 23.01310502 24.15934908\n",
            " 19.35848668 15.02650132 13.01264581 27.836466    8.80564442 14.79971956\n",
            "  8.28922763 18.82896475 18.64597461 32.49101586 17.71169695 30.64495913\n",
            " 49.85386147 10.57064168 26.18890466 21.48477258 14.29427059 37.10356724\n",
            " 20.51106732 12.8594055  39.20766876 26.62762042 43.1260521  30.02555607\n",
            " 40.17344155 10.89265945 11.98702181  9.73635708 15.0531085  21.91884495\n",
            " 32.39477916 24.4605208  34.92362303 23.77740125 20.15191939 48.54476612\n",
            " 18.0133001   7.71464891 14.72139857 27.86756053  9.04068348 24.09350429\n",
            " 19.0338453   8.67976345 24.08040962 16.2256992  33.39273096 16.94358051\n",
            " 22.72156739 36.51841766 37.46498279 26.02549421 27.77198573 16.75031275\n",
            "  9.97134463 28.84371801 11.75169874 34.44571234 21.861895   24.4935553\n",
            " 32.1807878  38.84163533 46.81974648  9.57888124 20.70524213  7.54205443\n",
            " 29.6084168  11.20664941  8.91541468 48.74635783 16.89753735  9.83592027\n",
            " 12.90446518 26.45352751 44.03657436 34.25893988 24.28174138 23.11870327\n",
            " 21.72312336 38.8450538   8.92879633  9.233668   48.56930608 13.47427974\n",
            " 22.1237305  44.12584062 17.24169688 30.89230513 25.30847853 30.18475517\n",
            " 13.91907962 42.12371939 13.14422491 43.43312224 28.76301935 35.00598399\n",
            " 26.67052202 37.90810815 23.4465242  48.70794574 19.38228414 11.79314994\n",
            " 22.76533142 26.35379535 14.37670802 14.22016081 18.23645877 26.4950646\n",
            " 34.88992114 20.29576694 26.92735441 42.07625115 35.43146471 23.4537366\n",
            " 30.84528341 16.39275357 16.32088095 45.99737069 23.3446164  20.37740999\n",
            " 26.14563473 13.14833261 18.98259381  6.2675175  29.57747768 23.26460863\n",
            " 30.12353881 28.47265501 17.6702352  23.31498309 16.57907303 19.07176657\n",
            " 24.30445512 38.73424601 17.79393127  8.37469459 14.19784489 19.6944744\n",
            "  8.60522417 16.96614873 30.82750247 26.06969762 13.38303469 20.87954361\n",
            " 12.93620116  8.13969123 40.72934027 26.11946911 21.53484651 16.7278085\n",
            " 18.33923672 21.70158308 19.37336421 20.29081654  8.25974436 11.34650405\n",
            " 15.8029042  26.44601214  8.47984194 27.83384984 37.77703506 18.89312598\n",
            " 31.72694646 24.4579023  24.96099797 22.44618662 28.639372   29.68502542\n",
            " 35.11609693 29.28358206 37.38364413 23.3910541  26.07353923 11.99147274\n",
            " 30.74166087 31.67742703  4.69103487 27.00610308 18.93672217 49.13169773\n",
            " 10.54317911 20.30643217 28.09110867 32.93287577 17.13523423 23.98185456\n",
            " 18.9649462  11.17690371 31.76936574 31.08882208 17.6728872  48.71931539\n",
            " 16.24732268 24.2839225  14.772252   30.20044844  8.09319003 32.82644848\n",
            " 49.15350559 14.25889454 24.15689076 46.08453777 18.37403314  7.91727597\n",
            " 18.93405465 14.83243643 23.04934504 17.2998662  24.12406043 14.47912396\n",
            " 16.87100642 14.84192775 28.80038965 10.02238552  9.30680835 49.99998039\n",
            " 40.27783035 18.46905382 24.85162469 42.31040124 15.59937095 10.57165521\n",
            " 49.64079825 21.58103294]\n",
            "selection [118 574 854 643 609 264 310 801 443 391 353  66 339 313 112   6 743 357\n",
            " 141 715 495 126 881 303 874 823 234 832 580 684 813  31 836 144 816 721\n",
            " 682 441 167 746 758 195 377 718  45 478 759 154 892 431] (50,) [3.59935729 3.83004084 4.69103487 5.09866703 5.50358489 6.0053563\n",
            " 6.18586438 6.2675175  6.52454649 6.76230897 6.8964429  7.17145238\n",
            " 7.40121077 7.43117608 7.46997182 7.51832551 7.54205443 7.60844347\n",
            " 7.64222181 7.71464891 7.89374442 7.90397139 7.91727597 7.95935001\n",
            " 8.09319003 8.13969123 8.18175572 8.25974436 8.28248563 8.28922763\n",
            " 8.37469459 8.3776909  8.47984194 8.4814678  8.60522417 8.67976345\n",
            " 8.80564442 8.85194891 8.89670151 8.91541468 8.92879633 9.01353063\n",
            " 9.03758263 9.04068348 9.15498583 9.16862609 9.233668   9.30054769\n",
            " 9.30680835 9.32673143]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [207 243] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0]\n",
            "std (852,) [42.14319955 19.17451669 19.54780731 12.78616463 24.00809619 25.52544687\n",
            " 11.24650197 14.99008272 34.51303185 47.42156685 47.31577848 42.31728261\n",
            " 29.73198761 20.00184597 23.14625206 18.30084716 15.61478813 39.59068872\n",
            " 14.5185064  45.92984209 19.49740512 12.65976606 35.07459742 22.55996738\n",
            " 33.30311571 18.8708359  25.80618914 29.84195185 36.09215578 19.00239932\n",
            " 29.02359527 43.94759454  9.9408264  13.1573457  18.12111708 18.54442577\n",
            " 22.74964879 24.3962203  13.61785992 30.59849418  9.93153748 22.09808903\n",
            " 22.50456748 28.65344223  9.2726567  47.29091845 31.8233268  14.4513491\n",
            " 25.04524959 10.32938937 17.348557   20.53130407 16.30977571 16.42899825\n",
            " 41.9492371  22.04636239 45.84867355 14.45832374 29.52108491 13.13217631\n",
            " 30.60931715 33.08779138 24.22826565 11.09278976 23.2654653  28.36225086\n",
            " 28.90015355 18.3517909  12.51332463 23.7197095  49.94097028 14.54626605\n",
            " 40.35034808 30.79340508 16.90787227 15.44189577 28.4636485  18.84552634\n",
            " 26.80299618 31.00667033 17.47972674 18.67783847 23.16630419 38.64898829\n",
            " 38.04795021 12.82843757 34.93625569 33.31885519 38.13671013 10.49224799\n",
            " 25.34261759 49.98623435 41.43674244 37.85216598 30.51212822 20.37249718\n",
            " 27.50222653 18.59858227 25.5018996  24.15942895 38.4504311  31.67535508\n",
            " 30.31131073 16.2958454  23.18557199 16.31487661 31.36248435 17.55231305\n",
            " 28.21371157 45.90008778 25.14846655 12.25442309 41.93457162 10.65340749\n",
            " 18.33432938 30.51261646 22.76204764 18.0733     29.57741971 49.87520451\n",
            " 29.97046822 15.77455289 19.09151809 12.26977024 12.22186724 49.99946017\n",
            " 19.39197092 35.0820723  31.01242756 21.21434011 28.66542993 22.59529774\n",
            " 34.36379851 18.19268625 15.50433693 28.13886517 16.64850124 19.83673263\n",
            " 31.28741567 19.82747241 18.4565918  34.57924531 18.52136397 16.22792303\n",
            " 19.82940921 11.77477224 13.85463957 29.33629362 31.19948402 22.85461606\n",
            " 15.99156629 12.78515242 15.02753325 46.05252557 17.26664827 23.44876378\n",
            " 23.35300385 28.0641943  16.73106429 37.86055308 11.72565637 29.27855349\n",
            " 20.09010955 49.52995939 19.08537312 26.68827631 21.12326291 20.27632542\n",
            " 28.31365408 33.21498427 40.46061364 46.65414434 26.87285844 16.23009975\n",
            " 14.59630162 30.46864135 48.81150519 37.77635883 16.31059568 32.27649052\n",
            " 15.83721203 17.32208205 37.15810477 20.93229655 28.22354339 39.5474264\n",
            " 14.69403602 13.08005893 45.28228362 14.34175388 31.56083681 30.58342003\n",
            " 39.16109823 14.68399895 24.48908302 32.56721998 11.5986725  20.22722966\n",
            " 11.25327894 17.96824115 38.10917435 14.9169528  26.60746286 18.75587875\n",
            " 34.00853605 13.14698407 25.06015674 32.90991678 15.32433749 18.37030541\n",
            " 20.82047746 27.4114129  15.5894109  21.23177224 23.26321164 32.63272601\n",
            " 26.49673065 33.54886743 27.74854314 27.57111361 49.1971056  15.92109496\n",
            " 12.65811398 30.82765815 12.66274357 26.97640373 18.38689681 37.63824581\n",
            " 32.11982865 33.5884376  14.61395555 17.03489221 44.24699646 33.81358137\n",
            " 30.9074873  24.98674939 49.89679308 17.51713352 29.68030098 27.90640939\n",
            " 15.30209981 26.5237036  24.85253185 23.97521832 49.99476798 41.87777245\n",
            " 31.06788491 43.93507045 27.34036887 32.00915543 41.67337802 28.5991941\n",
            " 48.23794068 16.42803665 17.02953463 23.59786468 21.10085435 30.84864394\n",
            " 19.13685098 21.12994176 22.26132442  9.99266123 43.89965562 49.73731185\n",
            " 21.61914204 17.87496963 23.25559208 17.04883555 16.32783139 18.96164606\n",
            " 20.99461077 33.66491118 16.53583665 18.2524747  21.07056972 32.31771975\n",
            " 28.24402166 22.11072951 28.36225086 33.02569693 14.45775417 16.07415418\n",
            " 29.12600966 23.92004482 24.17585512 10.19695347 27.17893279 27.61242388\n",
            " 20.46665032 10.38380959 21.98310758 35.05185753 28.85567116 30.21337034\n",
            " 11.00183845 28.95954667 20.53283085 39.03770104 19.28113249 31.8936121\n",
            " 20.20338158 16.6937337  10.30519439 29.43881386 24.14247933 28.78483511\n",
            " 19.74000657 26.2786392  16.36078178 13.00073221 30.5847381  18.41278525\n",
            " 14.99032298 23.53280542 37.79132931 19.25126735 20.31470486 18.86707921\n",
            " 15.23185207 22.59580239 22.7751967  26.26178328 15.78991624 28.77767142\n",
            " 28.43892503 28.55789876 12.39753965 15.15786356 35.55403591 16.0069035\n",
            " 21.66078147 25.87224544 19.02669072 49.94891612 22.49762544 20.59905602\n",
            " 21.61217902 15.9863704  33.67432861 48.42064966 24.51776344 42.93264325\n",
            " 25.49973804 29.52584377 19.3232121  22.9328549  25.47010347 27.32361258\n",
            " 24.06427342 23.07528707 26.22778352 27.42907455 21.56497813 18.73923444\n",
            " 15.94794683 12.67618237 16.69132327 23.13287628 22.37947692 33.08160271\n",
            " 11.87082219 14.51263482 48.55695573 31.62266678 28.51732865 24.37434289\n",
            " 25.46474892 19.53597942 25.65180412 10.48439889 40.62651984  8.44441225\n",
            " 26.54325    21.02188663 22.35528572 16.15098995 32.39009258 35.71567296\n",
            " 21.84127628 21.40116618 39.33197633 13.27349021  9.79295129 20.77637063\n",
            " 48.52271047 34.36457819 39.24634004 16.09711549 37.17742189 42.16466502\n",
            " 10.4194797  10.48439889 26.45366569 22.37254342 14.07605611 37.2915759\n",
            " 19.53947577 28.85542243 30.75512769  9.00836901 25.01543477 40.68197115\n",
            " 24.77852326 23.32178346 27.07370288 12.99498035 13.48528317 35.12183519\n",
            " 48.56672454 18.6413907  11.44782136 27.00407383 24.26148588 32.88662151\n",
            " 17.75908199 46.09046843 14.78259819 13.41324788 14.59422697 28.44648622\n",
            " 16.74625585 32.98103509 17.97214016 49.33931481 44.51576508 11.58976569\n",
            " 31.67839116 18.60979023 12.95626553 19.86086036 23.8238066  19.67450402\n",
            " 12.62302867 42.58635771 11.11114703 37.12197473 23.80650864 34.22736953\n",
            " 17.24490653 27.72989996 17.34657844 13.01737362 23.54537029  9.89082687\n",
            " 19.70106412 16.09774123 17.95111727 18.76043399 21.24415481 18.26320669\n",
            " 29.82644486 36.35948987 27.04749084 21.55707802 41.93358683 33.97039623\n",
            " 32.14878509 31.15594364 28.93410183 19.66246391 10.27366782 19.01551568\n",
            " 35.9392994  29.61347604 23.10458186 18.38595482 42.50456991 24.04524873\n",
            " 21.6396528  26.77311458 23.86702775 33.5855892  33.21991761 20.94119923\n",
            " 41.41934319 18.79494218 13.00073221 19.19228376 14.09547894 25.16545597\n",
            " 26.65444282 35.84504104 48.33712242 23.48280216 18.26320669 15.44202828\n",
            " 12.74126754 29.79955339 22.12561212 37.88818674 49.99831199 23.71696107\n",
            "  9.55130854  6.95125558 42.27048456 30.17607395 11.37912525 13.59081416\n",
            " 34.37421584  9.48599876 14.55661821 37.27326306 20.1896788  20.75434772\n",
            " 12.89776679 40.22546116 30.99217537 16.38617393 16.17751921 40.24932382\n",
            " 34.1274766  21.22522441 29.07137469 27.37866552 31.13582035 23.76134672\n",
            " 12.92978501 14.45616132 23.15729922 26.92503852 17.35851411 23.94255868\n",
            " 41.07128206 31.63074166 37.52543885 21.24023482 34.42421065 11.18460325\n",
            " 49.99860046 38.1722986  14.08203225 10.77317955 36.38242402 18.5072509\n",
            " 20.09375809 37.40946596 19.30175137 22.37339771 15.1801284  18.63976973\n",
            " 39.0456953  19.73781044 14.12324222 48.99092155 14.0924222  42.51390208\n",
            " 23.26524681 25.38757731 22.69300777 26.94610149 17.55432222 30.05326617\n",
            " 19.89190037 31.53628771 10.89844877 12.52495264 30.17407391 15.291381\n",
            " 30.55574522 17.58022242 34.74188968 18.75092165 36.19367745 16.34863118\n",
            " 15.76299915 31.25125342 17.44043174 14.55929207 42.08796335 23.55761967\n",
            " 48.31053025 27.8320079  13.81103426 30.01453954 20.99461077 49.87552527\n",
            " 25.53422959 19.78512249 24.54552063 41.71660058 29.58080997 36.57690015\n",
            " 36.43218612 17.50032033 46.89397554 20.63870914 21.26420543 35.67648814\n",
            " 48.64564481 28.59162505 18.2720711  11.04967639 34.35332197 16.52256246\n",
            " 36.81358562 39.55986508 27.51549937 39.50336229 39.42816954 47.7078621\n",
            " 10.15424432 11.61119423 29.22284621 26.17397064 18.66445803 22.1051791\n",
            " 15.18959078 14.46846371 45.53667123 17.2491703  18.09391882 33.89330238\n",
            " 17.23095066 16.37879276 25.03115472 33.03518216 24.72639459  8.70662049\n",
            " 38.28400134 36.23124255 40.2678304  31.67048689 13.82948789 38.98261781\n",
            " 20.09311635 16.78795608 11.19651953  9.63752783 49.99172751 47.37282086\n",
            " 35.95665503 23.32218169 48.41474051 30.31781084 20.46027479 21.0613566\n",
            " 20.3656643  16.97869148 35.6365574  29.786361   27.12397246 26.08409836\n",
            " 11.03443262 24.53641239 11.5523563  25.97063887 26.27292487 19.48374342\n",
            " 13.74206284 14.14165274 28.06969739 15.58132469 22.51638025 19.82580917\n",
            " 33.91821697 19.24358721 33.58343888 49.94669877 11.45387368 26.96245251\n",
            " 21.45106695 17.6359247  36.06227748 23.04770093 10.8242258  40.36398093\n",
            " 28.03200462 45.44700554 31.48648026 42.96097731 11.4774764  10.34675787\n",
            " 12.12543082 16.81299197 21.47592798 32.68235709 25.12420885 37.85584524\n",
            " 24.90902228 21.70533648 49.2167187  18.79375873 17.07748741 30.86377688\n",
            " 25.71467227 19.80640908 27.54531455 16.34707113 34.71116471 21.27296457\n",
            " 23.54537029 38.18222955 38.20077534 25.92888129 30.13664266 17.36373866\n",
            "  9.24304355 31.53354017 13.30602378 36.23156759 24.35600823 24.21874966\n",
            " 36.27140226 42.46560986 48.03300321  9.63094911 16.38337274 30.69128473\n",
            "  9.81867977 49.39657406 18.9430803   9.33842199 17.82053133 28.22462068\n",
            " 45.41050492 35.05648905 23.75311365 23.33531555 22.65179047 39.63835663\n",
            " 49.2959414  16.23772946 25.11413163 46.15540866 18.18874191 35.20416998\n",
            " 27.60042482 32.56721998 14.65447885 44.22494989 12.87224889 43.86911156\n",
            " 29.02356176 36.79449659 25.82529602 40.3272397  25.78091993 49.32402624\n",
            " 19.78165212 13.77795925 24.34575081 26.71542085 15.09855166 13.87175143\n",
            " 19.91773286 28.91736637 34.96107531 20.93301838 29.81081183 44.46750945\n",
            " 36.81200736 24.11631634 34.31261154 12.67018653 19.3734     46.47326992\n",
            " 25.72548796 19.92183193 29.52236235  8.07889108 18.66445803 30.58474907\n",
            " 23.82222431 31.67026172 30.93656825 17.96824115 24.04524873 18.74907142\n",
            " 20.60888848 27.25224714 39.4298133  16.96088546 16.90787227 20.20840985\n",
            " 18.23375837 33.62182365 26.99214126 14.94132692 24.06517798 13.66973194\n",
            " 43.10090123 27.16959879 23.20101063 19.78467376 20.06942529 20.20384984\n",
            " 19.56404333 21.17767675 12.86896836 17.45936348 27.18886898 30.5329905\n",
            " 38.9704378  20.52473357 31.94695006 26.02648212 26.2403094  24.68622218\n",
            " 29.04969973 31.42242618 37.06286376 30.2357936  38.82982237 24.55892454\n",
            " 28.28201367 12.89339917 33.90125988 32.48545945 27.30445731 20.26357263\n",
            " 49.54893094 11.34947646 23.06901293 28.41473382 35.65122984 15.48131204\n",
            " 24.58420146 19.92257916 13.4688857  33.54982741 34.01792893 14.41989906\n",
            " 49.45602387 16.64434599 26.2576963  16.18931116 32.59787424 33.34160672\n",
            " 49.57395189 16.70264662 25.09466854 47.55436679 20.13113143 21.67141322\n",
            " 13.92321891 24.42946277 19.84659533 25.46548433 15.67823984 18.60205599\n",
            " 13.93625307 26.99925904 11.78245459 49.99999067 43.02004927 20.93491975\n",
            " 25.23937054 44.82086568 16.37034115 11.25791166 49.83950676 20.25093136]\n",
            "selection [493 759 371 617 399 696  44 711 499 492 705 627 382 708 443  40  32 261\n",
            " 600 285 460 302  49 671 289 390 369 391  89 113 531 664 554 294 642 591\n",
            "  63 434 527 626   6 198 849 811 496 410 658 670 644 425] (50,) [ 6.95125558  8.07889108  8.44441225  8.70662049  9.00836901  9.24304355\n",
            "  9.2726567   9.33842199  9.48599876  9.55130854  9.63094911  9.63752783\n",
            "  9.79295129  9.81867977  9.89082687  9.93153748  9.9408264   9.99266123\n",
            " 10.15424432 10.19695347 10.27366782 10.30519439 10.32938937 10.34675787\n",
            " 10.38380959 10.4194797  10.48439889 10.48439889 10.49224799 10.65340749\n",
            " 10.77317955 10.8242258  10.89844877 11.00183845 11.03443262 11.04967639\n",
            " 11.09278976 11.11114703 11.18460325 11.19651953 11.24650197 11.25327894\n",
            " 11.25791166 11.34947646 11.37912525 11.44782136 11.45387368 11.4774764\n",
            " 11.5523563  11.58976569]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [229 271] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.64976958525345, 78.57142857142857, 78.11059907834101, 78.80184331797236, 79.72350230414746, 80.4147465437788, 79.72350230414746, 79.49308755760369, 79.95391705069125, 79.72350230414746]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-18.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 19, using model = LogModel, selection_function = MinStdSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 10) (25,) unique(labels): [13 12] [0 1]\n",
            "Val set: (1277, 10) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 10)\n",
            "Validation set: (1277, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 67.511521 \n",
            "Classification report for LogisticRegression(C=2.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.71      0.76       321\n",
            "           1       0.41      0.58      0.48       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.62      0.64      0.62       434\n",
            "weighted avg       0.72      0.68      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[228  93]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 1 1]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 1 1]\n",
            "std (1277,) [15.57429005 26.83501805 17.33197715 ... 49.99972755 13.59645922\n",
            " 25.04588007]\n",
            "selection [ 320 1129  501  730 1198  238  995 1156  705 1265 1018  231  950  650\n",
            " 1147  364 1252  295 1022  792 1076   25 1172  476  123] (25,) [0.17008401 0.18337276 0.45023367 0.57773903 0.64692693 0.72196478\n",
            " 0.72262925 0.73216561 0.87090279 0.88278625 1.04224593 1.04524886\n",
            " 1.11207803 1.12157651 1.19038926 1.21309873 1.38981002 1.55588253\n",
            " 1.55588253 1.5910176  1.6138593  1.61545551 1.71330637 1.75451181\n",
            " 1.78856525]\n",
            "trainset before adding uncertain samples (25, 10) (25,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [24 26] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 74.654378 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.82       321\n",
            "           1       0.51      0.58      0.54       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.69      0.68       434\n",
            "weighted avg       0.76      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[259  62]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1252,) [ 9.656557   40.52775952 16.89470313 ... 49.64002211 34.88883658\n",
            " 14.99133229]\n",
            "selection [ 976  217 1221  299 1201  397  385  477 1090  921  751  138  739  475\n",
            "  855  868  803   32  238   74  540  254  168  723  301] (25,) [0.02166933 0.05800857 0.15133644 0.19996935 0.31612997 0.34931016\n",
            " 0.38633224 0.45918067 0.47626269 0.49741861 0.55960581 0.57943635\n",
            " 0.92246582 0.99161782 1.19519294 1.23624857 1.27854254 1.42935837\n",
            " 1.4992483  1.54232304 1.68919279 1.79685971 1.82980556 1.83266638\n",
            " 2.16138483]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (75, 10) (75,)\n",
            "updated train set: (75, 10) (75,) unique(labels): [36 39] [0 1]\n",
            "val set: (1227, 10) (1227,)\n",
            "\n",
            "Train set: (75, 10)\n",
            "Validation set: (1227, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 76.958525 \n",
            "Classification report for LogisticRegression(C=0.6666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1227,) [38.07231769 41.82504929 28.51051371 ... 49.99789903 43.20737302\n",
            " 13.78498432]\n",
            "selection [ 682 1119 1095 1086  480  896  414  580  175  803  489   26  511  402\n",
            "  894 1034  634  409 1030  639  860 1078  846  610  719] (25,) [0.03961682 0.08564168 0.0858427  0.12769407 0.23427875 0.23829778\n",
            " 0.40423304 0.51188654 0.66911888 0.68553942 0.69355219 0.81448201\n",
            " 0.97696044 1.05592263 1.15393925 1.19644933 1.21240308 1.2760381\n",
            " 1.37547397 1.45496581 1.47508442 1.54730409 1.94391163 2.01362117\n",
            " 2.0221565 ]\n",
            "trainset before adding uncertain samples (75, 10) (75,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [47 53] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.419355 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1202,) [40.62502509 41.55955459 29.6309823  ... 49.98847923 40.40248082\n",
            " 20.14841954]\n",
            "selection [ 210  769  322  441  999  377 1081  173  899  265  127  878  309  425\n",
            "  429  238  367  594    7  997  215 1078  845   42  381] (25,) [0.10639147 0.11632703 0.33913264 0.36375192 0.37567575 0.5722918\n",
            " 0.61732632 0.70505629 0.80713346 0.91321458 1.01556507 1.02241996\n",
            " 1.11801523 1.21944379 1.41639743 1.57726133 1.61176017 1.66679995\n",
            " 1.74275283 1.74931856 1.82904064 2.02081011 2.25298634 2.25628638\n",
            " 2.31224376]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (125, 10) (125,)\n",
            "updated train set: (125, 10) (125,) unique(labels): [57 68] [0 1]\n",
            "val set: (1177, 10) (1177,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.62      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1177,) [37.38878273 40.07552246 28.65216903 ... 49.94613134 42.11389496\n",
            " 19.99311664]\n",
            "selection [ 892  568  133  711  692  671  784  123  730 1140  386   26  585  311\n",
            "  573  197  458  610 1043   23  267  799 1115 1080  877] (25,) [0.00896774 0.04791622 0.06916942 0.29890355 0.38211035 0.43568865\n",
            " 0.47526064 0.69943089 0.80853508 0.87273618 0.96858966 1.08252791\n",
            " 1.18639974 1.20290245 1.27730416 1.30608863 1.41651993 1.43247847\n",
            " 1.46634819 1.46634819 1.49456095 1.5357509  1.67619233 1.7000423\n",
            " 1.71035711]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [63 87] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.62      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1152,) [37.57997426 37.53000285 27.02047317 ... 49.79779782 38.48719942\n",
            " 20.34124306]\n",
            "selection [ 970  462  684  805  409  776  187 1108  559  497  518  426 1113  301\n",
            "  750  438  771  307  718  656  337  458 1103  739 1068] (25,) [0.01598065 0.01936035 0.0924114  0.19460954 0.31247506 0.42836491\n",
            " 0.58784232 0.68721312 0.86266793 0.9174198  0.97628451 1.17407833\n",
            " 1.18093184 1.52895759 1.5572379  1.59753324 1.68684343 1.77532316\n",
            " 1.79826408 1.82917207 1.83428532 2.10745324 2.22728082 2.23807533\n",
            " 2.3805288 ]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (175, 10) (175,)\n",
            "updated train set: (175, 10) (175,) unique(labels): [ 72 103] [0 1]\n",
            "val set: (1127, 10) (1127,)\n",
            "\n",
            "Train set: (175, 10)\n",
            "Validation set: (1127, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.2857142857142857, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.62      0.49      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1127,) [42.25400893 35.54910973 26.15422284 ... 49.91208914 35.94864619\n",
            " 20.86552491]\n",
            "selection [ 930  161  520  961  499  178 1120  802  207  591  594  769  780  410\n",
            "  858  497  840  716  565  516  436   13  192  461  662] (25,) [0.14924623 0.15081187 0.44979356 0.73457765 0.83503975 0.92649262\n",
            " 0.99520762 1.15857272 1.16263852 1.41823394 1.51948099 1.54926133\n",
            " 1.78881437 1.89416906 1.89764566 2.00750211 2.0545414  2.52344307\n",
            " 2.58112485 2.65945409 3.10771342 3.1566259  3.24243627 3.39403058\n",
            " 3.41924911]\n",
            "trainset before adding uncertain samples (175, 10) (175,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 84 116] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.007 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.64      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1102,) [38.66687923 33.76107214 24.60791439 ... 49.55758163 34.28455538\n",
            " 18.78361063]\n",
            "selection [1053  693  666  912  873   54  413  104  586  651  235  377  942   35\n",
            "  582  556   45  908    8  129  313  549  207  913  289] (25,) [0.06813801 0.69907258 0.72988853 0.75516186 0.79600093 1.05530624\n",
            " 1.23057313 1.34237668 1.48366863 1.67980049 1.75458711 1.84679555\n",
            " 1.94931722 2.11411151 2.2880173  2.64446865 2.69561606 2.75307264\n",
            " 2.84445358 2.85661786 2.93913357 3.15417972 3.16655949 3.40251389\n",
            " 3.6358298 ]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (225, 10) (225,)\n",
            "updated train set: (225, 10) (225,) unique(labels): [ 95 130] [0 1]\n",
            "val set: (1077, 10) (1077,)\n",
            "\n",
            "Train set: (225, 10)\n",
            "Validation set: (1077, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.2222222222222222, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1077,) [41.75892303 23.99635639 22.75981512 ... 49.67887088 28.69047749\n",
            " 16.53890936]\n",
            "selection [ 678  775  504 1029  478  447   71  703  885  644  133  109  362  302\n",
            "  193  643  146  763   66  260   64  664  469  124  434] (25,) [0.00464892 0.04198791 0.07598833 0.18435678 0.19410251 0.19410251\n",
            " 0.23738103 0.34287485 0.35669961 0.38497741 0.45426843 0.45462723\n",
            " 0.46503742 0.53217661 0.53572277 0.60645545 0.70049052 0.88020003\n",
            " 0.90616794 1.16659581 1.25239314 1.31847815 1.4357646  1.52504869\n",
            " 1.62984258]\n",
            "trainset before adding uncertain samples (225, 10) (225,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [103 147] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1052,) [39.66186038 27.33020543 23.77280855 ... 49.44967091 31.31781327\n",
            " 18.45710063]\n",
            "selection [ 365  942   87  197  961  171  411  998  459  965  875 1020 1040  946\n",
            "  200  583  468 1033  221   12  767  708  735  923 1044] (25,) [1.37633354 1.79304077 2.01145839 2.05108605 2.08869892 2.18274687\n",
            " 2.18656496 2.64755441 2.80911264 2.94235368 2.9708398  3.70435795\n",
            " 3.78906861 4.06348776 4.26979091 4.29965457 4.41440896 4.43698088\n",
            " 4.62202585 4.88233281 5.11129037 5.28154943 5.59011262 5.67668783\n",
            " 5.77229075]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (275, 10) (275,)\n",
            "updated train set: (275, 10) (275,) unique(labels): [114 161] [0 1]\n",
            "val set: (1027, 10) (1027,)\n",
            "\n",
            "Train set: (275, 10)\n",
            "Validation set: (1027, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.18181818181818182, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1027,) [40.19596466 23.4541376  21.01634293 ... 49.31450914 28.72298431\n",
            " 17.84141886]\n",
            "selection [ 750   39  225  615   37  625 1023  902  445  745  763  693  899  176\n",
            "  163  734  711  270  449   94  645 1003  674  273  219] (25,) [1.75945811 2.04750704 2.20950566 2.38518074 2.52362121 2.82550557\n",
            " 2.87192798 3.01323124 3.123136   3.9331166  3.99609625 4.07422496\n",
            " 4.2583067  4.40087371 4.58152592 4.7813048  4.84290928 4.88397319\n",
            " 4.90400253 4.95339046 5.0476753  5.11940302 5.18181141 5.22569006\n",
            " 5.24575949]\n",
            "trainset before adding uncertain samples (275, 10) (275,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [122 178] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1002,) [39.02488161 19.09235617 19.00083227 ... 48.82503006 24.96285991\n",
            " 16.91499037]\n",
            "selection [496 987 367  91 826  68 231 982  22 412 515 132 469 951 704 724 828   4\n",
            "  84 827 923 533 508 483 926] (25,) [0.03470328 1.48377032 1.55840166 2.21713567 2.39219521 2.76897933\n",
            " 3.30910469 3.41557331 3.57631332 3.69325159 4.50183024 4.60963242\n",
            " 4.69073177 4.74213446 4.77903287 4.84679926 4.97679651 5.05651529\n",
            " 5.17609573 5.3180746  5.39688421 5.4397296  5.4621436  5.51008472\n",
            " 5.57458895]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (325, 10) (325,)\n",
            "updated train set: (325, 10) (325,) unique(labels): [129 196] [0 1]\n",
            "val set: (977, 10) (977,)\n",
            "\n",
            "Train set: (325, 10)\n",
            "Validation set: (977, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.15384615384615385, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0]\n",
            "std (977,) [39.35500977 14.17898571 17.84031349 17.01112482  6.35760741 22.20528677\n",
            " 24.0265346  13.90339297 13.73277111 44.63046705 45.75952274 45.21904217\n",
            " 40.63768295  8.26307341 22.96110252 11.31149863 19.76523005 17.80322733\n",
            "  9.03053938 35.81204481 11.33688424 45.10952983 21.33667825 10.19027797\n",
            " 30.69668602 19.71839744  9.03053938 27.87720193 18.24682024  5.32678893\n",
            " 20.82407139 29.76450804 29.67042804 26.12971108 41.72078021 12.42523748\n",
            " 12.16235926 14.42292336 18.31354636 26.67633857 20.03519718 20.98360053\n",
            " 28.67342492  6.45419256  7.83540921 10.18575441 23.84101258 17.41628348\n",
            " 11.20798684 22.21155932 33.22873197  6.38253749 45.18809709 27.62691222\n",
            "  7.83448154 26.66473273 14.05393524  7.14019586 20.94088615 17.19674884\n",
            " 11.92438329 36.96898959 23.69911068 42.97461178 11.13240468 14.14131526\n",
            " 20.96410641 13.13137332  3.26250573 26.14480011 26.56463234 24.99360083\n",
            "  8.36378927 20.35433815 11.57736571 33.95481601 24.86901025  8.99851066\n",
            " 27.50448375 14.06222529 11.95810914 21.86278074 49.66952745 11.11278297\n",
            " 34.16578434  3.48499786 16.8015252  18.87186968 11.52167502 18.51104927\n",
            " 24.96702322 16.9004536  26.29020205  6.45658974 30.41627744 21.77390347\n",
            " 26.01240347 34.00483093 37.30434474 14.10612884 31.25739159 24.91663725\n",
            " 49.68489351 34.68726772 12.50971627 22.60295304 49.92439267 36.91084762\n",
            " 35.66319775 16.85758433 25.93354162 27.43603663  3.15999332 23.19310591\n",
            "  9.34385741 24.42970214 29.57157305 33.16819575 24.58689278 14.99302988\n",
            " 20.57102982 15.49844503 29.52655933 15.79827098 10.55724822  9.03053938\n",
            " 26.43100134 15.55965259 43.11598554 25.97461846 10.00648368 40.66163886\n",
            " 10.58502036 13.09666462 25.37596281 21.46639889 15.72324901 22.30932421\n",
            " 49.70268581 14.1402944  23.65075385 12.67179435 20.54208913  9.67744205\n",
            " 10.03814854 11.51326034 49.98941295 19.81451262 34.02438691 25.11597539\n",
            " 19.14376661 26.70617378  9.03053938 15.20630163 31.59051472 13.46074437\n",
            " 12.00729658 11.47881019 24.4628025  12.76649011  7.23131123 29.11045615\n",
            " 18.37236181 14.44902216 32.57108536 17.16730797 13.252247   19.62740558\n",
            " 14.55523834 15.08535442 15.06189116 26.13154846 30.36955218  9.03053938\n",
            " 20.5387681  11.25247462 12.06351738 16.12719216 42.75540785 16.32942185\n",
            " 19.99009152 17.93671691 32.58334833 10.31952607  9.02479196 23.93122722\n",
            " 21.82507422 47.5662879  15.88049816 20.2659146  21.08020989 15.41645438\n",
            " 28.53745404  9.100117   22.15325196 36.87015329 44.05308298 24.78980788\n",
            "  7.27257982 13.54552796  7.90219032 15.03133072 28.21277072 47.9199391\n",
            "  9.03053938 35.27283893 20.72288866  8.85949636 25.8820623  14.43117033\n",
            " 15.91318036 31.17892634 19.49118312 23.91989455 33.42138534 12.38419705\n",
            "  8.99610249 12.14153376 44.71952098 14.28931566 29.5733643  28.70738704\n",
            " 33.99526119 14.01837496  8.08155135 24.98465673 32.93596069 12.10159922\n",
            " 10.75740976 18.72177731 10.83141055 21.13200892 36.79202731 25.21454854\n",
            " 28.33836803 13.29006187 24.61729068 29.87695351 14.9134009  11.29195081\n",
            " 21.5970599  20.19208767  7.65300485 22.73366992 14.59606929 29.54995838\n",
            " 27.24679831 21.88130139  7.40895591 22.29744838  8.67622073 47.91879126\n",
            " 16.28076463 10.80197941 11.0075314  29.3118095   7.84971185 18.29837139\n",
            " 21.15371753 27.56435276 29.87853719 29.86344482 23.80014506 14.76795085\n",
            " 41.90102538 32.6085767  24.35280215 19.9451377  49.51570998 17.44332112\n",
            "  9.75719967 33.57574852 18.62285798 21.80468629 23.66408777 13.10504825\n",
            "  9.03053938 49.93570435  9.03053938 35.52294689 32.57904888 36.10981251\n",
            " 26.37220269  9.03053938 26.91432125 40.88163461  9.72865174 17.51857334\n",
            " 46.24355593 16.10809944 12.62541513 14.33776707 18.80054423  8.42395341\n",
            " 28.53597564 11.93697715 21.96415627 18.66613807  5.54674698 43.97186098\n",
            "  6.24626743 49.31243265 20.57533424 17.1315137  18.83906704 13.50817571\n",
            " 13.16669863 14.90723711 35.34681444 15.91790957 19.51776384 19.37438005\n",
            " 17.08505066  9.03053938 28.86021826 19.04990849 24.86901025 27.7205369\n",
            "  9.7664343  18.34578431 24.90858269 22.10529397 15.31849164 13.97787728\n",
            "  5.39591104 25.71371273 27.68134507 19.52929289 12.079357   16.85886924\n",
            " 30.00202166 23.68596659 29.03719738 10.53904791 20.72389262  9.03053938\n",
            " 32.6443461  19.44262753 29.39936877 16.26093116 19.41132814  8.41353882\n",
            " 13.56641243 25.9843953  23.34770137 24.46994252 13.35523847 22.44393825\n",
            " 15.2131625  12.04115486 11.73615859 25.43392687 20.31060946 19.23293007\n",
            " 29.55295197 13.67980397 13.49725281 12.88937276 17.71614161 20.00392992\n",
            " 23.75417306 17.55017122 26.27156522 12.55452017 23.78656216 31.83508895\n",
            " 17.05858654 13.89001773 31.74624963 14.08734244 18.56655095 49.77919735\n",
            " 19.2664594   7.57472489 17.30857541 19.4498183   5.58949769 46.46535064\n",
            " 25.20097337 37.53113824 21.50908081 21.72866186  9.03053938  9.03053938\n",
            " 19.69570022 25.10236651 16.36276769 25.28980441 16.68665256 21.47209859\n",
            " 27.881434   11.70751632 16.41345708 17.24098645 15.38940498 19.7251674\n",
            " 20.89461239 19.2565618  30.10932758  7.72471607  8.09684086 17.66367853\n",
            " 47.01910768 27.01437466 30.61444627 25.19975427 22.88138374 13.51344143\n",
            " 20.19456419 19.23468427 14.19614727 36.3140193  13.01962178 20.71055351\n",
            " 26.52090779 16.32963616 18.43651617 25.16122615 33.67099963 18.80910943\n",
            " 27.27131741 15.24644998  9.03053938 20.81334635 34.774429   13.28514789\n",
            " 12.21928621 32.94149047 47.47877906 29.77670795 37.04222035 22.23064463\n",
            " 29.38066011 40.99002751  8.59952326 30.25134098 27.97352184 17.27806241\n",
            " 30.47813948 23.00952221 27.43819264 30.01710484  9.50991996 17.95591767\n",
            " 37.39151028 23.46610575 18.8527496  25.12305077 18.03553265  8.16024391\n",
            " 13.65223852 23.05392919 25.89660892 47.41668375  6.67660208 15.3372975\n",
            " 11.62760122 18.35270355 20.4574526  28.62626677 16.92306435 42.39389531\n",
            " 49.96922726 24.95897842  7.40034873 10.73002598 15.35793882 24.56173824\n",
            " 14.61745603 10.0980655  20.43230968 13.2708384   8.21667377 19.88293244\n",
            " 48.17270603 18.84587362 42.20802926  9.03053938 12.9564993  29.60617208\n",
            " 17.61088882 17.24348807  8.58954271 19.46212395  6.0173569   9.03053938\n",
            " 14.70901414 19.93602583 39.66054884 15.20153963 36.48705713 20.3247943\n",
            " 27.99880498 20.6184744  10.22057486 27.07835114 13.10770477 16.2553558\n",
            " 18.828466   16.16244507 14.86917296 16.32836766 16.98292711 18.93967173\n",
            " 16.61751797 23.80771826 31.22089448 25.86829018 12.80983818  5.70914614\n",
            " 22.03175943 38.1059475  33.56668039 29.00331081 26.07482906  9.03053938\n",
            "  9.03053938 23.45741053 15.21849622 16.08528196 31.06710761 34.08959166\n",
            " 22.5167266   7.59846644 18.73866225 17.33485455  7.74926734 41.01980881\n",
            " 22.42346233 24.07009883 24.79850926 16.28775434 31.10802565 31.53279022\n",
            " 20.63132368 37.20006631 20.04721867 11.73615859 14.6612558   9.08227377\n",
            " 24.3589378  32.65155994 21.43157283 31.23692098 47.91607336 13.9553461\n",
            " 19.13573668 19.83898619 16.61751797 13.08144398  8.92375595 19.19750831\n",
            " 17.11703058 27.98994192 11.12144654 20.90830888 39.21158142 49.9719733\n",
            " 24.81276174  5.50176455 22.49513464 37.66389054 24.94587389 11.86089745\n",
            "  1.78070992 35.17719213  9.97322678 15.02251935 21.08536074 16.77867083\n",
            " 14.48546235 15.71500776 36.20210374 23.9326031  18.20676443 11.38487879\n",
            " 14.82235474 37.36180915 31.22832108 25.71835483 11.06232073 23.16425555\n",
            " 28.62594971 17.56905898 14.98583478 10.85498784 17.49150754 22.20423218\n",
            " 25.67192933 17.8261977  14.69551268 17.3967576  19.75605271 33.96309643\n",
            " 26.23416376 39.68922309 14.33201144 31.36633409 12.65925248 16.07878238\n",
            " 49.98153772 35.33493199 12.11379073  8.53270844 32.93901128 16.73377713\n",
            " 16.66961194 32.99284969 18.74397201  4.71769288 22.03005523 10.73882691\n",
            "  4.71929227 20.65683704 35.59907866 21.47035012  9.74625899 49.38545088\n",
            " 14.61575218 36.74020878 23.86872456 12.18815643 24.79386734 24.05092813\n",
            " 14.01941732 26.90436605 12.44235    33.47597148 27.91505525 12.27023152\n",
            " 10.34739058 30.04041856 13.91399202 26.28341822 18.78202567 26.3223038\n",
            " 19.36920788 31.81779588 12.77149706 13.19250306 28.54917957 18.79009469\n",
            "  5.76452796 19.14285418 36.47506433 20.2562444   8.8747211  45.23323837\n",
            " 23.76084825  9.0195216  28.05284262 16.07436759  2.60939952 14.90723711\n",
            " 49.38106835 22.88952192 19.37210815 21.40843488 39.38194838  8.03595024\n",
            " 17.71651358 33.80644592 32.99657219  5.59613534 43.18290655 20.30927563\n",
            " 29.72716104 21.49671443 46.97962331  9.03053938 17.88080748 25.92218706\n",
            " 21.10647609 17.76728026 29.11938711 20.52120188 31.38655161  9.03053938\n",
            "  9.03053938  6.64182669 38.13525943 28.94561647 34.67209343 35.59037439\n",
            " 44.23221715  6.20170386 11.62459665 24.08109548 22.25839631  8.26307341\n",
            " 11.5058387  20.90980437  9.44123097  9.62743233 13.01376204 19.3605576\n",
            " 42.82421718 16.19667865 22.8186266  33.39220406 11.75320122  6.52595371\n",
            " 12.25242894  8.35545572 15.10705088 22.58965057 32.64671519 19.04392675\n",
            " 34.46992951 33.55442581 30.20867559  5.52615166 33.68665288 15.36155773\n",
            " 14.47753257 15.94894152  7.98536965 24.0279529  49.93521949 44.93638164\n",
            " 32.50232648 18.91655166 46.88381632 11.1649937  27.61838463 16.15441806\n",
            " 16.80396803 35.38408303 29.97789939 24.97532774  9.03053938 23.39087241\n",
            " 12.42361055 23.58158839 10.41431201 20.41302448  9.03053938 21.47865917\n",
            " 17.47764177 20.56857933 32.77982889 11.23951672 26.05643299  7.43192963\n",
            " 23.78197016 10.32164118 17.57283867 19.72264055 29.66274986 22.35516617\n",
            " 30.64839459 49.81158454 12.02956646 18.66852887 11.19186245 23.91671917\n",
            " 27.22742151 32.75400835 17.49447537 26.98265152 38.87142358  9.03053938\n",
            " 30.3735674  42.44812444 15.37597478 20.27328403 41.23708086 14.09584747\n",
            " 19.36431517 11.99494563 15.17355868 21.60743669 23.91446528 27.11287657\n",
            " 32.78099827 19.8619278  25.84003993 48.20882725 19.96303274 12.5425485\n",
            "  3.07176878 30.46775736  8.44941584 22.20616164 21.08974249  4.29147923\n",
            " 21.20449428 20.821942   30.49393555 22.87316654 18.828466   35.50186454\n",
            " 23.712822   14.23948581 30.57381671 18.98438468 18.49505762 26.9784337\n",
            " 15.55386386 35.58852758  2.71902412 20.7907833  29.00686777 29.22501716\n",
            " 40.1551628  46.0603899   8.13478916 14.8841765   7.45102406 13.20005025\n",
            " 19.50309644 11.33083581 48.43964645 17.99268227  9.1183032  15.30587672\n",
            " 10.81575976 23.700483   41.51652584 24.61113699 16.85495756 26.08537315\n",
            " 21.32297086 30.84961957  7.98741093 48.229584   10.28160498 17.86235519\n",
            " 40.52948125 20.15306554 31.17703603 21.23040363 15.29050855 32.93596069\n",
            " 10.20050501 40.94220504 16.48445295 42.1922766  12.03552574 27.69555636\n",
            "  3.3921393  35.0386064  23.8675981  48.19547669 21.31702941 17.82584498\n",
            "  8.39012186 25.24411828 24.80041337 14.24888815 12.53173925 18.86927416\n",
            " 17.05944131 25.93382999 33.46835737 18.94416672 23.25182234 41.15472022\n",
            " 36.79084272 18.64692835 33.24654312 10.68188791  9.03053938 40.15113515\n",
            " 35.86438079 22.33501131 11.61859674  9.03053938 18.26466184 10.57485639\n",
            " 11.5058387   5.71214844 25.02338799 23.52769122 26.24427893 28.124425\n",
            " 21.13200892 22.42346233 20.92042019 19.80819953 21.56893982 10.79744156\n",
            " 38.02397974 11.52167502 19.91213825  8.63728189 15.78685065  9.95879381\n",
            " 33.565549   24.38403945 18.79637738 31.20976323  5.9061782  11.71343536\n",
            " 39.42752758 27.84696551 18.49787234 14.90961132 19.69550019 11.484269\n",
            " 20.32452726  5.3521264  12.90521371 14.25469586 25.48787466 23.82806412\n",
            " 32.4483862  18.71791381 36.43496536 20.76349507 23.84504061  9.03053938\n",
            " 17.57143642 28.1296109  26.42704285 32.94620381 28.74956637  8.61991589\n",
            " 36.60283009 25.8612783  23.78040049  9.14277387 32.1738901  29.31970426\n",
            " 32.45689684 18.90541884 17.73801235 15.57076275 48.7796055  22.03653208\n",
            " 20.59016576 34.40305678  7.88510159 21.27868317 18.43825382 12.86344293\n",
            " 28.98346661 34.57452475 48.47215819 15.31558575 17.46005264  5.81304827\n",
            "  9.41789493 30.20771443 35.3272624   4.46010363 48.78730184  9.9744683\n",
            " 21.94993872  6.33361564 45.46669094 15.6632397  18.4635824  10.97241134\n",
            " 21.26320652 12.07831866 20.3466861  19.2067103  16.63420569 23.71852922\n",
            " 15.90821555  9.03053938  9.05336703 18.34312517 15.34159106  4.30195327\n",
            "  9.03053938  5.48139901 49.99335777 18.00011483 18.49509943 42.07103118\n",
            " 17.06437757 16.20415257 49.36982309 20.21674262 15.3282295 ]\n",
            "selection [564 652 800 780 112  68 840  85 785 965 945 609 612  29 901 324 967 559\n",
            " 711 298 376 663 509 871 642] (25,) [1.78070992 2.60939952 2.71902412 3.07176878 3.15999332 3.26250573\n",
            " 3.3921393  3.48499786 4.29147923 4.30195327 4.46010363 4.71769288\n",
            " 4.71929227 5.32678893 5.3521264  5.39591104 5.48139901 5.50176455\n",
            " 5.52615166 5.54674698 5.58949769 5.59613534 5.70914614 5.71214844\n",
            " 5.76452796]\n",
            "trainset before adding uncertain samples (325, 10) (325,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [142 208] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "std (952,) [38.76554542 16.36705459 18.16156815 17.92641357  7.36163168 24.13242157\n",
            " 26.00545465 15.72221704 15.14832261 46.76433491 46.28226802 45.12434561\n",
            " 41.3154926   8.57847687 26.60728263 15.19881548 22.20114285 20.26560848\n",
            "  9.32209476 37.44329218 13.17681426 45.21239683 21.01700657 12.30759952\n",
            " 33.17947456 20.428906    9.32209476 28.93579039 17.97949638 24.4542822\n",
            " 30.10848508 34.08915083 28.25418111 42.86039678 12.47915752 11.31538176\n",
            " 15.41626987 20.44333768 25.84919649 22.24023465 21.09041645 31.57807812\n",
            " 10.26788913  9.2388941  12.10815637 25.40158066 17.13581374  8.7198907\n",
            " 24.83123822 32.61972483  9.59329343 45.23333574 31.24939523 10.63917601\n",
            " 27.03304586 15.83883444  7.33978887 21.63335797 16.5220662  15.12083744\n",
            " 38.76134384 26.43542484 43.41303232 10.00031812 16.28463445 23.0826346\n",
            " 13.6790705  28.54299286 30.18295424 22.52007407  6.38558778 21.1308631\n",
            " 12.45901503 37.2843281  26.61085628 11.33106556 29.42005459 19.29070102\n",
            " 14.90169561 24.38021357 49.70150155 13.28542807 35.86656828 16.66495717\n",
            " 22.42361249 14.43507656 17.32485792 27.69848543 17.41537839 27.29505232\n",
            " 10.95446452 32.91963181 19.34121082 24.58698349 34.81609818 37.11594506\n",
            " 13.36130632 33.8170195  25.06090205 49.65245469 35.50804598 13.84287645\n",
            " 23.32261748 49.9197563  38.61922845 36.57469137 18.03191368 26.61706699\n",
            " 27.49024245 23.63851724 10.15940453 25.01294113 32.8652819  33.95387973\n",
            " 28.217999   15.85295034 21.58270529 17.69341826 30.998308   16.86087872\n",
            " 13.31802187  9.32209476 28.22430614 20.35793437 43.09297183 26.8129215\n",
            " 11.41926709 41.49318552  8.77193444 12.99758095 28.90336969 22.33148525\n",
            " 15.94631859 23.97271359 49.7365235  13.75088175 27.41017594 13.17660492\n",
            " 20.46683345 11.20441659 11.3894008  10.30187221 49.98745227 20.89212537\n",
            " 33.65006513 28.50409355 20.72979499 27.87980272  9.32209476 17.72621092\n",
            " 33.91749079 13.71070148 11.16202543 12.59774995 27.11312519 14.13470435\n",
            "  9.54790669 30.97851765 18.32530523 14.15675577 34.28825187 17.55153606\n",
            " 13.61266992 19.31276078 15.22876186 12.29083094 15.82483658 29.09657635\n",
            " 31.14501314  9.32209476 22.01865995 13.22479558 16.83863536 16.73385036\n",
            " 45.35373901 15.66727053 22.02497475 20.97792954 34.16842234 12.48434878\n",
            " 12.26751265 27.5560344  22.77900203 48.52077578 17.74304649 22.38451235\n",
            " 21.52736983 16.34371134 28.78262677 10.45250275 23.78785664 38.23598285\n",
            " 44.75209171 25.42178925  6.94209382 15.74624719  7.17233933 14.06447773\n",
            " 29.1767313  48.71244162  9.32209476 35.18218706 20.10969379 11.61545968\n",
            " 28.28819688 15.73955251 17.12169773 32.33605088 19.79174068 26.00381501\n",
            " 33.82018617 13.89202455 11.65805426 13.69282309 44.92498856 17.11932\n",
            " 31.48991625 30.91747485 35.74595096 13.04477418  7.41292491 25.10521213\n",
            " 31.66021129 13.80732094 11.50407311 19.41779084 10.12960355 20.573648\n",
            " 38.30858656 26.39039892 31.18696219 18.07949277 25.60353799 31.83205402\n",
            " 15.43733795 14.87095098 21.34999532 22.67442195 13.38580586 22.6152427\n",
            " 16.38806478 30.38616142 30.84439587 20.52649531 11.22862895 24.64642812\n",
            "  9.78920877 48.15392647 17.43187543  8.93103808 14.2114934  30.54023389\n",
            "  9.52572049 19.71845752 20.19111209 31.34442261 31.63256734 32.27111777\n",
            " 23.49962311 15.64872173 42.009841   33.67582943 27.92030205 23.02194848\n",
            " 49.43829807 18.19762176 13.64930023 35.22251559 18.59909315 25.11948523\n",
            " 24.70581851 17.52517742  9.32209476 49.93728687  9.32209476 36.9008237\n",
            " 34.11059346 38.76220382 27.35645018  9.32209476 29.77983219 41.38855333\n",
            " 10.36185221 21.95227278 46.53339078 16.5408236  14.98974426 16.60097059\n",
            " 21.31147397  8.87129195 28.74629379 12.49269758 26.61161724 19.3915185\n",
            " 45.18711832  9.69617357 49.37186828 21.85791152 19.94110694 20.15036193\n",
            " 15.266204   15.23885787 16.43286381 34.65310738 16.3513692  20.91986299\n",
            " 19.64646194 18.13703338  9.32209476 29.46987999 21.42524176 26.61085628\n",
            " 30.03796183 13.88474792 18.65334393 27.37699316 23.37506532 17.21839761\n",
            " 12.70614913 27.97287483 29.39862385 21.35113534 12.18542331 17.64630043\n",
            " 29.98728268 26.86366631 29.94453269  7.87069766 20.61681036  9.32209476\n",
            " 35.79430778 22.89392992 32.18918367 17.18489621 18.34422838  7.70215149\n",
            " 12.38903363 28.3786684  26.70294306 26.7156114  14.90134926 27.21330186\n",
            " 16.17062404 12.09967223 11.19729331 27.66570402 20.14546185 20.71824203\n",
            " 32.18823325 16.19246807 16.26810479 14.59172607 19.61865007 21.86119045\n",
            " 26.45026594 17.00965922 27.26220557 12.73151734 26.79266675 34.69883079\n",
            " 20.22060805 14.29361268 34.69088916 13.65009061 19.41342102 49.78053519\n",
            " 21.19705886  6.94282119 19.42499778 21.25524823 48.19262894 26.47512469\n",
            " 38.34770272 22.2929693  23.61869398  9.32209476  9.32209476 21.44040809\n",
            " 23.62223873 19.03072617 27.32523262 18.16596765 22.92947363 27.56227207\n",
            " 16.55710211 16.88566216 20.45977481 14.4152276  19.31448975 22.11366142\n",
            " 21.48742867 31.98891081 10.55649724  8.77880924 16.72080991 47.30379259\n",
            " 29.68226765 29.57305828 24.98631487 24.15873089 14.25915298 22.53200054\n",
            " 19.78978757 15.32035998 37.29914187 12.66649464 22.94549867 28.11294341\n",
            " 18.97047127 17.96789962 26.78220886 34.53279644 18.77481241 26.80730619\n",
            " 15.80880671  9.32209476 21.83854744 36.5004061  16.74722908 11.19014097\n",
            " 31.32629272 47.65917217 31.17908537 35.97990544 20.15853374 30.65894321\n",
            " 40.90626917  8.40510614 29.55370475 30.53910626 16.64131887 34.00189641\n",
            " 22.65176087 30.60171698 31.96703642 11.29160608 22.5016652  38.33776632\n",
            " 24.10825234 21.53753968 25.69466353 19.97074305 10.30534636 17.09248593\n",
            " 26.60457666 27.78542949 47.66607891 11.34866369 17.19341761 11.51031752\n",
            " 21.98344886 21.06829743 31.5740277  14.9754713  43.36761334 49.97175264\n",
            " 28.07458593  8.89884215 12.35499223 11.54667745 23.89213137 14.70647965\n",
            " 12.59673155 24.54705379 14.84829987 10.27567781 22.05942207 48.05329448\n",
            " 22.01613957 43.59618589  9.32209476 17.2075309  31.34043251 18.91892727\n",
            " 15.5407953  12.84395162 21.59857801  7.18667156  9.32209476 15.79366988\n",
            " 20.17166266 40.6869906  13.65737779 37.13861989 22.18913882 31.92634064\n",
            " 23.75825386  9.3268523  28.34728239 13.7641651  15.76364935 21.359543\n",
            " 17.14762262 14.81979966 18.9382612  18.20917317 21.00008318 17.36848525\n",
            " 27.44008722 32.22368731 27.62886792 12.72343179 22.68177139 40.73708206\n",
            " 34.91230876 29.40886662 27.64757243  9.32209476  9.32209476 25.54732473\n",
            " 17.08010756 17.22677658 32.88973534 34.53151747 24.94058401 11.82706313\n",
            " 21.48373193 17.27517187 10.1052265  41.92583053 23.65202919 23.70237282\n",
            " 26.76655236 17.22407512 33.60313641 32.01642105 21.08881391 37.89973202\n",
            " 19.95673555 11.19729331 18.85785466 10.51588507 27.07308913 34.93413759\n",
            " 24.18077283 34.19426777 48.80072372 13.84740078 20.4851042  22.63868446\n",
            " 17.36848525 14.68904974  8.79803431 19.64949007 17.42799686 29.63748802\n",
            " 13.04090709 21.78817435 40.59656305 49.9717451  25.5676114  25.9955392\n",
            " 38.70021577 28.64885568 10.98879208 37.21214534 11.710644   13.60439114\n",
            " 21.86128141 18.99983257 14.17442983 15.973424   37.22474127 27.78794241\n",
            " 19.99597182 12.19698762 17.07010626 37.84357109 33.25707898 24.55338405\n",
            " 13.15374771 26.01895476 29.9974252  20.4090831  14.65520476 11.96493476\n",
            " 17.82667713 22.9345313  27.3564387  18.29311897 14.40001815 18.38282303\n",
            " 21.4613101  36.98251782 29.35459006 41.28485513 18.29081847 34.24145155\n",
            " 12.14983139 15.79900071 49.97881703 36.27154888 13.21402704 12.69608365\n",
            " 33.32455865 17.12608457 21.78023306 34.90181217 19.5240423  23.42751501\n",
            " 13.61665416 22.76877263 36.2747909  21.96209836 10.59399081 49.20863324\n",
            " 13.78436874 39.06744952 24.42138732 14.33733713 25.47857141 26.76204088\n",
            " 15.73256884 29.025191   14.22513043 34.00160466 29.13929903 15.39398435\n",
            " 12.09282895 31.91400903 18.22242946 28.21140313 19.62662222 29.33106814\n",
            " 18.47179679 34.65629388 14.48679888 14.61389626 29.96554218 17.6618546\n",
            " 19.05508187 37.86510573 21.76458188 10.15977696 46.09984512 25.5288421\n",
            " 10.99612192 29.58175008 18.78676988 16.43286381 49.48123184 25.12796515\n",
            " 20.08908367 22.03389826 41.34061797  9.73745106 23.72540722 33.96727776\n",
            " 34.61597188 43.51153793 21.26909909 32.11067862 21.02784689 47.21077168\n",
            "  9.32209476 17.71796226 28.79867232 22.22450673 18.58691025 29.14197448\n",
            " 18.11159654 30.5974505   9.32209476  9.32209476  8.04456276 38.67145246\n",
            " 28.50282008 35.406458   37.77499714 44.73716648 10.19012642 10.64772769\n",
            " 26.42777063 24.49875015  8.57847687 13.58677588 22.21222788 10.50493684\n",
            "  9.54979688 14.05071015 17.32649037 44.46724972 15.93016582 23.65688429\n",
            " 33.31063084 12.24006009  7.84047463 15.08595474 10.61706564 16.10428835\n",
            " 24.29913485 33.45426289 21.340521   35.23136878 33.40821422 32.42645312\n",
            " 36.40982357 18.14851098 15.68879048 15.41047033 12.35657414 20.67435824\n",
            " 49.93953246 45.60588418 34.94624514 20.38670444 47.49107698  6.0728461\n",
            " 28.40130112 18.15179144 18.42708748 36.63231218 30.89229254 26.32076299\n",
            "  9.32209476 25.49912821 14.45878662 27.59313114 10.04352586 23.17262516\n",
            "  9.32209476 23.87787342 18.80676475 18.4988525  34.1321056  11.90639384\n",
            " 27.06292481  9.7021816  25.46632323  8.53497051 17.41639157 19.34043349\n",
            " 31.59052334 22.40406753 32.3169013  49.81741543 12.51614955 23.65705479\n",
            " 13.32399058 24.21704128 30.64581769 33.71566972 19.59952479 23.45822313\n",
            " 39.08653656  9.32209476 31.40518376 42.89089926 17.46040938 24.00463515\n",
            " 41.63079951 13.84557696 18.52528754 12.41314288 15.75359647 20.26617447\n",
            " 28.40907125 27.51622704 36.54097315 21.807189   26.90503246 48.24314306\n",
            " 18.73603787 14.87734219 31.34353401 11.39652767 24.48872991 23.09045235\n",
            " 24.93264711 23.34555525 33.33761952 23.32780039 21.359543   38.33846635\n",
            " 26.38916137 18.55982311 31.1925908  17.02057486 16.75869694 29.84711919\n",
            " 15.31884963 36.8666259  24.56845405 31.55024601 32.20043209 39.74729963\n",
            " 46.51190835  9.04259813 19.96272999  7.90437189 11.94897448 24.05626923\n",
            "  9.45241999 48.58245227 18.08320843 10.39051464 16.66485147 10.66679693\n",
            " 25.83534568 42.56349566 26.5374966  18.13378159 26.69062534 22.2967508\n",
            " 34.69534162 10.32384971 48.34957614 11.08121898 22.25475916 41.76134246\n",
            " 20.29354138 31.2272629  23.96598313 16.64209661 31.66021129 12.1069145\n",
            " 41.32036671 19.87880641 44.29578163 15.98252341 29.05815206 36.66031125\n",
            " 25.3173917  48.45936072 20.45246989 19.01573504 10.45754233 24.93756583\n",
            " 25.95639599 17.28785481 13.1088357  20.511616   19.02948158 27.67956638\n",
            " 35.01757905 20.50973312 26.21373361 41.55933204 40.38602461 20.83267812\n",
            " 33.80834381 10.98629205  9.32209476 42.81566457 37.79293967 25.28298144\n",
            " 14.6865081   9.32209476 18.81634182  9.86284091 13.58677588 27.7631196\n",
            " 24.7751758  31.2264734  30.1927065  20.573648   23.65202919 21.46693848\n",
            " 20.96970519 24.27573938 11.1067788  39.7964636  14.43507656 20.92567904\n",
            " 11.45141223 17.6225807  12.20092333 34.91178421 26.83954526 18.3883523\n",
            " 33.52441454  6.25095722 12.85879833 39.80656785 28.34670731 19.31289543\n",
            " 18.0021797  22.59618176 13.10284807 20.49325739 12.48265858 15.83592225\n",
            " 24.87406941 27.59786966 34.90050921 20.22916583 37.92228477 22.87050893\n",
            " 24.46017325  9.32209476 19.93079075 29.87608327 29.25291909 35.863169\n",
            " 30.47910327 12.16001816 38.66154049 29.06873808 25.14242963  7.93178674\n",
            " 34.23757752 30.6686074  32.77891595 17.6357972  19.14809986 17.46996542\n",
            " 48.94532813 24.07480346 22.53979769 35.33181074  8.97778877 23.52401763\n",
            " 19.02961424 13.20504814 30.11772318 36.29934338 48.62051097 16.17907699\n",
            " 18.83023773  7.06457616 12.96340536 32.22855207 37.2336003  48.9529465\n",
            " 14.37376299 23.49368552  8.38248319 46.21249455 15.79081204 19.25067309\n",
            " 14.59927205 23.29917723 14.56303272 24.15885929 21.41636419 19.52661362\n",
            " 24.5599878  16.86241606  9.32209476  9.90403987 23.10724938 16.95460771\n",
            "  9.32209476 49.99876619 19.9544965  21.49799368 41.94658689 20.36963283\n",
            " 17.03165928 49.44696105 22.66195985 15.84896289]\n",
            "selection [707 871  70 194 367 919 196 477  56   4 220 335 686 327 789 899 664 926\n",
            " 427 729 674  13  47 128 393] (25,) [6.0728461  6.25095722 6.38558778 6.94209382 6.94282119 7.06457616\n",
            " 7.17233933 7.18667156 7.33978887 7.36163168 7.41292491 7.70215149\n",
            " 7.84047463 7.87069766 7.90437189 7.93178674 8.04456276 8.38248319\n",
            " 8.40510614 8.53497051 8.57847687 8.57847687 8.7198907  8.77193444\n",
            " 8.77880924]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [146 229] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.88       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 0 0]\n",
            "std (927,) [39.4911433  14.79684574 18.16175172 21.86794034 23.5366834  25.69390223\n",
            " 14.59620261 15.21441375 45.28933302 46.60398317 45.20540176 41.27137127\n",
            " 26.26244302 14.76497001 21.10197331 19.34083861  9.2774698  38.12527573\n",
            " 13.08990846 45.84178826 19.66611324 11.35267962 32.6253099  19.137096\n",
            "  9.2774698  28.97953717 17.93421539 22.18320662 31.01901701 32.50630143\n",
            " 27.21163336 42.82741987 13.17848992 10.83676394 15.70035935 19.73391075\n",
            " 23.56415452 21.66953378 18.67546094 28.88263798  6.0890611   8.53499043\n",
            " 11.40522181 23.89425594 20.47053442 24.9959571  34.44760147  8.96653846\n",
            " 45.56329061 30.04629644 11.24302215 24.91498725 14.69722504 20.94959153\n",
            " 16.14087997 12.6128417  39.52179863 23.5749382  43.72488026  7.71296722\n",
            " 14.66039126 25.12796928 13.74277788 28.30287614 31.5792952  22.00893258\n",
            " 21.19124043 12.63709855 37.68897409 25.74279629  9.16297954 28.42102813\n",
            " 18.60669681 14.8345715  23.33177349 49.77239182 12.13263425 35.47904224\n",
            " 16.95389904 23.53422309 14.36629276 16.94996332 28.68894088 16.11342574\n",
            " 27.10521531  5.84717208 32.03198511 17.63191097 25.96780788 35.72223628\n",
            " 37.67776872 12.64923063 32.02048701 27.18716464 49.74032598 36.06324854\n",
            " 14.97104951 23.60861946 49.93744308 38.09642171 36.09992445 18.60193008\n",
            " 26.17012961 21.78055411 24.65385027 11.47350768 23.49750421 35.17817818\n",
            " 33.54477924 27.51800507 15.9366181  22.84137461 15.24395445 30.62670495\n",
            " 15.78174018  8.05439381  9.2774698  26.41310439 17.24184531 43.52551766\n",
            " 25.77134828 10.5881887  42.15775449 17.21210065 27.87056183 21.87754933\n",
            " 15.01697991 25.63618744 49.78671911  8.68140463 28.58346123 13.07728459\n",
            " 20.2985093  13.17941173 12.43853714 10.29702656 49.99099986 17.82876089\n",
            " 35.02331298 28.11471506 19.90029945 29.05508305  9.2774698  16.89894939\n",
            " 34.1012091  14.19603206  9.33875633 13.36801983 26.02877847 14.25612207\n",
            "  9.12611339 30.96281393 17.2531941  15.08446643 35.25937184 16.85166355\n",
            " 14.62678887 18.06065006 16.06425961  8.80942545 15.06907384 27.72162099\n",
            " 31.85620703  9.2774698  21.79090557 13.70384865 16.6653081  16.31579405\n",
            " 44.08156058 16.38364284 21.30261548 20.82498202 28.88090742 13.6615438\n",
            " 10.7647227  25.84704909 20.63364895 48.64948409 17.49843441 22.7731011\n",
            " 21.56901141 18.75589424 27.55041252  9.32095389 24.06456707 39.0963783\n",
            " 44.81779735 24.91412983 15.17040226 14.04117118 29.07505076 48.43983894\n",
            "  9.2774698  34.8749178  19.72940867  9.98531209 28.42955822 14.80638756\n",
            " 16.13422447 32.35638227 20.43262073 26.67145739 35.40785106 13.57140946\n",
            " 11.58114503 13.75905213 43.66729179 14.86898213 29.57330798 28.90946231\n",
            " 36.43767274 13.59967892 24.08039937 32.72394158 12.71364736 10.49433958\n",
            " 18.03452415 10.18814294 20.07177831 36.57837215 25.18815273 31.6709656\n",
            " 15.93322947 25.38144864 31.99244886 15.15195113 14.90044054 20.43517412\n",
            " 22.84990651 13.79703883 21.28131287 19.76356404 29.65797126 30.91162241\n",
            " 23.12567201 11.63271051 24.29860642  9.07362644 48.40750732 16.85013252\n",
            "  8.19779804 13.42301201 30.24212856 10.54812142 22.23157076 19.89443925\n",
            " 31.78257402 31.71746369 33.23922858 20.87400393 14.97220993 42.6020825\n",
            " 31.02100484 29.11285094 24.41716703 49.49903631 17.22188132 17.03949205\n",
            " 31.901688   17.51437811 23.22492114 24.34532458 17.78889225  9.2774698\n",
            " 49.95296399  9.2774698  37.89047081 31.68722906 40.62949682 27.76111362\n",
            "  9.2774698  29.51835017 43.36469039  9.54541476 22.19883924 47.03676067\n",
            " 13.66763872 14.12745078 18.68916072 19.96636402  9.88710037 27.24991628\n",
            " 16.06943126 22.93966713 20.26466795 42.94050402 10.14714614 49.47197679\n",
            " 20.23218023 19.0057492  20.23107373 14.72035932 15.74254195 18.04282046\n",
            " 35.5383011  14.4171269  17.59242983 19.72045323  7.6252034   9.2774698\n",
            " 29.00715258 20.64210213 25.74279629 30.10037744 13.02744401 18.0025682\n",
            " 29.05613809 22.13744322 18.69373362 12.05461672 25.84490962 26.99891818\n",
            " 22.33880932 10.45618171 18.41005158 31.3673142  26.10469527 29.45384729\n",
            " 19.93020154  9.2774698  36.19980179 18.77985692 30.57615953 17.10638807\n",
            " 17.97583921 10.66436257 28.2544815  24.14181052 27.28530265 15.12502735\n",
            " 24.51307543 14.91991573 11.70717355 11.22167846 28.09094993 20.11212278\n",
            " 19.80752321 33.07176613 16.01159005 17.17604223 13.89955791 19.66844444\n",
            " 21.24721737 25.50374585 16.48258941 26.86434459 13.63001513 26.88931237\n",
            " 32.28876646 15.13776535 13.88831728 33.27233776 13.82698001 18.40176312\n",
            " 49.83061584 21.62576467 18.75005339 22.20205629 48.0553746  24.17869672\n",
            " 39.75475487 21.92872706 24.75603094  9.2774698   9.2774698  20.25899132\n",
            " 22.29756281 20.29953417 26.08140271 16.81616279 22.79726442 27.88328114\n",
            " 16.92342957 17.72433826 20.05873772 14.61664903 16.25111152 21.42780377\n",
            " 21.17990893 31.66939578 10.96823924 15.82164945 47.58806394 28.62302511\n",
            " 29.84563737 24.13688832 25.1010997  16.91550357 23.05180866 20.24117132\n",
            " 13.99495719 37.62369212  9.45580861 23.97016464 23.87920498 19.23832106\n",
            " 16.84007268 27.50625657 34.76878923 19.96322504 24.28538892 16.88697421\n",
            "  9.2774698  21.55389404 37.33711934 15.32403281 10.08580596 31.49881477\n",
            " 48.01791569 32.33040042 38.58806207 18.96828009 33.38772265 41.36795087\n",
            " 27.68155591 27.56557077 16.07722664 33.78742898 21.89324668 28.95638782\n",
            " 29.95965795 10.9897664  21.84452595 38.80904699 23.18030989 20.81092645\n",
            " 25.66169119 18.99092529 10.3961325  11.20993635 24.358318   28.0110302\n",
            " 47.88701798  9.6699632  17.56888405 11.63452301 23.30793371 23.0799145\n",
            " 30.02953808 16.14674429 43.71204035 49.978899   25.03020379  8.97010414\n",
            " 12.19762347  8.08592174 22.51236249 14.38901177 10.98883125 24.2258081\n",
            " 14.78572783 11.52833562 21.54006927 48.20481974 24.85307769 42.71920543\n",
            "  9.2774698  15.61076647 29.57891301 18.95107623 14.38047632 13.66864629\n",
            " 23.49585313  9.2774698  18.06623817 20.9867729  41.57584868 12.21089485\n",
            " 36.50724667 22.58202575 31.36941444 22.64253618 11.48178266 27.35901472\n",
            " 13.34181851 14.75435751 19.93709221 19.38629249 15.46460507 17.19191355\n",
            " 17.64372601 19.71929089 16.35159104 26.57628296 32.77452112 26.6708534\n",
            " 10.32693065 21.53363949 40.13174351 32.7824139  29.41024579 28.38721449\n",
            "  9.2774698   9.2774698  25.2805568  17.13755556 16.6742531  32.96625714\n",
            " 33.99191775 25.5563153  12.75263465 22.52030299 18.23050191  9.97827156\n",
            " 40.48554985 23.07336483 24.22896414 28.24972882 17.39019379 31.6145689\n",
            " 32.12077929 21.25570307 39.05723565 17.08839218 11.22167846 18.34208431\n",
            " 11.7311889  24.39770573 37.01389863 23.31007554 33.55303805 48.53102474\n",
            " 13.42363437 19.26127199 21.84969575 16.35159104 11.06795239  7.40882981\n",
            " 18.89029472 15.85340361 28.4170345  16.59149351 20.2626296  38.45727159\n",
            " 49.97835908 22.98080916 26.43607588 40.04087047 25.94342144  9.97724561\n",
            " 34.87339868 10.41105271 14.36218906 20.94437019 18.48069928 14.13452889\n",
            " 14.67247278 37.92023688 27.54220194 16.98152241 11.68096032 16.88463644\n",
            " 37.85446855 32.10095461 22.57362844 13.1905711  26.66381952 29.58463338\n",
            " 20.49615097 13.42103348 10.19388958 15.65462243 18.41234132 27.45653095\n",
            " 17.66412271 13.61004779 18.87002027 21.36938846 37.64062927 28.59482323\n",
            " 39.75023004 16.9645999  33.03179074 13.32396019 14.35773901 49.9858119\n",
            " 36.58979582 13.13524607  9.25991227 33.22865946 15.98041452 18.7821297\n",
            " 34.92019825 18.41232066 23.43443274 14.10616653 21.96877428 38.07568536\n",
            " 21.26104785 10.51151674 49.20960685 13.36999338 39.17283255 23.79225362\n",
            " 18.31568887 24.48054405 30.10020662 15.80834022 29.17502665 13.48877109\n",
            " 33.43028415 28.99093538 14.01188916 11.25930919 30.32774146 17.8989525\n",
            " 28.16657279 19.76497159 30.14062117 17.50477234 34.95446144 14.1592133\n",
            " 13.98175112 30.06251032 16.30481448 17.2029174  38.62436419 22.14633176\n",
            "  8.91435154 46.64432773 24.76206766 11.13006637 27.7456043  19.73319053\n",
            " 18.04282046 49.58628697 24.17205179 19.49500258 21.13219189 40.42603367\n",
            "  9.33558934 23.02756081 35.08700202 34.64257633 44.55720303 20.91287426\n",
            " 32.45389533 18.68421358 47.72824618  9.2774698  16.35635424 28.1939324\n",
            " 21.47487886 18.30488985 27.74140405 17.27532554 32.6899309   9.2774698\n",
            "  9.2774698  39.48186755 27.50862265 36.18502389 36.99564764 45.34093201\n",
            " 13.36348368 10.24209244 25.4800779  23.83392518 13.98431766 20.61122755\n",
            " 11.53382413  9.27867997 13.37465835 18.08589453 44.12019978 16.30287689\n",
            " 21.07061327 32.83858774 14.40137739 10.10185819 11.07565623 15.59325156\n",
            " 24.36658308 33.30739283 21.31335139 35.68075518 35.65029241 31.10343358\n",
            " 36.08115741 16.20775763 13.86741524 13.9584982  11.07497879 15.84503466\n",
            " 49.95805106 46.12343972 34.08064614 19.82320252 47.61912555 27.94442584\n",
            " 18.28269092 17.14046847 34.94468917 29.19375427 25.08039351  9.2774698\n",
            " 24.04080992 11.89728771 25.08688009 13.10204211 23.73573106  9.2774698\n",
            " 23.85576991 15.47877017 17.92478435 29.94591529 11.95865794 26.25698014\n",
            "  8.99691977 17.81693217 20.24174009 19.43924169 32.18292244 21.18815389\n",
            " 32.23793377 49.85373306 11.65277688 23.4028772  12.86079808 22.50098228\n",
            " 26.84985961 32.84229943 20.70002774 20.32292252 38.57956344  9.2774698\n",
            " 30.23200269 43.42501398 17.53800572 26.11458438 42.56763578 12.3695247\n",
            " 17.1596126  11.47623399 15.56858287 19.61377169 27.0955317  27.09007469\n",
            " 35.49896404 22.64733362 24.92123765 48.30429294 19.27432341 15.02162119\n",
            " 30.37706323  9.60745788 23.68901857 22.14540982 24.50609196 17.94851022\n",
            " 32.54730747 22.18170054 19.93709221 37.61394997 29.7886958  18.7209003\n",
            " 30.94791628 15.85769276 10.44942514 28.43963838 15.27546179 35.2141076\n",
            " 25.14214861 27.1221473  32.27038903 41.25773729 46.96395066  8.87935804\n",
            " 15.71199263 11.1005906  26.25541442  9.69584587 48.77861797 17.34288842\n",
            "  8.00099085 16.40332124 10.0756004  26.25706798 42.98232679 28.10525125\n",
            " 20.21133594 23.79482668 21.73585268 35.65411612  9.71255623 48.67592072\n",
            " 14.01780144 23.12129372 42.74567295 19.43095168 30.77189149 24.30004163\n",
            " 10.42807728 32.72394158 10.73864834 42.4239327  17.81145284 42.77722181\n",
            " 20.33485201 28.42646572 37.10049431 24.82969242 48.65696195 20.2708733\n",
            " 17.74148918 10.49992833 25.02671205 25.47485862 13.74628528 12.94772221\n",
            " 21.89161313 17.37798712 27.48310423 33.93160486 19.9649716  26.64298376\n",
            " 41.98973941 37.43000406 21.48263138 34.48515605 10.91284118  9.2774698\n",
            " 43.66835548 38.30349442 23.81452468 15.23741793  9.2774698  16.63088677\n",
            "  6.22401839 13.98431766 27.02514903 23.77455608 29.97633262 29.40245732\n",
            " 20.07177831 23.07336483 20.42449931 19.90541305 23.79810467 12.00456572\n",
            " 38.88247258 14.36629276 18.25860455  9.6814159  16.48693475 11.51044826\n",
            " 32.19364117 25.67454549 18.06394209 30.69897229 12.58475962 36.86166679\n",
            " 27.52428418 20.64302934 17.16560891 22.25508196 14.93617676 20.08791767\n",
            " 11.25757645 15.22234675 25.47161672 27.21516362 35.93238747 19.05788052\n",
            " 32.15751235 22.39000908 25.20354617  9.2774698  19.88681733 28.39214945\n",
            " 30.27080867 35.04670382 29.43477753 11.35511322 37.73711808 24.07053401\n",
            " 23.79345978 32.2516502  31.19713849 33.0682867  15.44977802 19.84661163\n",
            " 18.14265286 49.13732215 21.81816471 20.20509734 33.91230799 10.92798326\n",
            " 22.10406727 19.33613758 10.94839283 31.24014782 37.18384504 48.72429268\n",
            " 15.75744207 17.14247157  6.50437867 31.2594545  34.21335334 49.1434735\n",
            " 12.94583795 24.12484659 46.61735481 15.56384103 19.36955663 10.74665958\n",
            " 21.18758043 13.80223504 20.21069885 21.29995732 19.06203383 24.54225587\n",
            " 16.84568619  9.2774698   8.89816716 24.30968133 14.74328874  9.2774698\n",
            " 49.99939221 19.6564186  22.18645882 41.75956397 20.70971586 17.05060877\n",
            " 49.56830084 20.7855842  13.80778381]\n",
            "selection [ 85  40 828 896 527 298  59 774 115 445 240  41 129 159 767 914 618  47\n",
            " 443 708 237 150  70 578 647] (25,) [5.84717208 6.0890611  6.22401839 6.50437867 7.40882981 7.6252034\n",
            " 7.71296722 8.00099085 8.05439381 8.08592174 8.19779804 8.53499043\n",
            " 8.68140463 8.80942545 8.87935804 8.89816716 8.91435154 8.96653846\n",
            " 8.97010414 8.99691977 9.07362644 9.12611339 9.16297954 9.25991227\n",
            " 9.2774698 ]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [156 244] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "std (902,) [40.7798457  14.84977867 18.49488784 20.7983845  23.65725011 25.51473669\n",
            " 15.12100108 15.89531543 44.63866253 47.21306253 46.27249336 41.81509886\n",
            " 26.26853946 15.11234715 22.07413094 18.53820064  9.98801741 38.62844325\n",
            " 13.9745778  45.95271882 18.5460657  10.35697835 32.77462944 20.25254501\n",
            "  9.98801741 30.45203587 18.28492504 23.06013685 32.33664525 32.56192833\n",
            " 28.31920653 43.33872915 13.40099775 10.87029834 16.90954778 19.18137633\n",
            " 25.11667867 23.14478669 19.57764671 29.65389143 12.40907322 23.76035615\n",
            " 21.62465977 25.44476128 36.15366869 46.10672406 30.27850554 11.38267284\n",
            " 24.28368957 14.5448906  20.60171186 16.95243831 12.88046602 40.29796666\n",
            " 22.86675162 44.77872635 14.07417449 25.72064885 13.67315055 29.1162448\n",
            " 31.68043099 25.36562034 21.91320723 14.29601643 38.17864783 27.46755864\n",
            " 28.13024483 17.57353396 14.77591644 24.12901435 49.85317859 11.7546629\n",
            " 37.55141559 17.19577591 24.14291408 13.67989997 16.47952998 29.19888071\n",
            " 16.43535091 26.78033641 32.3803163  17.82009175 27.36854356 37.00526617\n",
            " 37.98432262 13.97501507 33.29966874 29.44533533 49.82363795 37.2756229\n",
            " 14.21707646 23.71576089 49.96220411 39.28700991 36.52926071 19.03686557\n",
            " 26.97808933 21.25284604 25.07500929 12.83499452 24.4973125  36.2226358\n",
            " 34.35434011 28.6912055  15.72564526 24.28158529 16.91660986 31.83246393\n",
            " 16.88388901  9.98801741 26.44814519 16.66564115 44.55128741 25.7131502\n",
            "  9.45562087 42.58517862 17.91831098 28.87405505 22.07825498 16.62721982\n",
            " 26.0466946  49.81312085 28.72105284 13.39118804 20.1872052  14.26296706\n",
            " 12.43185858 10.70178576 49.99651558 17.79393811 36.04259128 28.21167361\n",
            " 19.6434562  30.83564116  9.98801741 17.51433078 34.68849927 16.02641348\n",
            "  9.42884822 13.67397873 25.94609959 15.85555356 31.50089603 19.03339492\n",
            " 16.37050028 36.22590085 16.71610668 14.77650033 18.78965968 16.51956198\n",
            " 14.23454224 28.56930248 33.32169418  9.98801741 21.31296856 12.9351874\n",
            " 12.20940366 15.15919102 44.3588173  16.82415477 22.52168822 22.31047297\n",
            " 30.16478881 14.7631045  10.87224403 26.77748725 21.33098468 49.05239308\n",
            " 18.09867667 24.32163546 22.75362654 19.35550266 28.98426069 11.49957032\n",
            " 24.42448774 39.90207942 45.72429736 25.50467196 16.37915374 14.18615553\n",
            " 30.50462051 48.54817594  9.98801741 35.66645656 18.99398457 10.82220609\n",
            " 29.77659299 14.08227259 15.84383327 34.31218708 22.23989035 26.92094757\n",
            " 37.17531676 14.47460695 12.64762286 15.49115338 44.79923399 15.2921598\n",
            " 30.27442789 28.9854664  37.08338351 13.42009898 24.09794417 33.63135863\n",
            " 12.82028178  9.56827883 17.82549265  9.87709938 19.02132231 36.90434842\n",
            " 26.23551369 32.05781316 13.0536533  25.50352359 32.96454812 14.47069973\n",
            " 15.11778245 20.69835344 23.48671319 13.38116712 22.3954152  20.81073679\n",
            " 29.93606722 30.62887673 25.00217299 10.73179002 24.80351982 48.71486902\n",
            " 16.31479969 13.75375148 30.74090807  8.51499574 23.67407478 20.15101724\n",
            " 33.45163768 32.11735227 33.69382817 19.82878958 15.14239572 43.4347534\n",
            " 31.68019917 29.14702182 23.9648292  49.72543943 17.94209397 21.68256336\n",
            " 31.53948682 17.4537314  23.49911697 24.15488748 19.35318503  9.98801741\n",
            " 49.97811289  9.98801741 38.72877318 31.35346353 41.28460613 27.82058831\n",
            "  9.98801741 29.85346847 42.45043241 10.03176724 24.25237411 47.58256309\n",
            " 13.82404793 15.01747151 19.56120415 20.83253254  8.76244426 30.27167569\n",
            " 16.72761281 22.37677274 21.90548072 43.42580262 11.02530424 49.58835328\n",
            " 21.05249867 19.67021864 21.00153494 15.2325876  15.60387773 18.69635839\n",
            " 35.79242798 14.72648128 17.98338137 20.22510273  9.98801741 29.63701369\n",
            " 20.96230752 27.46755864 30.41847822 13.65733539 17.52711255 28.9917362\n",
            " 22.0126879  20.55213095 11.22959221 26.27940727 27.09899837 22.02021295\n",
            "  9.70047472 18.91919184 32.75701506 27.33062302 29.43244068 19.36945159\n",
            "  9.98801741 36.93355069 19.67688948 31.27789025 18.00491937 18.39119046\n",
            " 10.04553494 28.54915177 24.54407766 27.53683346 15.65772301 25.4493297\n",
            " 13.88865567 13.26007428 11.49213313 29.62033212 20.06627338 21.98291865\n",
            " 34.92182433 16.42303491 16.99105964 12.99633423 19.75940995 21.22274083\n",
            " 25.44213118 15.74456345 27.55672193 14.8014921  26.81695654 31.92623017\n",
            " 14.4025807  15.40053502 33.07775335 14.59392906 19.57397129 49.89226007\n",
            " 21.44648808 20.02957885 22.0164045  48.19897849 24.18485639 40.7698561\n",
            " 22.29008864 25.46273957  9.98801741  9.98801741 19.77375897 24.37858195\n",
            " 21.41080122 27.19890547 17.45231848 23.24895569 30.22053641 16.34335822\n",
            " 18.21197909 18.47479401 14.15880747 17.72789371 21.20883047 21.33517452\n",
            " 31.94201742 12.90304299 15.42322717 47.95194339 28.227079   30.86831375\n",
            " 23.95776484 25.77557288 17.53585539 23.27020377 21.00491051 13.31631348\n",
            " 38.66625384  9.74591077 25.0265812  23.72135422 20.93510396 16.51920599\n",
            " 28.64926742 35.91640889 19.50684795 24.40422847 17.54468348  9.98801741\n",
            " 21.00412597 38.12616509 16.08384909  9.17174239 34.17602857 48.36275909\n",
            " 32.75352157 39.46312232 19.82005661 34.44644021 41.8055547  26.9378174\n",
            " 26.50532962 16.24887737 34.15034818 21.69507725 29.38414244 30.50178464\n",
            " 11.03474082 21.55960008 40.17200272 23.28934983 21.99976763 25.89832005\n",
            " 19.62610821 11.9755052  13.15059058 23.7142317  29.69897554 48.22173344\n",
            " 10.56645388 17.67034569 10.99619921 23.93113706 23.54349276 30.51706504\n",
            " 17.40180526 44.82153475 49.9890225  26.05322965 11.90075234 22.96991769\n",
            " 14.61619588  9.90200267 24.63804951 14.03268811 12.63583201 21.32381883\n",
            " 48.72073397 23.24764285 42.90773976  9.98801741 14.56939936 30.13128173\n",
            " 18.56284643 13.32945377 15.20673899 23.67622607  9.98801741 19.57128655\n",
            " 21.70546665 42.4647593  12.76267499 37.27865364 23.09856564 31.87111746\n",
            " 23.94218038 13.35646035 27.16618276 14.62700376 14.44256794 20.3274506\n",
            " 19.25314097 15.55645773 16.93897388 18.11303018 18.72298961 17.02870255\n",
            " 27.69447117 33.75273777 26.56545872 10.00590634 22.68899416 40.46813664\n",
            " 33.12828648 30.48963662 28.58731638  9.98801741  9.98801741 26.04905541\n",
            " 17.28061405 17.8480394  33.80225353 30.98448158 26.77837377 13.60078694\n",
            " 22.483795   20.13107439 10.92157102 41.10907395 23.30407078 25.21074652\n",
            " 29.11210853 17.92475244 32.27376586 32.72949601 21.14572055 40.16094519\n",
            " 18.04527753 11.49213313 17.61619424 11.04613555 24.48190905 35.4851117\n",
            " 24.58122297 34.16094044 48.65230248 13.33850496 17.71247911 21.42067367\n",
            " 17.02870255 11.0186353  17.59378135 15.5194637  28.78339631 17.50648654\n",
            " 20.36022579 38.58806772 49.99186068 23.8070747  27.19906411 40.84065955\n",
            " 26.03884767 10.83899491 35.08239134 10.89883004 14.89187873 21.00140343\n",
            " 18.25876294 15.70466119 14.10641957 38.99091763 28.89581152 16.97767064\n",
            " 12.22086329 15.58388357 39.34535501 32.36822324 23.29476166 13.63032251\n",
            " 27.71462256 29.29145843 22.15216611 12.41310012  8.99241848 14.85598133\n",
            " 21.20555974 27.35007579 17.27299559 14.7594535  20.18065221 21.76750157\n",
            " 38.32402146 29.79101522 39.71585727 16.9425297  33.42626393 14.31619371\n",
            " 13.48650044 49.99364574 37.5722842  12.96350139 35.3674592  16.61633535\n",
            " 18.57025476 35.76875458 18.32866577 23.30388845 16.96728468 22.51588984\n",
            " 39.05430652 20.44673017 12.18766077 49.49968222 12.98708191 39.95784072\n",
            " 23.88675184 19.79241281 23.99086373 29.52645486 15.57100627 30.34648691\n",
            " 13.18816833 34.30634754 30.97612906 14.27428958 10.88638533 30.07195826\n",
            " 17.03570073 28.12467629 18.87773829 30.73367379 18.00464614 35.08679643\n",
            " 15.06288344 15.15012123 30.41049092 16.89836328 18.43251563 39.63215376\n",
            " 22.57681562 47.20247081 25.33117035 11.36614905 27.50924345 21.3795806\n",
            " 18.69635839 49.70557957 25.04193289 19.17233945 20.86793    40.91462265\n",
            " 10.29402771 22.4115411  36.59783337 34.98264958 45.36943758 20.19113895\n",
            " 33.21491733 18.20748192 48.06663251  9.98801741 17.38750343 27.65751002\n",
            " 22.8119391  17.65118958 30.49341702 17.51664307 35.22656338  9.98801741\n",
            " 39.81134412 27.24248355 37.66113593 37.53461333 46.27721598  9.047452\n",
            " 10.1944193  27.11001181 25.09238262 14.31675597 20.85928166 13.51455928\n",
            "  9.22110615 12.54919871 19.43375902 45.11362138 16.96830761 21.85700284\n",
            " 33.68574178 15.00568192  8.5151479  10.82843375 15.23974042 25.67841548\n",
            " 35.21874819 21.2803873  36.73761808 35.93201986 31.48909305 36.73313696\n",
            " 17.04942175 14.81004344 14.5284369  10.13722145 15.02712867 49.97573361\n",
            " 46.29067201 35.15804808 19.68479559 48.08631666 28.60921863 17.61315105\n",
            " 16.96129035 35.58598744 30.12408954 24.78789903  9.98801741 23.78746029\n",
            " 11.59430244 25.53255399 12.41004922 24.8635151   9.98801741 25.21416488\n",
            " 16.01477039 19.25582124 32.97284232 11.96007534 26.22568637 17.50039009\n",
            " 21.25705324 20.60061717 33.02771285 21.0465687  33.8924568  49.89628757\n",
            " 10.33022845 24.0645359  12.43372429 22.08030946 25.47546488 33.80731983\n",
            " 21.99573327 19.71744777 39.49006084  9.98801741 30.31103185 44.44802727\n",
            " 19.05205891 27.36418636 43.02432447 12.02389489 17.33192393 11.75629705\n",
            " 16.27386889 21.14667349 28.07246894 27.39121625 36.20275947 23.10522308\n",
            " 22.96865274 48.71157595 19.4559633  16.18584946 30.78563162 10.43863083\n",
            " 24.8141778  21.57722265 25.40212488 18.17304661 32.73007144 24.47839683\n",
            " 20.3274506  37.39611613 33.09933459 19.5960777  32.17071169 16.87631717\n",
            " 13.39281096 29.45165792 14.93882827 35.88840102 25.17582771 25.67976514\n",
            " 33.35938908 41.68516821 47.31749733 15.44692285 11.90921988 27.58413007\n",
            "  9.71608981 49.09017585 18.65868055 17.64813194 11.23137982 26.23678607\n",
            " 43.23329942 29.97398901 20.55468917 23.75826053 21.85970969 36.73715412\n",
            " 10.24580198 48.96238774 14.85437252 23.67706151 43.26802401 19.10782316\n",
            " 32.65101958 25.40706846  8.77677364 33.63135863  9.55307803 42.8685168\n",
            " 16.68303753 43.47611385 21.70489778 28.73107533 37.89986016 25.71006873\n",
            " 48.8950516  22.90097078 17.79230014 11.70411748 25.82464695 25.6026397\n",
            " 14.44124999 12.46848528 23.59260587 16.91685745 28.60811517 34.27847726\n",
            " 20.08436195 27.85699178 42.94418286 36.77139306 21.78287232 34.8412293\n",
            " 10.89000061  9.98801741 44.55270202 38.61717791 24.59272398 14.99603188\n",
            "  9.98801741 17.20114676 14.31675597 27.42707396 23.44321059 29.88304242\n",
            " 30.65693559 19.02132231 23.30407078 20.103639   21.00949449 25.13283502\n",
            " 11.2445771  39.0978057  13.67989997 18.00613186  9.83636091 16.54377004\n",
            " 11.3687944  33.62020524 25.2504862  17.42165986 29.82109973 13.86057971\n",
            " 42.12336881 27.55090677 21.36874535 18.15412227 22.08502375 15.99177724\n",
            " 19.85825709 11.32906729 16.49543665 26.40525883 28.61943355 36.71634266\n",
            " 18.64280545 32.60459617 22.56269795 26.02911381  9.98801741 21.5346847\n",
            " 28.92372025 30.21115247 35.54225231 29.77018864 12.25796848 38.23622868\n",
            " 24.56264699 25.72405341 32.63047384 31.94887975 35.51399176 12.23993252\n",
            " 23.3726811  18.12850227 49.22488711 22.20047304 22.90111339 35.70654577\n",
            " 11.95458853 22.7255593  19.55250641 12.47454315 31.11398067 37.89404091\n",
            " 49.09670251 15.62126882 17.62025536 32.22036781 33.9583756  49.26662513\n",
            " 12.39374436 23.14822546 47.07122471 17.28585954 19.08606403 10.40122861\n",
            " 21.50163318 13.44164074 17.61465836 21.72971875 18.82180902 25.44887108\n",
            " 18.44760699  9.98801741 24.08842092 14.39591692  9.98801741 49.99977127\n",
            " 20.58323801 22.32918059 43.29939122 21.56647254 16.62921244 49.64595941\n",
            " 20.07281051 14.41246571]\n",
            "selection [231 650 268 770 544 635 393 642 138 114 772 205 300 750 379 820 207 433\n",
            " 629 351 844 621 350 109 799] (25,) [8.51499574 8.5151479  8.76244426 8.77677364 8.99241848 9.047452\n",
            " 9.17174239 9.22110615 9.42884822 9.45562087 9.55307803 9.56827883\n",
            " 9.70047472 9.71608981 9.74591077 9.83636091 9.87709938 9.90200267\n",
            " 9.98801741 9.98801741 9.98801741 9.98801741 9.98801741 9.98801741\n",
            " 9.98801741]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (425, 10) (425,)\n",
            "updated train set: (425, 10) (425,) unique(labels): [169 256] [0 1]\n",
            "val set: (877, 10) (877,)\n",
            "\n",
            "Train set: (425, 10)\n",
            "Validation set: (877, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.11764705882352941, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0]\n",
            "std (877,) [41.13474063 15.96028616 19.04209268 21.66016905 25.3948323  26.3994694\n",
            " 15.86021692 15.61611913 45.58049934 47.28642214 46.54928208 43.30786878\n",
            " 29.06084915 18.48418719 22.96280981 18.47845639 10.00904515 38.9952325\n",
            " 14.61228344 46.22170358 20.73096958 10.58411355 35.0071239  21.04864452\n",
            " 10.00904515 32.66318436 19.42693966 24.65989787 32.05388037 34.48436048\n",
            " 29.57271938 43.94818393 15.01909147 11.43305452 16.80658062 19.02380711\n",
            " 25.16850826 23.38611081 19.8030301  31.72498532 12.61795532 24.38732401\n",
            " 23.20418517 27.47003818 35.06941766 46.38703381 32.56096087 12.68380676\n",
            " 26.81509366 15.37769918 21.85739257 18.24880581 15.13575606 42.3090842\n",
            " 24.92715157 45.06231669 14.69487691 28.13314683 13.15461741 30.53671046\n",
            " 33.30830166 24.99425958 23.05220871 14.7118336  40.91192356 27.59212263\n",
            " 30.29803039 21.75946152 15.90120758 25.09764408 49.86461808 13.85728585\n",
            " 38.26403538 19.37270448 28.22763239 14.88469085 16.87226987 29.26086498\n",
            " 17.8982283  29.40171556 34.29611646 17.08788328 26.00649688 38.11110715\n",
            " 39.3100535  13.86145771 34.77549286 29.98687385 49.83160563 38.05923008\n",
            " 15.67221232 25.27705913 49.96335875 40.81039145 38.93432886 19.4026473\n",
            " 27.44363616 25.4614087  25.10657804 12.17180241 25.91436416 39.31759365\n",
            " 34.4714779  29.61723109 16.77723748 25.04222312 17.64642759 32.33434401\n",
            " 18.48379684 29.22801204 14.80859172 45.08482671 28.24273941 42.67458868\n",
            " 19.11667561 29.88615421 22.99029838 15.7414151  26.9971897  49.82351117\n",
            " 29.67112137 13.06454526 20.14498453 16.00654573 12.65518815 11.03628199\n",
            " 49.99671336 19.93317687 36.27998044 29.88613692 20.27156516 30.69320397\n",
            " 10.00904515 19.18926721 35.77367108 15.65005075 14.39396143 27.58064551\n",
            " 16.14682771 32.96542913 19.5788679  16.50945101 35.22181911 17.59221832\n",
            " 14.22322166 20.74750139 18.89236282 15.14193615 29.2433774  32.92528447\n",
            " 10.00904515 22.12243049 15.15652571 17.64808485 15.85152314 46.34964846\n",
            " 16.53981663 23.20896233 22.14255642 28.9303147  15.33474508 13.24005899\n",
            " 29.0871846  20.41475807 49.50041393 18.6032309  25.03285838 22.41453904\n",
            " 18.8003944  29.37907875 13.11383975 26.04025561 39.96040676 45.82473667\n",
            " 27.22431503 16.11507182 14.40963292 30.93925588 48.81911514 10.00904515\n",
            " 36.90670214 19.05843249 10.36222377 30.85854308 15.82188588 17.38757658\n",
            " 35.27390629 21.59564775 28.26835987 36.9007359  15.28842192 12.64002377\n",
            " 15.89771259 45.41652063 17.31397052 32.33264668 31.02134687 37.97576632\n",
            " 14.67505517 24.852739   33.07226636 12.75602213 18.99299489 19.42799591\n",
            " 39.31558627 26.36213669 33.14601723 18.12107088 26.38068662 34.03796279\n",
            " 14.9523686  18.07057489 21.47776017 26.53961904 16.2245477  22.95147193\n",
            " 22.38611274 31.21297735 33.0450647  24.15448934 12.51964206 26.61148595\n",
            " 48.78694378 16.94049235 13.9594142  31.98639115 25.59767179 21.08520588\n",
            " 36.87841172 32.81236478 35.02725553 20.65465501 14.92014061 43.76157274\n",
            " 34.53126087 31.51873808 24.97394071 49.73997322 19.0825954  24.2987323\n",
            " 34.12622108 17.57144025 25.56350305 24.55828389 21.33036094 10.00904515\n",
            " 49.97907944 10.00904515 39.12750899 33.92610779 41.82709125 28.88145667\n",
            " 10.00904515 32.04007206 43.55306551 11.86949239 24.81971634 47.7818018\n",
            " 14.94326221 16.70116611 21.33905932 20.96417633 30.53276526 16.35951096\n",
            " 24.84656446 22.1667669  44.9696603  11.15501713 49.60895551 22.02106076\n",
            " 21.39731077 22.31347857 17.13558373 17.20907961 19.31512308 35.79850793\n",
            " 15.09977498 19.51164456 20.3765319  10.00904515 30.74782215 23.08434551\n",
            " 27.59212263 33.47636397 14.71842317 17.85627022 30.61452709 23.21792601\n",
            " 22.52874701 11.17190662 28.00653774 29.87087963 21.97246575 20.46746665\n",
            " 33.92700724 28.58084361 31.2304926  22.21089906 10.00904515 38.89393955\n",
            " 21.3124484  31.23488577 18.71538399 17.42882435 10.62174272 30.26219299\n",
            " 26.73699528 29.4570717  17.66755079 26.02073386 15.08152084 13.56066011\n",
            " 12.0706534  29.89166543 19.97486998 22.00754828 36.55521713 16.79232798\n",
            " 17.42748636 14.09274503 22.07164533 22.43987366 28.50461963 16.57055398\n",
            " 28.80474485 13.06857997 29.60827447 33.62880087 17.69872279 14.43591018\n",
            " 34.93589502 15.10073279 19.71218907 49.89611351 22.45534594 20.5747844\n",
            " 22.93173454 48.74332661 24.75677773 41.31762635 25.10396025 28.02013079\n",
            " 20.74033312 24.50817357 22.78054798 27.8209708  19.68358485 25.7526539\n",
            " 29.49074917 20.5546285  19.27357453 22.0466883  14.83276353 16.47842706\n",
            " 22.47140061 22.4952224  34.25464531 14.05114583 16.42656683 48.20404952\n",
            " 30.69800733 31.48957278 26.4284034  25.84530295 18.77574857 24.64954801\n",
            " 21.44981828 14.98878259 39.47983739 26.07024172 26.82489843 21.83451233\n",
            " 16.90918257 31.15614842 36.97380341 21.06410693 25.37564837 19.48266259\n",
            " 10.00904515 22.22658263 38.51162259 16.21657927 30.19462938 48.32524247\n",
            " 34.82422537 40.28253252 19.73561959 36.81397552 41.4610317  27.92558523\n",
            " 28.69372523 16.90127518 36.82886082 21.45634652 32.60593988 32.1912326\n",
            " 12.20117122 23.92680116 40.43980738 24.20605111 23.26159157 26.98974475\n",
            " 22.47883541 11.94495974 14.10815487 26.92032419 31.53480277 48.26696619\n",
            " 12.38383872 18.49034612 11.47541362 25.89696214 23.79624335 31.53154714\n",
            " 17.76626341 45.11856581 49.9897573  27.74347862 13.64068431 23.20653426\n",
            " 13.14773365 26.96754272 15.23842951 13.55915922 23.17623888 48.78724968\n",
            " 26.59146192 44.67144751 10.00904515 16.05194094 32.98109854 19.17065991\n",
            " 14.12635644 15.3692249  24.27045643 10.00904515 19.60437982 20.98384836\n",
            " 42.64111269 11.50260446 38.55552454 25.6070137  31.4977454  25.67983435\n",
            " 14.6826803  28.60274928 16.01224541 15.06423209 21.60834287 20.26733203\n",
            " 15.90437498 18.48479211 19.11686761 21.91155758 17.47664971 29.30419897\n",
            " 34.93567495 28.35754448 11.59910047 22.60740996 41.86654413 35.96218592\n",
            " 32.84258046 31.09912101 10.00904515 10.00904515 26.45074664 20.56455345\n",
            " 19.37122363 35.47202112 32.68958601 27.17768242 13.30097219 24.17582823\n",
            " 19.62933835 10.88161171 42.82881198 23.86819725 25.00862169 29.46253552\n",
            " 19.77408047 34.58291979 35.02244375 22.10783755 40.97788916 19.75536329\n",
            " 12.0706534  20.29259701 12.07798171 27.61191584 36.93912252 26.50063437\n",
            " 36.23412256 49.19107834 12.40356472 21.65302332 23.64626546 17.47664971\n",
            " 14.70490061 17.44363335 15.53270983 30.76883809 19.33624733 21.69636397\n",
            " 40.27504896 49.99235558 24.76750415 28.02891058 40.90618669 28.88771222\n",
            " 10.50196507 36.10387068 12.55736123 14.57479297 20.34089604 20.45023954\n",
            " 16.73013492 14.18687993 40.02327391 29.70882084 20.80122131 12.7001957\n",
            " 16.99855194 40.25986732 34.26960017 23.73403744 12.32148795 27.8460805\n",
            " 31.58535395 23.11458099 12.79225151 15.17512518 22.35243862 28.74321224\n",
            " 17.88696402 15.96321959 19.22126456 24.34429307 41.3121692  30.89827514\n",
            " 37.87030635 19.29609972 34.77771896 12.20624113 13.30278392 49.99421925\n",
            " 38.14554075 12.77372885 35.51521932 17.57995826 22.38212231 37.45168786\n",
            " 18.84077186 24.40227717 18.29024141 23.13339913 38.39187064 21.43065559\n",
            " 11.91980891 49.41458145 14.11876674 41.06795822 24.75024576 22.60792419\n",
            " 24.56407283 30.28815592 16.06964437 30.44655578 17.20344249 34.0887006\n",
            " 31.65277609 14.89977551 11.89845595 32.31251247 19.29234923 30.08945667\n",
            " 19.46616736 33.88373706 19.25674798 35.23658796 15.87865179 16.00777142\n",
            " 32.74528847 17.66911976 17.60782614 41.99497952 23.14810921 47.56801382\n",
            " 26.52507405 10.51025613 30.90420038 21.31989851 19.31512308 49.73564279\n",
            " 26.13504267 21.37653469 23.64680133 42.16426396 10.35856467 26.42512231\n",
            " 36.07568809 37.44900742 45.65595882 21.68005222 35.81178746 16.9562417\n",
            " 48.07406007 16.82648309 29.36830583 23.21006923 18.55539338 31.74796855\n",
            " 17.34437568 35.4533962  39.91755715 29.5161134  38.48678383 39.48171158\n",
            " 46.61013794 10.1795501  28.82158446 26.70305748 14.76277059 22.88704899\n",
            " 13.63110307 13.66127953 19.01261991 44.50496811 16.28058752 23.26710499\n",
            " 35.8384866  16.05018651 10.53194578 16.45099974 26.90506971 34.17245333\n",
            " 23.48223251 37.4957543  37.05385623 32.8282091  39.39397574 17.26072641\n",
            " 15.16606823 15.51588539 11.60554364 16.2865392  49.97774768 46.53778284\n",
            " 35.97563089 22.01599393 48.07272879 29.9980498  19.45687617 17.07155392\n",
            " 36.64093487 31.03573276 27.00468596 10.00904515 24.57102073 13.75447178\n",
            " 26.28546143 11.744494   25.87489975 10.00904515 25.94729537 17.17178233\n",
            " 19.54152788 33.18284438 13.21326212 27.80539674 19.4271048  23.82845263\n",
            " 21.98371566 33.96493368 20.7422897  34.05688237 49.90102832 10.96324058\n",
            " 23.24453793 12.55663615 23.63306985 26.57282968 37.59926327 22.85202645\n",
            " 19.06626276 40.32610433 10.00904515 31.62693981 44.8122111  20.06953419\n",
            " 29.37000327 42.42832109 11.73157739 16.97108413 11.47711025 15.91085621\n",
            " 21.1407766  29.8498837  26.41964009 37.62365491 23.38100108 26.94566955\n",
            " 48.79750907 19.93137499 16.5989542  33.12128757 12.00734649 25.84849575\n",
            " 22.20075654 27.41893368 19.91625304 35.17388904 22.82661697 21.60834287\n",
            " 40.09954457 34.28457509 23.81941246 32.53283817 17.55080154 13.0506646\n",
            " 31.47219754 13.72782886 37.66114465 27.06305226 30.63672975 36.01804266\n",
            " 40.50978193 47.55381052 19.07961682 11.17101983 29.69793212 49.1180614\n",
            " 18.49079628 17.53552934 11.91648463 26.99077288 44.13474184 33.32461954\n",
            " 21.04382289 23.77044894 23.00845347 38.62794001 11.92260042 49.03297985\n",
            " 15.38998722 23.82624911 43.90013495 19.32688312 33.56057422 26.47218231\n",
            " 33.07226636 43.66502675 18.09768769 44.8458021  24.99401238 29.53457859\n",
            " 39.1987512  25.54910802 48.98078819 25.17413876 18.25516209 13.07551446\n",
            " 26.59623447 26.95019049 16.20235943 13.21686766 23.0421948  18.23566936\n",
            " 29.49576256 35.93952292 20.90944534 29.02918532 43.61419635 38.88972192\n",
            " 24.30023216 34.63446134  9.41580987 46.27317557 40.4682811  25.95707205\n",
            " 15.14792973 10.00904515 18.50708299 14.76277059 29.93528478 25.38577878\n",
            " 31.61326812 30.70799298 19.42799591 23.86819725 20.21374787 21.70100376\n",
            " 27.41622061 11.25757893 40.74538182 14.88469085 20.88365689 18.29479144\n",
            " 14.10287391 34.81930433 27.58019712 17.43671345 30.84705322 13.70892657\n",
            " 40.73182325 29.1130872  23.03204854 19.18165175 23.40644095 16.55149197\n",
            " 20.56446173 11.80774722 17.10352663 27.67759071 29.66832371 39.05106569\n",
            " 19.57035444 35.19128084 25.07954514 27.12344073 22.61003726 30.78193672\n",
            " 31.72587849 37.02559854 32.07351596 12.62073043 40.07780438 27.51368804\n",
            " 27.79223054 34.59050576 34.09919703 36.10781322 15.94706233 24.3135657\n",
            " 18.57323193 49.30194038 22.6489962  23.92796164 36.74061375 12.20742838\n",
            " 23.76447567 19.8123162  11.90292053 31.70154192 36.91065035 49.11323374\n",
            " 14.99360396 20.01533429 33.50876322 35.78746199 49.32465742 13.64147158\n",
            " 26.52823467 47.21035231 18.25469854 20.84655951 13.58302166 21.28678659\n",
            " 17.19453618 22.202682   23.26211937 20.43875477 26.03048909 18.51789171\n",
            " 10.00904515 26.88391922 16.87404218 10.00904515 49.9999861  20.71596747\n",
            " 23.75674029 43.82353224 22.98497792 16.43110148 49.69975546 23.42564251\n",
            " 16.27015558]\n",
            "selection [776 686 657 663 245 247 252 279 298 864 132 378 781  24 867  16 150 179\n",
            " 435 428 464 465 619 598 182] (25,) [ 9.41580987 10.00904515 10.00904515 10.00904515 10.00904515 10.00904515\n",
            " 10.00904515 10.00904515 10.00904515 10.00904515 10.00904515 10.00904515\n",
            " 10.00904515 10.00904515 10.00904515 10.00904515 10.00904515 10.00904515\n",
            " 10.00904515 10.00904515 10.00904515 10.00904515 10.1795501  10.35856467\n",
            " 10.36222377]\n",
            "trainset before adding uncertain samples (425, 10) (425,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [193 257] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0]\n",
            "std (852,) [44.82860359 15.67521184 20.96135163 25.1380471  29.73709378 29.72300875\n",
            " 15.75753918 18.0769967  46.28760276 48.66265953 48.33823484 44.00234535\n",
            " 32.23062897 19.00767333 26.51576415 22.30967194 43.75296531 17.94487958\n",
            " 48.15378363 23.70748099 14.52654728 37.43131083 23.19602284 33.66862504\n",
            " 21.11598668 27.28835727 35.11906849 38.96731446 32.61751289 46.31616545\n",
            " 17.24594228 13.37195526 20.57323012 23.19200962 28.4625039  26.62123626\n",
            " 19.4776113  31.9801734  11.84008919 27.78079471 23.3869421  30.35620429\n",
            " 40.0218616  48.66095032 35.75478224 14.6501993  29.12692928 18.79133161\n",
            " 25.00198938 20.19423066 15.57989522 43.3460514  26.85407675 47.41913264\n",
            " 17.85688748 29.79083051 16.56810617 32.9484148  36.2396325  25.67689703\n",
            " 26.70897631 15.83348734 42.00187842 31.03272485 32.94678047 18.10529086\n",
            " 16.33823735 26.74859467 49.98014166 15.38567692 42.15107081 21.66671251\n",
            " 29.80404154 18.84220088 19.27753471 33.66357224 19.23585049 31.34704628\n",
            " 35.91018033 20.1776324  27.76560553 41.38058644 42.08218604 15.2878982\n",
            " 37.90313796 35.4103365  49.9817306  41.00262812 14.1886142  27.7945751\n",
            " 49.99694324 43.85169806 40.27666235 21.7264696  29.54116476 25.78407425\n",
            " 27.82647195 14.91857974 28.13638535 39.64379113 37.3398709  33.53484248\n",
            " 18.81493304 28.47723418 17.55056268 35.51703965 20.43817314 31.08468873\n",
            " 17.53517931 47.43197638 29.78237179 45.55287803 22.33174289 33.75124922\n",
            " 25.4149201  18.92615375 34.36284871 49.98015414 34.01508168 16.78900113\n",
            " 23.05090977 16.11386502 13.12095591 12.51248514 49.99984328 21.7496453\n",
            " 39.96405001 33.41344443 24.14968738 32.88179192 21.30558555 38.47922199\n",
            " 17.51521082 16.89867269 30.9615475  18.88533784 35.65437284 22.46347459\n",
            " 17.19420314 39.44957713 20.58094737 17.09271359 23.58293936 21.00901401\n",
            " 17.82907167 33.33642037 35.22051305 25.41159635 16.71106972 18.01726241\n",
            " 19.35442828 47.02208811 18.50432264 26.57055123 25.43202737 32.93912589\n",
            " 18.83538906 13.53907504 32.95647171 24.75257301 49.60963568 19.76064556\n",
            " 26.65428092 25.66734766 20.04269447 32.50428347 13.97182035 28.69449425\n",
            " 43.96015608 48.01532547 29.31060087 18.7713725  16.76244786 34.12053909\n",
            " 49.41913209 41.19622153 21.60878809 34.77960044 19.12092536 18.56642863\n",
            " 39.38180435 24.55160504 31.04731462 43.0978761  17.72158801 15.54005878\n",
            " 14.56999145 46.95462785 17.58059063 35.70207082 33.22149431 42.65266746\n",
            " 16.39891769 28.09065166 39.12492333 14.90858935 20.21523733 22.56495054\n",
            " 41.9657776  29.55270335 35.56055316 18.89390552 28.84601674 36.68474249\n",
            " 18.50686289 17.94366311 23.95165    28.34242635 14.40139684 25.08759824\n",
            " 23.69502458 33.19315105 36.51598768 31.18081715 13.38069009 28.9339591\n",
            " 49.66533158 20.07696983 14.48526787 34.11040622 26.6666967  22.5229148\n",
            " 36.26808056 36.51375291 37.10812897 23.22283803 18.86122551 46.44974969\n",
            " 35.55896882 33.65330516 30.43913789 49.9434962  21.76348628 23.14876933\n",
            " 34.52364349 20.48613767 29.30747421 28.21702851 24.83496766 49.99800018\n",
            " 44.93039638 37.27212572 46.02148438 31.56046418 34.02800501 46.64054953\n",
            " 11.14610348 29.23817365 49.04808399 17.11642397 19.69097125 23.16515841\n",
            " 24.84902401 32.93145673 18.15103472 27.25487967 25.09303483 46.44028208\n",
            " 15.78918036 49.92271291 25.21957306 21.44779947 26.25537581 15.97134549\n",
            " 20.49582375 21.40120519 41.189154   16.93706773 17.94165547 22.12548818\n",
            " 32.13689318 23.31567078 31.03272485 35.29288114 15.62865352 21.11488029\n",
            " 34.16422725 25.39719432 26.35026316 14.21201548 31.29025339 33.27059769\n",
            " 27.84996597 23.09407324 38.51556139 31.96997799 33.78456089 24.28405496\n",
            " 40.27783022 22.77568235 35.50285858 20.66248422 22.51537022 13.62461805\n",
            " 33.62336844 28.05986606 31.51020595 20.06609191 29.75057748 19.34303861\n",
            " 16.69954543 14.31005173 33.31611032 22.81750125 25.30958057 38.10460184\n",
            " 19.68721649 23.68779559 17.18321555 24.23165986 25.74187293 29.41502197\n",
            " 19.39361863 31.25712346 18.02065389 30.26633853 35.14939766 19.28527974\n",
            " 16.61985027 38.11116332 15.79245486 22.40780166 49.98354022 25.34202303\n",
            " 23.31452587 28.1975272  49.19853238 27.50337283 45.57357413 27.20498381\n",
            " 30.38659857 24.18410938 26.89196348 24.04735475 30.84948501 20.85036736\n",
            " 27.84990699 31.59253842 18.0211783  21.20733517 21.30436277 16.26221063\n",
            " 21.1821634  25.59449914 24.75414013 36.26777276 17.78389439 18.62176069\n",
            " 49.31678562 34.12056324 32.81515036 27.82860205 30.59193271 19.85759068\n",
            " 27.4403286  26.09649063 18.12206193 43.16172853 29.8949206  28.37285488\n",
            " 23.85954226 19.80859804 32.0482509  39.17275171 24.80625577 27.64584996\n",
            " 21.55516033 25.25097926 43.11228965 17.12411438 28.74210213 49.4079208\n",
            " 36.53373051 42.72748298 19.67038408 37.5173573  45.99795018 30.95159169\n",
            " 29.77856874 19.84754263 38.99137801 24.66598512 31.52736736 34.14230366\n",
            " 10.73896279 27.63566961 43.5424162  27.43071302 25.96195254 29.390265\n",
            " 22.98754429 14.06953039 15.41445074 31.31016736 36.87991577 49.4300764\n",
            " 12.51569409 21.61389869 14.53827818 27.30569643 26.08084929 34.81536303\n",
            " 18.92831071 47.39654159 49.99924835 28.97703908 14.31427365 26.26097181\n",
            " 16.87697772 30.67861653 19.15501423 15.77987421 22.84520397 49.66546419\n",
            " 32.28438685 46.47099604 17.39200588 35.14741173 22.29874932 16.97733167\n",
            " 20.39571153 28.22000126 22.91817665 25.1026315  45.28720751 14.41025214\n",
            " 41.00539563 27.59333636 35.97818694 27.48569005 13.25596237 31.55574461\n",
            " 18.76878568 16.92881566 24.09086691 22.53320255 17.9412569  21.94147238\n",
            " 23.43295807 24.96412734 19.57980957 32.78782966 39.50553561 30.61051683\n",
            " 11.76547803 25.0715443  43.78209129 38.50962951 34.17624847 33.19896996\n",
            " 29.43307312 20.12520982 21.51142178 37.07034597 40.26165435 29.88826797\n",
            " 14.31656627 28.22111664 22.10585162 14.71442189 45.25195098 26.61482914\n",
            " 28.00210744 32.01983327 21.23156547 37.75447452 36.53885163 24.07735146\n",
            " 44.44127187 19.18320555 14.31005173 20.95347228 15.50986451 28.10117672\n",
            " 40.58568051 29.49539506 38.28739982 49.32222521 16.10985461 22.57490776\n",
            " 26.65994928 19.57980957 13.72036114 21.54663946 16.62729673 32.10304009\n",
            " 19.4428191  24.06643074 41.78063    49.99925736 28.43343616 32.97412735\n",
            " 45.58670414 31.45342918 12.64038643 37.84847856 12.01656946 16.77298882\n",
            " 23.64125226 23.74425848 17.95207509 17.75438881 42.82479111 33.25507312\n",
            " 20.35783161 13.08290585 20.84441776 43.25516201 36.40787261 25.63003332\n",
            " 16.46733027 31.61910865 34.29397127 25.77529786 16.57933756 18.42799817\n",
            " 25.2041827  31.25562593 21.04275415 17.00085631 18.40586463 26.82527028\n",
            " 42.33720774 34.4277214  42.72709651 21.7209412  38.6031419  16.43743305\n",
            " 16.22234953 49.99962527 41.9921393  16.83381883 39.54337083 19.51904403\n",
            " 24.03876626 39.17819759 22.21031764 26.42693431 20.15208903 23.59724155\n",
            " 42.77035619 24.29882078 13.31643274 49.69025216 16.04987264 44.75961473\n",
            " 27.79186878 22.55764214 28.03355107 33.18122667 19.4479199  34.18730492\n",
            " 20.13009474 37.37949317 33.87574881 15.31998782 12.87787274 34.66893671\n",
            " 20.03835222 32.83608094 22.6616817  35.60930799 20.83945733 38.75533437\n",
            " 19.47315912 18.09059187 34.64957389 19.23467921 17.51742771 42.68145323\n",
            " 25.95295437 49.00785254 27.29104358 12.98304105 33.8265713  26.60181176\n",
            " 21.40120519 49.95403049 29.45408165 23.50689262 26.7052054  43.68186686\n",
            " 30.19167556 40.55735384 38.9697133  48.02682321 24.59150306 37.79235167\n",
            " 17.76011653 49.45994128 19.47120261 32.63286515 26.04542768 21.7456632\n",
            " 36.25213821 19.5692165  39.27330119 44.16341835 31.2661813  42.00464306\n",
            " 40.93937302 48.64029679 31.76363075 29.36018351 17.02009861 25.81600201\n",
            " 14.67415657 17.42860518 21.36311531 46.69306823 17.91797683 23.34981774\n",
            " 38.29228566 16.74410942 12.28537865 19.07201485 29.75135145 36.96917422\n",
            " 27.00873643 41.46584722 39.83577842 35.03780648 40.99571242 19.36392034\n",
            " 16.68140147 17.14239762 13.30750544 19.15298917 49.99783492 48.98356831\n",
            " 39.08301591 23.88044459 49.18284858 31.82160397 23.21441012 19.40694919\n",
            " 39.46240084 33.98292124 29.84093869 28.39597694 13.98380953 28.25363746\n",
            " 14.25696673 29.53070227 28.90032046 20.56619149 17.66667147 36.66841124\n",
            " 15.65965394 30.47885652 22.41478751 26.38445655 23.50958647 37.58876789\n",
            " 24.19625689 36.5772148  49.98760231 14.76641724 26.02152995 12.08119076\n",
            " 25.74364326 28.74748233 36.97778212 26.78345484 20.69309886 42.96376593\n",
            " 33.5593458  47.07906467 22.66578852 31.21045172 46.48822933 14.75878403\n",
            " 16.48944268 14.57882097 18.99974865 23.14252432 32.11724211 32.1028707\n",
            " 40.88215739 26.32644185 29.24193125 49.64261136 22.04587967 19.56096872\n",
            " 36.18299746 12.2127865  28.82999208 24.53345442 31.08729317 21.65615697\n",
            " 37.33441374 25.86246456 24.09086691 41.83467483 35.05388234 24.01801768\n",
            " 35.05443893 18.65825869 12.41668617 34.99997618 17.27399949 40.14892978\n",
            " 28.65563405 31.40768495 38.83649386 46.69943475 49.14847059 14.00745459\n",
            " 13.22579603 30.50336481 49.72974678 20.79457272 22.06909993 13.27735718\n",
            " 30.76132259 47.47950451 34.23505232 22.92842774 27.3369487  25.58824153\n",
            " 39.96330928 11.27978493 49.67800215 18.5820257  30.23462741 47.88500085\n",
            " 22.28985114 37.84057703 31.10649222 39.12492333 46.86382257 17.44583591\n",
            " 45.6259376  25.56297421 30.8144762  42.89743901 30.01247221 49.73497727\n",
            " 26.3434167  21.51659327 15.17868963 29.36874349 29.26691185 17.47504453\n",
            " 15.2355478  25.63148496 22.17341488 32.76234407 37.55264646 23.07188834\n",
            " 32.13952722 46.47977743 41.40121526 25.64039147 40.26693168 46.66053262\n",
            " 41.95466755 29.33306932 16.89685524 19.96607208 17.02009861 32.12669771\n",
            " 27.58933934 34.58252407 34.40150865 22.56495054 26.61482914 23.92883562\n",
            " 24.25344103 30.23434105 14.15290456 42.23782057 18.84220088 23.17813271\n",
            " 20.26541684 15.79046633 37.45994657 30.28306504 20.38432908 33.89026462\n",
            " 15.54953613 42.73236247 31.44943748 24.99532513 22.73046328 24.61652972\n",
            " 17.02438311 22.7240489  14.69420678 19.80679632 28.86716831 32.97574469\n",
            " 40.19558943 23.75156185 37.0552216  27.8070443  30.95124575 26.73055389\n",
            " 32.28264769 35.46838448 39.75963737 33.69349812 16.32914018 41.57120753\n",
            " 29.16891823 30.13562897 38.36843679 35.02001809 40.37135568 16.15196541\n",
            " 24.47330191 21.67326299 49.88063659 27.45992471 26.56021287 38.81366905\n",
            " 12.07256386 26.30145266 22.18973623 14.24762127 38.95642382 40.77859067\n",
            " 49.74086577 18.35540148 20.75476753 36.76541896 37.92886752 49.87311026\n",
            " 17.96927869 29.9728128  48.82117665 19.43234085 23.68447719 14.94293071\n",
            " 26.34047191 14.2795079  24.42559784 26.08031235 23.67993045 30.03297036\n",
            " 21.06156534 26.27214342 17.4637056  49.99998701 24.38629833 25.98494477\n",
            " 46.50514316 24.0092247  19.38038665 49.95467258 23.9190486  18.08023032]\n",
            "selection [384 246 721 444  38 496 816 659 685 614 698 123 396 494 556 573 505 122\n",
            " 708 430 713 626 542  31 214] (25,) [10.73896279 11.14610348 11.27978493 11.76547803 11.84008919 12.01656946\n",
            " 12.07256386 12.08119076 12.2127865  12.28537865 12.41668617 12.51248514\n",
            " 12.51569409 12.64038643 12.87787274 12.98304105 13.08290585 13.12095591\n",
            " 13.22579603 13.25596237 13.27735718 13.30750544 13.31643274 13.37195526\n",
            " 13.38069009]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (475, 10) (475,)\n",
            "updated train set: (475, 10) (475,) unique(labels): [205 270] [0 1]\n",
            "val set: (827, 10) (827,)\n",
            "\n",
            "Train set: (475, 10)\n",
            "Validation set: (827, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.10526315789473684, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.88       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 0 0]\n",
            "std (827,) [45.70732747 18.05656014 22.6580633  25.27510619 29.96273312 30.34857663\n",
            " 16.80306729 19.41068434 46.73828971 49.01209569 48.79743588 45.64081661\n",
            " 33.39687269 21.70206221 28.38260058 22.68565702 43.80551357 18.96320116\n",
            " 48.34767079 23.42661936 14.29971728 38.96938779 25.65486315 36.83673157\n",
            " 22.50310953 29.96602979 36.41688226 40.11577539 34.6310981  47.0259514\n",
            " 16.96654006 22.48324752 22.91007969 29.71104815 28.92638338 20.89812878\n",
            " 35.59392751 28.09261542 26.15500958 31.95890674 40.80549127 48.80098924\n",
            " 36.64731999 16.3693869  30.13893062 19.17915471 25.00329118 21.50294949\n",
            " 18.37850108 44.86335424 27.81720374 48.02660726 18.19810775 32.39287846\n",
            " 15.95414811 34.91451806 38.23042215 28.54465673 27.61686761 18.40011959\n",
            " 43.59680833 33.60875252 34.00953582 21.64588491 17.38023141 28.41124514\n",
            " 49.98772436 17.42399176 43.8033518  23.48523834 32.89049787 18.80764671\n",
            " 19.2678372  34.50117324 21.89064731 33.42539685 37.6549376  20.5190662\n",
            " 30.35672618 43.04731587 43.31675972 17.22683735 39.84225957 37.08170003\n",
            " 49.98604905 42.57952617 16.03936465 29.03404608 49.99797794 45.03719241\n",
            " 41.85303051 23.68431481 31.93535171 25.9950018  29.3080244  16.07683537\n",
            " 30.31346595 42.2340606  38.60159913 35.41999929 19.78563799 29.2568136\n",
            " 20.66606051 37.23723386 22.66852538 33.1414892  17.09584691 47.95792052\n",
            " 31.21515944 46.05258894 22.52844116 35.60136212 26.93951941 20.20492918\n",
            " 34.33198084 49.97659939 35.2199196  16.97524449 23.38029832 18.67975278\n",
            " 49.99994979 23.78729514 41.08282219 34.62924841 24.49820056 35.3299385\n",
            " 24.68159373 40.07434857 20.04730983 17.6138107  32.17293642 21.12678882\n",
            " 37.41127717 24.82866076 20.83025313 39.9687696  21.28874332 18.11971701\n",
            " 24.87229882 22.00485234 17.84470021 34.95898499 37.58851328 26.43480893\n",
            " 18.45415122 18.10966014 18.58413204 48.14343186 20.13067444 28.56081854\n",
            " 27.72307605 33.7484944  19.88511576 14.88318674 34.53698403 25.82995147\n",
            " 49.81132535 23.31223825 30.22834671 27.46854599 23.47552288 34.37519809\n",
            " 17.04074962 30.29363652 44.53421359 48.46610473 31.0995524  20.70342275\n",
            " 17.04052023 36.02904578 49.57966403 42.15630557 21.60412005 36.53970788\n",
            " 18.7908761  19.7865509  41.39246553 26.83755055 32.36081082 43.35050143\n",
            " 19.02133253 16.91573571 17.62758044 47.86767338 19.18359969 37.38872387\n",
            " 35.52182056 43.17145027 17.0454659  28.65163722 38.52055764 16.68807743\n",
            " 23.63996374 22.29498724 43.04353175 31.52336763 37.82664129 18.89591657\n",
            " 30.16662685 38.62610481 17.97586254 20.09098737 24.88175634 30.62188091\n",
            " 16.70927375 27.204923   26.23596176 34.57020373 37.87840766 31.77621643\n",
            " 31.21771061 49.73178168 19.89734692 16.63822682 35.83166123 29.79812712\n",
            " 23.57213793 40.1815569  37.68176014 39.3510916  22.54398774 19.05982834\n",
            " 47.23693185 38.38878836 35.44660707 30.29982477 49.97439666 22.9882752\n",
            " 30.43611363 36.26529047 20.11025475 30.29825691 28.77783653 27.78526873\n",
            " 49.99925077 44.94600751 37.71090577 46.51161571 32.31867023 36.43440594\n",
            " 45.93368082 32.07979598 49.33254439 19.080682   21.51825746 25.73818032\n",
            " 26.46834011 35.97063848 20.55308747 27.7403819  27.46248789 47.19117807\n",
            " 16.71634036 49.93651726 26.88094874 23.49130108 27.85517546 19.08392921\n",
            " 21.01244378 23.67399928 40.37385973 19.35143761 22.6252679  24.76225489\n",
            " 34.26935743 25.79003621 33.60875252 37.07541529 17.65563812 20.47089292\n",
            " 35.07080622 27.72700679 27.66691892 12.8759935  32.90469859 34.34807827\n",
            " 26.72119855 24.21198767 39.67610994 34.04984131 34.81583433 24.66887288\n",
            " 42.58489058 24.60911534 36.98961661 22.9701347  22.23311557 12.49809322\n",
            " 35.21688107 29.9372608  33.32070954 21.54769368 31.50542446 18.46143875\n",
            " 18.17775511 14.65524792 35.60272124 22.97376764 28.26709347 40.77499001\n",
            " 21.73660964 23.02764147 17.66966596 25.93477366 26.56022838 31.1887297\n",
            " 19.14911216 32.89762052 17.65872334 32.70807938 36.16316674 18.59600299\n",
            " 18.67294672 39.36974337 18.71586814 24.20989728 49.99066796 26.46346035\n",
            " 25.56163647 27.55240589 49.46759749 29.3846801  45.9500984  28.71730944\n",
            " 32.17471433 24.27338592 29.17452159 28.10298157 32.93622396 25.03552882\n",
            " 29.64782954 34.2184814  22.97013282 22.3455111  22.91605351 16.53824135\n",
            " 22.03712474 26.59066308 26.76681954 37.7629295  19.73566498 18.46052183\n",
            " 49.47564169 35.14129558 35.4466683  29.38884254 31.04591984 22.17195345\n",
            " 29.08085681 26.30176049 18.24564969 44.28071523 31.54835861 28.2412467\n",
            " 26.56056309 19.7811933  35.57822834 40.8745619  24.68103764 28.26030932\n",
            " 22.0519296  25.71508348 43.53408897 18.56093358 31.93842899 49.50947642\n",
            " 38.33026544 43.79227744 21.80580695 40.27283895 45.95106323 31.48077045\n",
            " 30.72121128 19.68522186 40.72584193 24.55298325 34.42046023 36.30267185\n",
            " 28.67463058 44.76997741 28.36229072 28.20788572 31.03982327 27.32451269\n",
            " 16.19326337 17.45369169 32.50551711 38.78577128 49.50104091 22.05323574\n",
            " 13.55689247 30.80394603 27.82612018 36.6900688  20.44125935 48.12231728\n",
            " 49.99965094 31.1532072  15.38241596 26.86007527 16.9108513  32.97442603\n",
            " 18.76785844 16.94105145 24.7980839  49.77355553 31.18728828 47.35632237\n",
            " 17.50749757 37.43582483 22.61803101 15.9815319  22.51052451 29.37450153\n",
            " 24.70431857 25.83209678 46.17950559 13.95236757 42.27726614 30.0476619\n",
            " 37.36053079 31.47033023 32.52505712 21.15928325 17.231126   26.80022632\n",
            " 23.24006352 19.01612087 22.34817819 24.03711825 25.28632707 21.42261279\n",
            " 34.73350269 40.87604911 32.26644871 27.06146495 45.02967739 39.81696821\n",
            " 36.23081228 34.87277193 31.9631188  22.54113928 23.89388492 39.6026477\n",
            " 36.13585974 32.14953391 15.11632628 28.98655506 24.34293015 15.7769711\n",
            " 46.15116092 28.20586519 29.03541099 33.90295591 25.19748283 39.21083713\n",
            " 37.88236109 25.4524875  44.91772446 23.82701768 14.65524792 23.33756678\n",
            " 15.10132972 30.61124985 41.82060901 31.71138082 40.12397862 49.61964864\n",
            " 14.92207969 22.87992254 27.67129972 21.42261279 14.83451233 20.06187826\n",
            " 16.72759405 34.95210338 23.77488469 25.95099319 43.21983098 49.99981045\n",
            " 30.03196406 33.96525271 45.74097922 34.17718605 40.29859119 17.09005798\n",
            " 24.50549346 24.15622551 20.013901   16.55798564 44.38892178 35.41289296\n",
            " 21.96722889 19.77096657 44.39766137 38.41322778 27.22703278 16.09186956\n",
            " 33.34567769 35.28451079 28.31670924 15.3555773  18.38116929 28.38347229\n",
            " 32.28082328 21.09574968 18.69623043 20.77371846 27.76815491 44.05698175\n",
            " 36.27189999 42.80303622 24.32948763 39.63932732 17.09861143 15.4439893\n",
            " 49.99986194 43.03698721 16.06690281 41.34224161 21.43090609 25.03712992\n",
            " 41.17765433 22.71221782 28.19959982 23.68966633 25.30344142 43.33908934\n",
            " 24.6374408  49.81603407 16.66099088 45.51036394 28.05522046 27.01649387\n",
            " 27.91996756 34.18482561 19.98117389 35.81079575 20.81802659 38.73874947\n",
            " 36.85726787 16.42154191 35.88811879 21.08493524 34.33600538 22.23184072\n",
            " 37.92728316 22.2752502  39.86856345 20.45570485 20.11093332 36.02916136\n",
            " 20.74285516 19.76754335 44.69901996 27.39021351 49.30160065 31.94107599\n",
            " 35.53613213 27.67653784 23.67399928 49.96930495 31.05716695 24.23170635\n",
            " 27.3812739  45.17252574 31.80486262 41.59699382 40.63076465 48.4322597\n",
            " 24.95361105 39.34988573 15.65277002 49.51453676 21.01079248 33.53644481\n",
            " 28.26747411 21.96026524 38.63548505 19.92155153 40.90879988 44.16320547\n",
            " 32.54504182 43.45139658 43.01692897 48.94082887 34.28208446 31.56016277\n",
            " 19.9526166  27.94959818 17.52507236 16.61284239 22.84166982 47.57259429\n",
            " 20.34955841 24.87219488 39.73509546 19.47929165 19.58627122 31.54594435\n",
            " 39.08777975 27.62329752 42.73202916 41.61345037 37.02147196 42.61261616\n",
            " 23.03245537 19.55449783 18.35173147 18.30976018 49.99891053 48.85966153\n",
            " 40.86922952 26.36584654 49.40845008 34.24971707 23.41723792 21.33736795\n",
            " 40.92515254 35.82506134 31.06599978 29.60108458 15.13494917 30.3146321\n",
            " 13.7001923  31.38095538 30.98729317 21.97026562 19.97071889 39.3694763\n",
            " 16.02317755 32.03406183 21.65951992 27.26558294 25.18065165 38.80303106\n",
            " 24.10126903 39.03415497 49.99075639 13.45362151 28.57504093 26.94856185\n",
            " 28.83150354 40.67837887 28.14551948 19.89547656 44.33562515 34.9159049\n",
            " 47.85742439 25.13913471 34.28161422 46.55069401 14.47929018 17.37055528\n",
            " 14.34285026 19.62663163 25.60017318 35.85180974 31.61071374 42.27557707\n",
            " 28.23538612 30.04751767 49.73608286 22.60073052 21.12606885 37.11513609\n",
            " 30.87306864 26.03487741 32.57709496 22.45132693 39.07822842 28.30706298\n",
            " 26.80022632 42.91891312 39.64486367 27.42789455 36.89060656 20.48114038\n",
            " 36.70802222 16.57499175 41.61960139 30.78627638 32.11540649 40.29060798\n",
            " 45.98879595 49.23593572 19.86704185 34.64847172 49.81723191 22.33514413\n",
            " 22.63402856 31.62631986 47.56606351 37.38040743 26.00279608 28.31860468\n",
            " 26.84558288 42.6782508  49.78485696 19.8571531  29.98821762 47.88207114\n",
            " 22.52672667 39.7953006  32.25736283 38.52055764 47.05998726 18.15180928\n",
            " 47.1747754  30.23480238 32.25815474 44.04442354 31.31101316 49.78235711\n",
            " 29.55301473 22.69122375 17.11323459 30.89946613 31.13844364 18.84959821\n",
            " 16.43017204 28.12274674 22.62622351 34.65997662 39.79103534 25.03966225\n",
            " 34.7540043  47.2031067  42.06904922 28.19820741 40.22074518 47.98706122\n",
            " 43.26624645 31.21039468 17.05049294 22.97083895 19.9526166  34.3959258\n",
            " 28.97819133 36.67378489 36.33228615 22.29498724 28.20586519 23.37393787\n",
            " 26.17553102 32.4562063  12.96094333 43.72277748 18.80764671 24.43528789\n",
            " 22.07470687 13.56867255 39.4762961  31.64394878 19.68180692 33.96616131\n",
            " 17.46437425 45.21023029 32.51963318 26.46493625 24.55979938 25.74293089\n",
            " 21.84943811 23.90115674 14.52529629 21.82056304 31.20307637 35.24631559\n",
            " 42.35138732 23.61289644 38.25811107 29.28374862 31.80396814 28.79779945\n",
            " 34.83546934 36.68595852 41.0067083  35.63815101 17.650727   43.13931189\n",
            " 30.66060075 33.24289944 39.60126211 37.52223304 41.85875719 12.39068495\n",
            " 29.80850226 22.97110027 49.871168   28.61856695 30.20080593 40.94080977\n",
            " 28.65194528 23.61913828 15.73925727 38.29478376 40.80202361 49.83649715\n",
            " 19.23081368 21.15986265 38.24884497 39.57064156 49.88004404 18.50063311\n",
            " 30.70731968 49.00037779 22.08272453 23.81216974 14.37147768 27.22948449\n",
            " 16.67309114 22.99218398 27.68968279 24.16646882 31.1710629  23.42063361\n",
            " 29.66834379 17.13916589 49.99999904 25.69316242 28.72915187 47.35342836\n",
            " 28.52728941 20.01848532 49.96038649 25.65397307 18.57865631]\n",
            "selection [785 287 273 746 639 390 751 624 417  20 654 808 652 764 460 295 472 153\n",
            " 468 462 446 622 501 398 515] (25,) [12.39068495 12.49809322 12.8759935  12.96094333 13.45362151 13.55689247\n",
            " 13.56867255 13.7001923  13.95236757 14.29971728 14.34285026 14.37147768\n",
            " 14.47929018 14.52529629 14.65524792 14.65524792 14.83451233 14.88318674\n",
            " 14.92207969 15.10132972 15.11632628 15.13494917 15.3555773  15.38241596\n",
            " 15.4439893 ]\n",
            "trainset before adding uncertain samples (475, 10) (475,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [212 288] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "final active learning accuracies [67.51152073732719, 74.65437788018433, 76.95852534562212, 77.41935483870968, 79.03225806451613, 78.80184331797236, 78.80184331797236, 79.49308755760369, 80.18433179723502, 80.64516129032258, 80.87557603686636, 81.33640552995391, 81.33640552995391, 81.10599078341014, 80.64516129032258, 81.10599078341014, 80.18433179723502, 80.87557603686636, 80.64516129032258, 80.87557603686636]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-19.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 20, using model = LogModel, selection_function = MinStdSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 10) (10,) unique(labels): [5 5] [0 1]\n",
            "Val set: (1292, 10) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 10)\n",
            "Validation set: (1292, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 76.036866 \n",
            "Classification report for LogisticRegression(C=5.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.55      0.48      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "std (1292,) [46.59338762 40.76231217 25.6688108  ... 49.99764576 41.85129611\n",
            " 12.5050758 ]\n",
            "selection [1223   52  828  819 1097   96 1058 1047   49  339] (10,) [0.04193593 0.17682648 0.18541024 0.34311479 0.40384169 0.42818602\n",
            " 0.44198547 0.47040321 0.72319084 0.85156905]\n",
            "trainset before adding uncertain samples (10, 10) (10,)\n",
            "trainset after adding uncertain samples (20, 10) (20,)\n",
            "updated train set: (20, 10) (20,) unique(labels): [ 6 14] [0 1]\n",
            "val set: (1282, 10) (1282,)\n",
            "\n",
            "Train set: (20, 10)\n",
            "Validation set: (1282, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 75.806452 \n",
            "Classification report for LogisticRegression(C=2.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.83       321\n",
            "           1       0.53      0.59      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.70      0.70       434\n",
            "weighted avg       0.77      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[262  59]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "std (1282,) [45.91438486 47.73887304 41.04154128 ... 49.99972961 43.29895988\n",
            " 11.65886094]\n",
            "selection [ 558 1017  648  294  441  469   46  189  293  771] (10,) [0.04687766 0.05336083 0.18438739 0.2137984  0.38297709 0.40393369\n",
            " 0.46331711 0.53268371 0.54850287 0.77959526]\n",
            "trainset before adding uncertain samples (20, 10) (20,)\n",
            "trainset after adding uncertain samples (30, 10) (30,)\n",
            "updated train set: (30, 10) (30,) unique(labels): [ 8 22] [0 1]\n",
            "val set: (1272, 10) (1272,)\n",
            "\n",
            "Train set: (30, 10)\n",
            "Validation set: (1272, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.649770 \n",
            "Classification report for LogisticRegression(C=1.6666666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1272,) [49.92806588 41.61687404 41.60795715 ... 50.         43.03953177\n",
            "  2.28480659]\n",
            "selection [1033  493  858 1116  534  834   93  619 1230  254] (10,) [0.09450241 0.19792179 0.34464347 0.41201516 0.53262004 0.65775644\n",
            " 0.72579351 0.85159764 0.98599762 1.28978223]\n",
            "trainset before adding uncertain samples (30, 10) (30,)\n",
            "trainset after adding uncertain samples (40, 10) (40,)\n",
            "updated train set: (40, 10) (40,) unique(labels): [15 25] [0 1]\n",
            "val set: (1262, 10) (1262,)\n",
            "\n",
            "Train set: (40, 10)\n",
            "Validation set: (1262, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=1.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1262,) [49.96559484 35.48613642 40.05086298 ... 50.         37.56984313\n",
            " 29.5144754 ]\n",
            "selection [1104  600 1140  557  562  231  286 1228  966  848] (10,) [0.55680561 0.64354078 0.67978892 0.74218621 0.81887281 0.85529073\n",
            " 0.97868717 1.28678314 1.34991419 1.40609751]\n",
            "trainset before adding uncertain samples (40, 10) (40,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [21 29] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1252,) [49.95335677 29.909028   42.27574556 ... 50.         37.80693516\n",
            " 32.91904265]\n",
            "selection [1039   49  745 1086  224   82  287  609 1174  876] (10,) [0.03412764 0.34021857 0.46666217 0.80916395 1.03714076 1.06260167\n",
            " 1.11414933 1.22599282 1.3702123  1.47129454]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (60, 10) (60,)\n",
            "updated train set: (60, 10) (60,) unique(labels): [24 36] [0 1]\n",
            "val set: (1242, 10) (1242,)\n",
            "\n",
            "Train set: (60, 10)\n",
            "Validation set: (1242, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.8333333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1242,) [49.93203869 25.44590864 41.67030096 ... 49.9999999  24.5930456\n",
            " 24.18608012]\n",
            "selection [ 872  423  386  898 1056  593 1006   83 1219   65] (10,) [0.14966879 0.25400034 0.36966274 0.45866421 0.58184669 0.67530706\n",
            " 0.70221038 0.7824347  0.95978973 1.23481909]\n",
            "trainset before adding uncertain samples (60, 10) (60,)\n",
            "trainset after adding uncertain samples (70, 10) (70,)\n",
            "updated train set: (70, 10) (70,) unique(labels): [30 40] [0 1]\n",
            "val set: (1232, 10) (1232,)\n",
            "\n",
            "Train set: (70, 10)\n",
            "Validation set: (1232, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.7142857142857143, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1232,) [49.85973317 25.96875976 40.22969817 ... 49.99999596 35.13756745\n",
            " 33.90359234]\n",
            "selection [ 373  128  923   84 1011 1204  192  783  352  844] (10,) [0.17762884 0.20341717 0.26702729 0.53598175 0.79334751 0.92113553\n",
            " 1.02340713 1.03061925 1.09107428 1.09161273]\n",
            "trainset before adding uncertain samples (70, 10) (70,)\n",
            "trainset after adding uncertain samples (80, 10) (80,)\n",
            "updated train set: (80, 10) (80,) unique(labels): [32 48] [0 1]\n",
            "val set: (1222, 10) (1222,)\n",
            "\n",
            "Train set: (80, 10)\n",
            "Validation set: (1222, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1222,) [49.07992468 31.63428843 38.36773471 ... 49.99946707 38.40195731\n",
            " 30.85293744]\n",
            "selection [ 421  611 1125  107 1022 1118   42  444   16  852] (10,) [0.030577   0.03166077 0.47447946 0.56073605 0.61290474 0.6264977\n",
            " 0.66728285 0.72191643 0.79228442 0.7990999 ]\n",
            "trainset before adding uncertain samples (80, 10) (80,)\n",
            "trainset after adding uncertain samples (90, 10) (90,)\n",
            "updated train set: (90, 10) (90,) unique(labels): [39 51] [0 1]\n",
            "val set: (1212, 10) (1212,)\n",
            "\n",
            "Train set: (90, 10)\n",
            "Validation set: (1212, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.5555555555555556, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1212,) [47.95449661 25.10279771 33.73965945 ... 49.99021125 34.29872808\n",
            " 27.14746389]\n",
            "selection [1190  156  104  572 1150  606  238  732 1196  379] (10,) [0.10895325 0.41444069 0.52202912 0.53150594 0.56736588 0.9829617\n",
            " 1.01620336 1.01801362 1.01857913 1.25897151]\n",
            "trainset before adding uncertain samples (90, 10) (90,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [41 59] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1202,) [45.57766314 25.63308807 29.89624365 ... 49.88286176 32.49889565\n",
            " 26.76334311]\n",
            "selection [638 542 473 230 198 490 215 545 250 462] (10,) [0.01160366 0.06858714 0.19870209 0.22034426 0.25684529 0.32104029\n",
            " 0.45762763 0.46882392 0.54410391 0.59682227]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (110, 10) (110,)\n",
            "updated train set: (110, 10) (110,) unique(labels): [42 68] [0 1]\n",
            "val set: (1192, 10) (1192,)\n",
            "\n",
            "Train set: (110, 10)\n",
            "Validation set: (1192, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.45454545454545453, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1192,) [45.76415485 28.95009282 32.69150735 ... 49.79747423 36.06907358\n",
            " 26.8826279 ]\n",
            "selection [976 394 391  71 163 440 312 848 210 277] (10,) [0.04429775 0.09269815 0.12228024 0.14950929 0.31506312 0.39470607\n",
            " 0.48336108 0.5784272  0.60954541 0.74364936]\n",
            "trainset before adding uncertain samples (110, 10) (110,)\n",
            "trainset after adding uncertain samples (120, 10) (120,)\n",
            "updated train set: (120, 10) (120,) unique(labels): [45 75] [0 1]\n",
            "val set: (1182, 10) (1182,)\n",
            "\n",
            "Train set: (120, 10)\n",
            "Validation set: (1182, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.4166666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.67      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1182,) [42.9322941  29.26710134 30.88653759 ... 49.34056654 34.24446423\n",
            " 25.98538144]\n",
            "selection [1174  549  969  195   57  720  604  180  355    8] (10,) [0.16979695 0.22104761 0.28062634 0.36986044 0.48239034 0.54943744\n",
            " 0.67947671 0.74719708 0.75384238 1.00542279]\n",
            "trainset before adding uncertain samples (120, 10) (120,)\n",
            "trainset after adding uncertain samples (130, 10) (130,)\n",
            "updated train set: (130, 10) (130,) unique(labels): [49 81] [0 1]\n",
            "val set: (1172, 10) (1172,)\n",
            "\n",
            "Train set: (130, 10)\n",
            "Validation set: (1172, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=0.38461538461538464, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.49      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1172,) [35.84197904 31.29848402 28.1940331  ... 43.45560888 37.11041956\n",
            " 22.19701128]\n",
            "selection [ 533  217  776 1127  254  570  691  622  365  133] (10,) [0.02823131 0.1800138  0.25957001 0.273861   0.38158504 0.39141919\n",
            " 0.40744224 0.44305002 0.56465634 0.58757378]\n",
            "trainset before adding uncertain samples (130, 10) (130,)\n",
            "trainset after adding uncertain samples (140, 10) (140,)\n",
            "updated train set: (140, 10) (140,) unique(labels): [54 86] [0 1]\n",
            "val set: (1162, 10) (1162,)\n",
            "\n",
            "Train set: (140, 10)\n",
            "Validation set: (1162, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.35714285714285715, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1162,) [37.77132871 30.72048471 27.80033907 ... 47.96396604 36.14784716\n",
            " 18.93719019]\n",
            "selection [  94  420  297  642 1123  979  926  716  316 1133] (10,) [0.18159734 0.31176622 0.38694812 0.44822376 0.49277012 0.51316139\n",
            " 0.527687   0.56237042 0.74190752 0.75459436]\n",
            "trainset before adding uncertain samples (140, 10) (140,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [59 91] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1152,) [36.13646509 32.134958   28.95928756 ... 48.25338101 36.7050398\n",
            " 19.98392481]\n",
            "selection [ 549 1143   29  917  464  148 1115 1125  460  857] (10,) [0.26628554 0.28884686 0.3213145  0.40049094 0.46249742 0.48787244\n",
            " 0.6429805  0.66555594 0.80729206 0.86503176]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (160, 10) (160,)\n",
            "updated train set: (160, 10) (160,) unique(labels): [64 96] [0 1]\n",
            "val set: (1142, 10) (1142,)\n",
            "\n",
            "Train set: (160, 10)\n",
            "Validation set: (1142, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.3125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1142,) [39.17115581 30.62797369 29.54905426 ... 49.06367312 35.25664771\n",
            " 17.84111779]\n",
            "selection [ 999  648  936  774  954   30 1030  762 1072  872] (10,) [0.01950057 0.19468872 0.21673612 0.40769099 0.44575346 0.5076378\n",
            " 0.63974298 0.673258   0.71042374 1.00486835]\n",
            "trainset before adding uncertain samples (160, 10) (160,)\n",
            "trainset after adding uncertain samples (170, 10) (170,)\n",
            "updated train set: (170, 10) (170,) unique(labels): [ 69 101] [0 1]\n",
            "val set: (1132, 10) (1132,)\n",
            "\n",
            "Train set: (170, 10)\n",
            "Validation set: (1132, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.29411764705882354, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1132,) [39.49867457 29.26373686 27.28545635 ... 48.78540265 34.2667925\n",
            " 16.1123767 ]\n",
            "selection [491  28 109 739 955  96 145 567 595 299] (10,) [0.08492853 0.17797028 0.34685555 0.35637445 0.47000048 0.49019924\n",
            " 0.58965819 0.74461952 0.76963501 1.00295099]\n",
            "trainset before adding uncertain samples (170, 10) (170,)\n",
            "trainset after adding uncertain samples (180, 10) (180,)\n",
            "updated train set: (180, 10) (180,) unique(labels): [ 74 106] [0 1]\n",
            "val set: (1122, 10) (1122,)\n",
            "\n",
            "Train set: (180, 10)\n",
            "Validation set: (1122, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.2777777777777778, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1122,) [38.27639494 27.33661674 23.47836786 ... 48.57425401 33.77054446\n",
            " 16.48758353]\n",
            "selection [959 579 373 672 489 678 737 551 293 112] (10,) [0.10116669 0.15571395 0.27594139 0.28677856 0.29511544 0.34510385\n",
            " 0.39231183 0.43672166 0.55218423 0.61052989]\n",
            "trainset before adding uncertain samples (180, 10) (180,)\n",
            "trainset after adding uncertain samples (190, 10) (190,)\n",
            "updated train set: (190, 10) (190,) unique(labels): [ 76 114] [0 1]\n",
            "val set: (1112, 10) (1112,)\n",
            "\n",
            "Train set: (190, 10)\n",
            "Validation set: (1112, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.2631578947368421, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.65      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1112,) [38.27949754 25.40422755 23.06039442 ... 48.58750232 32.14872768\n",
            " 18.00746756]\n",
            "selection [463 494 615 102 787 922 804 221 155 540] (10,) [0.53289278 0.53289278 0.89181607 1.0709905  1.90081148 1.92471388\n",
            " 2.00484858 2.03227362 2.19256275 2.2199927 ]\n",
            "trainset before adding uncertain samples (190, 10) (190,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 79 121] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1102,) [36.9630235  24.01970822 23.70933184 ... 48.0414866  29.34257111\n",
            " 17.14421733]\n",
            "selection [767 538 280  51  50 884 248  93 540 330] (10,) [0.12714533 0.55867921 0.70560663 0.96363448 1.16358288 1.60763418\n",
            " 1.60763418 1.77683311 1.92957024 2.21744284]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (210, 10) (210,)\n",
            "updated train set: (210, 10) (210,) unique(labels): [ 82 128] [0 1]\n",
            "val set: (1092, 10) (1092,)\n",
            "\n",
            "Train set: (210, 10)\n",
            "Validation set: (1092, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.23809523809523808, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1092,) [38.96795031 26.3029919  25.26101771 ... 48.52934955 32.33366811\n",
            " 17.99869427]\n",
            "selection [ 894  282  641  447  544 1049  730  228  697  992] (10,) [2.70072201 2.91241868 3.48581517 3.54234157 3.82503419 3.88961582\n",
            " 4.03676073 4.15188713 4.32016806 4.49473897]\n",
            "trainset before adding uncertain samples (210, 10) (210,)\n",
            "trainset after adding uncertain samples (220, 10) (220,)\n",
            "updated train set: (220, 10) (220,) unique(labels): [ 84 136] [0 1]\n",
            "val set: (1082, 10) (1082,)\n",
            "\n",
            "Train set: (220, 10)\n",
            "Validation set: (1082, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.22727272727272727, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1082,) [39.28508286 24.72655725 24.86829119 ... 48.75607475 31.289492\n",
            " 18.09070896]\n",
            "selection [ 234  956   23 1029  748  711  679  717 1068 1053] (10,) [3.37681459 4.04193168 4.04193168 4.37877791 4.94704838 5.06542046\n",
            " 5.06587849 5.21678994 5.26519539 5.32167936]\n",
            "trainset before adding uncertain samples (220, 10) (220,)\n",
            "trainset after adding uncertain samples (230, 10) (230,)\n",
            "updated train set: (230, 10) (230,) unique(labels): [ 87 143] [0 1]\n",
            "val set: (1072, 10) (1072,)\n",
            "\n",
            "Train set: (230, 10)\n",
            "Validation set: (1072, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.21739130434782608, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1072,) [38.55158966 21.2451525  22.75336764 ... 48.74118545 27.89075677\n",
            " 17.1733585 ]\n",
            "selection [ 43 885 397  17 753 195 151 886 185 779] (10,) [3.9924196  4.19782083 4.43975132 4.70103422 4.70103422 4.70335444\n",
            " 4.76249395 5.22545041 5.31811134 5.38391878]\n",
            "trainset before adding uncertain samples (230, 10) (230,)\n",
            "trainset after adding uncertain samples (240, 10) (240,)\n",
            "updated train set: (240, 10) (240,) unique(labels): [ 92 148] [0 1]\n",
            "val set: (1062, 10) (1062,)\n",
            "\n",
            "Train set: (240, 10)\n",
            "Validation set: (1062, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.20833333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1062,) [38.83832461 19.99789066 22.21609744 ... 48.72572479 27.08914431\n",
            " 15.34325134]\n",
            "selection [1031  329   62  661  936  418  777  265  759  587] (10,) [3.30462856 3.63410321 3.65649969 4.09579184 4.28401628 4.37451333\n",
            " 4.67435842 4.95545478 4.98078269 5.23689583]\n",
            "trainset before adding uncertain samples (240, 10) (240,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [ 96 154] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1052,) [38.74928662 22.09313682 23.36872946 ... 48.77305908 28.07962526\n",
            " 16.97653232]\n",
            "selection [217 543 589 690 556 710 549 464 773 467] (10,) [4.15563309 5.00680454 5.81786585 6.12533656 6.36529352 6.37519554\n",
            " 6.40872464 6.49594292 6.61760195 6.62282669]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (260, 10) (260,)\n",
            "updated train set: (260, 10) (260,) unique(labels): [ 99 161] [0 1]\n",
            "val set: (1042, 10) (1042,)\n",
            "\n",
            "Train set: (260, 10)\n",
            "Validation set: (1042, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.19230769230769232, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1042,) [37.87429623 22.51379588 23.3557174  ... 48.51032918 27.61479565\n",
            " 16.29745249]\n",
            "selection [285 661 426 686 855 812   4 653 146 755] (10,) [5.04005903 5.07455432 5.23112376 5.54931681 5.92373402 6.32335217\n",
            " 6.35869266 6.48834323 6.51624097 6.52022927]\n",
            "trainset before adding uncertain samples (260, 10) (260,)\n",
            "trainset after adding uncertain samples (270, 10) (270,)\n",
            "updated train set: (270, 10) (270,) unique(labels): [103 167] [0 1]\n",
            "val set: (1032, 10) (1032,)\n",
            "\n",
            "Train set: (270, 10)\n",
            "Validation set: (1032, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.18518518518518517, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1032,) [36.81282275 22.49317107 22.44413918 ... 48.87082371 28.42807788\n",
            " 14.95877753]\n",
            "selection [408 999 377 622 990 750 529 642 749 997] (10,) [1.45080123 3.84476397 4.25007855 4.73764296 5.43393198 5.97257858\n",
            " 6.21789853 6.34723507 6.43166082 6.46584489]\n",
            "trainset before adding uncertain samples (270, 10) (270,)\n",
            "trainset after adding uncertain samples (280, 10) (280,)\n",
            "updated train set: (280, 10) (280,) unique(labels): [108 172] [0 1]\n",
            "val set: (1022, 10) (1022,)\n",
            "\n",
            "Train set: (280, 10)\n",
            "Validation set: (1022, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.17857142857142858, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1022,) [36.35219366 19.4761999  20.23124064 ... 48.86963346 25.42896768\n",
            " 14.16193359]\n",
            "selection [890 640 726 585 591 924 947 551 941 395] (10,) [4.26511422 4.74699038 4.83633721 5.48942335 5.87766943 6.22703785\n",
            " 6.40859758 6.42276516 6.42667924 6.47533098]\n",
            "trainset before adding uncertain samples (280, 10) (280,)\n",
            "trainset after adding uncertain samples (290, 10) (290,)\n",
            "updated train set: (290, 10) (290,) unique(labels): [113 177] [0 1]\n",
            "val set: (1012, 10) (1012,)\n",
            "\n",
            "Train set: (290, 10)\n",
            "Validation set: (1012, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.1724137931034483, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1012,) [37.27569612 20.03847628 21.74053268 ... 49.07614679 26.88348915\n",
            " 14.17769159]\n",
            "selection [391  65 210   5 765 977 909 160 362 334] (10,) [6.48061391 6.57350639 7.05888928 7.11432542 7.22399098 7.27080116\n",
            " 7.29124005 7.40611159 7.45271185 7.50091801]\n",
            "trainset before adding uncertain samples (290, 10) (290,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [117 183] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1002,) [34.80889024 14.86963626 17.46026964 ... 48.77688697 21.67195251\n",
            " 13.85974779]\n",
            "selection [170 414 604 466 205 829 469  56 926 708] (10,) [4.39495871 4.82597619 4.84924282 4.99645184 5.17099319 5.2771832\n",
            " 5.32356086 5.45507932 5.89738416 5.98340512]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (310, 10) (310,)\n",
            "updated train set: (310, 10) (310,) unique(labels): [118 192] [0 1]\n",
            "val set: (992, 10) (992,)\n",
            "\n",
            "Train set: (310, 10)\n",
            "Validation set: (992, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.16129032258064516, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (992,) [35.90871719 17.26092999 19.53162391 25.41252752 10.67121196 21.02106153\n",
            " 25.72165729  8.02093548 16.1809266  14.42837958 11.98997847 45.27383314\n",
            "  9.51226771 43.18473826 43.30703543 26.57716452 17.44441542 18.56348569\n",
            " 17.06961374  7.81495849 12.50925556 34.89196348 13.75110656  8.94830357\n",
            " 22.49242411 10.42214586 34.83604455 19.69211077  7.81495849 32.63522554\n",
            " 20.27474958 12.12228415 18.86253985 30.51967542 30.34628446 19.15079669\n",
            " 26.00334405 41.52503461 12.89633047 11.9954405  17.43909798 21.62572593\n",
            " 20.51390288 21.11053102 29.19140785 12.57246104 18.97911889 22.72281605\n",
            " 11.46528902 26.45000421 34.692559   43.09057251 26.80540336  9.62969609\n",
            " 27.36729392 13.60844802 18.16443514 20.95348689 16.59328601 14.7888337\n",
            " 42.27294196 26.62733159 41.10447598 12.69411998 29.03726741 13.04701994\n",
            " 28.26676387 30.84445246 23.63990068  8.73868185 12.01117963 36.78919024\n",
            " 22.98360484  7.28277887 28.73036167 26.01639158 16.40587554 15.2917334\n",
            "  8.27043454 25.7729357  49.31766861 10.92294324 31.22531897 17.69724507\n",
            " 21.54500909 27.29014528 13.14809174 18.99763148 25.82920782 18.28898849\n",
            " 28.83025389  7.81495849 27.6982017  20.42715008 18.47069805 27.1165841\n",
            " 33.30349598 37.20697111 12.73961084 30.0987255  23.298124   49.2571282\n",
            " 35.49453534 18.51008642 26.57579145 49.76456061 35.17630553 39.46464081\n",
            " 29.0508784  17.9096485  13.07588864 12.74550958 27.33540225 28.50874374\n",
            " 26.66233077 10.44839031 23.41133196 36.1912414  32.68237088 10.31864718\n",
            " 23.99820412 17.11305325 20.13094081 16.50045194 29.22105761 14.14176434\n",
            "  9.27549013  7.81495849 27.6752621  42.09894147 24.9912025  10.63974612\n",
            " 40.43359815  8.81507023 13.00173774 16.62399726 24.99014025 23.04039817\n",
            " 14.18317225 19.43219314 49.43718533 12.41968855 26.2809778  12.9408881\n",
            " 21.6499947  17.47361957 14.11598842 49.92362829 18.10893819 31.99931405\n",
            " 28.8156941  20.43837143 27.47475013  7.81495849 17.26879065 33.2850687\n",
            " 13.38471774 13.83316974 26.62528986 14.28609573 12.73262902 10.56031309\n",
            " 20.18460794 11.0190573  30.40641009 14.99935557 15.40109186 34.52605711\n",
            " 18.27817889 15.35788007 16.97990463 18.30681603 13.73308217 10.86217539\n",
            " 13.09498869 24.16967937 30.94362989  7.81495849 23.19331696 15.88100477\n",
            " 21.55672304 16.25987476 42.8181066  17.52165427 19.43951686 18.75720551\n",
            " 13.69188458 28.06178316 12.40236297 35.85883985 12.95295737 22.57979942\n",
            " 17.75509133 49.28287448 13.35415721 21.95237616 18.70700561 26.46040584\n",
            " 22.56573211 31.77900844 36.02519263 42.15217222 26.2480903  12.11389082\n",
            " 16.02196886 28.24000936 48.53601611  7.81495849 33.22445333 19.16619421\n",
            "  9.06157121 27.20125959 14.39755541 20.02693361 28.16940353 17.17724392\n",
            " 13.74349468 27.60778818 11.62634252  7.85607629 15.2680539  43.09132949\n",
            " 16.18245736 28.39109209 33.4708357  26.42236848 30.83633981  9.35945746\n",
            " 10.81175529 20.68636577 11.53030686 21.87492183 36.61898358 15.16514233\n",
            " 26.44919803 21.05364706 33.1148058  18.61443046 25.13645804 30.96219396\n",
            " 16.07783834 18.24188911 22.31430735 25.64924459 18.7641554  21.41365606\n",
            " 19.57906596 30.03286207 29.91412241 27.43352903 33.76431464 20.88647319\n",
            " 12.56751247 25.88715714 46.73066637 11.64621672 16.50243965  9.38033515\n",
            " 13.22639398 30.72575771 11.95451562 24.64486448 20.22954607 37.88142525\n",
            " 31.26908189 33.06014886 14.72598044 14.48093946 39.95050213 34.97524922\n",
            " 28.6685318  21.91417593 48.58164883 15.73853454 19.40453561 28.73060991\n",
            " 16.15214128 21.75524201 25.60351081 12.71817677  7.81495849 49.7483703\n",
            "  7.81495849 31.30211738 31.12971092 35.40321961 29.13749639  7.81495849\n",
            " 28.80685549 43.68281494 12.11607545 33.59163934 17.41954525 45.35143721\n",
            " 14.69132358 10.64813346 20.59181016 16.30147217  9.30307492 25.84308741\n",
            " 18.85633854 22.64069463 18.50474038 43.36368252 48.81365136 19.8430258\n",
            " 21.15584588 18.39495431 18.88952888 17.93551808 16.17446909 17.54142519\n",
            " 32.84101861 15.648173   20.52928101 20.15376188  7.81495849 28.41434821\n",
            " 22.76077523 22.98360484 32.71386116 12.92194343 17.986418   28.63427925\n",
            " 23.41896341 17.06382148 12.97285533 25.33905643 25.17974044 18.88995977\n",
            " 10.14901573  9.34790976 21.18649692 28.58823712 23.66495864 31.88460306\n",
            " 21.67091526  8.08798889 23.61769336 22.93867778  7.81495849 37.171873\n",
            " 19.14483542 28.61583034 16.72171193 15.24093029 11.81496919 26.89361451\n",
            " 25.83931152 28.64203974 16.39556958 15.36176553 16.08798949  8.40287355\n",
            " 12.00633372 26.24430878 20.55560053  7.56169872 26.5231161  16.21970388\n",
            " 34.44186929 12.52418905 14.36790801 23.35808463 13.23736375 14.38410758\n",
            " 21.22672049 21.13741886 26.51640817 18.7616242  28.49514534 29.37704636\n",
            " 27.5900754  15.68240465 13.61758984 34.53459278 14.7431172  20.12756825\n",
            " 24.7878711  17.29311804 49.53207016 21.73510875 16.45447895 19.69164858\n",
            " 16.61827624 31.62139272 46.76978265 25.25771903 36.12929377 25.15106744\n",
            " 28.68293505  7.81495849  7.81495849 16.91265013 21.31691179 21.57751551\n",
            " 24.45768587 26.03897801 20.32255707 26.01764201 26.50060965 24.01529437\n",
            " 18.11965943 23.4919095  16.42553673 11.1549062  10.45241372 11.2540532\n",
            " 23.70542504 19.74901931 32.67307173 17.74964223 46.31841779 30.94581734\n",
            "  8.00765583 31.72111988 27.31237066 23.35021098 18.90756728 24.63203041\n",
            " 18.24660926 14.91136568 35.58505518 13.90948426 21.98048635 26.6815919\n",
            " 18.677444   17.81562538 31.83995624 35.29748387 19.5038751   7.55613767\n",
            " 24.5040305  16.99840934  7.81495849 22.00437789 33.74461855 14.57937176\n",
            " 21.80741037 32.34822325 16.47206457 46.77419252 35.14726362  8.65649065\n",
            " 40.16397616 37.27150842 37.56876416  7.47746479 31.26567761 29.53840341\n",
            " 15.69415306 35.74877079 22.60613771 30.01251407 27.91074141 13.51989296\n",
            " 18.96895973 36.28353669 25.17630442 19.44542224 28.77617699 21.06424118\n",
            "  8.97689966 21.93709978 23.0845073  46.49039269  8.3656381  16.32098999\n",
            " 12.19002069 21.70931474 24.83619034 30.29373561 20.04202901 40.8205809\n",
            " 49.89500899 26.25892454 11.0557231  13.97383702 24.71076528 14.11761458\n",
            " 12.0390973  22.05671965 14.51484122 27.21714394  9.44149745 24.64394011\n",
            " 45.93134634 25.48574974 42.50911554  7.81495849 14.75099682 30.02305976\n",
            " 18.23573183 17.39313116 21.01570771  7.81495849 15.25492356 18.4072503\n",
            " 17.67107192 39.82601576 11.79480982 36.96493604 21.43596782 22.25097082\n",
            " 14.91863576 28.37575147 10.58104474 15.24712298 19.25297655  9.90863365\n",
            " 21.05194773 16.90651406  9.73166337 14.20485442 18.70886928 15.80634809\n",
            " 24.58344701 29.55563397 26.87624982 17.33536311 11.05843107 21.57314926\n",
            " 40.75906787 33.69107301 33.01493045 31.48090802  7.81495849  7.81495849\n",
            " 27.01297406 20.54302566  8.36366855 15.58891682 35.8814106  35.23285266\n",
            " 25.83142449 19.9318767  15.46061551 39.09400391 24.24255591 22.73258541\n",
            " 11.61996406 26.27170871 21.02264495 30.07889651 35.81502304 23.0249761\n",
            " 37.46808211 16.27796345 12.00633372 21.1487339  12.51770428 24.65831026\n",
            " 35.32605423 22.05212398 33.07987551 46.69975533 12.89808073 22.01846886\n",
            " 21.28139156 15.80634809 13.88968262 18.84348702 17.23627529 29.20259172\n",
            " 20.26402199 20.98172348 38.21186779 49.87054322 20.22519214  7.64774787\n",
            " 24.12124947 34.49715027 25.31178151 10.67609632 10.93506807 32.65589222\n",
            " 12.25996071 16.53068123 39.63519981 20.71633222 16.7320062  14.41613952\n",
            " 14.51656934 36.01974784 25.78864511 21.05524178 14.10855057 17.25747573\n",
            " 35.961314    7.80732048  9.31455583 34.37534216 25.85033011 31.02198247\n",
            " 23.16140014 32.58621874 18.86065987 14.54015035  8.74315666 14.76328623\n",
            " 27.49006528 17.50428549 13.77059597 20.28218539 21.88730867 13.51954528\n",
            " 40.64856743 28.18791941 36.78641396 12.57677065 30.97931531 10.00179947\n",
            " 13.62506759 49.91216572 33.52793862 11.93656086  7.96027764 28.40298609\n",
            " 17.21433468 20.01928902 36.40963911 18.90091106 20.7937176  10.96975843\n",
            " 13.1667081  24.10122429 37.23770045 21.13990664  9.21582301 48.87037244\n",
            " 15.45340904 36.41703492 22.40308513 25.33985321 29.1643463   8.2965674\n",
            " 16.01338568 26.53152414 11.14726211 32.48679012 27.97576463 13.63437549\n",
            " 12.93718273 28.90221909 18.99319945 30.44204289 19.8201865  33.23516644\n",
            " 20.31110803 35.3665698  12.40642397 12.36105762 31.94042504 18.89041287\n",
            " 12.20358762 42.35881466 22.27424663 44.30552532 26.73194272  8.36398751\n",
            " 28.46623961 16.9617327  17.54142519 48.85113222 22.69421847 20.11464759\n",
            " 24.15807459 40.96240853 22.4507874  32.39159721 36.99768925  8.74806648\n",
            " 42.67899833 21.68673811 24.15502152 34.71206646 20.024248   45.37459973\n",
            "  7.81495849 15.45708304 27.04600595 20.23990561 17.75031518 13.67947925\n",
            " 23.77789961 20.88659797 12.617094   31.62006293  7.81495849  7.81495849\n",
            "  7.46444263 37.42129737 31.44476213 33.82143633 39.24409729 41.93256887\n",
            " 13.439299   10.46191015 23.30584102 22.76265153 10.25793387 14.51958154\n",
            " 18.4359022  13.40843258 13.67252473 17.53904689 43.58016431 16.79930693\n",
            " 24.04306869 33.68382564 16.47855934  9.14703581 10.6523739  11.19972379\n",
            " 15.56525331 21.96025646 31.25894259 22.1208929  32.69513792 10.82506592\n",
            " 36.65829746 36.31084135 29.24149002 37.10784625 14.54417424 12.41310863\n",
            " 14.40573795 11.50236271 15.56915009 49.83757911 42.99186216 32.03202962\n",
            " 22.88111126 46.44548969 16.54279066 31.08412352 19.14830724 19.55451722\n",
            " 18.57712652  8.46039737 14.19735625 35.20985548 28.60296859 27.88025848\n",
            "  7.81495849 24.86418004 25.19705774 14.56017479 20.61821083  7.81495849\n",
            " 23.43360144 15.20377824 23.81140883 26.86693103 12.96796961 29.19986142\n",
            "  9.77691818 19.30083001  9.04150321 21.21153406 31.72259434 21.31766215\n",
            " 30.99792256 49.56569304 11.85948867 25.1702439  13.31976064 22.53542413\n",
            " 10.81176161 35.80956218 34.27248116 17.8203881  22.91348641 38.90719125\n",
            "  7.81495849 27.25581752 40.80639282 15.68186682 27.93369454 38.60527694\n",
            " 10.93362503 19.92290647 10.49846736 14.84228264 24.21947863 27.21227921\n",
            " 24.54428455 33.34817585 22.91624754 25.33253325 46.77459307 22.2469023\n",
            " 12.4276736  30.30411837  9.6760052  22.65154411 17.47599233 11.17334145\n",
            " 21.91731491 18.65051718 32.90411459 16.7554755  19.25297655 36.21002447\n",
            " 33.2427694  24.03264661 30.15149742 18.44772134 26.45075138 15.15501287\n",
            " 35.369039   25.2876877  31.10459671 33.93479347 44.95615374  8.25912652\n",
            " 21.60616758  7.80918062 23.69958508 12.09904652  8.04413501 47.6294364\n",
            " 16.9099774  11.77737706  8.68975861 27.83542482 40.32497591 33.56359165\n",
            " 23.33826016 23.2360596  22.49045838 36.49078518  9.42771648 11.62037291\n",
            " 12.17147552 47.72684063 13.80130184 37.17987749 20.70779725 27.67526349\n",
            " 21.57621295 11.31880791 30.83633981 40.27414218 18.63925082 38.81029969\n",
            " 23.44165692 35.1311378  31.74020806  9.04454385 34.92971283 22.19911535\n",
            " 47.3059669  21.1244769  18.672948    8.53624981 25.18572957 26.71715313\n",
            " 15.72658897 13.34921742 18.06302703 16.05063991 25.83342511 34.81436584\n",
            " 20.46654247 24.58649639 39.12074617 38.25249394 21.5087846  30.08132895\n",
            " 11.62840187  7.81495849 45.74745773 40.17172444 21.55650213 16.48976698\n",
            "  7.81495849 18.09423968 29.95659124 16.67821216 14.51958154  7.9770497\n",
            " 28.90441874 23.70355967 25.38587002 10.70934554 26.84791512 21.87492183\n",
            " 24.24255591 20.4225417  19.20489437 22.06001347 11.32386672 39.88453104\n",
            " 22.79779493 13.17467012 13.14809174 16.18573694 10.45142372 13.81734433\n",
            " 15.12069533 30.95834892 26.57649345 18.08262529 17.69307846 11.52094279\n",
            "  8.28902889 32.36429149 28.2582656  23.2548486  13.44428557 24.50268645\n",
            " 16.29982197 22.24728885 19.62402361 12.32486688  7.97896071 13.36409923\n",
            " 29.82539316 25.09400556  7.81495849 38.11854236 19.35137372 34.86111598\n",
            " 23.6050978  12.67093121 24.85619896  7.81495849 26.04800325 29.69776252\n",
            " 35.02629996 30.33166305 38.45096352 25.50595753 22.64619442 14.22631818\n",
            " 30.09116526 31.95322553 29.68942885 22.18581629 20.57224037 18.12460743\n",
            " 47.97103584 10.44784812 18.84684218 21.47002748 33.95017328 11.97670067\n",
            " 21.60479578 19.91900106  7.90390611 26.7854725  35.07421992 16.07396258\n",
            " 47.00146682 15.83194255 22.79724581 20.26594781 29.95162699 31.95709551\n",
            " 47.94856974 24.63293426 45.08989753 15.90110065 20.74730048 10.71159841\n",
            " 17.22012455 22.11833776 22.31687971 15.21377396 22.94619946 12.82409378\n",
            " 14.5137462   7.81495849 10.2297543  30.06810638 17.36045268  9.91587223\n",
            "  7.81495849 49.99998228 15.05069619 37.40102988 17.93617088 21.91804357\n",
            " 38.87047926 20.19249828 16.2054886   8.28706035 10.59784681 48.92402138\n",
            " 24.8932701  13.77833595]\n",
            "selection [ 73 690 447 431 357 569 589 817 495 876] (10,) [7.28277887 7.46444263 7.47746479 7.55613767 7.56169872 7.64774787\n",
            " 7.80732048 7.80918062 7.81495849 7.81495849]\n",
            "trainset before adding uncertain samples (310, 10) (310,)\n",
            "trainset after adding uncertain samples (320, 10) (320,)\n",
            "updated train set: (320, 10) (320,) unique(labels): [126 194] [0 1]\n",
            "val set: (982, 10) (982,)\n",
            "\n",
            "Train set: (320, 10)\n",
            "Validation set: (982, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.15625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (982,) [35.543475   14.16969588 16.87873666 21.89216525  9.95361326 20.81861124\n",
            " 22.85376768  7.28208972 14.63696202 12.89095534 14.91992308 44.41364143\n",
            "  7.8556433  43.01964325 40.90154716 25.1911054  16.76193019 18.35912156\n",
            " 14.38849066  8.00226076 11.87664796 33.69471521 10.4765071  10.03598181\n",
            " 19.66639382  8.79870749 32.36943109 17.94042374  8.00226076 30.43438995\n",
            " 17.89504206 10.10088836 18.56818173 28.0868293  29.4391375  16.24817239\n",
            " 25.1271648  40.07335278 12.76281923 12.02818157 14.97283678 21.05575191\n",
            " 19.41194362 18.72598169 26.78095502 11.1971376  17.11851413 20.05481027\n",
            "  9.23198811 24.44251615 32.6485974  42.69592841 25.92069556  9.33710276\n",
            " 24.741032   12.4866936  16.95815075 18.62857668 15.91832231 13.96996338\n",
            " 40.33672601 23.81486263 40.6485659  11.51678971 26.33079404 11.08280216\n",
            " 26.14206864 28.6453121  21.97180161  7.42144107 11.76493281 35.57546558\n",
            " 22.19588831 26.31638631 21.90917874 14.44457374 12.82963639  5.60058858\n",
            " 23.09649477 49.30687267 10.51894142 31.92553772 17.12796619 21.10921519\n",
            " 26.01538136 11.91951134 15.74459781 23.90514155 16.36096168 26.8925471\n",
            "  8.00226076 26.22858793 16.52276042 16.2998741  23.57133013 32.65226263\n",
            " 35.3855773  11.91193594 29.6960103  23.99530971 49.250788   33.85676061\n",
            " 15.01020571 23.5854925  49.75212491 35.51664364 37.14453027 28.33552885\n",
            " 15.37948337 10.15252897 12.91415732 24.06166178 27.25322234 22.68587838\n",
            "  9.87611282 22.38952637 34.72167885 30.2887951   8.60150467 23.66390943\n",
            " 15.01846914 20.09415306 15.55052773 27.65059058 14.67347749 10.04294836\n",
            "  8.00226076 25.8783398  41.54384972 23.14539503  9.4138752  38.34366375\n",
            "  6.10743078 10.57692099 15.80062652 24.38268659 20.28486391 12.83712492\n",
            " 20.30298108 49.28386464 11.42595114 24.46819325 11.0077259  18.27309897\n",
            " 15.55690282 11.02015237 49.9420214  17.06281788 30.53799165 26.47441371\n",
            " 17.90778602 25.80377828  8.00226076 15.87669411 31.32885723 12.05532178\n",
            " 12.4529922  24.27713276 13.39136253 13.64950178 11.37273297 16.94373827\n",
            "  9.51088652 28.72168024 15.45511737 12.97846618 31.45944365 15.86753545\n",
            " 12.31042018 17.34454711 17.12176995 10.70508847  9.59966055 11.27986668\n",
            " 23.41587489 28.3274962   8.00226076 19.83553788 14.07705058 18.02196612\n",
            " 13.56599229 42.13997912 14.27306794 18.96703528 17.60697387 13.20610152\n",
            " 25.39222854 12.61482207 34.03084151 12.57985186 23.62894054 16.19899268\n",
            " 49.09271816 11.69763084 19.96258376 17.58629753 25.06661389 21.77419112\n",
            " 29.83119383 34.49075207 41.61186398 23.86025614 11.73754335 13.56260465\n",
            " 26.60942676 47.75122606  8.00226076 32.26625587 15.57256446  8.42998256\n",
            " 26.20290337 13.21515661 17.11623131 28.86195428 16.4088535  10.7717898\n",
            " 24.8113125  11.84049029  8.34208165 13.86199245 42.4856113  15.50635637\n",
            " 26.12772046 32.66240584 23.16967773 29.23160652  8.27337782  9.96269273\n",
            " 17.50485985  9.64391483 17.87090298 35.94248828 15.63403023 23.97487106\n",
            " 17.79423954 29.74175405 17.25323109 22.11712787 29.32118463 13.59486625\n",
            " 16.547635   19.62143352 24.35823275 15.65858555 19.83210151 17.65525817\n",
            " 27.18295207 28.4581976  23.8296916  31.28400727 20.61785506 10.9340901\n",
            " 23.72687809 46.47579739  8.6185522  14.45851277  7.84549879 11.28045657\n",
            " 27.81165054  9.7642487  22.5317275  17.67807308 35.93114599 28.85019754\n",
            " 30.7295398  12.85639737 12.46747254 38.95852612 33.07656797 26.42352884\n",
            " 20.32022492 48.77397866 15.53085083 20.52823399 26.66205797 14.20758544\n",
            " 21.05669181 22.05855764 14.85422884  8.00226076 49.78352943  8.00226076\n",
            " 31.94849352 30.24515015 35.23272438 25.88696461  8.00226076 26.58695377\n",
            " 41.16977423 11.61974442 25.93556887 18.23341247 44.83651204 13.37545587\n",
            " 12.1573518  18.76134071 15.93298488  6.55086462 25.75478638 15.07055386\n",
            " 21.71097571 17.90818882 42.77539109 48.60199249 19.09773044 19.52671055\n",
            " 17.97208008 15.81511661 15.78911951 14.76528414 15.19130792 31.08333946\n",
            " 13.60027399 17.74402528 16.89971918  8.00226076 25.57774052 20.12951769\n",
            " 22.19588831 30.79420144 12.07587798 15.53596174 26.61044377 20.40639152\n",
            " 18.37552361 10.41101882 24.37034204 25.4253713  17.40894269  8.27545847\n",
            "  7.87000465 19.09959563 28.45975392 23.33264339 28.85121436 19.34955478\n",
            "  6.79189099 23.60158012 20.83074381  8.00226076 34.89959389 18.7643123\n",
            " 26.41640251 15.00725043 13.60739292 10.17887182 25.58969586 24.17891674\n",
            " 26.11589074 15.4937793  15.94702278 14.00162501  9.85309681 10.57167054\n",
            " 24.86359715 17.37568675 23.03053478 16.67387724 32.63316345 11.13247206\n",
            " 12.7903713  21.86737702 13.04752451 12.41841179 19.55507999 19.05055061\n",
            " 24.4766569  15.78243668 25.81972495 26.28313085 25.13451315 16.24729567\n",
            " 12.05342037 31.85797429 12.58359951 18.01522841 24.70379591 16.28808613\n",
            " 49.46400199 19.02099008 16.09406187 18.69863043 13.42699548 30.94621247\n",
            " 45.94275729 22.24686536 36.09556056 23.35914975 26.54401057  8.00226076\n",
            "  8.00226076 15.21021909 20.58599948 19.36956243 23.05115409 23.0291067\n",
            " 18.45522931 24.11352825 24.82508686 20.35850482 16.2232933  20.78404093\n",
            " 13.74910019 11.55539742  9.18850803 11.22663545 20.89183612 17.73439766\n",
            " 30.40961514 15.41602533 45.81072089 28.43074166  4.81204674 28.81385774\n",
            " 24.56438382 21.59266087 16.45550387 22.07070547 17.58505789 13.51448077\n",
            " 34.54123883 12.71104139 21.26435589 25.6243971  17.99347188 15.18067313\n",
            " 29.36857187 33.3290152  18.03893054 22.09889531 16.65282368  8.00226076\n",
            " 19.14201588 32.80040305 13.4176895  21.32097889 27.08118586 11.48385907\n",
            " 46.02591264 32.15069993  6.89504268 38.01063434 34.37349085 36.11770865\n",
            " 26.99125703 26.61359878 14.62668037 33.89389826 19.08432134 27.55269674\n",
            " 26.11847703 11.50426347 18.29263591 35.31910186 22.18564116 19.0405181\n",
            " 25.31822559 19.62433758 10.99262967 22.07603105 24.80639992 45.89698771\n",
            "  9.55958952 14.77281455 10.30002662 19.85538897 21.0120316  27.73821803\n",
            " 17.37852987 40.55338074 49.89212223 24.89189559  9.9823501  12.14568668\n",
            " 21.60834407 11.3051905  10.68348942 21.98462119 12.71673668 26.56823419\n",
            " 10.02654904 21.69665861 46.22311075 23.65488936 41.77713503  8.00226076\n",
            " 13.28775273 29.39491038 15.74323307 14.3134119  19.24213855 14.89279922\n",
            " 16.68501249 14.70252812 38.1404677   9.87366549 35.18571785 21.22406443\n",
            " 20.96657204 13.13321099 25.38696776 11.80214762 13.20399621 17.55093847\n",
            "  8.48339366 18.09227475 14.23594843  9.95994965 14.55074534 17.58935643\n",
            " 14.22109626 24.34388292 29.17583766 24.33315943 15.23210318  9.606973\n",
            " 19.48431198 38.48171594 33.06426844 30.81751183 28.76263917  8.00226076\n",
            "  8.00226076 23.78491337 18.51745567  7.07362853 15.85678354 33.04903523\n",
            " 31.34590163 23.32587002 19.37747371 15.12931498 38.90127115 21.23247453\n",
            " 20.97217186 10.11332406 24.45913242 18.21073506 30.03085434 33.40100802\n",
            " 19.89808426 36.58533158 15.14082726 10.57167054 18.60804084 10.75943879\n",
            " 23.2243907  31.8563671  22.26372515 31.28228222 46.14832802 10.78102012\n",
            " 20.74349955 19.69791745 14.22109626 14.32754585 15.42418514 13.86715576\n",
            " 26.68782638 17.71826518 18.68499596 36.03506023 49.8964299  19.98225893\n",
            " 23.27417617 34.28270196 24.18098854  9.62228966  9.72251349 29.80188434\n",
            " 11.60319642 13.78794182 37.08291717 17.39377043 15.87092062 14.34511076\n",
            " 12.63394595 35.03167312 24.72143954 20.30783637 12.56299759 15.30486979\n",
            " 35.67974941  6.70258065 31.70868752 22.91339018 29.02491367 22.05450958\n",
            " 29.61703941 18.65860831 12.05461697  7.98429531 12.50232112 24.62560615\n",
            " 15.08370428 13.61978497 17.05987043 20.88236487 12.04359062 39.2479494\n",
            " 27.09110139 32.31149958 12.5576616  29.78789002  8.79642751 11.20603625\n",
            " 49.92185465 32.66853034 10.33265683  9.25197536 28.77456198 15.59289169\n",
            " 20.19617667 33.91950102 16.51937957 18.088641   12.44695468 14.28181208\n",
            " 21.28445155 34.90667992 17.98144743  9.1522317  48.46783724 13.12683428\n",
            " 35.90582795 20.5990777  22.06885225 25.83002581  7.12230332 13.87630406\n",
            " 24.99062782  9.4146424  29.67540267 26.8521078  12.49519616 10.70285136\n",
            " 26.74336749 17.01305641 27.4686495  16.76090457 31.3021296  18.11360418\n",
            " 31.4417261  13.05533683 12.58693396 29.6331347  16.67730419 10.65087755\n",
            " 40.45057129 19.60336271 44.1210919  23.32184043  6.54298159 27.53616821\n",
            " 12.29602164 15.19130792 48.81492431 21.82166452 18.35205741 22.04660978\n",
            " 38.65677047 21.95856827 31.01197327 34.60985992  8.16455807 41.94045127\n",
            " 18.91327738 26.49735561 32.92968436 16.61714097 45.01368064  8.00226076\n",
            " 14.09135745 24.39000035 19.23106154 15.1255802  10.94674067 25.94581049\n",
            " 17.34134865  9.78825021 31.2166584   8.00226076  8.00226076 35.34163028\n",
            " 28.19016054 33.1818024  36.91431883 42.29773544 11.72134351  8.33954929\n",
            " 23.67467058 22.38223781  7.51841663 12.3136239  18.30400394 12.62904781\n",
            " 12.08029893 16.09591961 41.20056945 13.58968249 22.04991765 32.66133548\n",
            " 14.16438297  8.7615678  11.61875121  8.98923971 13.86019254 21.96531102\n",
            " 28.92020495 20.46827374 31.80058171  8.23663763 34.20474174 36.33199999\n",
            " 26.85827981 35.63959921 13.27117054 11.56678289 13.68014327 10.08807758\n",
            " 15.04750805 49.82019004 42.53685807 30.61827849 20.46294911 45.58410491\n",
            " 14.30482703 27.9516757  17.14068698 17.36631606 18.4464838   5.85911972\n",
            " 12.07757376 33.15887747 26.91879924 25.23366441  8.00226076 21.81426479\n",
            " 22.83109437 10.81568766 20.42409577  8.00226076 22.09758294 14.61200245\n",
            " 20.32297151 27.64953459 11.8764149  26.00681676  8.31406436 18.95023122\n",
            "  7.76928039 19.89129159 29.76532692 18.16398157 29.24467401 49.48721605\n",
            "  9.82370354 20.58369332 10.94219377 19.91242096 10.27896622 34.2749288\n",
            " 33.17152833 17.9918757  18.85620366 36.94075635  8.00226076 24.69042521\n",
            " 40.22229276 15.86639207 25.93354897 36.84977832  9.12643027 16.24204399\n",
            "  9.48680513 13.06057541 21.53545191 25.50426575 21.89909033 32.47149302\n",
            " 19.76680832 24.00049943 46.64315751 18.90010015 12.50098764 29.50246759\n",
            " 10.0448559  21.62699442 15.02126742  9.47954142 22.23701086 18.13899135\n",
            " 30.87237775 16.42864227 17.55093847 34.51308471 31.11043995 22.93318067\n",
            " 28.25366652 16.26848309 26.53354045 11.97131317 34.11826311 23.43611543\n",
            " 29.77314774 33.31996181 44.48091019  6.52482382 18.38740186 22.31503144\n",
            "  9.31943897  7.53215297 47.33259694 15.29353654 12.63190789  9.29252732\n",
            " 24.43252038 39.7133335  32.13410596 19.36337007 20.20233251 20.00486277\n",
            " 34.3080125   7.77317187  9.43149775 11.46768209 47.31567309 13.31531593\n",
            " 37.93922297 17.65634615 28.1035975  21.37191652 11.41770491 29.23160652\n",
            " 39.47635032 15.1487225  37.77778134 21.78094664 33.73294533 27.92995157\n",
            "  6.19984641 34.16915836 20.70301509 47.02477052 21.94324988 16.32075909\n",
            "  9.94474752 23.34900216 23.67964424 15.45514566 11.1762965  17.41615877\n",
            " 14.51973201 24.73607876 31.93601035 18.02560873 23.75301015 38.79166427\n",
            " 36.05743968 19.88662257 28.54808777  7.71131962  8.00226076 44.66975953\n",
            " 37.69492697 21.14393233 13.77750438 16.36529824 28.52024522 13.2977489\n",
            " 12.3136239   7.00076804 26.99931445 21.30875507 23.76342029  9.24951186\n",
            " 25.22536898 17.87090298 21.23247453 17.72730123 18.38949402 22.64336201\n",
            "  9.30415231 37.4445409  19.24835366 12.02808491 11.91951134 15.70795878\n",
            " 10.24485551 13.09724059 15.0961118  30.44826907 24.35361845 15.02444149\n",
            " 16.59184989 10.92934427  7.51617569 33.74271426 25.38318817 21.12089861\n",
            " 14.33010401 21.49224165 13.69201991 19.0356822  17.44786027 10.99998262\n",
            "  6.71071558 13.53382247 26.52488464 24.42364551  8.00226076 35.88997205\n",
            " 17.17405236 33.69244264 21.98388178  9.52216196 23.49113263  8.00226076\n",
            " 24.20106027 27.48489003 32.85573278 28.31031451 36.44704784 25.1570501\n",
            " 23.19588715 16.47279804 29.84601058 29.510854   30.41587642 18.95197005\n",
            " 20.08999798 15.49818384 47.76943089  8.50289817 18.39982931 21.10621095\n",
            " 33.00893428  9.88154601 19.81345022 16.96414132  8.08205905 25.78121984\n",
            " 32.16966013 15.06961453 47.00015432 12.86471254 23.51283815 19.11574969\n",
            " 28.96507557 29.72143019 47.7500551  23.21544674 44.1930501  15.57857205\n",
            " 18.64233124 12.04236626 16.39905626 20.95564341 20.44428102 14.47237565\n",
            " 21.69604233 11.74373295 14.23363912  8.00226076  9.54155428 25.77918339\n",
            " 16.3403715   8.43772555  8.00226076 49.99996801 14.14038076 37.15194675\n",
            " 16.89933822 19.36524279 39.12600787 18.21550354 13.22135428  7.80425223\n",
            " 10.70548323 48.82464959 22.49678467 14.17541085]\n",
            "selection [412  77 731 132 840 807 652 297 583 906] (10,) [4.81204674 5.60058858 5.85911972 6.10743078 6.19984641 6.52482382\n",
            " 6.54298159 6.55086462 6.70258065 6.71071558]\n",
            "trainset before adding uncertain samples (320, 10) (320,)\n",
            "trainset after adding uncertain samples (330, 10) (330,)\n",
            "updated train set: (330, 10) (330,) unique(labels): [130 200] [0 1]\n",
            "val set: (972, 10) (972,)\n",
            "\n",
            "Train set: (330, 10)\n",
            "Validation set: (972, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.15151515151515152, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0]\n",
            "std (972,) [35.81773557 12.44853511 15.243339   19.7245892   7.54993054 20.01329781\n",
            " 20.81340269  6.07754651 13.0495795  12.54375709 25.82301279 43.84404238\n",
            "  6.6537855  42.98413723 39.65727778 23.26679828 14.44466607 17.62069836\n",
            " 12.64354856  8.10868636 12.83352635 33.55846668  9.93861987 10.33542684\n",
            " 17.9457993   7.1562212  30.02597892 16.61136054  8.10868636 28.67415602\n",
            " 16.3797309   7.51237897 17.18164045 27.37759094 27.83788572 15.49548052\n",
            " 24.02845405 39.16391981 12.73676918 12.62184724 13.33446208 20.56464542\n",
            " 18.74819672 17.15587795 25.0228444  10.21534893 16.96531587 19.56028549\n",
            "  8.75333311 22.56361037 31.37667304 42.96685016 24.86968674  8.25632528\n",
            " 22.73071358 11.13199539 16.49704857 17.31842046 15.77859395 11.3282167\n",
            " 38.65988054 21.33857833 40.5373608   9.78075366 24.36197397  9.72821306\n",
            " 24.50043172 27.1839473  21.59756574  7.03077685 12.03837302 34.62355027\n",
            " 21.57953723 24.56131121 18.71106869 13.08861614 12.47312447 21.39760373\n",
            " 49.33452154  9.45936675 31.75838635 16.57762507 19.66367973 24.03586043\n",
            " 10.34551375 14.37034953 23.21597533 14.60620985 24.98409228  8.10868636\n",
            " 25.84946292 15.47554616 14.69353531 22.19540589 32.37355928 34.40027466\n",
            " 11.98560937 28.44769408 24.66490003 49.32978984 33.0491033  13.69228918\n",
            " 21.66887315 49.7746622  34.64487306 35.17295767 27.51892911 14.70688233\n",
            "  8.47159644 11.23359213 22.56773835 23.98648001 21.08884016 10.01397866\n",
            " 21.53630747 33.81886978 28.97146367  8.88512894 22.80612581 13.24417564\n",
            " 19.89933212 14.34768727 26.72206606 14.54631219  8.0646578   8.10868636\n",
            " 23.91824104 41.43802052 22.35205637  7.93353017 37.63391219  9.77902625\n",
            " 16.14547109 23.25945996 18.59831641 12.03330286 21.23510635 49.31589214\n",
            "  9.41867669 23.6291358  10.11940838 16.4808328  13.85524966  9.99602225\n",
            " 49.95360201 15.37789841 30.32253457 24.17943241 16.00461486 24.7768759\n",
            "  8.10868636 14.71916224 29.8566829  11.80625477 11.17108494 22.08343353\n",
            " 12.95934471 14.29293712 11.84772166 15.6541776   8.74909915 27.40235598\n",
            " 15.55975449 13.00334433 30.22784669 14.46566147 11.32946539 17.33609468\n",
            " 15.97071651  9.57953412  8.48704218 10.64157595 22.47993275 27.20394604\n",
            "  8.10868636 17.88753691 12.02937277 15.08672917 12.20418275 41.37644205\n",
            " 13.58281372 18.28179286 17.07438493 12.09878039 23.52045033 12.21806373\n",
            " 33.8760138  11.17090669 22.41856614 15.64043857 48.73646462 12.07579574\n",
            " 19.15151745 17.44324118 24.47665624 20.66814869 28.23511432 34.25925732\n",
            " 41.38903594 22.61880911 11.74422065 12.25205175 25.97435977 46.93386096\n",
            "  8.10868636 32.18859179 14.74656532  7.27291395 25.69827317 11.99868561\n",
            " 14.71983702 28.77675412 16.69371744  9.21227506 23.04384322 11.83613882\n",
            "  8.73630422 13.05417668 41.47464417 13.95207724 24.5696085  32.44555619\n",
            " 21.17299293 29.34982691  7.57043278  8.69206401 15.58941023  8.71263291\n",
            " 16.26280079 34.28167359 14.70296946 22.56881369 16.42225199 27.55545134\n",
            " 14.68521749 20.89668921 28.24176998 12.00856926 14.53090704 17.9360354\n",
            " 22.1467201  12.93613343 19.337164   17.51043092 25.41428449 27.09942312\n",
            " 22.51755758 29.23116666 21.06126328 10.21061002 21.62881282 46.57798639\n",
            "  8.49543526 12.70238946  6.84781423 10.52696233 26.39031101  8.52226821\n",
            " 21.54691571 17.01243168 33.40041646 27.73520639 29.07355375 13.5080913\n",
            " 11.41968747 38.70263607 30.39753704 24.98427886 20.15554261 48.85952792\n",
            " 15.27858829 19.77872073 25.93550773 13.10293585 19.35698585 20.10928587\n",
            " 14.77505996  8.10868636 49.80665945  8.10868636 33.04914775 28.22022098\n",
            " 35.3536803  24.01730824  8.10868636 25.49858237 40.6592255  10.73088259\n",
            " 19.62392686 18.11321818 44.6683042  11.85993512 12.15233219 17.34500583\n",
            " 15.3661538  25.25643445 13.90180813 19.20367906 17.82376591 40.89891709\n",
            " 48.62832003 18.03942639 17.68702284 18.19855324 14.3724002  13.37246029\n",
            " 13.43794561 14.64390177 31.4560942  11.8622689  15.07568443 16.11738507\n",
            "  8.10868636 24.67133599 18.39737033 21.57953723 28.73440416 11.07251096\n",
            " 13.87048563 25.70439149 18.22061191 18.38332651  9.32908294 22.81942625\n",
            " 24.0210624  17.60473734  6.94958179  6.98883653 17.43514051 28.77654245\n",
            " 22.43110228 26.83519048 17.50706316  6.04718373 23.11351484 19.17618146\n",
            "  8.10868636 33.22358178 16.78805462 24.96960716 14.11567381 14.76003429\n",
            "  9.05022632 24.60082807 22.19964758 24.13718225 14.22687716 15.31508418\n",
            " 12.29772642 10.44287613  9.81660737 24.10179548 15.95998166 20.92605695\n",
            " 16.47591814 31.27751284 11.24335277 11.79617968 20.7667316  13.49247812\n",
            " 10.65160087 17.84274736 17.47736266 22.69856225 14.12470467 24.03332442\n",
            " 24.56201773 24.50809968 13.87898884 12.14811882 29.3380429  12.08639197\n",
            " 17.19881806 23.20250109 15.90792724 49.46856658 17.43893875 15.77035302\n",
            " 18.83774501 12.04184969 30.26121625 45.47641747 19.74991131 36.21086577\n",
            " 21.61777629 24.50787566  8.10868636  8.10868636 14.52348057 20.55942116\n",
            " 18.00254253 22.22512194 20.86115645 16.64123411 22.0007949  24.27993915\n",
            " 17.77213601 15.29453209 18.34371305 12.19612438 11.86725986  8.2155014\n",
            " 11.37141493 18.70713542 16.51636899 28.74894413 13.85363102 45.68437917\n",
            " 25.96384608 27.57247249 22.80959978 21.5638813  15.43349509 19.9933086\n",
            " 17.96500628 11.88031575 34.25146378 11.6596944  21.09866862 23.2358004\n",
            " 17.61295827 13.68257878 27.36351173 32.13242018 18.12723006 20.48259725\n",
            " 16.49876243  8.10868636 17.67551668 32.68929881 12.08101684 22.02506838\n",
            " 25.03278356 10.50177516 45.86293363 30.47170235  6.40310388 36.96976237\n",
            " 33.1344779  36.36365441 24.71592723 23.65087937 14.34151289 31.56847475\n",
            " 17.39060918 26.08022195 24.87502584 10.28099969 16.96964714 34.58283851\n",
            " 20.18408198 18.38488417 23.18142937 18.14702602  9.84036336 20.94924335\n",
            " 25.30586204 45.81926732  8.52226504 13.62852211  9.19842689 19.15700126\n",
            " 19.76061218 25.34930905 16.22784716 40.07621337 49.89985453 22.8299576\n",
            "  8.72170508 10.50730323 19.76521806 10.06384937  9.15381516 20.77352395\n",
            " 11.66714878 25.48509635 10.3852306  19.68633327 46.532712   23.74171568\n",
            " 40.21521847  8.10868636 11.50691495 27.67877219 14.43985232 12.7960888\n",
            " 18.95671068 15.32603665 16.82282461 13.04906752 37.49696313  9.0095013\n",
            " 33.92667721 20.31477289 19.72301105 12.90828157 23.52535619 12.12004823\n",
            " 11.8089054  15.93936781  7.80319398 16.53528596 12.98491429 10.80814489\n",
            " 14.74893919 16.70653238 12.86420706 23.26538605 29.11281719 22.60121592\n",
            " 12.68777957  8.83071757 18.85380862 36.61094657 31.1601569  28.99504006\n",
            " 26.94465442  8.10868636  8.10868636 21.67876865 17.12587923  6.1718785\n",
            " 15.46233834 30.92661519 31.52223843 21.61866304 18.99047091 15.2876616\n",
            " 37.68153074 19.24369112 20.49116102  8.69093875 23.24921175 17.14657863\n",
            " 28.43523007 31.26056386 18.09566758 36.42338066 14.70081849  9.81660737\n",
            " 16.85849349  9.55179728 21.26782235 29.99775771 21.23021432 29.69504361\n",
            " 46.30593433  9.38711463 18.60916949 17.99474796 12.86420706 12.74387778\n",
            " 13.42935722 12.23232027 25.00932843 16.21459028 17.26393895 34.2420798\n",
            " 49.90365423 19.45485766 22.548325   34.90139757 22.30290015  8.56188332\n",
            "  9.12390866 28.33806978 10.78179602 12.69773344 34.9758419  15.5065715\n",
            " 15.18719853 14.46852638 10.81420277 34.32272151 23.68111936 18.30585565\n",
            " 11.37125126 12.8736595  34.85231751 29.24126449 21.73315406 27.55374877\n",
            " 21.26471419 27.40738393 18.18144534 10.5968959   7.00722964 11.21468032\n",
            " 22.96250302 13.9055243  13.63801577 15.81020758 19.5991     10.36669841\n",
            " 37.24174363 25.52523447 30.86183985 12.04823069 28.37180268  9.37891649\n",
            "  9.78604146 49.93350145 32.092671    9.292635    7.97707166 28.8036609\n",
            " 14.08404892 17.84694599 32.34914761 14.92909273 17.88971814 12.79344424\n",
            " 12.74734617 19.41733735 33.75176663 16.76607809  9.52150861 48.05633363\n",
            " 12.08790775 35.48662956 19.39951384 19.88906979 24.19837284  6.14284419\n",
            " 12.21109703 24.3479912   8.29244729 28.860009   26.04895509 11.17723349\n",
            "  9.05184124 25.46248315 15.24743373 25.46937381 15.05673208 29.30517227\n",
            " 16.5345147  29.2595887  12.4124317  12.47724217 27.84454375 15.0058474\n",
            "  9.27704853 38.88161491 18.20984736 43.97279571 21.44489971 26.34694191\n",
            " 10.927318   14.64390177 48.8692899  20.86022283 17.07739505 20.57459517\n",
            " 36.94910423 19.83980753 30.58381503 32.83377621  7.68881322 41.7504678\n",
            " 17.34980011 23.95100149 31.04000818 13.96256202 45.18448777  8.10868636\n",
            " 13.80331999 22.75759493 18.69917527 14.23847537  9.18479187 25.90371699\n",
            " 15.76351794  8.74755033 30.86246767  8.10868636  8.10868636 35.20836069\n",
            " 26.13352554 32.95440473 34.71921493 42.59018856 11.7886755   8.12656692\n",
            " 22.76382377 21.60986853  6.69566387 11.08604293 17.6815906  12.25386611\n",
            " 10.66231192 16.43693881 39.86693908 12.80413505 20.11378015 31.52503449\n",
            " 13.18792667  8.38816717  9.61469361  8.7637468  12.47515649 21.2997408\n",
            " 27.94145151 18.97945369 31.68745944  7.21259155 32.50786511 34.57012991\n",
            " 25.53536386 34.09543244 11.64320724 10.14130251 13.34223686  8.83805359\n",
            " 14.28875261 49.82721559 43.01358782 29.57018981 18.48624492 45.15825549\n",
            " 10.55363065 25.68057317 15.71943838 15.74959725 17.76209521 11.32565277\n",
            " 31.60646021 25.9078147  22.95397047  8.10868636 19.47823071 20.59620416\n",
            "  9.69443391 20.01635033  8.10868636 21.24411795 13.18406232 18.99676961\n",
            " 26.75407269 10.6009689  23.8108134   7.68948207 16.19311004  6.8162134\n",
            " 19.42462459 29.17618572 16.40438138 27.98080181 49.51323765  8.49005759\n",
            " 17.91692551  9.57112268 18.68488238 12.50103263 33.02910693 31.5373792\n",
            " 18.06917062 17.94866785 36.13140315  8.10868636 24.28912887 39.96775475\n",
            " 15.70746741 24.14021628 36.89525283  8.43037481 15.26010572  8.1751424\n",
            " 12.0465287  20.23483631 23.17307957 22.09479074 30.85205596 18.38370161\n",
            " 21.98495377 46.74585    17.43517779 12.49339467 28.24123616  9.26864204\n",
            " 20.91049748 14.66408338  7.92903231 21.37577806 16.03798045 29.06661198\n",
            " 16.16038454 15.93936781 33.46796479 30.08892934 20.47030369 27.1673811\n",
            " 15.14280333 25.45315259 10.56404343 32.58080992 21.41510174 26.68778508\n",
            " 34.78951036 44.50894122 14.75105799 22.02990277  8.4765667   6.67830246\n",
            " 47.27578804 14.19560747 12.58618176  9.65434068 22.30447761 39.95792283\n",
            " 30.18649234 17.76317257 18.52504808 18.46505013 32.78189637  6.81621095\n",
            "  9.68943745 10.20925492 47.16799354 12.96852592 38.87845817 15.64758112\n",
            " 27.60754286 21.3075185   9.24143115 29.34982691 39.50734683 12.78548186\n",
            " 37.92244511 20.83481866 32.50657259 26.31680513 33.64307903 20.25197275\n",
            " 47.05540074 21.76976441 14.48095219  9.9167196  22.65637075 22.08694911\n",
            " 13.51024982 10.10679006 16.8874673  13.35348542 23.95216237 30.26815962\n",
            " 16.09720273 22.88663479 38.64074985 33.17940443 18.92305372 28.54177871\n",
            "  6.72814664  8.10868636 43.4162578  35.85178657 20.11320369 12.82857241\n",
            " 14.675418   27.33139748 10.77202855 11.08604293  6.69360405 24.70504064\n",
            " 19.99418165 22.49587518  8.16401315 24.48303726 16.26280079 19.24369112\n",
            " 15.83200736 17.58076324 21.86968271  8.19590603 35.51564458 16.29878453\n",
            " 10.00357411 10.34551375 15.48088216  9.1348252  12.54060281 12.97007793\n",
            " 28.75766697 22.12291968 13.65639538 18.50789584 10.95908432  7.13357258\n",
            " 34.37421841 24.04529389 19.50993279 13.77261141 19.48777146 12.92226208\n",
            " 17.00340681 17.44621354  9.64090784 13.32115699 24.54594255 23.4187875\n",
            "  8.10868636 34.26170675 15.4407916  30.90987691 20.35574074  8.68183045\n",
            " 23.37751905  8.10868636 23.63813852 26.47567664 30.88234386 26.38738963\n",
            " 34.4872186  22.65681756 22.65136309 16.63560981 28.21718942 28.4950113\n",
            " 30.69801282 16.25638937 19.53396129 14.13608165 47.92095321  7.25133571\n",
            " 17.14487975 19.60140418 31.5007553   9.24233371 18.11081385 15.63131887\n",
            "  7.50374041 26.72812806 31.08540413 13.48620504 46.97797938 11.42312887\n",
            " 22.18114381 17.75512244 27.80731502 28.14380031 47.87725947 22.68139712\n",
            " 43.88479082 15.49453555 17.10429881 10.41478413 15.60339273 18.13952889\n",
            " 18.61318825 13.86843162 21.63275934 11.74968919 14.41025634  8.10868636\n",
            "  8.40695687 23.19622907 14.54423658  7.69972518  8.10868636 49.99988306\n",
            " 15.41670846 36.34266465 16.30269951 18.07645217 39.07998199 16.98793448\n",
            " 12.12174875  6.92434817 10.9652315  48.90244152 20.7017753  13.47961685]\n",
            "selection [333   7 623 521 436  12 803 862 686 852] (10,) [6.04718373 6.07754651 6.14284419 6.1718785  6.40310388 6.6537855\n",
            " 6.67830246 6.69360405 6.69566387 6.72814664]\n",
            "trainset before adding uncertain samples (330, 10) (330,)\n",
            "trainset after adding uncertain samples (340, 10) (340,)\n",
            "updated train set: (340, 10) (340,) unique(labels): [135 205] [0 1]\n",
            "val set: (962, 10) (962,)\n",
            "\n",
            "Train set: (340, 10)\n",
            "Validation set: (962, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.14705882352941177, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (962,) [32.7481506  10.74139527 12.77124783 18.39216461  6.04748495 19.30509797\n",
            " 18.79256484 11.12843361 11.54669817 15.7478109  41.46599778 40.73225915\n",
            " 36.11425747 22.0311913  13.13386854 15.97139927 11.0062848   7.56518224\n",
            " 11.02264648 32.01054624  9.66081091  9.62513956 16.29313352  6.861364\n",
            " 27.63416484 14.01296773  7.56518224 24.66987115 13.38781299 11.01586605\n",
            " 14.1269146  23.69019713 27.84760497 13.88346288 21.48833047 36.83308249\n",
            " 12.34593511 11.41084053 12.14258274 16.84796885 17.29784251 12.99712168\n",
            " 20.35956935  8.39145501 14.67800579 16.65023989  6.99262183 20.97017136\n",
            " 28.34868263 41.31525028 23.13075942  6.81376459 19.41388382  9.84688178\n",
            " 14.16274449 15.55860078 13.73531603  8.93165197 36.11050652 20.22366889\n",
            " 37.90587322  7.63002513 22.32653942  8.62022105 21.75546403 25.84466748\n",
            " 16.21914035  4.72551715 10.34463045 31.56963148 19.16960578 21.95089882\n",
            " 15.9354599  12.09272173 13.94563477 20.95294249 48.92680525  6.75842315\n",
            " 29.23782788 14.65973473 16.4762612  22.56082646 10.92702596 12.10068101\n",
            " 21.59745848 11.152381   21.90698478  7.56518224 21.39871078 13.27837297\n",
            " 10.01459656 17.77085199 29.28473702 31.10103062  9.59746081 25.92109011\n",
            " 23.73533941 49.02124391 30.38021596 11.98276933 19.59935743 49.58620299\n",
            " 32.5186128  31.88093885 24.28315022 12.65660694  4.99642769 10.24518814\n",
            " 18.97428634 20.18025988 19.06522334  9.95071439 18.3287235  30.49698531\n",
            " 25.04922086 11.53621991 21.27171201 10.73761413 18.29579092 12.21670446\n",
            " 23.70599462 12.21822349  7.00738938  7.56518224 20.00172439 39.27868501\n",
            " 18.49681484  8.1797123  34.89637853  8.21999541 17.02845913 21.79085875\n",
            " 15.78044447 10.26076623 20.88305042 49.0184875   8.50041258 22.72453208\n",
            " 10.20368476 14.14998802 12.72870094  8.82798591 49.88629109 11.19962714\n",
            " 27.03431629 22.90082685 14.86482785 20.74865844  7.56518224 12.36942392\n",
            " 27.14218817  9.87301709  9.85582436 20.17631057  8.99082956 10.03660629\n",
            " 10.13683146 13.99654328  8.21668051 24.48455619 13.03789858 10.00621534\n",
            " 28.06598091 12.91753307 10.35136617 14.39514707 14.82430942  8.00898552\n",
            "  7.28653822  8.89338492 20.69139751 23.33786758  7.56518224 16.33742158\n",
            "  9.87015007 16.08368331 11.3178912  38.22247441 11.3609262  16.45701838\n",
            " 16.13750717 11.44725647 22.06033179 12.47531854 31.79185368 10.9494157\n",
            " 20.9335721  13.96126065 47.99268194  8.44993466 15.84705569 14.56666507\n",
            " 21.19710076 17.61036148 26.22739632 32.16369539 39.21253371 19.84440359\n",
            " 10.60461645 10.69777026 23.06191995 46.27093073  7.56518224 29.89109341\n",
            " 12.23967668  5.90937304 24.53796971 11.09387958 13.28175964 25.92666091\n",
            " 13.77236809  8.18147724 21.18409202 10.84648675  8.79887745 10.91489902\n",
            " 37.86199858 12.70531015 20.87097182 30.96527569 18.99230711 26.50949279\n",
            "  5.20665959  7.98376638 11.51228073  7.48611269 14.52174001 31.37636342\n",
            " 14.13877591 20.67893758 14.25638317 25.02526474 14.53031102 18.06004609\n",
            " 25.3321006  11.25983968 13.67181459 15.23724519 19.76157377 12.82675968\n",
            " 16.21662772 15.90249231 21.92342189 25.93557221 20.01402274 26.47258814\n",
            " 19.62243236 10.46339886 18.98226078 45.51382291  4.34725809 10.86741705\n",
            "  4.61181455 10.04724307 23.10923411 10.00173008 19.06866862 13.42662571\n",
            " 29.82326025 25.70023092 26.37563051  9.98383013 10.85624336 35.63114678\n",
            " 24.77963886 22.89614828 19.97144266 47.92072464 13.43619036 15.28268639\n",
            " 20.23219727 10.59089818 17.78478916 18.81061497 14.45800074  7.56518224\n",
            " 49.61929283  7.56518224 32.45339566 26.32382636 34.72994445 21.68030558\n",
            "  7.56518224 22.57718197 40.12659528 10.12130638 20.63178223 17.90503618\n",
            " 42.70198167  8.64289235 10.85538077 15.86132663 13.95763066 21.4569993\n",
            " 12.7515217  17.87772446 15.37979516 37.80049213 47.95899295 15.95786487\n",
            " 15.7486649  17.0195917  12.32346665 10.98448962 13.07130886 13.25306469\n",
            " 28.16054535  8.44282801  9.74829856 13.01732813  7.56518224 20.09225628\n",
            " 16.17461553 19.16960578 26.04172438 11.49545827 11.97376056 23.95468546\n",
            " 16.07082848 18.65335064  8.18652972 20.66316443 21.2240491  16.63522212\n",
            "  3.81534317  5.87661641 16.45290469 26.80146097 20.57104584 24.36274059\n",
            " 15.60809789 20.70312156 16.82723426  7.56518224 30.02076465 14.86355357\n",
            " 23.66656846 12.33205899 13.01076139  7.72400903 22.61271223 20.18659722\n",
            " 21.31527132 13.13568873 13.32886405 12.27589918  8.36105262  8.74523305\n",
            " 22.01096254 13.65123309 15.02587762 14.37509883 28.4906892   8.09702583\n",
            " 10.18684318 20.74521438 14.06250632  9.45129657 15.63054979 15.85225489\n",
            " 20.27858485 12.42460951 21.58226517 22.06934966 21.03318064 12.65058934\n",
            " 11.03411863 27.451393    8.66850795 13.90382165 19.94347139 13.87926026\n",
            " 49.04755129 15.69011332 14.07202403 17.68278924 10.31834619 28.60610961\n",
            " 44.14834024 15.763573   34.35757261 19.65116924 23.0319735   7.56518224\n",
            "  7.56518224 12.82622565 16.71406543 15.00421256 20.07498713 17.82570398\n",
            " 13.10453825 19.45379085 19.49976507 15.8533141  13.03321944 16.40108651\n",
            "  9.56360978 10.2792124   6.86572527  9.04921123 16.82968864 13.81076388\n",
            " 25.99051597 11.22186591 44.1863716  24.70659747 22.98225612 19.05515601\n",
            " 19.9909043  13.35867663 18.1463192  16.35989189 10.63664271 32.04914027\n",
            "  9.37894492 19.56583482 20.93696375 16.22605416 11.86726065 23.67834667\n",
            " 29.56432919 16.47774324 15.97832764 15.6235023   7.56518224 15.94654909\n",
            " 30.91903845 10.49503249 19.74039854 16.55059798  5.28067789 44.19604199\n",
            " 28.07677976 34.17454716 30.57786712 34.13701947 21.89546887 20.67988851\n",
            " 13.05032829 29.32419074 14.91769623 22.84881616 20.8774715   8.77596431\n",
            " 16.21277664 31.47461789 18.63908875 16.75783819 20.63063223 13.81455031\n",
            "  8.10174788 20.23346537 23.98112261 44.39814912  7.73539621 12.38790703\n",
            "  8.58158454 16.49742679 17.91040184 23.06765236 13.32024568 37.56026083\n",
            " 49.79654064 20.12216285  7.77250072  9.64153292 17.37779307  8.96993162\n",
            "  8.29504048 20.09196213 11.93023385 23.66186219 10.51255744 16.93949468\n",
            " 44.99810399 24.97348257 37.81531458  7.56518224 11.10360842 24.47932286\n",
            " 12.89648308 11.2270856  18.35739149 14.01814351 14.68060655 10.31960292\n",
            " 34.71472116  6.93022058 31.00042323 17.48889728 16.12043213  8.83808891\n",
            " 21.09826751 11.27845205  8.64790295 13.67570063  6.05153493 15.07923669\n",
            " 10.86185915 10.03295427 13.56043651 15.54761998 10.77006414 21.71275888\n",
            " 26.91330789 19.24286457 11.71142378  8.81182454 16.48823979 34.32185107\n",
            " 27.83690503 25.6978486  24.93353335  7.56518224  7.56518224 19.51373915\n",
            " 15.08379945 13.35201102 27.48094622 31.71739696 19.19076127 17.63647525\n",
            " 12.02654989 34.88324358 16.46106767 16.98605984  6.68213266 19.89316153\n",
            " 14.95622289 25.62562485 27.89223738 15.04264007 35.02368337  8.11927414\n",
            "  8.74523305 16.77779936 10.07654605 17.34154307 28.65148504 19.15325674\n",
            " 26.87820563 43.34005325  8.29550171 15.76547719 16.14931578 10.77006414\n",
            " 10.40274318 12.26205145 10.03705556 20.28711615 13.68824176 14.46413283\n",
            " 29.17971649 49.75692707 16.57314223 21.8919596  33.09620891 18.98036431\n",
            "  6.16886111  9.07216314 22.55638041  9.49966136 10.78716989 31.14343249\n",
            " 13.22965998 14.38871137 11.72540232  9.18467116 31.18106102 22.62107534\n",
            " 16.0733027  11.14510397 12.19464137 31.96576588 25.730559   18.55863552\n",
            " 24.94240349 19.22300029 25.47394693 16.59389166  9.96886461  7.27179143\n",
            "  9.07964968 20.30692724 12.32585253 11.08941279 12.26776537 17.71044213\n",
            "  9.57446969 34.58916395 23.90434265 30.25392832 10.98364841 26.97360289\n",
            "  8.52564814  7.45123636 49.84319807 29.04995273  9.24941948  7.12029725\n",
            " 25.12686007 11.32642932 17.39116701 29.36000134 13.17507691 14.99686078\n",
            " 10.96981013 11.69679009 16.58447677 31.44730783 14.62673841  8.14461867\n",
            " 45.19588639 10.0805251  34.2263904  16.87726054 17.62141069 22.68756314\n",
            " 11.43099552 22.02282411  6.87064833 25.62778074 22.65911262  9.4690536\n",
            "  8.37817862 22.57856334 14.24587073 23.6392558  13.45010202 27.06848655\n",
            " 13.43165681 28.24899941 11.60577333 10.93959377 24.92386841 11.98040312\n",
            "  8.42677104 36.0129022  16.17859352 42.51319641 16.2566294  24.1319176\n",
            "  6.82267804 13.25306469 48.37896498 19.20249981 14.55098815 19.52926354\n",
            " 33.50454564 20.64309151 27.32178656 29.85260945  6.71966765 40.75081002\n",
            " 15.56807849 19.69645638 28.77425454 10.82392445 43.87988476  7.56518224\n",
            " 11.53095253 21.01851331 16.31430933 13.07362721  6.46131266 22.6062007\n",
            " 13.21731147  6.35232775 28.42018235  7.56518224  7.56518224 33.11021612\n",
            " 22.73817649 30.48076187 31.0272711  41.17414906  8.23027658  6.77873435\n",
            " 20.68818905 19.28451751  9.6922995  15.60240419 11.18732386 10.41437959\n",
            " 12.82849222 38.71352539  9.78883332 16.96821793 28.05573156 10.63891284\n",
            "  5.47500536  9.14900186  9.81256477 10.974159   17.98358239 24.08137677\n",
            " 18.46825198 29.10968923  5.66576114 29.21047333 32.42357922 22.04375735\n",
            " 31.41471824  8.19452557  7.61231311 11.22569752 10.26211205  9.41975005\n",
            " 49.67042852 42.32194772 27.23347662 15.77327468 43.60517753  9.92902846\n",
            " 22.71094057 14.33846383 15.71270221 14.83667403  8.25275255 28.74666494\n",
            " 22.81914532 20.65872631  7.56518224 18.22749361 19.11346834  9.33790604\n",
            " 18.21700082  7.56518224 19.9198287  10.92609274 14.98569234 23.30324982\n",
            "  9.31194855 21.28526051  7.11458176 14.5870385   6.14575725 17.1316048\n",
            " 27.32377324 13.964955   24.6762777  49.18655404  8.36093457 18.44301634\n",
            "  7.66224335 15.09693034  8.67618541 26.9849713  28.31923247 16.39493536\n",
            " 13.31416349 33.2613146   7.56518224 20.11312151 37.03087479 14.0958195\n",
            " 22.01889587 34.73259954  6.71854964 11.83720717  7.02713998 10.38809405\n",
            " 17.67003142 19.8539736  20.20174945 29.2616846  16.72845732 18.93052605\n",
            " 45.38101535 14.99823323 11.98005989 25.65581596  9.12164315 19.08179937\n",
            " 11.32055034  8.93674714 20.45840056 13.62561549 26.33381427 12.3422153\n",
            " 13.67570063 30.83895277 27.41500974 18.71422315 23.10134499 12.07474791\n",
            " 23.35832026  9.27494139 29.20374158 19.32231436 23.4064296  33.88709912\n",
            " 43.10672899 10.21369991 18.62528898  5.74707588 45.9970431  11.61072176\n",
            " 12.25174181  7.57290906 21.49327258 39.14676081 27.39514809 15.87648078\n",
            " 15.37813584 15.91919027 29.80654923  4.7627239   6.45068397  9.68286195\n",
            " 45.83434868 12.43123992 38.66532322 13.12713917 24.95494266 20.3384631\n",
            "  9.04147856 26.50949279 37.78245759 10.44550132 32.96030598 19.34772196\n",
            " 29.64966999 23.68907735 31.70028387 18.672889   46.14470416 18.3009218\n",
            " 12.53565898  8.99721284 19.4330398  19.186722   12.68659888  8.13531211\n",
            " 13.75293751 13.12407058 21.37095888 26.06080378 13.01374746 21.11977059\n",
            " 35.9535681  31.98546559 16.00844755 26.4665782   7.56518224 41.08658506\n",
            " 33.21432156 18.37788702 12.46672529 11.25544856 25.67431431  9.22992722\n",
            "  9.6922995  21.92443522 16.91566498 19.81790613  7.55295135 22.35879978\n",
            " 14.52174001 16.46106767 13.94638305 15.10726428 19.74804083  7.38318642\n",
            " 31.73060493 13.51687957  8.11381676 10.92702596 13.17317887  8.1070826\n",
            " 10.29343523 14.56120714 24.78061449 19.45876457 11.57124397 14.56734536\n",
            "  9.70343252  5.10534648 28.75685036 21.00569867 17.37643184 12.64323025\n",
            " 18.96204993  9.91264839 13.89216669 14.25260863  8.69633632 11.77305383\n",
            " 21.17225808 21.95530314  7.56518224 31.60608356 14.75927348 27.26204587\n",
            " 18.93465577  4.6911565  21.30407577  7.56518224 19.01638641 25.28863297\n",
            " 28.69845053 22.13679307 30.83233613 20.22475357 19.67422776 14.40390395\n",
            " 25.51309348 24.41750451 27.74834749 15.19724247 14.67586755 13.18406985\n",
            " 47.41209198  6.70835938 15.70577196 16.42113007 27.53390782  6.01370808\n",
            " 14.80437224 13.39292082  5.33001953 26.36403163 27.90888273  9.73778648\n",
            " 45.48825426  9.9910678  18.25103609 16.89868615 25.04443585 23.35473891\n",
            " 47.21445232 21.71826821 42.49985856 12.8648904  15.64754568 10.36428201\n",
            " 14.68915891 17.81277031 16.68146417 12.55882232 19.3947778   9.8269542\n",
            " 12.38015886  7.56518224  6.35920848 21.63165462 13.84143703  8.23101069\n",
            "  7.56518224 49.99953615 13.91111578 33.41672872 15.55229065 15.88354388\n",
            " 35.92248614 13.08966493 10.40808607  6.19890596  8.57002018 48.52024599\n",
            " 19.12385848 12.13403234]\n",
            "selection [324 250 252 895  67 807 106 877 222 430] (10,) [3.81534317 4.34725809 4.61181455 4.6911565  4.72551715 4.7627239\n",
            " 4.99642769 5.10534648 5.20665959 5.28067789]\n",
            "trainset before adding uncertain samples (340, 10) (340,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [138 212] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.45      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (952,) [33.99015295 11.50883008 13.88744845 19.32031707  5.68390974 19.62213549\n",
            " 19.72149764 11.98764224 12.11051974 20.8852997  42.62334041 41.67482831\n",
            " 37.70686288 22.36958289 13.43902858 16.44376088 11.6091719   7.75184464\n",
            " 11.83698063 32.28084468 10.66083422  9.85613845 16.6123918   6.67238899\n",
            " 28.44587989 14.92024526  7.75184464 26.02840395 14.37937082 10.58198827\n",
            " 14.79078513 25.2948241  27.69665385 14.64347827 22.32408325 37.86611619\n",
            " 12.73450796 11.90712605 12.7239163  17.93946255 17.929997   14.28272885\n",
            " 21.79421291  9.25765994 15.65333705 18.49617462  7.75945767 21.59119498\n",
            " 29.590223   41.63392352 23.72368188  7.19960429 20.08734152  9.9920813\n",
            " 14.88725465 16.23158253 14.42983277  9.17220936 37.33546183 20.45447791\n",
            " 38.94467182  7.9241562  23.37430774  9.16497376 22.92404256 26.96048849\n",
            " 18.26817209 11.01711545 32.94289146 20.24378179 22.81549346 16.81579244\n",
            " 12.65274787 14.02760769 21.67621726 49.09034195  7.04002197 30.07563705\n",
            " 15.28858511 17.25382395 23.04215373 10.8884541  12.97535002 22.46112696\n",
            " 11.90415283 23.0941623   7.75184464 23.15363114 14.03429237 10.89355913\n",
            " 20.2631391  30.61384608 32.6618507  10.33144734 26.66286281 24.49079069\n",
            " 49.09760851 31.70445454 13.25397492 20.48355451 49.64945058 33.40688756\n",
            " 33.04463081 24.8794949  13.68171285 10.36713019 20.42108836 20.2775276\n",
            " 20.39293498 10.40222935 19.27932126 32.21211958 26.67919291 12.61846552\n",
            " 21.87498286 11.51053293 19.35403367 12.93875956 24.91724148 12.75550607\n",
            "  6.28593109  7.75184464 20.83542077 40.07508948 19.75052907  7.87104353\n",
            " 36.20082124  9.12618157 17.93832203 22.35377338 16.78202386 10.91584748\n",
            " 20.6111611  49.04781531  8.06602265 23.28196807 10.45344625 15.19613651\n",
            " 13.57757696  9.67866173 49.91831361 11.6698637  28.86003457 23.41360094\n",
            " 15.22988366 22.85381655  7.75184464 12.76003299 28.46815761 10.91535177\n",
            " 10.37033239 20.70419171  9.65265738 11.48150582 10.73479627 15.05997222\n",
            "  8.28886    25.70293214 13.77976605 11.36799034 29.20173724 13.34450442\n",
            " 11.08681303 14.82729914 15.28756151  8.63599119  7.67607725  9.439433\n",
            " 21.3075685  25.50222244  7.75184464 17.00351181 10.29834043 15.88630635\n",
            " 11.70595689 39.07067735 12.49894268 17.05921909 16.76692088 11.91821231\n",
            " 22.38106586 12.63344116 32.53776728 10.83114776 21.14600788 14.4938461\n",
            " 48.32819211  9.77839712 17.27279475 15.77812444 22.34288964 18.73378219\n",
            " 27.36373604 33.05123495 40.07667836 20.91647778 11.12906528 11.42867671\n",
            " 24.22259183 46.64507923  7.75184464 30.46101017 13.44448893  6.4599629\n",
            " 25.05601829 11.13722002 13.86082019 26.93882729 15.13121196  8.53011068\n",
            " 22.22601456 11.17554706  8.92013868 12.24297054 39.13316044 13.06700538\n",
            " 21.85387052 31.28751378 19.78294151 27.06934239  7.78872261 12.41519375\n",
            "  8.01725441 15.33476948 31.98924177 14.43788336 21.4340292  15.11212429\n",
            " 26.30359253 14.10743638 19.34980009 26.67964247 11.54586539 14.14029574\n",
            " 16.17020524 20.46371473 13.31704618 17.2172959  17.26687776 23.52350138\n",
            " 26.4174725  21.38765188 27.67926276 20.02028047 10.3965918  19.84292626\n",
            " 45.88798858 11.43692256 10.59992698 24.54949747  9.56331569 20.41402426\n",
            " 14.82176727 31.03686643 26.57452402 28.06040145 11.15429045 11.07976779\n",
            " 37.03960723 25.84441429 24.06968862 19.91987803 48.35543949 13.94331759\n",
            " 16.77833032 21.99195118 11.45767045 17.89871398 19.6101063  14.71967572\n",
            "  7.75184464 49.71689571  7.75184464 32.15626765 26.85114323 35.14998253\n",
            " 22.94399154  7.75184464 23.71973104 40.48636936 10.58587964 20.35307549\n",
            " 18.24902803 43.64627095  8.85303873 10.94924647 16.73675919 14.41113816\n",
            " 22.93253429 14.01370932 17.80159937 16.44400312 38.40741817 48.17823734\n",
            " 16.38956021 16.65651692 17.28193942 13.11055481 12.27774844 13.2297119\n",
            " 14.33312084 28.72516827  9.08292579 11.21859544 14.31922919  7.75184464\n",
            " 21.97996218 17.23005919 20.24378179 26.94108337 11.71782177 12.6848764\n",
            " 24.72916863 17.13119627 18.91035533  8.57490057 21.08866361 21.65384947\n",
            " 16.64565876  5.98282384 17.05013015 27.63450306 21.21849852 25.35052853\n",
            " 16.52471685 21.16798886 17.38981064  7.75184464 31.42772436 14.99959811\n",
            " 24.28289981 13.33429294 13.23499605  7.76721916 23.37180994 20.65777327\n",
            " 22.52243393 13.44310533 14.14734775 12.02596569  8.67977015  9.21551339\n",
            " 23.14456301 14.77916234 17.20013713 15.26576646 29.88512359 10.10588085\n",
            " 10.71126839 21.1046274  13.49048474  9.57450197 16.23805433 16.40544467\n",
            " 21.32388699 12.97620989 22.72243412 23.17661678 22.5679149  12.18724368\n",
            " 11.64383993 27.95173706  9.80033773 15.23446917 20.48727079 14.57274542\n",
            " 49.24494608 16.56109581 14.71480641 17.65036103 11.20215506 29.84338019\n",
            " 44.94477078 16.69722661 34.80078231 20.253139   23.69279513  7.75184464\n",
            "  7.75184464 13.4423587  18.09672691 16.16890983 20.96513705 18.73040222\n",
            " 13.57760745 20.14582582 21.68587114 16.53362306 14.05740614 17.32899884\n",
            " 10.65096886 10.687158    7.29644559 10.23256941 17.37398424 14.84341901\n",
            " 27.13067765 11.93579833 44.75421368 24.96024834 25.11648619 20.16034013\n",
            " 20.48584909 14.61104619 18.94689539 16.57829727 10.59919041 32.85343519\n",
            "  9.68969331 20.16857847 21.11612006 16.91218306 12.54298279 24.95776603\n",
            " 31.06622456 16.89955905 17.19022248 16.24257353  7.75184464 16.69407521\n",
            " 31.43570446 11.11478941 20.53818683 21.35244649 44.91679981 29.39754193\n",
            " 35.89965203 32.1708758  34.67036244 22.78948462 21.61691754 13.57762401\n",
            " 30.05275886 16.01594302 24.48017304 22.2648883   9.65361599 16.32939503\n",
            " 32.98000855 19.37162833 17.28264787 21.75566551 14.65456925  7.75466905\n",
            " 19.88541661 24.1100192  44.85672515  7.74480563 13.00561012  8.83827595\n",
            " 17.80657394 19.33435751 23.8249544  14.5997461  38.73485589 49.84364841\n",
            " 20.79754135  7.77389158 10.20623073 18.32158528  9.56601593  8.66321663\n",
            " 20.38720588 11.88739256 24.56803345 10.79704705 18.16003072 45.5233584\n",
            " 24.20052363 38.61623379  7.75184464 11.36299539 25.36140774 13.64396953\n",
            " 11.58908515 19.23904917 14.96491468 15.59410373 11.26770793 36.16358273\n",
            "  7.62317717 32.20889159 18.55815605 16.96507157 10.32597203 21.98343389\n",
            " 11.69572891  9.4957677  14.35867646  6.4095092  16.18113396 11.77574837\n",
            " 10.32938226 13.68931763 15.62799717 11.71657888 22.14964128 27.62563158\n",
            " 20.33506154 11.7006242   8.80501601 17.40617986 35.29452587 28.47456954\n",
            " 27.00196551 25.8751281   7.75184464  7.75184464 20.45807088 15.89942836\n",
            " 13.97457156 28.83953855 30.06019552 20.41383195 18.01194763 13.4236566\n",
            " 35.68568041 17.46663914 18.49569062  7.16391608 21.71900571 15.53921914\n",
            " 26.17844944 29.20711842 16.24444183 35.76419512  9.54969137  9.21551339\n",
            " 17.07982731 10.07963067 18.37118224 30.23119032 19.65909574 28.02523699\n",
            " 44.71976573  8.79562584 16.26510345 16.59024234 11.71657888 10.43905421\n",
            " 12.84743825 10.96920526 21.70012046 15.02491197 15.1848321  30.62866822\n",
            " 49.83938311 17.14332981 22.04255232 33.54243896 19.28521868  6.66042474\n",
            "  9.09699862 24.31584197  9.84104365 11.7465075  32.38044474 14.29513271\n",
            " 14.59385928 12.46735063  9.54630818 32.65988139 23.23169723 16.38580161\n",
            " 11.70218998 12.34466179 33.2665074  26.76364456 19.51266617 25.88430942\n",
            " 20.34796128 26.20590171 17.25420805 10.01453219  6.74543806  9.45083172\n",
            " 21.56481368 12.91826379 11.75173687 14.23682317 18.2937173  10.11095013\n",
            " 35.57874551 24.45915353 31.11713123 11.35145221 27.50952536  9.24789967\n",
            "  8.08323026 49.88814971 30.22138052  9.3983038   6.93793995 26.57898686\n",
            " 12.04298948 17.0465058  30.68573737 13.66470566 16.38017961 11.54050353\n",
            " 11.37614436 17.80760064 32.63338472 15.49511731  8.66966982 46.82100164\n",
            " 10.61035305 34.523189   18.14892712 18.51550923 24.47192741 11.81831196\n",
            " 23.12378436  7.35861903 27.0379784  24.24778682 10.06164578  8.75885025\n",
            " 23.66547034 14.68767281 24.42378286 14.29141599 27.91327911 14.36695252\n",
            " 29.31702327 11.71703131 11.38402319 26.11120325 12.95590161  8.86672772\n",
            " 37.23449745 17.20081026 43.2102022  17.81433944 24.41764791  9.21738973\n",
            " 14.33312084 48.56557677 19.83126909 15.2694848  19.66879041 34.81618676\n",
            " 20.27492445 28.69451311 31.05930405  7.36532428 41.51110099 16.25234966\n",
            " 19.67420459 29.70896768 12.36979144 44.29781054  7.75184464 12.25839825\n",
            " 21.79528726 17.19141725 13.64170685  6.6659861  23.32060067 14.26669342\n",
            "  7.58612275 29.92089554  7.75184464  7.75184464 33.75623667 23.86723817\n",
            " 31.64038064 32.23376916 41.75956375  9.07441204  7.26566109 21.53676972\n",
            " 20.00322956 10.22092483 16.19508762 11.75635848 10.32433766 14.16913518\n",
            " 39.81699457 11.0942763  18.02969844 29.24023108 11.80670453  6.1489642\n",
            "  8.39692658  9.92948094 11.49194897 19.06738854 26.1966113  18.67668399\n",
            " 30.12788531  6.45429128 31.1230058  32.85863958 23.47552321 32.37697017\n",
            "  8.90545753  8.55301953 11.76275144  9.87583333  9.86820046 49.74649303\n",
            " 42.12138856 28.28088889 16.35605393 44.40291012  9.05336284 24.0634464\n",
            " 14.73628389 15.76829487 15.49203294  9.15752381 29.69809591 23.82237083\n",
            " 21.17905847  7.75184464 18.72591371 19.81480754 10.33976022 18.95034744\n",
            "  7.75184464 20.66765252 10.79867341 16.78938386 24.01983181  9.56539006\n",
            " 22.10252531  7.55496289 13.8450542   6.01367701 18.07678645 28.04442389\n",
            " 14.90232796 26.40735333 49.30171906  8.23247446 19.15761945  8.66564556\n",
            " 16.10331036 10.35193399 28.74662286 29.10611252 16.89101722 14.59960612\n",
            " 34.37665813  7.75184464 21.82570565 38.38630584 14.66477735 23.14855623\n",
            " 35.62311353  7.12016944 13.22102137  7.40488172 11.08251398 18.57057216\n",
            " 20.66285171 20.57190082 29.88572884 17.76808314 19.59486248 45.80870985\n",
            " 16.15513577 12.33220765 26.43891987  9.18551947 19.71446743 12.63528109\n",
            "  8.80486383 20.76482326 13.48900611 27.28613246 14.14979091 14.35867646\n",
            " 31.87196667 29.11800878 18.992512   24.78639102 13.2372952  23.77770489\n",
            " 10.04141494 30.15312626 20.5103941  23.51138316 33.79606918 43.48940805\n",
            " 11.3066646  20.35620383  6.64690095 46.58295724 12.69777241 12.65986524\n",
            "  8.05512498 22.19602064 39.02185229 28.38048308 17.05130698 16.12897243\n",
            " 16.80324588 31.23330686  7.34436003 10.05350016 46.50388755 13.02977629\n",
            " 38.24301966 13.97990645 26.04722555 20.50601176  8.12228799 27.06934239\n",
            " 38.27855063 11.48355305 34.69962161 20.36107604 30.54258166 24.82444317\n",
            " 32.40116842 19.36784236 46.42538765 19.4973844  12.94877587  9.11696239\n",
            " 20.66458065 20.2783687  12.4841773   8.68679642 15.49809262 13.14276224\n",
            " 22.38267353 27.53614711 14.02542111 22.12849705 36.94242817 32.13428144\n",
            " 17.07108461 27.2016055   7.75184464 42.13442996 34.55409671 18.85573832\n",
            " 12.73434497 12.01218988 26.53211869  9.12039675 10.22092483 22.70924542\n",
            " 17.87864388 20.69199355  7.84614716 23.38692824 15.33476948 17.46663914\n",
            " 14.65642096 15.85728269 20.35355762  7.94988443 33.11175689 14.77918038\n",
            "  7.83373329 10.8884541  13.46084185  8.37954302 10.97504632 14.2503492\n",
            " 25.91527685 20.10551567 12.52582463 16.1755246  10.20994882 30.71800698\n",
            " 22.19173647 18.37551413 12.90080321 19.83345901 11.14995755 14.95365926\n",
            " 15.60832245  8.89795544 12.18215217 22.77229024 22.64852409  7.75184464\n",
            " 32.99419048 14.8780896  27.38057285 19.40031382 21.8898855   7.75184464\n",
            " 20.50968581 25.94487416 29.58145637 23.42385403 32.21476653 20.24068967\n",
            " 20.50817547 14.88361039 26.06056622 26.19304867 28.60617812 15.44989009\n",
            " 16.30937011 13.86768109 47.41292039  7.10488284 15.91469894 16.79678254\n",
            " 29.01401543  7.22918455 15.63372224 14.49668903  5.93433388 25.89393447\n",
            " 29.3649686   9.8445035  46.15383697 10.56912342 18.90626365 17.23291734\n",
            " 25.88668766 24.58352601 47.32506337 21.91563887 43.13513998 13.71620538\n",
            " 16.22408863  9.71238566 14.9216864  17.24377206 17.30090515 13.03694389\n",
            " 20.01623869 10.44054423 13.2046885   7.75184464  6.47467929 22.61141959\n",
            " 14.07561708  8.56754947  7.75184464 49.99971408 13.90558532 34.63272061\n",
            " 16.0520422  16.85037694 37.01550813 14.34916847 11.25083347  6.49077271\n",
            "  9.16920015 48.59154892 19.41419699 12.36772024]\n",
            "selection [  4 910 319 729 683 120 489 691 203 934] (10,) [5.68390974 5.93433388 5.98282384 6.01367701 6.1489642  6.28593109\n",
            " 6.4095092  6.45429128 6.4599629  6.47467929]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (360, 10) (360,)\n",
            "updated train set: (360, 10) (360,) unique(labels): [141 219] [0 1]\n",
            "val set: (942, 10) (942,)\n",
            "\n",
            "Train set: (360, 10)\n",
            "Validation set: (942, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1388888888888889, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (942,) [35.296015   13.50803157 15.2402858  18.3412905  21.51117972 21.77616351\n",
            " 13.65837039 13.25031695 24.10118297 44.01899969 43.31604411 39.14708512\n",
            " 24.27454811 14.73558471 18.28307575 13.22046158  8.41989291 12.61634433\n",
            " 33.4732395  11.0409633  10.75892765 17.94998999  7.7112668  30.47171365\n",
            " 17.06379587  8.41989291 27.37887765 15.23378411 13.6827554  17.0953425\n",
            " 26.56771877 30.59619732 16.31702599 24.38758734 39.64423018 13.43779098\n",
            " 12.72955369 14.74062113 20.32667765 20.01946508 16.09033547 24.37392608\n",
            " 10.51160456 17.98666107 18.63253038  8.25019019 23.37998462 30.90255537\n",
            " 42.52557941 26.37675988  7.41268431 21.60755423 11.39858742 16.13254101\n",
            " 17.94495091 15.25162867 10.72551649 38.80161809 23.1186842  40.49276987\n",
            "  9.01111115 24.15875596 10.5612812  25.0712704  28.2285427  20.47183428\n",
            " 11.77456138 34.35119546 22.70958476 24.69291548 17.43988993 13.66990882\n",
            " 10.96862563 24.76893361 49.34556407  6.93548111 32.53474951 15.45513667\n",
            " 19.34067936 23.93218597 12.55569172 14.03641351 23.81657151 12.63781335\n",
            " 23.82681095  8.41989291 24.94777108 16.12163261 12.37624519 21.47448235\n",
            " 31.7703495  33.70281228 11.09226295 29.51838582 25.98531668 49.30747352\n",
            " 33.46454952 13.34656907 21.94973988 49.76123176 35.85233517 34.7206845\n",
            " 26.66940522 14.7567437  11.30363576 22.23843531 22.80446823 22.13023296\n",
            " 11.41695364 20.89249713 32.73260518 28.63116733 14.10190532 24.25458502\n",
            " 12.0634678  20.88657531 15.4278913  26.92123032 13.63467081  8.41989291\n",
            " 22.2378964  41.38001932 20.93850548  8.97444758 37.56106176  9.40366586\n",
            " 18.63786794 24.91099741 18.22067969 13.25531334 20.7154454  49.22601506\n",
            " 10.34969688 24.37400027 12.42236019 16.7869596  14.48569993 10.46898831\n",
            " 49.95503572 12.65270226 30.13826153 25.75084753 17.21787588 24.39043631\n",
            "  8.41989291 13.59962362 30.26086475 12.5115277  11.15837383 22.86775533\n",
            "  9.77108651 11.04788787 11.51149897 16.08178196  8.37235505 27.15503689\n",
            " 15.19225631 11.80851273 31.20509349 14.71616939 12.12485651 15.26211731\n",
            " 16.26102509  9.45532028  8.78750678 10.24060464 23.8714642  27.44892776\n",
            "  8.41989291 18.55491673  9.94001305 15.60880936 12.91176705 40.89572992\n",
            " 13.39655388 18.81318704 18.92719249 13.08124478 28.31755866 13.74832645\n",
            " 33.45154179 12.20697755 23.57259773 16.9789959  48.82080378 10.12783016\n",
            " 18.71183316 16.93970127 24.50306546 19.35300962 28.73920744 34.52602562\n",
            " 41.91076083 22.690996   12.65743109 12.74698078 26.00242894 47.89191787\n",
            "  8.41989291 31.86248825 14.63965427 27.05011345 12.29340906 15.65329776\n",
            " 28.79795128 16.20976168  9.22871987 23.99622438 12.19497725 10.20019959\n",
            " 13.80013327 41.66128583 15.17066686 23.83778048 32.48967992 21.98445209\n",
            " 27.6201675   8.48876237 12.67176989  8.63029274 16.73619783 34.0685608\n",
            " 14.73244711 23.87295194 16.60454688 28.14739229 14.65916703 21.04141386\n",
            " 28.40620795 12.87389501 15.63902644 17.91184931 21.71259069 14.90824864\n",
            " 18.71506437 17.64855161 25.799332   27.76061142 22.60367147 28.99342436\n",
            " 20.32072122 10.55596474 21.43670201 46.66071032 12.68563405 12.2559599\n",
            " 26.51998113  9.81304943 21.05724169 15.79280637 32.34753464 28.22610113\n",
            " 29.52563068 12.73741372 12.98764559 38.34718301 27.39616888 25.28422577\n",
            " 20.23554983 48.88261203 15.14021848 16.54207692 24.45448227 13.08049337\n",
            " 20.35023298 21.93768611 16.24076481  8.41989291 49.83732748  8.41989291\n",
            " 33.17578583 29.69404851 36.15835353 24.82125771  8.41989291 25.25278253\n",
            " 40.4860741  12.09018784 14.96507286 20.57658777 44.62724921  9.87355904\n",
            " 11.73943091 17.48339961 16.45185419 25.95380647 14.32931318 20.73398233\n",
            " 17.31117909 41.28614785 48.58014921 18.11497466 18.54557506 18.1144763\n",
            " 15.19732078 14.0568894  14.38151922 15.35412178 28.69442428 10.03097634\n",
            " 12.79352061 15.27850143  8.41989291 23.54571774 19.19737072 22.70958476\n",
            " 28.35035937 13.88037729 14.24658536 25.07249892 19.38457663 20.5677946\n",
            "  9.62700306 23.32973854 23.53453637 16.76207651 18.60880463 28.6750773\n",
            " 23.34841641 27.3230008  18.26829065 23.34723459 18.31583444  8.41989291\n",
            " 33.00373167 18.19026566 27.54248774 15.11355278 13.63813071  8.91107284\n",
            " 24.73352123 23.22626188 23.83629909 14.58807781 18.05587909 13.68244806\n",
            "  9.00784093 10.30193512 25.14861203 16.3693023  19.80037645 17.49297623\n",
            " 31.94961357 10.10212344 11.87258027 22.8484494  13.72109517 10.31642456\n",
            " 17.10841646 18.20198729 23.35352472 14.07749739 25.03444616 24.50926288\n",
            " 25.80896675 14.94089952 13.22138704 30.55141571  9.91905577 17.53051461\n",
            " 21.92730986 16.19696541 49.47462892 17.88403456 16.33428492 17.59582637\n",
            " 12.74645862 32.42170261 46.40840197 18.63289055 35.58468345 21.78566477\n",
            " 25.12361302  8.41989291  8.41989291 15.22436465 20.01021657 16.62358196\n",
            " 23.40194997 20.90611243 13.95082603 21.40121412 23.49414508 16.73908957\n",
            " 14.94724359 17.85340929 11.19432565 13.59753562  8.66020253 10.97613279\n",
            " 19.00224272 16.08908896 29.04319819 12.88453945 45.57635149 27.1646279\n",
            " 26.27759946 21.09701609 21.30289364 14.73906842 20.39273211 17.03721004\n",
            " 11.64569083 34.41406155 10.86488704 21.06471399 24.18725129 18.45345317\n",
            " 14.27604516 25.79777083 33.26144603 16.4456912  19.25152319 17.07305958\n",
            "  8.41989291 18.15449724 32.73578327 13.06329203 22.04345501 24.62239223\n",
            " 45.79008148 30.83810908 36.57795175 32.96758378 35.79348666 24.97791347\n",
            " 24.12500646 14.73690143 31.92627863 18.02573206 27.22452146 24.85484412\n",
            " 10.35123458 18.08632737 35.0027806  21.83133139 18.9818127  23.51062697\n",
            " 14.67075892 11.3840767  21.55810646 25.39589496 45.75403227  9.20437252\n",
            " 14.37305492  9.71310335 18.42129039 20.55872691 26.79367732 15.44871503\n",
            " 40.58630384 49.90781186 23.85635272  7.8359896  11.49812995 20.89869958\n",
            " 11.37545129  9.66094333 22.0562063  13.23560746 26.97085875 11.59262465\n",
            " 19.27477301 46.43137305 23.35244095 40.43269239  8.41989291 13.07183939\n",
            " 27.21191865 14.99193036 12.67035623 20.15489456 15.47579904 16.25901058\n",
            " 12.16435577 37.68509973  9.48760273 34.30576272 19.00555218 18.29853984\n",
            "  9.3307252  24.06320024 12.91939536 10.17002432 16.24665492 16.86462071\n",
            " 12.45869624 12.3086433  14.57096188 16.83965593 13.76552674 24.32054337\n",
            " 28.74293816 21.87559345 13.30265625  8.85624276 19.36308965 37.68079522\n",
            " 30.44324045 28.7691741  27.26650732  8.41989291  8.41989291 22.64328671\n",
            " 16.81750456 14.93509775 30.51943716 29.43888262 22.45404637 18.21058311\n",
            " 14.03998635 38.04841723 19.17745642 19.46223006  7.88678189 22.7230715\n",
            " 15.96421559 28.5037419  30.85590356 17.17755107 37.33453906  9.62762168\n",
            " 10.30193512 18.44190266 10.80713134 20.45125028 30.58552352 21.3593307\n",
            " 30.35205323 45.83587539 10.49153163 16.18150817 17.99818982 13.76552674\n",
            " 11.6513165  14.62533524 12.9553659  22.94392815 14.39572307 16.69675568\n",
            " 33.0604432  49.91714681 19.20176522 23.60957629 34.34964518 21.06683819\n",
            "  7.71654988 10.13701818 26.6319461  10.9148964  12.81834021 34.25320885\n",
            " 16.53715276 16.08739557 12.79191393 11.11514306 34.06143357 25.71623164\n",
            " 18.10663583 13.73722877 13.53130803 35.49746505 28.73797808 21.34748009\n",
            " 27.71779218 22.03011355 28.07580769 18.83142252 11.4581647   8.09500247\n",
            " 10.86293139 23.22901143 14.37522605 12.35008779 15.6084116  19.7351041\n",
            "  9.62865527 36.80850046 26.90005228 35.51271381 13.00305463 30.22876359\n",
            "  9.96209908  9.18245258 49.93266027 31.77642443 11.04383904  9.2580039\n",
            " 28.73847661 13.38586573 19.51249357 32.67444351 15.43761771 17.45732345\n",
            " 12.99353805 11.8587449  19.55817262 33.96911506 16.85004756  9.5278434\n",
            " 47.8991164  11.12847114 36.26672699 17.54931815 20.67555965 24.95986877\n",
            " 13.21416793 25.06548795  7.21623756 29.23402762 26.36846783 11.49861594\n",
            " 10.27415997 26.00842469 15.54057866 26.20999782 15.57515938 29.15770801\n",
            " 15.67317751 32.17461316 12.84571676 12.4246573  27.73642082 14.59037354\n",
            "  9.16965944 38.42918892 18.83342668 44.28300309 18.30219567 25.72834002\n",
            " 10.67180961 15.35412178 48.93275209 21.95339999 16.21464764 21.31622028\n",
            " 37.13282978 22.45467408 30.13823529 32.64029253  8.46420731 42.92507292\n",
            " 17.53634286 25.03132807 31.56308097 15.09316017 45.00080419  8.41989291\n",
            " 13.7465388  23.82100221 18.88376968 14.95736567  7.70130391 25.57062117\n",
            " 16.01557562  8.76317518 31.75283817  8.41989291  8.41989291 34.73086307\n",
            " 25.3680208  33.28736547 34.07586632 43.11041686 10.57101641  7.97694154\n",
            " 23.91752363 21.62224713 10.93053067 17.57855972 12.58771961 11.60447258\n",
            " 14.2716274  42.93379365 11.86184364 20.51728834 30.65406185 11.5342322\n",
            " 10.38762577 11.22440614 12.70163167 20.08318271 29.12372802 20.48064503\n",
            " 31.5629842  31.52316923 37.02664344 25.99328359 34.26185869 10.21674944\n",
            " 10.62265657 12.75127044 11.50497283 10.52681658 49.83576508 42.73811438\n",
            " 30.95385646 17.00720352 45.8250257  11.49196273 26.25645259 16.78521091\n",
            " 17.20284347 16.58124305 10.06551511 32.33461075 26.13403365 22.92358499\n",
            "  8.41989291 21.38770038 23.2016397  10.60454037 20.14979834  8.41989291\n",
            " 22.74939863 12.84896295 18.11586555 28.75382823 10.24433454 23.89761724\n",
            "  8.65363231 18.30343618 19.16195521 29.46022976 17.06059564 28.8912656\n",
            " 49.49264764  9.29594137 22.937331    9.41607234 17.39754842 13.35542816\n",
            " 29.04246414 30.60227812 17.480339   15.87879035 36.27469755  8.41989291\n",
            " 23.76861696 39.85083558 15.77937007 24.26046689 36.83666201  8.36092075\n",
            " 14.41823635  9.1361676  12.74554008 20.14175943 22.63047852 21.92491509\n",
            " 32.69905611 19.38813435 20.04329836 46.70970107 17.34132471 13.69015438\n",
            " 28.18817347 11.17319597 21.6225084  13.69786289  8.95432089 22.72796452\n",
            " 16.58166006 29.30840744 16.82463511 16.24665492 33.76682173 30.59844828\n",
            " 20.02273281 26.54280371 14.97228554 25.9423149  11.5967026  32.37309769\n",
            " 21.71428669 25.49518681 34.46510221 44.24657024 13.13723122 20.72956696\n",
            "  6.52011821 47.37387627 14.83631636 14.73417673  8.34563131 24.44150972\n",
            " 39.88954395 29.41832459 17.60302561 18.78871738 18.31721157 32.73139103\n",
            "  7.53217364 11.33800946 47.1707482  12.90055744 39.00160981 15.52148327\n",
            " 28.37533623 21.84353973 11.10818871 27.6201675  38.87482165 13.57466109\n",
            " 37.19273743 19.96719313 32.83715136 26.65150531 33.8067374  21.36658296\n",
            " 47.11157874 20.71445783 14.59539585  9.59560431 21.62808982 21.831326\n",
            " 15.19791215  8.96695033 16.6098046  15.40704977 24.06651369 29.39368021\n",
            " 15.18434526 23.88452634 38.23471583 35.7575493  17.52056956 28.43402256\n",
            "  8.41989291 43.07283624 36.23570415 20.98912932 13.49609104 13.43849038\n",
            " 28.36733237 10.30542769 10.93053067 24.43365729 19.14305241 22.81464096\n",
            "  8.27818285 26.02204975 16.73619783 19.17745642 16.77227928 17.38779609\n",
            " 21.8829571   8.57640082 35.14813169 17.32566772  9.26350847 12.55569172\n",
            " 15.00003706  9.59982028 12.15390468 16.7216469  29.1314426  21.66434523\n",
            " 13.87166494 18.98523976 11.28419733 36.79070968 24.05969823 19.43622769\n",
            " 14.41224828 22.19254636 10.77592908 15.97748325 16.37577291 10.5791585\n",
            " 13.34677626 24.31308422 25.02311962  8.41989291 34.46811067 16.99495079\n",
            " 31.64056612 21.18133922 22.55852133  8.41989291 22.08135024 26.96547\n",
            " 32.34325285 24.95158201 34.50464989 23.81257055 22.34040641 15.98999972\n",
            " 28.41716282 27.31332747 30.39099971 16.38202954 17.33067671 15.233481\n",
            " 47.7428464   6.98540938 18.28038585 19.38935274 31.79239001  6.31717869\n",
            " 17.32315846 15.81065224 26.13470491 31.10690419  8.57149426 47.16095867\n",
            " 11.98213762 21.43623656 19.49485793 27.97594478 27.32294405 47.71442278\n",
            " 21.94188965 44.38227104 14.51488318 17.51473487 12.00464458 17.4738543\n",
            " 19.35300156 18.84536053 14.47862842 21.06531729 11.53122761 14.32973936\n",
            "  8.41989291 23.71624241 16.22147525  8.9804115   8.41989291 49.99984673\n",
            " 13.38069661 37.01653936 18.00696029 17.7141021  38.91872288 14.59100225\n",
            " 12.36700756  7.18423518  9.53481072 48.83549589 20.7190586  14.08560866]\n",
            "selection [899 780  75 895 937 608  50 792 652  22] (10,) [6.31717869 6.52011821 6.93548111 6.98540938 7.18423518 7.21623756\n",
            " 7.41268431 7.53217364 7.70130391 7.7112668 ]\n",
            "trainset before adding uncertain samples (360, 10) (360,)\n",
            "trainset after adding uncertain samples (370, 10) (370,)\n",
            "updated train set: (370, 10) (370,) unique(labels): [142 228] [0 1]\n",
            "val set: (932, 10) (932,)\n",
            "\n",
            "Train set: (370, 10)\n",
            "Validation set: (932, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.13513513513513514, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.45      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0]\n",
            "std (932,) [37.46012559 12.96722861 16.35163476 23.51231828 23.34482619 23.01213529\n",
            " 14.80652763 14.32414739 20.40972949 45.44971355 44.18631727 39.83925227\n",
            " 25.30376651 16.04360881 20.24714285 15.09862776  9.4119263  13.20451204\n",
            " 34.9426282  10.49041499 12.22879653 18.35147437 31.88765852 17.17569542\n",
            "  9.4119263  28.29038357 16.05244051 14.60092969 18.30825933 28.1557286\n",
            " 32.55908356 15.5050258  26.31259607 41.53481434 14.1205043  14.18912833\n",
            " 15.80378106 21.18199307 21.06131058 16.79506549 24.96499478 11.46985303\n",
            " 17.35951472 18.96226197  7.44830307 24.21297967 33.4586346  43.61485676\n",
            " 26.36320635 22.75044063 12.09207379 17.28816877 18.10487444 16.20627422\n",
            " 12.41121208 39.64824869 24.47264103 42.00002918 10.97046104 25.72911844\n",
            " 11.01322003 25.87342413 31.69655142 18.14981857 12.23376036 35.27177604\n",
            " 24.38563934 25.74591663 18.47082639 15.08802868 16.66975511 24.66882923\n",
            " 49.57107603 34.06938622 18.24270374 20.3952553  25.53278302 13.54426276\n",
            " 14.42185617 26.06748126 14.33401697 28.08714517  9.4119263  25.64393546\n",
            " 14.79280735 13.55385384 24.73845902 34.42999752 37.00250927 11.76621498\n",
            " 31.18077558 27.33896221 49.50801582 35.28361728 15.72582506 22.75406316\n",
            " 49.84106864 37.91553684 35.67265876 27.78854532 15.16819371 12.07377291\n",
            " 22.93177737 24.23775397 22.62064317 11.93946515 22.60930771 33.99278816\n",
            " 30.90820962 11.73473114 26.17333981 13.87788018 22.09209961 15.59669727\n",
            " 28.71699373 15.24118546  9.4119263  23.94815317 42.43606281 21.45271083\n",
            "  9.77703493 39.70423774  9.27575454 19.55438422 26.88754949 19.27463263\n",
            " 12.0997372  23.13763797 49.39522579 10.4232666  27.0930769  11.64817665\n",
            " 17.58993091 14.7075317  10.04223275 49.97679188 14.31308009 32.87646232\n",
            " 26.95598264 17.93425394 27.1825817   9.4119263  14.81469826 32.98773601\n",
            " 11.60125367 11.91097086 24.14407186 10.62589487 12.61893089 12.94925045\n",
            " 16.9319914   9.61197042 30.01409115 16.43884166 11.70943599 32.52901468\n",
            " 15.01906042 12.43998546 16.9017956  16.17794856  9.55238506  8.89271359\n",
            " 10.85111526 25.83469692 29.90984658  9.4119263  19.86847279 12.99340454\n",
            " 20.59520338 13.63340227 41.74495388 13.41331699 20.53365014 19.84976494\n",
            " 13.74803416 26.40563795 15.10761705 34.94011238 13.14316351 25.85100771\n",
            " 18.1183489  48.9011006  10.98370182 19.35368068 19.02683075 25.58990129\n",
            " 23.04587702 32.24402917 36.26184407 43.10988066 22.72545854 13.58026332\n",
            " 12.74944803 27.36545984 48.2867279   9.4119263  33.37097188 15.00544649\n",
            " 27.94746278 12.73856736 16.19478459 31.1682302  17.82544303  9.97221724\n",
            " 24.92457269 13.18508269 10.7391351  14.25753185 42.8483194  16.04488858\n",
            " 25.17603899 34.36630244 22.24932539 28.80714535  9.10291124 15.36144108\n",
            "  9.05598886 17.39046723 36.45514794 18.0884389  24.99315143 16.83378249\n",
            " 29.9946083  18.32800108 21.6343376  30.73880091 13.44666532 15.61996638\n",
            " 18.28585436 23.34215186 14.80299951 19.78796727 17.76652785 26.71778087\n",
            " 30.77270483 23.22192194 30.4458254  22.26403271 11.8371854  23.03563645\n",
            " 47.38620037 14.0196096  13.05322861 27.0909171  10.8285445  21.56380871\n",
            " 15.77533487 33.59099791 30.22498807 33.07608    11.8542501  12.55082297\n",
            " 40.87079324 29.78717128 26.88808945 22.20360171 49.2520094  16.41097763\n",
            " 17.68822407 24.46488392 13.2123567  20.80311702 22.4520505  18.65391469\n",
            "  9.4119263  49.90679447  9.4119263  34.4470859  32.01799075 38.88283652\n",
            " 25.6266318   9.4119263  26.28212753 41.86998161 11.81561541 28.07954746\n",
            " 22.02814838 46.07860687 10.89364538 13.73732702 18.60574914 18.26046804\n",
            " 26.30424067 15.38895049 22.49874297 19.21364013 42.72618889 48.9262673\n",
            " 19.93056564 19.95135483 19.3526817  15.04934141 15.2755292  14.89519629\n",
            " 15.66873903 30.08880966 11.67856641 15.14696771 15.90395686  9.4119263\n",
            " 24.14320987 19.1397481  24.38563934 29.73356581 14.38829763 14.9687976\n",
            " 27.88913815 21.09078064 20.99838467  9.53161139 25.48021426 26.43655195\n",
            " 18.25998051 18.79737029 30.39105463 25.2044484  28.04297814 18.92760706\n",
            " 24.27905602 18.95955029  9.4119263  34.5746207  18.81549652 28.99673803\n",
            " 15.22979216 13.52681382  8.57933637 27.36824517 24.22711127 25.57542454\n",
            " 14.89889728 18.22996837 13.93240068 10.40303113  9.87724763 27.01849418\n",
            " 16.79083209 20.30893335 19.03977001 32.12804923 13.7609815  12.84354707\n",
            " 24.80667752 15.04588349 12.09189833 18.51994694 18.85087628 23.81289428\n",
            " 14.73158584 25.37515144 25.15544347 25.08820829 16.09458148 13.05697823\n",
            " 31.96224278 10.54680682 16.92575897 24.49271423 17.15978319 49.67102508\n",
            " 19.15806922 17.67888806 19.10973562 13.19088923 32.28054977 46.63835119\n",
            " 20.75717815 37.75270662 22.13157379 25.82249885  9.4119263   9.4119263\n",
            " 15.36813345 20.56948723 18.56096369 24.62639722 21.47467999 15.95875467\n",
            " 22.60885986 25.15067107 18.31767528 15.26773715 21.60273005 12.54763047\n",
            " 13.75108438  8.07091366 12.62674846 20.2075994  17.51810269 29.83901017\n",
            " 13.52062019 46.60942722 28.3295018  29.01720068 22.26035836 22.36733146\n",
            " 15.8456885  21.92145889 17.75413004 12.56286388 36.12310569 11.02225835\n",
            " 22.77509939 24.53570608 19.13539343 14.51574908 27.53065825 34.66668865\n",
            " 18.40483753 19.66968147 17.73633476  9.4119263  18.91246954 34.47001986\n",
            " 14.15646289 22.65087244 26.09524905 46.74437198 31.6529525  39.06192427\n",
            " 33.62769331 37.57461653 25.63603214 26.98444958 15.62042353 33.55499713\n",
            " 18.40338754 26.37755778 25.58506607 12.21798693 19.16364067 37.24906386\n",
            " 22.13179283 20.34578681 24.56906951 17.00359419 10.67349354 24.40840925\n",
            " 27.51190624 46.50966343 10.35027743 14.89943195  9.89068044 20.10836051\n",
            " 21.12619517 28.02440247 15.11057513 42.59280259 49.94659364 24.66331467\n",
            "  9.30321123 11.90417247 20.59479754 11.33984631 11.8068475  25.24574345\n",
            " 13.27208403 29.1717722  12.12193898 21.63179928 47.2328315  24.60419509\n",
            " 43.07718386  9.4119263  15.26219971 30.25238207 15.82915919 12.76657792\n",
            " 22.58286651 17.08694424 17.9215706  12.74659422 39.71036228  8.59479639\n",
            " 35.85489302 22.72588535 19.62806191 10.17699487 24.84818526 14.44606359\n",
            " 11.31689272 17.65358605 18.13583107 13.29576861 11.02254234 16.07061628\n",
            " 17.14995736 14.33525433 26.11489979 31.31955458 23.40327743 14.26459928\n",
            " 10.08038856 20.01291752 38.65423664 33.07820582 29.16384001 28.12509883\n",
            "  9.4119263   9.4119263  23.78218093 16.72349294 16.89386342 32.02133424\n",
            " 28.10202663 23.28893815 21.04613587 16.07272168 40.16915223 20.50151657\n",
            " 21.58494534  8.55764653 26.43531895 16.78859972 31.06773112 31.73683635\n",
            " 18.78264866 38.01590187 11.1241498   9.87724763 20.42980053 11.34998148\n",
            " 21.52373523 36.87560831 23.45370651 31.22963425 46.03164821 10.53642293\n",
            " 20.25651022 19.45002124 14.33525433 12.7456944  15.72848446 12.39737941\n",
            " 24.88682959 16.98677131 17.48057448 34.49444185 49.9588583  20.90204257\n",
            " 25.32858499 36.40942486 22.983743    7.56397542  9.63415557 28.6227584\n",
            " 11.28772103 12.5316087  35.04647009 17.56398556 15.87957263 13.57995055\n",
            " 11.87910346 36.95517526 27.06837662 19.28605221 13.43190449 15.62263633\n",
            " 37.3399495  30.60291565 21.93598444 28.80560317 24.70012296 29.03262923\n",
            " 20.13380183 11.5320682   7.98464474 11.5717937  24.42529307 14.63803949\n",
            " 13.03391251 16.91219928 19.90348401  8.6938222  38.366315   28.65638465\n",
            " 35.98117749 13.98429228 32.10445273 11.3284418  10.21610425 49.96532223\n",
            " 34.47227387 10.83506989  9.9984231  30.81086721 14.12091523 21.00680368\n",
            " 33.28699088 15.88014487 17.94879007 11.59016456 14.86487174 21.05911137\n",
            " 36.5313563  17.56698677  9.82446388 48.65059589 11.4889263  37.80897394\n",
            " 19.34518118 21.65674831 29.53162946 14.26697167 26.84691336 30.52946649\n",
            " 28.19499686 12.96152287 10.41809591 26.63884462 18.0331423  27.22395732\n",
            " 16.84351074 30.57133453 16.06756075 33.0435924  14.82748053 13.64765223\n",
            " 28.60675498 14.97967129 10.74599005 39.23255645 19.48400836 45.8401649\n",
            " 21.40753895 28.18664951 11.80194179 15.66873903 49.25770272 23.516955\n",
            " 16.98358014 20.97563043 38.18433271 24.01831944 32.50700638 33.65089666\n",
            "  8.77210585 44.18445475 18.50104647 22.29096153 32.22999726 16.71872283\n",
            " 46.21199791  9.4119263  14.68844302 25.00194438 20.31717572 15.15942324\n",
            " 27.45861565 15.66990576  9.31375054 33.19399906  9.4119263   9.4119263\n",
            " 36.09082038 26.32538897 35.140374   35.85889853 44.14692586 11.44941936\n",
            "  7.4717969  25.84684145 23.26288626 12.40846765 20.35817658 13.16867981\n",
            " 11.96819577 15.68466088 43.23294297 12.51575915 21.14292806 32.84720708\n",
            " 12.96344386 11.6695701  10.65752981 13.59724617 22.66203378 30.57763595\n",
            " 20.5726656  33.72993913 36.39845    38.73556509 26.8864176  34.93388728\n",
            " 12.47118637 11.80320123 13.73096687 12.66226268 13.02665478 49.90738726\n",
            " 43.50603652 32.56257703 18.94110943 46.61060954 10.6538354  27.43803146\n",
            " 16.42427903 17.75626398 18.2848651  11.50610805 33.88350297 27.57022628\n",
            " 24.13524159  9.4119263  22.52308838 24.54913095 11.55910827 22.44141283\n",
            "  9.4119263  23.74290261 12.82117034 18.23066447 28.46434964 10.83493001\n",
            " 25.02450365  9.3868485  18.14050474 19.79137175 30.56246511 17.50967526\n",
            " 31.05768303 49.64774708  9.80268524 22.56855204 11.71891442 18.2632773\n",
            " 12.02675203 30.89825098 32.43486571 19.14615146 16.5350203  37.54084566\n",
            "  9.4119263  23.8685642  41.96475059 17.23851617 25.85976374 39.00164612\n",
            "  8.26640166 14.82350783  9.30511895 12.56600067 21.03954696 25.21382562\n",
            " 22.09708646 34.8065724  20.18679884 25.16598413 47.3031807  17.51949673\n",
            " 14.5999801  30.26024881 11.10323892 22.79337265 15.06815379 11.77931534\n",
            " 24.47528994 17.16702085 30.556665   18.67292407 17.65358605 34.50264715\n",
            " 30.48832937 20.94569358 28.78364417 14.4595192  27.87151058 12.01190364\n",
            " 34.41928845 25.4795201  28.2258281  35.96055364 45.38519906 14.80000002\n",
            " 22.41565731 48.07869619 14.52432107 15.94932063  9.41953412 25.1983812\n",
            " 40.93478516 30.22920269 19.29165241 19.14463703 19.09326268 34.23988215\n",
            " 12.25848203 48.11509888 15.6126104  40.33404051 16.69387817 30.78666591\n",
            " 22.84266069 11.2617228  28.80714535 40.60833514 13.91981833 37.1073756\n",
            " 21.5986436  34.21441366 27.19311198 36.14839117 22.84696154 47.76014767\n",
            " 21.63273468 15.55112754 11.25116814 23.71575332 22.82841798 15.6329515\n",
            " 10.27654775 19.85108517 15.35581945 26.11945663 30.75760688 17.22270897\n",
            " 26.45029418 40.44485108 37.26603987 18.89866469 30.70035875  9.4119263\n",
            " 44.12166701 37.39075923 22.98601546 14.20983346 14.74420928 29.83191561\n",
            "  9.02112011 12.40846765 26.21575891 20.23598826 24.78182317  8.69308827\n",
            " 27.40106679 17.39046723 20.50151657 17.34338025 19.05571493 23.93588996\n",
            "  9.4027345  36.65358186 17.35685074 10.63711059 13.54426276 14.41633148\n",
            " 10.87751022 12.81692379 16.50494468 31.12547756 23.61696659 14.44949235\n",
            " 17.73036641 11.87087311 30.60907558 24.3167066  20.07866045 16.71955721\n",
            " 23.43620045 12.71168762 17.69093612 16.88557014 10.35874334 14.73796584\n",
            " 25.10771271 26.62536362  9.4119263  35.57715969 17.23017111 32.01465833\n",
            " 21.72042401 23.54260944  9.4119263  22.73025567 29.51804531 33.03349595\n",
            " 26.64322274 35.82688653 24.59959105 23.97755534 17.13823475 31.51859251\n",
            " 28.19763058 31.29249436 18.14653113 17.29317943 16.17232901 48.21053768\n",
            " 20.91152301 18.8012839  33.38499281 18.72648627 16.63191714 27.29742788\n",
            " 32.7369533  10.82761472 48.00136995 12.84639224 23.06938689 18.98531328\n",
            " 29.86167477 28.75648748 48.27381989 24.46406406 45.29680279 15.38024467\n",
            " 17.99031946 12.4787928  19.42251527 21.63901729 19.90537094 14.88815852\n",
            " 22.19645635 10.98271144 15.49955412  9.4119263  24.54920403 16.73719982\n",
            " 10.84521288  9.4119263  49.99992797 14.43741022 38.93849532 19.15438186\n",
            " 19.50572371 40.32544264 17.27015175 13.30483206 10.60762759 49.15155217\n",
            " 22.11011935 14.65380675]\n",
            "selection [ 44 660 543 566 385 738 511 326 473 833] (10,) [7.44830307 7.4717969  7.56397542 7.98464474 8.07091366 8.26640166\n",
            " 8.55764653 8.57933637 8.59479639 8.69308827]\n",
            "trainset before adding uncertain samples (370, 10) (370,)\n",
            "trainset after adding uncertain samples (380, 10) (380,)\n",
            "updated train set: (380, 10) (380,) unique(labels): [142 238] [0 1]\n",
            "val set: (922, 10) (922,)\n",
            "\n",
            "Train set: (380, 10)\n",
            "Validation set: (922, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.13157894736842105, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.45      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (922,) [40.22599622 13.80172742 18.12984612 25.56479457 25.47585903 25.13529767\n",
            " 15.94130316 15.77574384 23.85238899 46.87348773 45.9285758  41.5712846\n",
            " 27.0036092  16.74989291 22.14717797 16.90923693 10.57876018 15.23133912\n",
            " 37.53841685 11.28713307 13.58963838 20.19763768 33.82628286 18.95106039\n",
            " 10.57876018 30.16844374 17.75960377 15.27719125 19.94443291 30.74235839\n",
            " 34.54500577 17.34558348 28.45727083 43.46482774 15.49911962 15.92130785\n",
            " 17.7172132  23.78348509 22.94011648 18.42705961 26.63146429 12.25147644\n",
            " 19.45819659 20.5823608  25.98932172 36.3639614  45.53866897 28.31118454\n",
            " 24.78358021 13.4635786  19.10315322 20.02280906 17.91268434 13.14293631\n",
            " 41.21755227 26.07339531 44.07101008 12.35941833 27.37461539 12.58737282\n",
            " 27.89211121 33.69911145 20.43167296 13.35323373 36.953838   26.70678444\n",
            " 27.79526913 18.52326952 16.0082355  17.59951044 26.29898463 49.8005884\n",
            " 36.59299116 20.0024181  21.98626573 26.80953705 14.82354333 16.28927085\n",
            " 28.39812498 15.78831584 30.21195844 10.57876018 27.78766963 16.95696102\n",
            " 15.40704729 27.48498934 36.924836   39.43441255 13.2151031  33.38163366\n",
            " 30.01887455 49.77751478 37.63546491 16.72194096 24.69670765 49.93900913\n",
            " 40.10837719 37.57308388 29.97955775 16.78932728 12.66037973 25.11286509\n",
            " 25.90693268 24.78960699 13.23713502 24.70068389 35.51847992 33.51047192\n",
            " 12.46906245 28.27688286 15.37413117 24.27923153 16.68527485 31.12818964\n",
            " 16.71992378 10.57876018 25.81897549 44.39660082 23.40674927 10.86285036\n",
            " 41.99154502 10.60115557 21.39055104 28.99666457 21.20286862 13.77308085\n",
            " 25.73578979 49.70938409 11.41436107 29.17498694 13.11732342 19.67353794\n",
            " 15.58183205 10.85970838 49.99350363 15.82294251 35.75665775 28.88820331\n",
            " 19.78462228 29.65137461 10.57876018 16.00823152 35.20815937 13.02254788\n",
            " 13.17801242 26.10207933 11.95381794 14.96294634 14.41804949 18.7037636\n",
            "  9.9610418  32.25092567 18.38809451 13.05619575 35.08870731 16.65823168\n",
            " 13.96780612 18.7639117  17.51687069 11.14379089 10.12502281 12.27959814\n",
            " 28.09149759 32.47898146 10.57876018 21.77318466 14.03987779 21.28424512\n",
            " 15.29688559 43.03259388 15.10308757 22.46459457 21.5895798  14.37549993\n",
            " 28.86075683 16.46768801 37.57552601 13.64185595 27.83295913 20.37293354\n",
            " 49.20470207 12.27531271 21.07932511 21.30517357 28.07734308 25.15976116\n",
            " 34.28123777 38.81404105 45.00885388 24.71052011 14.9838261  14.38614693\n",
            " 29.75226416 48.84655339 10.57876018 36.01944653 17.0556626  30.06265283\n",
            " 14.08313549 17.49845878 33.80720851 20.01925269 11.49142724 26.91726507\n",
            " 14.46843253 11.75576925 15.2362655  44.66304161 17.00629224 27.19981212\n",
            " 36.86347076 24.46077039 32.01312901 10.14762793 16.91611769 10.2306669\n",
            " 19.46688105 38.54429307 19.67494906 27.16086907 18.82892501 32.0204661\n",
            " 18.98736458 23.75973908 32.99683539 15.04792115 16.21091365 20.29096495\n",
            " 24.89909254 14.78562016 21.84689005 19.14759641 29.03677247 32.73464927\n",
            " 25.23891024 32.29865589 25.14985016 12.23647596 24.83438244 48.39967356\n",
            " 15.69434615 13.89149661 29.30152892 11.73204445 23.11191096 17.70094274\n",
            " 34.77092926 32.56071806 35.20981378 13.91324632 14.124917   43.15630686\n",
            " 31.77795541 28.67035238 24.16910368 49.6370222  18.11725037 18.32123354\n",
            " 26.52597204 15.09305258 22.53430005 24.63003328 20.09022825 10.57876018\n",
            " 49.96626463 10.57876018 37.33622874 34.30069967 41.19756548 27.84187338\n",
            " 10.57876018 28.14250155 43.71961825 12.52807635 28.88835668 23.83418087\n",
            " 47.3922463  12.21687361 15.02787994 19.96935894 20.16985012 28.76272274\n",
            " 16.87847906 23.92351107 21.23815316 44.28079415 49.42826057 21.85713325\n",
            " 21.20728335 21.19924661 17.15650743 16.31554863 16.20365514 17.16163786\n",
            " 33.24860949 13.13785793 16.42042876 17.69205553 10.57876018 26.41061296\n",
            " 20.40663826 26.70678444 31.52664866 15.10786513 16.86785324 29.97361206\n",
            " 23.05567849 22.69149248 11.08471839 27.53751076 28.6623473  20.37095982\n",
            " 20.4303035  33.06688852 27.20774255 30.19835226 20.43947858 26.44833584\n",
            " 20.67313033 10.57876018 36.35995753 20.10267057 31.34380775 16.81370446\n",
            " 15.65667433 29.49963366 25.70864103 27.4920673  16.13436219 20.15430691\n",
            " 15.44881781 11.87007957 11.14533015 29.28355428 18.86611164 22.56050967\n",
            " 21.10000827 33.97127529 15.87401788 14.17204697 27.44116836 16.65343443\n",
            " 13.37537926 20.01866401 20.65516202 25.36355319 16.4881154  27.50432023\n",
            " 26.597139   27.14467419 17.13993361 14.5028651  33.9792739  11.82780919\n",
            " 19.08473259 26.53196057 18.97779656 49.85405735 20.96132013 19.35635804\n",
            " 21.07674821 15.00288122 34.47741905 47.46509236 22.9139302  40.31468149\n",
            " 23.83865875 27.51720993 10.57876018 10.57876018 17.15873108 23.03216484\n",
            " 19.95651702 26.75950115 23.70055816 17.17904733 24.32114473 27.81110348\n",
            " 18.47018844 16.86279963 22.74205111 14.18586651 16.03471693 14.55931956\n",
            " 22.08239085 19.2159294  31.85340701 15.24379922 47.77514439 30.23526912\n",
            " 31.55107369 24.22554518 24.61884806 17.23329575 23.71280884 19.75624719\n",
            " 13.87413482 38.54738607 11.71998904 24.75812024 26.23738747 20.57929162\n",
            " 16.37804697 29.25235229 36.89021145 20.33701078 22.01584505 19.26373785\n",
            " 10.57876018 20.81092762 37.06383003 15.21036571 25.43625483 29.23618195\n",
            " 47.93859547 33.6042718  41.28656729 35.34875883 40.49823369 28.08629064\n",
            " 28.84302952 17.47273638 35.28987926 20.69202304 27.70380922 27.76768583\n",
            " 12.90505078 20.6202748  39.71305333 24.23188571 21.9994654  26.68284621\n",
            " 18.22732742 11.41266808 26.24682645 29.94666344 47.75406655 10.62062156\n",
            " 16.45848993 11.21760082 21.55453174 23.11156616 30.14825732 16.85439775\n",
            " 44.52147532 49.98186142 26.28584669  9.96649856 12.79215301 22.88393381\n",
            " 13.08093229 12.49525301 27.05137082 14.62422898 31.2991121  13.09863107\n",
            " 23.06165795 48.35762847 26.18578043 44.63242681 10.57876018 16.25298048\n",
            " 32.32958525 17.58596343 14.49784573 24.67047172 18.92510119 20.18019818\n",
            " 14.58290713 41.91812442 38.1045596  24.60167562 21.13417767 11.02056812\n",
            " 27.01038816 15.98506941 12.91285601 19.27021029 19.83774539 14.85513519\n",
            " 12.3375986  17.8884821  18.59651535 16.00359369 28.02923331 33.93851226\n",
            " 25.42485529 15.27378235 10.29972279 22.0123996  40.42429151 35.22274407\n",
            " 31.17094736 29.99105322 10.57876018 10.57876018 25.79399819 17.60795651\n",
            " 18.48678546 33.99223205 30.57250747 25.27789012 22.90356331 18.13419912\n",
            " 42.20190507 22.52876895 24.08679779 28.79083221 18.06524746 33.24629365\n",
            " 33.80158007 20.69765261 40.2928114  12.48884266 11.14533015 21.37566085\n",
            " 12.45497529 23.03285551 39.16730798 25.29270077 33.19220234 46.99426261\n",
            " 12.23120375 21.74854274 21.09382505 16.00359369 13.34331542 17.79836874\n",
            " 13.83399725 26.88757306 18.0904445  19.2516806  36.71703245 49.98666777\n",
            " 23.19343502 27.38641266 39.23711958 24.62619683 10.68193541 31.0123896\n",
            " 11.87225315 14.22585898 37.08679048 19.72399433 17.30850172 14.96698559\n",
            " 13.58814312 39.29370523 29.0135017  20.3422845  14.48100254 17.19951854\n",
            " 39.74211928 32.65307032 24.12505931 30.92580133 26.96605724 31.0628391\n",
            " 21.76377919 13.11712344 13.24218725 26.54236703 16.38833729 14.37131107\n",
            " 18.49135837 21.52673293  8.2878815  39.82506734 30.70522944 38.71702276\n",
            " 15.09645278 34.33298278 13.25012674 11.91254772 49.9893862  37.12724566\n",
            " 12.26331489 10.41394663 33.72017728 15.67013248 21.96165051 35.26755811\n",
            " 17.68752031 19.82069431 12.45383947 16.18782517 22.65802975 39.14433898\n",
            " 19.53069456 10.84027837 49.28492105 12.87958862 39.97145491 20.38590169\n",
            " 23.93744647 31.78044468 15.77860001 29.23081169 33.17085055 30.50313374\n",
            " 13.88477927 11.23832176 28.78545846 18.99100864 29.18498821 18.76444743\n",
            " 32.20792699 17.84283449 35.24952977 16.35022736 14.96400956 30.62165542\n",
            " 16.7840163  11.14014584 40.75398422 21.38395738 47.15388373 23.21418097\n",
            " 30.18079283 13.74589212 17.16163786 49.62018552 25.5371569  18.63500012\n",
            " 22.67329746 40.0684982  25.28429943 35.43293658 35.55766606  9.15742339\n",
            " 45.86708826 20.34216732 24.26754584 34.05889988 18.87777612 47.60342803\n",
            " 10.57876018 16.54655648 27.02926748 22.33385002 16.85544898 30.20594286\n",
            " 17.77660616 10.52594318 35.88700475 10.57876018 10.57876018 38.7426801\n",
            " 28.49765724 37.63078904 37.71287247 45.88742416 13.57836348 27.92081324\n",
            " 25.14125082 13.56950809 22.3239675  14.18951569 13.31854509 17.8771739\n",
            " 44.87612489 14.17943608 22.81185051 35.2032954  14.16272852 12.26860225\n",
            " 11.40773561 15.0840924  24.79014715 33.30484682 22.20674536 36.35821322\n",
            " 38.80600329 41.20252208 29.09980511 36.68235435 13.82929261 13.24628853\n",
            " 15.25720402 13.19172716 15.44434199 49.96552234 45.43503046 34.83139836\n",
            " 20.45980741 47.74862985 11.63411489 29.56487167 18.14984973 19.2768157\n",
            " 20.21066649 13.03816837 36.17543742 29.94575313 26.07612371 10.57876018\n",
            " 24.57940271 26.23957473 13.06777905 24.43784895 10.57876018 25.63273745\n",
            " 14.26477072 19.90176973 31.00240705 11.9901511  27.08486955  9.8948996\n",
            " 19.77157956 21.58446075 32.77763523 19.74077835 33.4599853  49.84394193\n",
            " 11.11122555 24.06953996 12.67501935 20.24746618 13.94521952 32.59393949\n",
            " 34.57251231 20.99171418 19.09815807 39.80608462 10.57876018 26.21633875\n",
            " 44.02692093 18.77266837 27.44601042 41.73999089 16.41511689 10.71075819\n",
            " 14.20121859 23.0960721  26.90262406 24.74354016 36.8425418  22.06877129\n",
            " 27.26042897 48.35517472 19.51003499 15.96074142 32.55345287 11.59982577\n",
            " 24.68593241 16.91296079 12.01766688 26.29370212 18.52260191 32.48137739\n",
            " 21.27411219 19.27021029 36.36426487 32.10310738 21.64625605 31.31466126\n",
            " 16.27435057 29.94951035 13.86930787 36.63765749 27.20514685 29.74423641\n",
            " 39.49932327 46.87675028 14.76639067 23.82319916 48.86983399 16.35296923\n",
            " 17.90954902 10.54747732 27.26609837 43.03588482 31.7594709  20.94224366\n",
            " 21.4534114  20.98561638 35.9318721  12.80096919 48.88422314 17.18995294\n",
            " 42.71439365 18.70273485 33.55736999 24.81784534 12.29159618 32.01312901\n",
            " 42.81997664 14.82231488 39.02771764 22.5275771  36.42645985 29.08353419\n",
            " 38.46145188 25.20683897 48.64610988 23.5696546  17.25553222 12.18684772\n",
            " 26.08504912 24.84962164 16.5718206  11.50407369 22.06921246 16.92225764\n",
            " 28.38091284 32.90899539 18.98031113 28.51016983 42.70715107 38.95578105\n",
            " 20.35955933 33.73463588 10.57876018 45.12851056 39.21001338 24.91491873\n",
            " 15.15268267 16.29506679 32.05743648  9.04493649 13.56950809 28.01622718\n",
            " 22.13315892 26.65254398 29.79330564 19.46688105 22.52876895 19.45205434\n",
            " 20.93109357 25.77164763 10.76370118 38.62542939 18.35684753 11.35440443\n",
            " 14.82354333 15.84938382 11.3364547  14.13936004 17.40521856 33.54901531\n",
            " 25.49362702 16.42519349 20.22117351 13.07197064 33.4812326  26.52259638\n",
            " 21.78341104 18.30234414 24.97834217 13.77876243 19.61979247 18.66316503\n",
            " 11.75449703 16.21459566 27.2328042  28.56878626 10.57876018 37.29457253\n",
            " 19.01816031 33.99742808 23.38790301 25.71052265 10.57876018 24.75845925\n",
            " 31.6059719  35.08201736 28.67766979 37.79218688 26.14185518 25.96800653\n",
            " 18.70713087 33.90273032 30.19013407 34.01604552 19.61023274 18.70549608\n",
            " 17.78004245 48.97852636 23.14809033 20.40886648 35.71286107 20.51803887\n",
            " 18.46558698 30.09321712 35.43082132 11.50024843 48.84106475 14.50673101\n",
            " 25.28302108 20.21230773 32.14030487 31.10535551 49.0196023  26.29051524\n",
            " 46.72733    16.85641436 19.70741459 13.03884816 21.65194873 22.8635522\n",
            " 21.59538058 16.36312493 24.42500486 12.38718725 17.19625112 10.57876018\n",
            " 25.42161822 17.86049193 11.10416768 10.57876018 49.99996728 16.96869221\n",
            " 41.11091063 21.01722174 21.0799913  42.66888131 18.61751676 15.00686361\n",
            " 11.97373054 49.55949766 23.30086243 16.03615531]\n",
            "selection [566 819 629 707 150 447 160 213 215 488] (10,) [ 8.2878815   9.04493649  9.15742339  9.8948996   9.9610418   9.96649856\n",
            " 10.12502281 10.14762793 10.2306669  10.29972279]\n",
            "trainset before adding uncertain samples (380, 10) (380,)\n",
            "trainset after adding uncertain samples (390, 10) (390,)\n",
            "updated train set: (390, 10) (390,) unique(labels): [147 243] [0 1]\n",
            "val set: (912, 10) (912,)\n",
            "\n",
            "Train set: (390, 10)\n",
            "Validation set: (912, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.1282051282051282, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.46      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (912,) [40.52539803 15.11925875 18.11062509 23.22281732 25.94713726 25.38888367\n",
            " 16.22856741 16.88412581 31.34644431 46.93312162 46.30094788 41.8232616\n",
            " 26.6333057  15.26722916 22.99147951 17.59721351 11.11402755 16.53043343\n",
            " 38.156164   11.44956607 13.84821667 20.98237354 33.79558522 19.86105769\n",
            " 11.11402755 29.61146936 17.35935888 15.85010465 20.49206082 31.01539268\n",
            " 35.90578485 18.89371057 29.14247939 43.87341456 15.38420486 16.88167441\n",
            " 18.47373044 25.45515225 24.2720634  19.61638024 27.99492684 12.77438063\n",
            " 21.37963288 18.98151105 25.72320765 36.08828392 45.97629033 29.01725782\n",
            " 25.42671559 13.77307999 20.29553395 20.62724772 18.47611078 12.61698668\n",
            " 40.60112307 27.89244557 44.38081164 12.2379632  25.97621629 12.67276158\n",
            " 27.99482572 33.32160369 21.20616841 13.96610415 36.13044003 27.99370166\n",
            " 28.16509868 17.40291371 16.12017561 14.87485644 28.09568367 49.81647339\n",
            " 37.28545146 19.56091342 22.25223011 25.17174996 14.8294612  16.89623313\n",
            " 28.77178093 15.60454299 29.49811558 11.11402755 28.14828443 19.72052472\n",
            " 16.76128526 27.44300908 36.74728814 39.25185703 14.34845585 34.4189112\n",
            " 30.18022264 49.81039069 37.96579338 16.37367052 24.63358571 49.95034595\n",
            " 40.29129264 37.40126469 31.21688303 17.24694463 11.56156673 25.9432465\n",
            " 26.77685491 25.37731087 13.83765565 25.56649726 33.47803379 34.09949411\n",
            " 11.75436587 29.09095982 14.4242066  23.88120552 18.31429894 31.84869089\n",
            " 17.04596096 11.11402755 25.71450985 44.69302937 23.58399917 11.07315345\n",
            " 42.28435689  9.92123495 20.4966469  29.97449775 21.4445681  15.12947228\n",
            " 25.77465291 49.76127414 13.64197864 29.32610835 14.38219158 20.14236203\n",
            " 14.05597766 10.92428071 49.99470377 16.07947308 35.65254444 29.08867307\n",
            " 20.54787798 29.0785972  11.11402755 16.23316298 35.24552249 13.64413068\n",
            " 12.31970439 26.55053377 12.82238338 13.83848201 14.72119718 18.64795164\n",
            " 32.19643493 19.33450833 13.54391123 35.8885073  17.37143396 14.53267303\n",
            " 19.06600547 16.66212585 11.90572899 12.81015159 29.43203393 32.79591442\n",
            " 11.11402755 22.36019457 12.14905218 20.42285372 15.9002514  43.59499493\n",
            " 15.82467266 23.24541552 22.96897738 13.98299419 33.99785994 16.30180151\n",
            " 38.13582359 13.9878625  28.11686696 23.03396818 49.04291282 12.95419581\n",
            " 21.2094328  21.99046665 29.51923059 24.20026609 33.97807201 39.36613242\n",
            " 45.5106288  25.25604521 16.30604318 14.73593998 30.6048959  49.10242568\n",
            " 11.11402755 37.04786844 18.21287064 31.0366885  14.23632866 17.7913107\n",
            " 34.07040939 20.5762714  11.79216461 26.77970144 15.01594189 12.83115104\n",
            " 15.51388145 45.288706   17.81155846 28.24122042 37.28187562 25.52464675\n",
            " 32.58690113 17.1683157  20.61590211 38.81753267 18.37878112 28.93777478\n",
            " 19.83437314 32.07707852 18.42851151 24.40840705 33.16682547 15.46979351\n",
            " 15.3918629  20.88370228 23.42510773 13.97713194 23.28672565 17.8124725\n",
            " 29.45159251 32.76259233 25.43771417 31.30709082 25.06241777 12.38480059\n",
            " 24.47497499 48.56592421 15.51063481 15.48469297 29.85101138 11.51778317\n",
            " 21.74067253 18.04414246 33.15417757 33.36727822 34.75147418 16.41607237\n",
            " 15.45550253 43.30288747 31.49042915 27.75543817 24.52849069 49.67422544\n",
            " 18.87683507 14.28420516 28.45857573 15.69162087 23.11083523 25.7437155\n",
            " 19.71761956 11.11402755 49.97126218 11.11402755 38.19348652 35.06619637\n",
            " 41.03959742 27.90260038 11.11402755 28.43310868 43.59201578 12.77037081\n",
            " 24.15270756 24.68515672 47.40719534 13.07801427 15.05956794 18.55149012\n",
            " 21.55653154 30.07366697 16.03097441 25.16808616 21.28168486 44.9023873\n",
            " 49.50463216 22.8679247  21.31300707 22.10141495 18.67140385 15.49903298\n",
            " 15.82607072 17.14795792 33.70204178 13.62323785 16.87931159 18.55711516\n",
            " 11.11402755 26.92819164 20.79807912 27.99370166 30.60495276 16.29603584\n",
            " 17.12960565 29.33788343 23.89744258 22.3171135  11.93485119 28.59472001\n",
            " 28.93676255 20.98667354 20.09387391 33.11000413 27.82013034 30.59421106\n",
            " 20.85030302 27.79900329 20.32112177 11.11402755 35.93534782 21.92992739\n",
            " 33.48944641 17.3199331  18.14888663 29.75231675 26.96862006 26.63515564\n",
            " 15.55223548 21.97109869 16.41678121 12.00786137 11.37902276 29.9002927\n",
            " 19.4067135  24.33532251 22.18749839 33.65849289 14.87330699 14.48116542\n",
            " 27.91944154 17.38653975 13.26035334 19.13303952 21.06392783 25.30854076\n",
            " 16.92062418 28.02576312 26.4969918  29.46277295 18.21740623 16.62536767\n",
            " 34.79520353 11.81537437 20.26670585 26.44615477 20.36490701 49.86978452\n",
            " 20.86909529 20.30980705 21.26215672 15.93677144 34.81988074 47.74990471\n",
            " 23.79839991 40.12613607 23.55779765 26.59421592 11.11402755 11.11402755\n",
            " 18.25339556 24.47565232 18.93635055 28.29993957 24.59840666 16.9924823\n",
            " 23.47357549 28.16789863 17.41643363 16.29064834 21.75622076 13.53418127\n",
            " 19.21707852 14.13730609 22.37979614 19.20908849 31.84158243 15.25182388\n",
            " 47.8950599  30.68814028 31.697967   24.05985761 25.44416523 15.69158078\n",
            " 23.20959068 20.49458185 13.99224224 39.17613131 13.27262148 24.86423802\n",
            " 27.61088485 21.17323225 17.34063    28.15305906 37.28564022 20.19496768\n",
            " 23.09246015 18.90162692 11.11402755 21.30305197 37.50797472 16.03880305\n",
            " 26.90063078 30.14203527 48.06674653 33.24020059 40.43190669 34.36880498\n",
            " 41.43924866 29.86584552 29.79825426 18.46020083 34.7019678  21.7660568\n",
            " 28.54708677 28.89534324 13.0427031  20.56919802 39.84119399 25.31146829\n",
            " 22.71562371 26.97714469 17.35066275 14.11182981 27.18510508 30.29200118\n",
            " 47.98566463 10.45049382 16.20314827 11.26486821 20.73471753 23.12897235\n",
            " 31.21691497 16.39236121 44.6235319  49.98519326 27.7042375  12.42531884\n",
            " 24.31125644 14.36113752 12.76475308 27.07701763 15.22538952 31.81425944\n",
            " 13.01279709 22.97489031 48.54654105 25.91809812 44.6405806  11.11402755\n",
            " 16.81367874 32.3731121  17.8298494  15.47389105 24.79265126 18.58925287\n",
            " 20.99288558 14.56592841 42.05232975 38.70271342 23.39674286 21.34675206\n",
            "  9.40897927 27.83150838 16.54471356 12.73420486 19.92610607 18.61763413\n",
            " 14.61951703 14.2844227  18.64828497 19.04969975 16.57286253 28.57330511\n",
            " 34.23108637 25.62072684 16.02265906 23.89609563 40.95823774 35.30264371\n",
            " 30.73353343 29.57937994 11.11402755 11.11402755 26.57537493 16.97713902\n",
            " 18.54339593 33.80772986 33.52139889 25.24468057 22.29212701 17.8752459\n",
            " 42.60532217 22.92189744 24.20298107 27.78102948 18.58814786 33.56566457\n",
            " 33.0755457  20.35055951 40.56448898 12.97370043 11.37902276 22.08560148\n",
            " 11.9948477  23.25137389 38.46059863 25.32755709 33.38791592 47.34555804\n",
            " 12.79283627 20.29544533 20.88163809 16.57286253 13.44442651 18.62138543\n",
            " 15.20504374 26.98205411 15.45543281 20.14238136 37.64602329 49.98796157\n",
            " 24.89819223 27.91379258 39.45979909 25.24649225 11.25569351 32.64115505\n",
            " 12.39214771 14.36035009 37.30370839 20.86798873 17.66310788 14.98810984\n",
            " 13.86910866 39.09011159 29.96931912 20.64651833 15.32350488 16.29140461\n",
            " 39.77702464 32.79095954 26.19094443 31.48033271 27.07921709 31.29948256\n",
            " 22.09903678 14.26987839 14.62418804 26.48977068 17.15052114 14.81605021\n",
            " 18.76912329 20.91070227 38.41429736 31.37510153 41.9330305  15.67333795\n",
            " 35.37594896 14.4732225  12.54945175 49.99061444 37.26529315 12.9009966\n",
            " 11.44886689 34.21043479 15.83440332 22.24943536 35.47546079 18.56993209\n",
            " 20.65122451 12.03408496 15.04746119 23.17720337 39.05039946 20.47064935\n",
            " 11.7689292  49.25496935 13.14305278 40.5565179  17.35290866 24.66253275\n",
            " 30.32209397 15.82216172 30.03345765 34.75709457 30.98413345 14.33116548\n",
            " 11.74464733 29.79783549 18.68718052 29.43876323 18.78252166 31.09751211\n",
            " 17.98039491 36.35587788 16.30198361 15.52894538 30.12580682 17.25518936\n",
            " 10.57882166 40.09506801 21.61100863 47.15528719 23.26938152 30.85511684\n",
            " 15.04816925 17.14795792 49.66152199 26.46476312 18.34143286 23.3974761\n",
            " 40.71800063 25.03187909 35.6031291  35.25367224 46.06432769 20.62507976\n",
            " 26.21768675 33.39567501 19.90706298 47.72593104 11.11402755 18.35917449\n",
            " 27.65270072 23.39829532 17.78793703 30.7384758  18.99615556 11.46592035\n",
            " 35.51708928 11.11402755 11.11402755 39.47486904 28.83738443 37.86728432\n",
            " 37.66623013 46.08765065 16.75649762 28.27901883 25.48040237 13.67423683\n",
            " 23.07131928 14.37832852 13.66319184 18.19998285 46.11180921 14.83461946\n",
            " 23.93662585 35.0372325  12.63956061 13.0845338  13.05745435 15.05992884\n",
            " 24.10651772 34.46755991 22.33968665 36.87273452 37.58437832 42.6607127\n",
            " 30.16525253 36.40296093 14.23986671 13.77807995 16.33434655 14.66418725\n",
            " 17.17157038 49.96926402 45.94000172 35.96245619 20.0646829  48.04063923\n",
            " 13.62240474 29.89833105 19.24527099 19.4822077  20.70811617 13.94305158\n",
            " 37.53395915 31.46508255 26.42615222 11.11402755 25.92145708 28.17002336\n",
            " 12.30984245 24.32324298 11.11402755 26.52630735 15.76033508 20.75982445\n",
            " 33.93966714 11.30313934 27.63215137 23.44545024 21.9826898  33.4934888\n",
            " 20.91558295 33.97986576 49.87087764 11.8143713  26.26835532 12.807472\n",
            " 21.16402758 18.187135   31.45516028 34.44118134 20.64845339 22.00586368\n",
            " 40.83030535 11.11402755 27.52228648 44.09853979 19.00235426 25.90116497\n",
            " 42.34414755 18.14622279 11.03192817 14.73908935 24.45401451 26.90585421\n",
            " 27.18999383 37.51082303 22.49609905 26.71383869 48.59439768 19.64650766\n",
            " 16.64895414 32.66881547 12.55106731 25.76636982 17.90231983 11.49039825\n",
            " 26.44697148 20.88679078 32.59595162 22.55887671 19.92610607 36.67557737\n",
            " 31.16783637 19.81019094 31.64964645 17.13360351 30.47334254 14.72427947\n",
            " 37.27883578 26.22175197 30.01943885 40.51150216 47.02313539 14.99571847\n",
            " 22.29556723 48.96081958 17.01805991 18.70367501 10.84631971 27.85180342\n",
            " 43.69376681 29.90186084 20.64271243 23.86097484 21.30336374 35.35964731\n",
            " 12.84363125 48.89178762 15.81252943 43.20730511 19.12125889 34.03093045\n",
            " 25.47316422 14.76036597 32.58690113 42.82129268 15.54700267 40.72408787\n",
            " 19.84902831 37.30557675 30.48772511 38.68480044 26.76102572 48.79366658\n",
            " 23.10526571 18.01620497 11.71470615 26.40357786 25.38828912 18.08503674\n",
            " 11.45139736 21.28191396 18.47545714 28.84679679 33.57902312 18.70273947\n",
            " 28.65121325 42.8223732  40.52564136 19.33364628 34.52006996 11.11402755\n",
            " 44.27120005 38.8372458  25.84069633 15.90876961 16.91101897 32.22577237\n",
            " 13.67423683 27.54945359 22.49800657 27.34971023 31.38881395 20.61590211\n",
            " 22.92189744 20.18400855 21.68071584 25.4700111  10.17613365 38.91434145\n",
            " 18.95269673 13.1402723  14.8294612  17.30909253 11.66997611 14.44267672\n",
            " 17.04501405 34.63383032 25.35599482 16.93485398 23.97574669 14.16627998\n",
            " 37.41545077 27.3303481  20.66714855 18.55263884 25.84297801 12.93611\n",
            " 19.52204273 19.1630345  12.40769233 16.84330836 26.95202254 29.26515953\n",
            " 11.11402755 36.59733311 20.03844131 36.50134491 23.41517428 26.12198559\n",
            " 11.11402755 25.63810668 31.76971689 35.67417742 28.24533817 37.84963214\n",
            " 27.88780004 26.41264673 18.30688058 34.399825   29.73359399 34.43163949\n",
            " 19.72207558 17.70728541 18.0027097  49.09920634 24.64502332 21.43626422\n",
            " 36.24762373 20.86210113 18.75735774 31.36359418 35.82590831  9.90291589\n",
            " 48.9433936  15.5038025  26.28011613 21.2020148  32.73041342 32.89390418\n",
            " 49.12275253 26.13538724 46.97667344 17.19050014 19.42616586 13.77221751\n",
            " 23.83510026 23.59269973 21.28083301 16.49028735 25.4033111  13.24314847\n",
            " 17.97368688 11.11402755 24.9114563  18.53977236 11.00790648 11.11402755\n",
            " 49.99987562 18.06172018 41.43055301 22.2213254  20.82829111 43.08205166\n",
            " 17.77672109 15.67596932 12.50647721 49.60821754 24.21619549 16.80915297]\n",
            "selection [468 875 121 820 433 606 760 133 898 722] (10,) [ 9.40897927  9.90291589  9.92123495 10.17613365 10.45049382 10.57882166\n",
            " 10.84631971 10.92428071 11.00790648 11.03192817]\n",
            "trainset before adding uncertain samples (390, 10) (390,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [150 250] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (902,) [41.69014069 14.91519531 19.05215513 24.84518339 26.28914994 25.92594903\n",
            " 16.89964616 16.94230277 29.66769984 47.49866277 46.88581526 42.77012527\n",
            " 27.51304012 16.80664087 23.64693574 17.60777523 11.42993615 17.00183315\n",
            " 38.80219863 11.41020022 14.49748494 21.04183098 34.74639797 20.6486055\n",
            " 11.42993615 31.75907088 18.81061668 14.08114253 21.7120625  32.48261833\n",
            " 35.56865764 18.69628169 30.12158505 44.40592545 15.86059279 17.38869051\n",
            " 18.34920289 26.41807829 24.63930198 20.6898394  29.05400198 13.39001981\n",
            " 21.14055569 21.10100991 26.70099129 37.69843502 46.46963755 29.49589034\n",
            " 26.26531781 14.20362482 20.8355824  20.91886749 19.35194893 14.0062652\n",
            " 41.8975646  27.30026697 45.20430671 13.07256765 27.68941974 12.99547418\n",
            " 29.14055232 34.08651354 23.57501335 14.86316884 37.78735232 28.69574797\n",
            " 28.90025961 18.6689641  16.45933804 15.09056661 27.50429773 49.87330798\n",
            " 38.44543713 20.84918358 23.66487086 27.01846957 14.552221   17.25186198\n",
            " 29.43299961 17.18723221 30.97233227 11.42993615 29.581947   19.06629136\n",
            " 17.96484523 29.05313895 38.26311965 40.39707882 15.12515056 35.19000821\n",
            " 31.31345201 49.86023373 38.95579338 16.84974134 25.49118387 49.9669694\n",
            " 41.23130661 38.69307694 32.20812421 17.83997183 12.62789053 26.91434231\n",
            " 27.86353628 25.77181747 14.01087631 26.67482561 36.0672352  35.21200463\n",
            " 11.18326301 29.7273505  15.8605471  25.20253233 18.70930782 32.87270849\n",
            " 18.31579802 11.42993615 27.358238   45.43847958 24.94679987 10.74529856\n",
            " 42.99234903 20.92753177 30.43799127 22.40837519 15.50450982 26.65026782\n",
            " 49.80556766 12.76020664 29.84555851 13.7720125  20.63666924 15.54031806\n",
            " 49.99728169 17.73713408 36.99903232 29.58932374 20.58332497 31.11012277\n",
            " 11.42993615 17.36240075 36.27012863 14.45898386 13.42146152 27.10139912\n",
            " 14.21749226 16.79924493 15.86508482 19.21719593 33.42848757 20.46579726\n",
            " 14.65950615 36.44366503 17.62433182 14.5603864  20.5902867  17.83180632\n",
            " 11.80206475 12.9963076  29.76220816 34.08356107 11.42993615 22.51634857\n",
            " 13.93688572 19.656711   15.63463799 44.33946447 16.27453682 24.03154919\n",
            " 23.09750543 14.59517035 32.19915021 16.8446552  38.83723399 14.14079985\n",
            " 29.09695413 22.5539203  49.36820954 14.06888486 22.73590885 23.06987256\n",
            " 30.3607922  26.07231492 34.89305081 40.08544723 46.04267032 26.16517915\n",
            " 16.48520659 15.13485288 31.6290997  49.09461304 11.42993615 37.7278142\n",
            " 18.18488436 31.52799651 14.49304684 18.06835814 35.58759119 21.89419432\n",
            " 11.67269998 27.58947966 15.56751252 12.71895981 16.48339986 45.98120307\n",
            " 18.08345931 29.08721712 38.0547971  25.76993898 33.86648701 18.55562199\n",
            " 20.3138841  39.82165878 19.38122241 29.01713085 19.9812119  32.95300827\n",
            " 18.42095558 25.05605089 34.33864982 15.40431283 16.23228954 21.6325271\n",
            " 25.44558382 14.17415038 24.0621731  19.36344056 30.39042605 33.34357672\n",
            " 26.24325734 32.98730111 26.5601897  12.14303646 25.84754445 48.80229423\n",
            " 16.1925633  14.87095904 30.77574971 10.71643601 23.79650699 19.19758322\n",
            " 35.75104186 33.85552638 35.91532055 16.03386942 15.06627075 44.22428266\n",
            " 33.86380528 29.03939454 24.70537863 49.78715753 19.58875814 19.67923968\n",
            " 29.23256548 16.26788906 23.71947692 25.54500916 20.81375128 11.42993615\n",
            " 49.98267384 11.42993615 38.73791567 35.39463385 41.79277602 28.64904942\n",
            " 11.42993615 29.46526577 43.80219339 13.04132842 23.42533924 25.14924605\n",
            " 47.92270827 14.09348762 16.15939569 20.16126465 21.77285685 31.48456115\n",
            " 16.91665314 25.10557039 22.74316563 45.45410757 49.60960247 23.55342255\n",
            " 22.15765551 22.62262657 18.55011868 16.76269531 16.29302701 17.78097323\n",
            " 35.05443386 14.80617298 18.55325017 19.2890418  11.42993615 28.32479713\n",
            " 21.41152221 28.69574797 32.27061581 15.73878489 17.56115412 30.51482987\n",
            " 24.11595305 23.2522843  11.68584011 29.14785009 30.12126954 21.3360384\n",
            " 20.85050634 34.41113249 28.69589996 31.27025039 21.14916587 28.67032067\n",
            " 21.41171858 11.42993615 37.39192435 22.10596722 33.09051233 17.90187637\n",
            " 17.91535065 30.5976276  27.27687654 28.18856669 16.59550993 22.01951714\n",
            " 15.83871459 13.49355795 11.75773117 30.77796158 19.85295453 25.58536548\n",
            " 23.09861549 35.25952283 16.71538144 15.09896223 27.62931672 17.25571906\n",
            " 13.67377553 20.61780726 21.53529914 26.20527756 17.16686171 28.7752641\n",
            " 27.42928242 29.18226458 18.28011566 16.37535039 35.05836078 13.29985482\n",
            " 21.08557628 28.18544876 20.83340581 49.91115198 21.5202668  20.9478861\n",
            " 22.00901252 16.0157427  35.75059986 47.90843045 24.77555757 41.37956148\n",
            " 24.69428092 27.96085002 11.42993615 11.42993615 18.21902176 25.70659525\n",
            " 20.9328687  28.63662186 25.31783413 18.82900267 25.14719293 30.11793346\n",
            " 18.55187828 17.57446351 22.62791472 14.66264443 18.69969318 15.96521065\n",
            " 22.9476698  20.26214901 32.91198111 16.17406757 48.26168241 30.99426553\n",
            " 33.25806088 25.54360045 26.04310494 17.40740195 24.27155269 21.29610539\n",
            " 14.43751866 39.96657768 13.53500543 25.93066161 27.90647115 21.97931672\n",
            " 17.40608162 30.42497561 38.11902842 21.10882951 24.17920456 19.79902959\n",
            " 11.42993615 21.56019842 38.34435075 16.41076252 27.59782163 32.1999276\n",
            " 48.41645505 34.34671298 41.86478437 36.00190427 41.94145008 29.70563047\n",
            " 30.03503137 18.63521707 36.00074967 21.93858711 29.18860736 29.81030449\n",
            " 13.47555245 21.04877267 41.01404001 25.33854846 23.47377191 27.75699798\n",
            " 19.83674931 14.15893156 27.29190156 31.53283312 48.29409986 16.87864037\n",
            " 11.34877563 22.3383751  23.77005738 31.60305474 17.9961492  45.45712815\n",
            " 49.99087618 28.27396196 12.81616145 24.4969639  14.06843325 12.70077296\n",
            " 27.74290067 14.75105872 32.63974363 13.50634849 23.91831705 48.86950447\n",
            " 25.85712717 45.30646653 11.42993615 16.46720326 33.7391558  18.16398053\n",
            " 15.3249975  25.12691522 19.81314995 21.68269111 15.4697825  42.95766165\n",
            " 39.49567768 25.40891845 23.07534384 28.24037596 17.17215313 13.947444\n",
            " 20.58558714 19.71336046 15.52871174 13.589495   19.19226673 19.24523158\n",
            " 17.1702552  29.39901795 35.35480107 26.72731034 16.06913952 24.12356124\n",
            " 41.43520155 36.64441888 32.42271128 30.64723966 11.42993615 11.42993615\n",
            " 27.12059985 18.15587616 19.85866925 35.24929941 31.96192338 26.42244891\n",
            " 23.46748971 19.80663757 43.37476844 23.7702251  25.68578927 29.65856344\n",
            " 19.41428717 34.74501481 34.78244707 21.58424901 41.30961318 15.55488576\n",
            " 11.75773117 21.6827771  11.96572362 24.63972035 38.89304624 26.68594826\n",
            " 34.43314321 47.82563927 12.83695176 22.02750783 21.73646154 17.1702552\n",
            " 14.57359681 18.22219376 15.0169891  28.72289424 18.04706896 20.81287776\n",
            " 38.70378603 49.99392987 25.50348315 28.42849034 40.46501333 26.43104804\n",
            " 11.02756637 33.62168392 12.84613686 15.06369342 38.58166571 20.9801941\n",
            " 17.91408217 16.6622577  14.23021015 40.42883778 30.36481748 21.55373819\n",
            " 15.0590274  16.82216829 41.00920625 34.00889726 26.48367624 32.45565441\n",
            " 28.11306249 31.8687059  23.14928141 13.71614404 14.54031729 27.39730932\n",
            " 17.25759305 16.0562704  19.97490247 22.23580237 40.31049531 32.0804445\n",
            " 40.45153932 15.90261461 35.57416447 14.47788213 12.82646834 49.99502077\n",
            " 38.51037714 12.57873829 11.73037558 35.80297684 17.02082316 22.5881222\n",
            " 36.55793626 18.79768972 21.15634653 13.93314855 16.7099557  24.00154486\n",
            " 40.09731071 20.52492341 12.37349502 49.54485148 13.72933672 41.1107611\n",
            " 20.41604299 24.93817733 31.38563219 16.07949682 30.81678938 35.17679089\n",
            " 32.38003475 14.83651606 11.56375588 30.22400126 19.11357646 30.01820069\n",
            " 19.08535045 32.79140317 19.23042049 36.06436373 17.19510695 16.34163119\n",
            " 31.55157609 18.26888336 41.61260204 22.32487327 47.64506192 25.2235411\n",
            " 31.57104265 15.33057859 17.78097323 49.74252708 26.9824247  19.47454825\n",
            " 23.57783688 41.45044938 25.10248081 37.03618235 36.53270097 46.44801566\n",
            " 20.99379777 27.92918931 34.83226166 20.07106149 48.10465685 11.42993615\n",
            " 18.62056121 27.8551808  24.19012282 17.57916495 32.6167277  19.25312202\n",
            " 11.78974551 37.18292069 11.42993615 11.42993615 40.04759273 29.8127665\n",
            " 39.00945457 38.99076039 46.66885354 16.50511518 29.42357329 26.69783375\n",
            " 14.27405066 23.75129272 15.32565244 13.51603564 19.8254849  45.90078238\n",
            " 15.65112384 24.66671686 36.59649005 14.54793097 12.81532887 11.71356587\n",
            " 15.60350987 26.22513423 35.38353804 22.65418507 37.86404928 39.23778925\n",
            " 42.6627582  30.8405449  37.71970035 15.61137266 14.63062368 16.89377245\n",
            " 13.26826456 18.19349409 49.98058591 46.20809712 36.45927131 21.41530559\n",
            " 48.27811154 13.21772529 30.83937921 19.38398494 19.39334574 22.22602049\n",
            " 14.5419151  37.92453558 32.05259195 27.14453311 11.42993615 25.64647335\n",
            " 27.79425482 12.45163046 25.58253772 11.42993615 27.01725667 16.24929055\n",
            " 21.91637535 34.43580033 12.30011445 28.29210821 22.39002671 23.05370549\n",
            " 34.1736126  21.07816981 35.10666043 49.90508581 11.25232882 24.89872039\n",
            " 13.20907942 21.91778319 16.65618479 34.56908112 35.8525939  22.1409801\n",
            " 21.94661217 41.39029591 11.42993615 28.22663508 45.04530324 20.1274878\n",
            " 27.89562211 42.87231648 18.37901135 15.25473564 25.12759501 28.45872224\n",
            " 26.93330053 37.99010312 22.94316675 27.86575731 48.85302633 20.44789797\n",
            " 16.9908495  33.80044095 12.45627236 26.34377467 18.35571775 11.11054296\n",
            " 27.25652123 20.8622955  33.60200571 23.74957631 20.58558714 37.39450381\n",
            " 33.41253741 21.94965739 33.1196308  18.02360635 31.49108381 14.53604084\n",
            " 38.26519822 27.53830639 30.99423683 40.85360359 47.47421198 16.31234763\n",
            " 24.73883576 49.18707211 17.91269012 18.95437774 27.8497445  44.00332845\n",
            " 32.55608021 21.50712963 23.72535852 22.14119176 36.9338642  13.22576438\n",
            " 49.15229363 17.18978137 43.63981133 19.7177387  35.26516872 26.16660573\n",
            " 13.45602213 33.86648701 43.66774464 15.46007504 41.50421361 22.35802565\n",
            " 38.08588569 30.70967643 39.58952153 26.90201872 48.98945618 25.51581238\n",
            " 18.4431741  12.96713941 27.69398194 26.18559488 18.02337264 12.13949546\n",
            " 23.25052087 17.76727717 29.94093586 34.67320516 20.02025158 29.64415069\n",
            " 43.83251081 40.05107272 21.14808207 35.1329949  11.42993615 45.61120387\n",
            " 39.90154037 26.41627897 15.65689791 18.08926424 32.96705622 14.27405066\n",
            " 29.02905492 23.40308075 27.98201522 31.67047863 20.3138841  23.7702251\n",
            " 20.39363504 22.64945058 27.04965425 39.91612717 19.44765605 12.89393341\n",
            " 14.552221   17.57519244 12.0333493  15.22561566 16.82680373 35.73874518\n",
            " 26.41996473 17.17656699 23.0073496  14.52500338 38.79043996 27.98602772\n",
            " 22.29512742 19.40800044 25.48716111 14.76643547 20.62983469 19.92665881\n",
            " 12.51117734 17.67423195 28.43211111 29.95633723 11.42993615 38.03200073\n",
            " 19.78539039 36.65583723 24.14234275 27.18282647 11.42993615 26.99582684\n",
            " 32.29593152 36.25234912 30.11618858 39.04610613 28.22835857 27.98754954\n",
            " 20.31036281 35.3490837  31.57106609 36.11683829 19.37869848 21.08932997\n",
            " 18.22685372 49.23491143 24.75128275 23.04196299 37.62574427 22.14876034\n",
            " 19.40872528 31.4371319  36.80698361 49.18944567 15.44145842 27.94892173\n",
            " 21.26922805 33.76787772 33.45902948 49.27754093 26.73467296 47.32061776\n",
            " 18.63078374 20.1430352  13.69223703 23.32808366 23.07624913 22.39738455\n",
            " 16.96813193 26.2080057  13.55678486 18.89614341 11.42993615 25.61130116\n",
            " 18.4884586  11.42993615 49.99997696 18.55335528 42.47918306 22.26731026\n",
            " 21.84178164 44.15176014 19.94995441 15.76977842 13.60958264 49.68895594\n",
            " 24.34490558 17.3159793 ]\n",
            "selection [237 119 528 731 108 700 432  19 362 710] (10,) [10.71643601 10.74529856 11.02756637 11.11054296 11.18326301 11.25232882\n",
            " 11.34877563 11.41020022 11.42993615 11.42993615]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (410, 10) (410,)\n",
            "updated train set: (410, 10) (410,) unique(labels): [154 256] [0 1]\n",
            "val set: (892, 10) (892,)\n",
            "\n",
            "Train set: (410, 10)\n",
            "Validation set: (892, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.12195121951219512, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.46      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0]\n",
            "std (892,) [43.52805664 16.45832751 21.13887923 29.65947282 29.07331707 28.45899076\n",
            " 18.49869431 19.0629076  30.03386224 48.27573394 47.65196024 44.51453218\n",
            " 29.80612234 18.35780346 26.12382371 20.51630844 12.78934888 18.38300001\n",
            " 40.97373128 15.91674379 24.13617047 37.38935401 22.06634591 12.78934888\n",
            " 33.6015654  20.60695245 16.09828146 23.27499748 34.47664423 38.76515029\n",
            " 20.49464474 32.62371862 46.11260155 17.48848862 19.39363107 20.5882127\n",
            " 28.15332166 26.72961856 22.34054406 31.25644479 14.64189653 22.6203463\n",
            " 22.15812851 28.81705084 39.69622986 47.55542463 31.08570712 29.48522436\n",
            " 15.76270499 23.3989346  22.949202   21.44333607 15.75589234 43.45307641\n",
            " 31.04376815 46.37540919 15.17160721 30.20235319 14.07324087 30.98072371\n",
            " 37.65297975 22.0108818  15.79670859 39.4306193  30.91392663 31.62923951\n",
            " 21.51197562 18.74370585 22.54551983 29.72948537 49.93292644 39.98155055\n",
            " 24.23965104 25.56380793 29.52144469 16.65379413 19.56304503 32.18639338\n",
            " 19.59592425 35.17612181 12.78934888 31.26289596 21.10663035 20.17567216\n",
            " 31.98944421 40.57771087 43.20314764 16.61956753 37.43104008 33.05397224\n",
            " 49.92856959 41.11540684 20.47095938 27.98887165 49.98444735 43.0646855\n",
            " 40.85056605 34.9481617  19.57075202 13.81464159 29.14320809 31.01755068\n",
            " 28.14194871 15.04606235 29.29438595 37.60773918 37.71339868 32.07394591\n",
            " 18.00989499 26.4441931  19.8804377  35.1875743  20.23026031 12.78934888\n",
            " 30.13758591 46.57359836 27.04920125 44.92365771 22.76273535 33.01154633\n",
            " 24.74774059 15.32510097 29.59929253 49.90196647 15.15622495 33.19449807\n",
            " 14.79284243 22.91811064 16.18260967 49.99888066 20.13608075 39.41836029\n",
            " 32.15957491 22.98555945 33.06027685 12.78934888 19.57130386 39.02549208\n",
            " 14.22695471 14.43845354 29.8368361  16.32826461 17.89430175 17.07984481\n",
            " 21.42164963 36.43994022 22.12054395 15.71872696 38.48311885 19.767974\n",
            " 16.35875774 22.91383975 18.90048573 13.84424678 15.00123669 32.45258216\n",
            " 36.43401964 12.78934888 25.66778833 17.01439063 26.90730042 18.25019108\n",
            " 45.87237525 17.96946348 26.29577887 24.9665228  15.99453541 33.87012184\n",
            " 18.37848778 41.33560282 16.07676751 31.51582115 25.27483586 49.51290208\n",
            " 15.61606326 24.01950528 25.3995379  32.55039498 29.24178768 38.41592604\n",
            " 42.11530267 47.05197176 28.07578262 18.00052737 16.58112319 33.82393085\n",
            " 49.51227411 12.78934888 40.32610818 20.71059607 33.71529896 16.34990883\n",
            " 20.37129954 37.63800073 23.50686522 14.13308637 29.86609661 17.06693277\n",
            " 13.71872236 17.19464066 47.00867212 19.99047745 32.02487034 40.4630399\n",
            " 28.23740719 35.94744645 22.33680039 23.54502024 42.34929077 23.24829871\n",
            " 31.79783723 22.40107435 35.81605327 23.6317498  27.33496608 36.90566012\n",
            " 17.60865775 17.29570093 23.62827499 27.51540333 15.44808079 26.47081709\n",
            " 20.15941055 32.66349084 36.98921904 28.75699306 35.32263332 28.65813237\n",
            " 14.79522281 28.28650586 49.24486058 18.26507079 17.08695565 32.94566526\n",
            " 25.00052279 20.67570325 37.43353679 36.87257539 39.11390324 17.70768501\n",
            " 16.38636108 45.93250001 36.64998101 31.53248828 28.07600215 49.87101028\n",
            " 21.71113964 18.06726874 31.50174586 17.4501499  25.43441166 28.26691863\n",
            " 22.70834578 12.78934888 49.99170521 12.78934888 40.77781422 38.5585293\n",
            " 43.92985812 30.98927831 12.78934888 31.9700923  46.05366579 13.73740623\n",
            " 37.51913226 26.86284282 48.65975438 16.19419837 18.03545388 21.74739397\n",
            " 24.21132812 32.3213152  18.89525434 28.13870055 24.69611854 46.82602973\n",
            " 49.77960618 26.23930028 24.26363153 25.22187787 20.36332527 18.37408939\n",
            " 17.96660494 19.26686005 37.39497064 17.12537333 21.45680883 21.52762828\n",
            " 12.78934888 30.27372149 23.11957802 30.91392663 34.64443931 17.43852209\n",
            " 19.47772786 33.85278902 27.44584877 24.15834861 13.43430595 32.26519954\n",
            " 33.33005887 24.03309857 22.46049187 36.62736851 30.9955571  33.89659428\n",
            " 23.67685347 30.66281529 23.88370011 12.78934888 39.57828164 23.90598566\n",
            " 35.98046335 19.16295076 20.10339978 33.85994085 29.87138321 30.65875046\n",
            " 17.7872078  22.65070826 18.36077011 14.43805151 12.51109501 33.07484298\n",
            " 21.76538593 27.36771015 24.72501307 36.32433015 18.83090269 16.75386311\n",
            " 31.2781465  19.80874686 16.46094143 22.83732826 23.64337121 28.29693051\n",
            " 19.73590388 30.77467349 30.01473144 31.05064757 20.58473173 17.76255268\n",
            " 38.03770787 14.6906807  21.60933868 30.98831914 22.8426809  49.95641631\n",
            " 24.00980958 22.89045464 24.65588513 17.89913734 36.82566657 48.50714134\n",
            " 27.93797781 43.19185211 26.72757731 29.95830296 12.78934888 20.05172846\n",
            " 27.20603916 23.08832168 31.07424382 27.60265366 21.74828907 27.3299279\n",
            " 31.50106943 22.0153116  18.67639161 27.22060858 16.78324125 20.1774676\n",
            " 16.26094821 25.76320075 22.4704535  35.15636475 18.10816472 48.91371268\n",
            " 34.09128635 36.28835442 28.36440192 28.41290699 18.96662638 26.74440322\n",
            " 23.19043487 16.67644252 42.13481217 15.61344784 28.2598398  30.23541255\n",
            " 23.41540972 19.49070861 32.8618096  40.21888168 24.38098319 26.14744207\n",
            " 21.35801624 12.78934888 24.21688832 40.47166025 17.78931205 29.4704601\n",
            " 32.79279656 48.96745142 36.6907175  43.94086062 38.00924484 44.02044413\n",
            " 33.28866918 34.04903122 20.95542293 38.39732763 24.18344949 30.62744328\n",
            " 31.92665204 16.0937525  23.16550908 42.76176576 27.81463903 25.57648548\n",
            " 30.50248003 22.56311674 13.89566852 31.39473418 33.95304206 48.86808539\n",
            " 18.15300114 24.54414024 25.96284782 34.20205855 18.67319761 46.64498475\n",
            " 49.99616271 30.31964282 14.2762474  26.4112477  15.42137775 15.76927004\n",
            " 31.23909791 16.77109007 35.08003676 14.27085725 27.23580743 49.25107333\n",
            " 29.87393087 47.10660742 12.78934888 19.55892577 36.92784521 20.31474588\n",
            " 17.91935664 28.37420704 21.37754392 24.11597621 17.27382646 44.66262109\n",
            " 41.72796684 28.64675082 25.02737179 31.00037973 19.15413659 15.74093562\n",
            " 23.05520542 21.82330067 17.35744673 14.22011739 21.50601127 21.74914749\n",
            " 18.58895976 31.74123617 38.14510201 29.51640135 18.77020682 26.39940532\n",
            " 43.40504584 39.47882112 34.12281093 33.18781824 12.78934888 12.78934888\n",
            " 29.71999034 19.64629486 22.01306767 37.66985011 36.24398307 27.9712502\n",
            " 26.58309624 21.14716846 45.24895314 26.31052398 27.96999594 32.43292596\n",
            " 22.35496649 37.4195826  36.73036548 24.22869746 42.76712077 17.41298607\n",
            " 12.51109501 25.69837187 13.75011342 26.84723234 43.48802781 28.9517795\n",
            " 36.43199952 48.39075077 13.77306919 26.40540985 24.29245309 18.58895976\n",
            " 16.41062674 21.1352497  16.41606843 31.57932159 20.36129276 23.29950579\n",
            " 41.05583534 49.99717264 28.22637014 30.96911076 42.37877891 29.47991811\n",
            " 36.60917856 14.11180833 16.00406477 40.73261057 23.39000224 19.36954238\n",
            " 17.73653339 15.86945078 42.6883446  32.58654298 24.1377269  16.02084552\n",
            " 19.58140126 42.59175107 36.84387532 29.36054306 34.88020637 30.61275955\n",
            " 34.79588709 24.78549761 15.99168904 16.990037   29.87763263 19.26991888\n",
            " 17.38245185 21.43224406 23.49046461 42.11000852 34.4870273  43.05805132\n",
            " 17.90993962 38.32683929 16.39554976 14.95168766 49.9979734  40.82845044\n",
            " 13.59129412 12.38956381 37.44892211 18.58181213 25.13555759 38.50124222\n",
            " 20.88211539 23.44298904 11.98999728 19.32477019 26.37966491 42.24737044\n",
            " 23.41009015 12.99891285 49.68681979 15.77139818 43.12505994 21.98170734\n",
            " 27.64330752 35.41761542 18.17805394 33.13903695 37.70661276 34.39917465\n",
            " 16.5431921  13.16372322 32.82595033 22.47324008 32.88845686 21.73190586\n",
            " 35.15837113 20.93556012 38.69791097 19.09900412 17.94293074 33.66430736\n",
            " 19.93053737 43.24465014 24.22608048 48.48244773 28.88529427 35.37222055\n",
            " 17.42024815 19.26686005 49.86050568 29.47359517 21.55234343 25.99232227\n",
            " 43.3725204  28.20941744 39.05657497 38.80887681 47.54309621 23.78668327\n",
            " 25.87459281 36.58655649 22.33277601 48.78277402 12.78934888 20.75605835\n",
            " 30.80702617 26.42136294 19.94166832 34.20944329 21.17314907 12.7819634\n",
            " 38.45484875 12.78934888 12.78934888 42.27068544 32.85190884 40.98331004\n",
            " 41.36541748 47.4900564  19.29873641 31.55079916 28.86787215 16.62966128\n",
            " 27.28338092 16.26348464 15.45428661 21.68500558 46.94553322 17.42809193\n",
            " 26.64458043 39.0973844  16.17114897 15.36613274 13.17474839 17.57649888\n",
            " 28.20354475 37.05752074 24.59445762 40.28225901 42.57856463 44.66334372\n",
            " 33.17820929 39.4875792  17.71996973 15.97828499 19.09204699 16.39905719\n",
            " 21.96418467 49.99180989 47.37087493 38.79073323 24.59603207 48.84768024\n",
            " 15.27634666 33.45946229 21.04952045 21.97016404 24.59541833 16.85413511\n",
            " 40.52097643 34.73735664 30.19452148 12.78934888 28.79733142 30.64712608\n",
            " 14.49488097 27.8739294  12.78934888 29.09743304 17.63237553 23.61463939\n",
            " 35.32229096 13.4508704  31.30159542 24.89798492 24.93926151 36.57050041\n",
            " 23.21028    37.25777475 49.95383953 27.08549608 15.81247773 24.66489412\n",
            " 17.94253397 36.95219179 38.56606925 23.79493045 25.44374603 43.51569483\n",
            " 30.27594241 46.38918946 21.81472073 29.73373721 44.79529697 20.69657643\n",
            " 15.92461921 27.71567583 31.26361182 29.46492318 40.40850717 25.18406698\n",
            " 33.20137492 49.24806482 22.34346359 18.40899911 36.53675689 13.14495205\n",
            " 28.63290048 20.97186    29.49000944 23.05573325 36.11059643 24.96268736\n",
            " 23.05520542 39.67705488 33.86161943 23.49113709 35.45921956 19.1352608\n",
            " 33.92954277 16.3175967  40.76041706 31.03139837 34.97499799 42.82171701\n",
            " 48.31965313 19.10064743 26.25797207 49.49754978 18.35205206 20.35028303\n",
            " 30.54059342 45.76776946 33.92604432 24.57403598 26.37201141 24.3402963\n",
            " 38.96397675 14.75100658 49.51640463 19.54099635 45.31067761 22.19666902\n",
            " 37.54407712 28.15504497 15.86606103 35.94744645 45.41883905 17.4144598\n",
            " 42.8419536  24.24773376 40.29031316 33.47701385 42.03169567 29.5786757\n",
            " 49.38585929 26.51637943 20.76821944 14.15727475 30.50497321 28.90105695\n",
            " 19.73574139 14.44449469 25.14319464 20.00079391 32.44246161 37.36650401\n",
            " 22.56026206 32.38214698 45.47484255 42.90730975 23.22491477 37.91766678\n",
            " 12.78934888 46.65528097 41.93000404 29.11055725 17.86284276 20.4146015\n",
            " 35.54364258 16.62966128 31.64465367 26.10900258 30.82999529 34.07892038\n",
            " 23.54502024 26.31052398 22.42633756 24.90390238 29.14325279 42.18612861\n",
            " 21.24353689 16.30874859 16.65379413 19.31102414 13.81732679 16.79091014\n",
            " 17.71241306 37.91035602 29.51531253 19.28364236 24.65121459 15.80675577\n",
            " 33.76319401 30.33225792 23.73040527 21.54042569 28.52158182 16.93574241\n",
            " 23.43564458 22.11598628 13.58681012 19.39519898 30.59039244 32.1210608\n",
            " 12.78934888 40.01487056 22.03000969 39.0264665  26.31300749 29.46667937\n",
            " 12.78934888 29.05021999 35.79138221 38.2531606  32.50455895 41.04220878\n",
            " 30.55445769 29.99290968 21.20732396 38.40224873 33.56258829 37.21573218\n",
            " 23.91222048 20.28870144 20.47734224 49.56322236 28.07485237 23.25303724\n",
            " 39.34213763 24.23782432 21.50528968 34.58527058 38.61205275 49.48798489\n",
            " 17.74592078 29.91431695 22.73345307 36.16806432 36.45942344 49.58877328\n",
            " 30.93281822 48.1425137  19.96574049 22.1085742  15.04441199 26.50455656\n",
            " 27.49735966 24.37846645 18.32789917 28.57496972 14.49200526 20.44574347\n",
            " 12.78934888 28.57178967 20.58543501 12.78934888 49.99999261 20.9942078\n",
            " 43.99608605 24.49630006 24.62546091 45.31266254 22.51274987 18.11158412\n",
            " 15.09685853 49.84018376 28.0873584  18.85615251]\n",
            "selection [566 559 498 322 623 479 358 828 135 680] (10,) [11.98999728 12.38956381 12.51109501 12.51109501 12.7819634  12.78934888\n",
            " 12.78934888 12.78934888 12.78934888 12.78934888]\n",
            "trainset before adding uncertain samples (410, 10) (410,)\n",
            "trainset after adding uncertain samples (420, 10) (420,)\n",
            "updated train set: (420, 10) (420,) unique(labels): [161 259] [0 1]\n",
            "val set: (882, 10) (882,)\n",
            "\n",
            "Train set: (420, 10)\n",
            "Validation set: (882, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.11904761904761904, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.47      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (882,) [43.50963578 17.62723842 21.59607472 27.7182974  28.73646893 28.92314633\n",
            " 18.70461692 19.56002011 29.93735251 48.35430474 47.81882532 44.51516127\n",
            " 29.5198097  17.61120149 26.35000437 21.2100298  12.82775468 19.02010041\n",
            " 41.0547205  15.65512029 24.15804074 37.37958415 23.20168262 12.82775468\n",
            " 33.54878506 20.67915853 17.51853718 23.93413124 34.80499595 38.99910323\n",
            " 21.61402247 32.78857593 46.16619623 16.65471453 19.59907284 21.54463512\n",
            " 29.1562723  27.55329143 23.18055412 31.99974817 15.04291233 23.7320747\n",
            " 21.36758817 28.88861668 40.11105402 47.59578724 31.38853704 29.42527512\n",
            " 16.30703803 23.48613951 23.37009711 21.13406824 15.87061904 43.20751025\n",
            " 31.45938304 46.49744312 15.39990669 29.49254334 14.97944644 31.39968318\n",
            " 36.93977819 24.1347048  15.99482069 38.5873122  31.72464052 31.61488934\n",
            " 20.39198848 18.42025228 17.37231078 30.8071084  49.93749898 40.41160189\n",
            " 22.92747131 25.6579239  28.28502607 16.86211039 20.09205806 32.24233142\n",
            " 19.6249782  33.92891374 12.82775468 31.10605134 23.21855762 21.37716384\n",
            " 32.32959752 40.19715134 42.68533089 17.10087702 37.83433169 33.08733572\n",
            " 49.93428607 41.18249    19.28864769 28.12392392 49.98657914 42.98900841\n",
            " 40.68509337 34.85607326 20.13845889 12.34068901 30.02787397 30.7878239\n",
            " 28.99967489 15.57923268 29.3328377  36.12997954 38.03199484 32.4259712\n",
            " 17.6980348  26.21231055 21.07797999 35.54353615 19.79023676 12.82775468\n",
            " 29.78918527 46.63372167 26.62113174 44.97007295 21.67263245 33.4163898\n",
            " 25.17595403 17.53295899 28.75194715 49.90751892 15.86533686 32.78489265\n",
            " 16.32619643 23.72778212 15.9638908  49.99908712 20.11018901 39.18978631\n",
            " 32.51957612 23.83616945 33.10590479 19.73414324 38.83952592 15.6975856\n",
            " 14.49676681 30.23621926 15.91860044 16.73650363 17.04552791 21.74593864\n",
            " 36.01569194 22.47446728 16.36196228 39.32376568 20.40074873 17.29582738\n",
            " 22.15574171 18.50355879 14.47163879 15.08951829 33.00808146 36.84881034\n",
            " 12.82775468 26.13179536 15.60597462 23.83192648 18.55832479 45.66496835\n",
            " 18.84213763 26.60671002 25.96298603 15.11884969 37.21131898 18.22383944\n",
            " 41.17572866 15.69653858 31.28810114 26.76289232 49.47774211 15.87992511\n",
            " 24.66362002 25.67449374 33.25896009 28.09592889 37.67775088 42.30804434\n",
            " 47.29616692 28.47156145 18.75962023 17.34621393 34.24877953 49.58407381\n",
            " 12.82775468 40.3126747  21.3437029  34.07448484 16.31885524 20.86397455\n",
            " 37.65167011 23.93238597 14.43168089 30.04610795 16.99945926 14.22066767\n",
            " 17.63850497 47.29049737 20.11019346 32.30132747 40.32832307 29.19335941\n",
            " 36.20750659 22.29800813 24.15690973 42.03673376 20.83363277 32.80504408\n",
            " 23.14043836 36.07746621 21.24843544 27.829223   36.79081692 18.15022435\n",
            " 17.01573189 24.38081491 26.77919075 15.28105029 26.96632118 19.50004158\n",
            " 33.06233419 36.20253425 28.9574982  34.8378044  28.74104495 13.80284396\n",
            " 28.33381748 49.28066294 18.50093406 17.77192707 33.3841857  24.51109714\n",
            " 20.94536877 36.83854338 36.99866214 38.56625487 18.45638687 17.86275591\n",
            " 45.8726897  36.45396023 30.78800649 27.40429635 49.88693758 21.60744579\n",
            " 16.94044031 31.72512294 18.31888914 25.98801251 29.26369528 22.03765675\n",
            " 12.82775468 49.99311537 12.82775468 40.83505551 38.36560078 43.62329868\n",
            " 31.29607641 12.82775468 31.83626455 45.47950625 13.5423284  30.27888535\n",
            " 27.4773903  48.64322419 16.86309179 17.35932595 21.26555144 24.8701741\n",
            " 33.57590859 19.08375908 28.34202467 24.61721633 46.98266855 49.79518652\n",
            " 26.52285063 24.17429845 24.97725823 21.92798308 18.41019335 17.91107101\n",
            " 19.5811121  37.18785493 17.83772452 21.95911896 22.26053724 12.82775468\n",
            " 30.61111915 23.50253432 31.72464052 33.99577282 17.99686052 20.06482366\n",
            " 32.802804   28.01709877 23.75544392 14.20149747 32.39729909 32.67356661\n",
            " 23.74824789 22.69842432 36.24470091 31.16697849 34.12944508 23.20680973\n",
            " 31.21167152 23.28012792 12.82775468 39.35290939 24.98917943 37.11778852\n",
            " 19.96787616 20.95780836 33.36838503 30.22685505 30.28328919 17.69176096\n",
            " 24.40542007 18.9693326  14.24299488 33.49535635 22.60556382 28.89589324\n",
            " 25.55669089 36.66718861 18.40298424 17.38896448 30.67327352 19.46022616\n",
            " 16.38295048 22.2934628  24.1201891  28.01459145 20.06913152 31.45352942\n",
            " 29.63374208 31.93746186 20.51491801 19.19203898 38.4366664  14.90257886\n",
            " 23.27617981 30.39033912 23.52601411 49.96057222 24.13073045 23.27486584\n",
            " 23.9151563  19.24371034 37.10698148 48.5481661  28.77111808 42.94791611\n",
            " 26.52398391 29.66001607 20.64080162 28.05357819 22.79909723 31.79006606\n",
            " 28.63822114 21.54663545 26.9837995  32.24420626 20.72769449 18.67738306\n",
            " 25.29937938 16.58620147 22.23648637 16.94127209 26.14875519 22.56660874\n",
            " 35.04115522 18.27973753 48.90469922 34.22394893 36.13540934 27.94475285\n",
            " 28.58517336 18.30422176 26.78847006 23.13727904 16.70278827 42.19867554\n",
            " 15.73483241 27.91892165 30.52585887 23.72890977 20.39087233 32.3620659\n",
            " 40.35740814 23.05611993 26.98155975 20.48789801 12.82775468 24.45071735\n",
            " 40.52101301 18.52053295 30.08568338 35.2073243  49.02844198 36.46445057\n",
            " 43.45187279 37.40979358 44.26270649 34.12545747 33.9256299  20.96629309\n",
            " 38.07524311 25.32863745 30.8404478  32.50494212 15.58788702 22.97587413\n",
            " 42.92623623 28.79252282 25.7187011  30.94623367 21.6865134  16.0530054\n",
            " 30.65455839 33.55060277 48.94144218 18.42986009 23.86998554 26.40892213\n",
            " 35.16668298 19.21802947 46.737422   49.99677713 31.12176048 14.23883449\n",
            " 27.86638595 17.24335176 14.95034083 30.56694002 17.10925645 35.27341149\n",
            " 13.88843358 26.59968719 49.30335786 27.86694654 46.8600813  12.82775468\n",
            " 19.22308597 36.3189326  20.67427972 18.43846802 27.86193746 21.03795439\n",
            " 24.36049992 17.7413348  44.73427392 41.81400856 27.08323079 25.35942343\n",
            " 31.47010014 18.85289875 15.86138908 23.64503101 21.37479063 17.6050854\n",
            " 14.95209703 21.23640034 21.18273253 19.57082018 31.82365403 37.80222645\n",
            " 29.50134018 18.95152858 27.41958562 43.68675419 38.99172796 34.09472307\n",
            " 32.84638784 12.82775468 30.74470845 18.98064033 21.53484145 37.77069219\n",
            " 35.9914918  28.80169663 25.32753779 21.14188055 45.15758713 26.97246866\n",
            " 27.9602293  31.83278803 22.41269261 37.1705758  36.50319    24.20405105\n",
            " 42.81258172 17.35712467 25.07367736 13.6137426  26.69973856 42.40055367\n",
            " 28.66902585 36.5498871  48.25670499 15.11508325 24.0405218  23.99720796\n",
            " 19.57082018 15.46831581 21.91206115 17.81738362 31.44788903 18.9622707\n",
            " 23.83235244 41.30699354 49.99771397 28.70016846 31.05870315 42.29823048\n",
            " 29.45825847 37.23076276 13.94712391 16.8875346  40.894699   24.78358062\n",
            " 19.36593776 17.38734041 16.63796929 42.38097089 33.16085763 23.41923782\n",
            " 16.67635714 19.17292452 42.6486994  36.97715261 30.22471384 34.97666489\n",
            " 30.68919855 34.79312335 24.93522553 16.84478324 17.87248479 29.90406492\n",
            " 19.90146577 17.12410572 22.12653276 23.16823672 41.32383389 34.88338808\n",
            " 44.59982328 17.84489207 38.57910593 17.23787771 15.62286152 49.99819936\n",
            " 40.74469575 14.66594878 37.85176312 19.15592117 24.61892822 38.72643216\n",
            " 21.80139976 23.57363619 18.12489302 26.70333839 42.46455466 23.73951385\n",
            " 13.59566547 49.73728195 15.84605269 43.14687841 20.24916911 28.3748108\n",
            " 34.14100502 18.65614028 33.59167626 38.46521344 34.74694758 16.69296154\n",
            " 13.79965128 32.98404186 21.40268224 32.98605081 21.88252996 34.43507184\n",
            " 21.35610745 39.63199646 18.87387654 17.92919179 33.38376544 20.73526834\n",
            " 42.9038682  24.82985182 48.42944004 28.86105352 34.57591151 18.57884022\n",
            " 19.5811121  49.86792664 29.75260564 21.13137317 26.1037353  43.69452664\n",
            " 27.55946836 39.36621394 38.49711253 47.59793983 23.79561774 28.4440429\n",
            " 36.33528527 23.4525376  48.79660382 12.82775468 21.73056388 30.95372387\n",
            " 26.90742771 20.37306371 34.49553453 22.48695164 38.64339763 12.82775468\n",
            " 12.82775468 42.37548285 32.79643445 40.96714315 41.35371337 47.54846499\n",
            " 20.6000155  31.58222465 28.74506134 17.01664934 26.73598259 16.62009109\n",
            " 15.76799349 21.63125579 47.69372433 18.40438895 27.24883059 38.56481463\n",
            " 15.48741176 14.56902605 13.91147512 17.67993872 27.60106048 38.24015098\n",
            " 24.71321826 40.22813539 41.70048193 45.15791236 33.76368869 39.20854137\n",
            " 18.59564077 17.04122241 19.11488527 16.46646492 21.69065807 49.99252984\n",
            " 47.37711891 39.27086052 24.14314684 48.99463498 17.00485844 33.9419691\n",
            " 22.11451359 22.01075249 24.50724989 17.45936044 40.97997214 35.26950138\n",
            " 30.29648839 12.82775468 29.8309607  31.74402616 14.49402117 27.51976595\n",
            " 29.60798049 18.94989391 24.21856914 37.45293648 13.34571702 31.66604768\n",
            " 26.78676224 24.77305086 36.67602048 24.50293393 37.76758239 49.95831657\n",
            " 29.56440958 15.53308506 24.87354653 19.09147216 35.67485443 37.80969445\n",
            " 23.31061453 26.43875078 43.77649051 30.69707205 46.38098911 21.69754905\n",
            " 29.26629068 45.01743888 21.47725828 17.18489798 28.58772195 31.5540375\n",
            " 30.58057214 40.59804505 25.91394672 31.22796901 49.31402598 22.84588524\n",
            " 18.66845336 36.03593686 13.52980862 29.00931275 21.1961016  29.18815079\n",
            " 24.19010109 35.9875778  26.45113491 23.64503101 39.23352148 34.34269637\n",
            " 22.48383204 35.65286711 20.19979227 33.84392454 17.64571066 40.75993635\n",
            " 30.01123355 33.92055254 43.18526251 48.29733215 19.15134255 25.25806896\n",
            " 49.53380583 19.81522947 20.91705346 31.26069167 45.67167147 33.17125508\n",
            " 24.67036879 28.05778542 24.72886727 38.75237976 14.46458286 49.51539977\n",
            " 18.38798272 45.18388352 22.8897996  37.70725594 28.20229157 16.56898693\n",
            " 36.20750659 45.11834034 18.00722943 43.2158266  22.46717051 40.53373663\n",
            " 34.0190654  41.78169511 30.31644995 49.41206679 26.00584875 21.68108044\n",
            " 13.43675762 30.28658134 29.22276448 20.59897732 14.37325685 25.00278711\n",
            " 20.91775594 32.49355295 37.57488153 22.73778035 32.2179658  45.36190157\n",
            " 43.30335633 22.29999745 38.0288424  12.82775468 46.28245313 41.73544009\n",
            " 29.27936018 18.02643171 20.99860443 35.19647223 17.01664934 31.43922382\n",
            " 25.99356388 30.96275545 34.96436685 24.15690973 26.97246866 23.43098736\n",
            " 25.15825982 28.54526193 42.24794641 22.15943063 16.27077483 16.86211039\n",
            " 19.42818273 13.39645302 16.77979928 17.40073573 38.48517392 29.25804596\n",
            " 19.89201885 25.6874512  16.39818939 39.32217992 30.70359095 23.50216144\n",
            " 21.40440322 28.73031864 16.68190483 23.63215579 21.72621776 14.57533857\n",
            " 19.54638762 30.85855547 32.50014829 39.61417574 22.92039096 40.03090165\n",
            " 26.2813379  29.19347137 12.82775468 29.22028543 35.23353683 38.78826937\n",
            " 32.20500183 41.10322842 31.22205985 29.888405   20.48874126 38.209135\n",
            " 33.13631248 37.59019424 22.34443817 20.96938929 20.97511028 49.56106488\n",
            " 28.6397722  25.050498   39.78774033 24.83633647 22.09777176 34.37131188\n",
            " 39.2398851  49.53535615 18.92883664 30.41798293 22.95246993 36.27926858\n",
            " 36.94670868 49.59132284 29.23611007 48.23941353 19.9092003  22.00510849\n",
            " 14.81150199 27.50615938 26.37196381 24.56138136 18.31844073 28.72328078\n",
            " 14.87998324 20.7949797  12.82775468 28.19441432 20.42608878 12.82775468\n",
            " 49.99998092 20.9184736  44.19992885 25.13840777 24.42510391 45.49780131\n",
            " 22.08310282 18.7886314  14.87084795 49.83775255 27.44534415 18.81063348]\n",
            "selection [ 99 259 443 308 475 869 287  80 254 667] (10,) [12.34068901 12.82775468 12.82775468 12.82775468 12.82775468 12.82775468\n",
            " 12.82775468 12.82775468 12.82775468 12.82775468]\n",
            "trainset before adding uncertain samples (420, 10) (420,)\n",
            "trainset after adding uncertain samples (430, 10) (430,)\n",
            "updated train set: (430, 10) (430,) unique(labels): [171 259] [0 1]\n",
            "val set: (872, 10) (872,)\n",
            "\n",
            "Train set: (430, 10)\n",
            "Validation set: (872, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.11627906976744186, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.47      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (872,) [44.22617528 18.13925421 22.92001802 30.6424837  30.28617981 30.72874919\n",
            " 19.65574195 19.9987038  32.58418264 48.64275325 48.02265599 45.26648521\n",
            " 31.19846781 18.82198227 27.1632036  23.18163519 13.19627644 19.34331985\n",
            " 42.20747153 16.18392766 26.3970295  38.84682395 23.75381751 13.19627644\n",
            " 34.51666673 22.25096274 17.79335239 24.91802176 35.86863103 39.96077424\n",
            " 22.50810723 33.7376139  46.78121877 17.77668051 19.83246433 23.32868086\n",
            " 29.96206117 27.7369159  24.24548287 32.90742615 15.15926796 25.10262603\n",
            " 22.3112247  30.21618441 41.0333106  48.03052742 32.77595962 31.6398465\n",
            " 17.71642755 24.50005142 25.1522865  22.04222491 17.10352585 44.09077233\n",
            " 32.91590641 46.89837453 17.47251433 30.90140766 16.35268299 32.54505191\n",
            " 38.33008429 23.55640603 15.69591461 39.9885859  32.04324742 33.51197409\n",
            " 22.38072417 19.54212976 21.48406339 31.09768068 49.95348244 40.68762874\n",
            " 24.52567675 26.7128262  29.63267664 18.33357537 21.90433279 33.55488065\n",
            " 21.11417381 36.06302083 32.79393761 23.99965938 22.77225425 33.43869309\n",
            " 41.13219457 43.95938881 17.26350054 38.54224838 33.30777181 49.95236224\n",
            " 41.92245055 20.9626472  29.70990615 49.99083228 43.70436388 41.97149972\n",
            " 35.91816602 21.0102712  31.06119967 33.13230093 30.09900205 15.29406141\n",
            " 30.40366986 37.42902478 39.28171859 33.13533445 19.70341545 27.12847119\n",
            " 20.75251308 36.45792638 20.52937189 13.19627644 31.79224612 47.03996063\n",
            " 28.63019823 45.83214941 22.46431004 34.19156071 26.72498669 17.33366643\n",
            " 30.37456187 49.94456949 17.51001813 34.19429785 16.68634948 25.42329957\n",
            " 16.38947192 49.99929703 22.09972648 40.39032711 33.95794439 25.36139266\n",
            " 34.03972358 20.51416776 40.07223779 15.20206597 15.70779349 31.87005635\n",
            " 17.21899593 17.81455956 17.14205959 23.11113986 37.4589839  22.74249602\n",
            " 16.35072118 40.17901361 21.75824237 18.08475857 23.58505604 19.68454415\n",
            " 16.23255393 16.95117542 33.8858984  37.64869568 13.19627644 27.82304367\n",
            " 18.0175215  27.67230458 20.62959163 46.32463548 19.62899051 27.27177341\n",
            " 25.91873945 15.94757091 36.97458096 18.54515777 42.36901875 16.37640969\n",
            " 32.44046193 27.36760107 49.55312734 16.51782497 25.01422583 26.57220231\n",
            " 33.95954891 30.11163372 39.19753098 43.16605436 47.59164489 29.66015012\n",
            " 18.74063033 18.51105699 34.96992837 49.67562087 13.19627644 41.28892701\n",
            " 23.25350354 34.54052975 18.16744098 22.26661161 38.23893396 24.29534905\n",
            " 16.61974971 31.50996577 17.43309823 13.92603162 17.35534561 47.59909039\n",
            " 20.87154082 33.83020435 41.43263836 30.68873507 37.72711538 24.07904691\n",
            " 26.19540527 43.31106325 23.39136287 33.36577471 24.79895425 37.29662571\n",
            " 24.55548772 29.34075963 37.86172463 19.87279301 17.89492644 25.8460795\n",
            " 28.4094715  16.09574678 27.71569765 20.10618404 34.65182536 37.83978459\n",
            " 30.24003796 36.34283723 29.53798492 15.03739674 29.77709888 49.42968656\n",
            " 20.5810484  17.85825998 34.68792618 25.20834523 22.51468959 37.6743822\n",
            " 38.19803394 39.88493301 20.60782362 18.57811608 46.50571517 38.17988741\n",
            " 32.54918487 29.1939437  49.89943492 22.55928358 15.51812484 33.72406909\n",
            " 20.00698687 27.37291752 30.66968561 22.2434341  13.19627644 49.99433088\n",
            " 41.76436917 40.04471755 44.48079259 32.9819784  33.15620756 46.8075418\n",
            " 14.04068723 38.79706946 27.12171007 48.89454322 18.18825022 18.11732489\n",
            " 22.21197233 25.54105442 33.2852326  19.73122314 30.11152638 25.17905818\n",
            " 47.56045635 49.8527214  27.51592775 25.15246994 25.75641839 23.30716728\n",
            " 19.42192105 19.25940054 20.16525264 39.17017541 19.128829   23.34277506\n",
            " 23.16627482 32.02400959 24.47847752 32.04324742 35.72462077 17.8615539\n",
            " 22.07656423 34.7050893  29.40815331 23.8667227  15.90657635 33.67830995\n",
            " 34.61486407 25.67037499 23.77875172 37.20701808 31.92213789 35.75183452\n",
            " 25.24313581 31.69438586 25.55126639 40.41363686 25.55904661 37.74005488\n",
            " 20.43096966 21.75360593 34.83068893 31.27175445 32.07090562 18.66165946\n",
            " 24.89624145 20.72820216 14.5879073  34.02216015 24.2206939  29.75250559\n",
            " 25.43914454 37.01669002 19.52225491 18.19364232 33.18474198 20.72916783\n",
            " 18.36712749 24.05179672 25.63481085 29.67536908 22.0448507  32.56670548\n",
            " 31.14383316 33.473675   22.50409336 18.63232969 39.81646675 15.6487984\n",
            " 23.96611959 31.99225306 23.97157938 49.97121318 25.8109225  23.63298046\n",
            " 25.8520014  20.26496179 37.36267645 48.80450027 30.56795054 43.89623886\n",
            " 27.98794655 30.93692002 22.32128736 28.38127822 23.6240341  32.38329547\n",
            " 30.12459999 22.68514969 28.64951511 32.71308717 22.05256348 19.96446146\n",
            " 28.02544093 18.69640982 22.15103266 16.7866912  27.87119221 24.07002595\n",
            " 36.55929765 20.34154139 49.14215661 35.87841335 37.3815863  30.06816837\n",
            " 29.71320745 19.45943826 28.33424106 24.21130321 18.57176192 42.91977168\n",
            " 16.32218553 28.82494289 32.26979622 23.67566678 21.96782743 33.56881502\n",
            " 41.11389334 25.19573753 28.93609325 21.53332259 13.19627644 26.31342696\n",
            " 41.56085518 19.03961181 30.75848786 35.48884728 49.24256618 37.77438691\n",
            " 44.4945834  38.30474418 45.21099555 35.9055226  36.07957628 22.21242797\n",
            " 39.4826765  27.0447547  31.7898726  33.77603039 16.38939452 24.61684458\n",
            " 43.59797739 30.04900871 26.26656618 32.39631197 23.15137188 15.28056276\n",
            " 32.45757354 34.17823422 49.16972513 19.81380081 24.95681588 27.47731457\n",
            " 36.22717029 20.00287402 47.10321073 49.99773355 31.87923934 15.44070577\n",
            " 29.02004459 18.01795808 16.68783887 31.76963819 18.52900982 35.93299601\n",
            " 13.86406244 28.36981628 49.40386228 30.30747583 47.56129979 21.01370698\n",
            " 37.78942221 22.39521385 20.47693321 29.15350276 21.45022267 25.45956784\n",
            " 19.70885154 45.40049642 42.75128076 28.90047916 25.99886523 33.14877748\n",
            " 19.03120724 17.91082136 24.69081088 23.19564451 18.99301197 15.73375397\n",
            " 22.36605156 23.27240413 20.36886262 32.63625493 38.89767123 31.40029376\n",
            " 20.31064414 27.84233208 44.4989137  40.58232803 35.30945501 34.36883061\n",
            " 31.57199829 20.18000485 22.29861311 38.87627935 38.97376861 29.57323642\n",
            " 27.24622944 21.68893091 45.95913287 28.47766306 29.38475251 33.43869804\n",
            " 23.03131846 38.56024285 38.1036419  26.14479243 43.48994177 18.55356551\n",
            " 26.28052196 15.03688058 28.47255342 44.23169301 29.60139178 37.72716142\n",
            " 48.62749305 16.44862456 27.43916688 26.01552041 20.36886262 16.98653453\n",
            " 24.28204189 19.10747307 33.13085313 20.05503206 25.22509453 42.69271972\n",
            " 49.99803794 29.76784896 32.04545149 43.33273217 31.06248441 38.64040444\n",
            " 14.41097213 17.85759262 42.03938098 26.15253498 20.76982417 17.80657646\n",
            " 18.65851616 43.27772282 33.53716554 24.97935165 17.05115656 21.89216103\n",
            " 43.35895682 38.48116399 31.14683899 36.01104321 31.72087176 36.50241717\n",
            " 25.17294116 18.60651201 19.71824826 31.77801202 21.50362827 17.71067642\n",
            " 22.7025792  24.7188052  42.6025802  35.63197017 45.04457623 18.7418292\n",
            " 39.65407854 17.46684201 17.77863951 49.9987086  41.82865079 15.69752348\n",
            " 38.37980511 20.25189584 26.12157556 39.65580065 23.23285241 25.04780465\n",
            " 19.64194073 27.89547617 43.40694341 25.73620368 13.04420946 49.80159455\n",
            " 17.41491178 43.8656942  21.07376878 30.39565917 36.35854454 20.0741095\n",
            " 34.40874171 39.3114522  35.16659721 17.66595893 14.70696209 34.70024238\n",
            " 23.34031155 34.49002478 24.14574641 35.85440064 22.74223058 40.53371581\n",
            " 19.70830635 18.25857662 35.07031754 21.92409857 43.70667349 26.02886478\n",
            " 48.70205553 30.2526367  36.26865782 20.0334333  20.16525264 49.89970165\n",
            " 30.6548716  23.18551884 27.59207453 44.50033505 29.133719   40.26461567\n",
            " 39.90848912 47.89790763 25.8518839  28.54771941 37.62000986 25.67898604\n",
            " 49.0745085  13.19627644 22.18040366 32.78723547 27.46810089 21.96905087\n",
            " 34.81262572 23.72139065 39.00325293 13.19627644 13.19627644 43.51909548\n",
            " 34.72856522 41.62353645 42.47967179 47.81499354 21.68317641 32.1961064\n",
            " 29.588619   17.69341395 28.13063895 16.03240189 17.48852653 22.56815627\n",
            " 47.69597999 19.12585616 28.30706637 39.95658699 16.60287622 16.91147031\n",
            " 14.1202451  19.35074776 28.96454324 38.57607931 26.09801222 41.12951845\n",
            " 43.21330981 45.93870588 35.02565509 40.37578436 19.41105256 17.56833009\n",
            " 19.99787118 17.21665663 24.52847777 49.99488353 47.97033504 39.92610339\n",
            " 25.93907822 49.13967455 17.59468304 35.09618635 23.0100184  23.65076939\n",
            " 25.1334377  18.8901374  41.86392351 36.19035632 32.13638664 31.19100058\n",
            " 32.4656842  16.12301976 28.50041637 29.99545457 19.81485721 24.84156119\n",
            " 36.9095395  14.74636254 33.24894957 28.07175126 25.55311122 37.59974758\n",
            " 26.17128597 38.34457835 49.97184049 29.36341693 17.00683246 26.87990638\n",
            " 21.10313141 37.28000336 39.60261919 24.18412159 28.46321903 44.3908527\n",
            " 32.32572047 46.85127617 21.99795113 30.22043891 45.86312732 22.60208378\n",
            " 17.9110032  28.97947772 32.54367869 31.75009304 41.46420221 26.87803765\n",
            " 34.48347701 49.43157701 24.35956976 18.78399315 37.64580847 13.36574234\n",
            " 29.62880731 23.09596645 30.17856897 25.32372799 37.47375307 26.59345155\n",
            " 24.69081088 40.79820976 33.50195011 23.55859795 36.78672553 20.88356146\n",
            " 34.90090053 19.09166497 41.8831076  31.8474154  36.60682856 44.31450073\n",
            " 48.67640404 20.41174338 25.99751344 49.62363319 20.27639341 21.31545139\n",
            " 32.56594354 46.4187786  34.05482535 25.7039581  29.43387337 26.28968621\n",
            " 39.54446801 14.82052834 49.62916589 19.40249375 45.92285449 24.82477085\n",
            " 38.36797374 28.92931083 18.69062178 37.72711538 46.03019297 19.93362385\n",
            " 43.70871992 23.01036816 41.41992356 35.10085744 42.71391408 31.08151954\n",
            " 49.54795273 26.27850114 23.01944821 13.73693163 31.67710501 30.72154349\n",
            " 21.01828355 15.96650763 25.87103158 22.07101723 33.48605754 38.90214347\n",
            " 24.46213317 32.98346939 46.01837938 44.36358366 23.91616823 39.5010515\n",
            " 13.19627644 46.80197776 42.9325369  30.24925476 18.99096067 22.10111565\n",
            " 36.62852341 17.69341395 33.02198233 27.96387357 32.53367521 35.47374795\n",
            " 26.19540527 28.47766306 25.29072773 25.99784306 29.52451844 43.44614136\n",
            " 23.1364375  17.94624276 18.33357537 20.80907287 14.06232418 18.03673265\n",
            " 18.61242999 39.33257385 31.39200272 21.93210981 27.65579802 16.26466337\n",
            " 35.68676423 32.33782347 24.87202115 22.15609534 29.87958912 17.09654946\n",
            " 25.63477252 23.60103201 15.58852489 19.89308666 32.08051086 32.98184942\n",
            " 40.65474051 24.43951472 41.05478849 27.67477983 30.39819942 13.19627644\n",
            " 30.59947906 36.72915647 39.85894199 34.106411   42.29505973 32.37284929\n",
            " 30.37566323 20.77331713 39.68492851 34.52201236 37.90139357 26.14448754\n",
            " 20.13126446 22.14101882 49.70173091 29.84721271 24.58953133 40.42835025\n",
            " 26.12548062 23.38252769 35.99934274 40.5097002  49.61291523 20.08174258\n",
            " 31.04473566 23.7495599  37.38337628 38.62797576 49.71310335 31.47951329\n",
            " 48.54907217 20.12170113 23.86418841 16.06703462 28.44067806 29.46205967\n",
            " 25.93796173 19.89586347 29.73536124 16.05865151 20.92503824 13.19627644\n",
            " 29.42603292 21.87275053 49.9999904  22.66297991 44.62652454 25.64611229\n",
            " 25.76904546 45.87730183 23.16195726 20.30130376 15.46806663 49.88994833\n",
            " 29.08072035 19.72060535]\n",
            "selection [556 184 388 111 857 768 250 610 815 609] (10,) [13.04420946 13.19627644 13.19627644 13.19627644 13.19627644 13.19627644\n",
            " 13.19627644 13.19627644 13.19627644 13.19627644]\n",
            "trainset before adding uncertain samples (430, 10) (430,)\n",
            "trainset after adding uncertain samples (440, 10) (440,)\n",
            "updated train set: (440, 10) (440,) unique(labels): [180 260] [0 1]\n",
            "val set: (862, 10) (862,)\n",
            "\n",
            "Train set: (440, 10)\n",
            "Validation set: (862, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.797235 \n",
            "Classification report for LogisticRegression(C=0.11363636363636363, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.88       321\n",
            "           1       0.73      0.48      0.58       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (862,) [45.44028653 19.8185253  24.41254025 31.1143753  32.17085914 32.55143982\n",
            " 20.56792301 21.59361241 34.40919744 49.04652363 48.65303456 46.08656\n",
            " 33.04584933 19.9033722  29.1842174  25.12333222 14.34850332 20.9803653\n",
            " 43.75682822 17.55475078 28.22336899 40.39314845 25.8546474  14.34850332\n",
            " 35.91415454 23.47047085 20.48090849 27.07925856 37.25442016 42.01370617\n",
            " 24.58371301 35.58322508 47.58530445 18.66689314 21.62993821 25.36574034\n",
            " 32.01283433 29.92735878 25.42006223 34.79564231 15.94949464 27.31060744\n",
            " 22.70475466 32.04167847 42.42796514 48.67555586 34.8846043  33.3700897\n",
            " 19.51543355 26.11096949 26.96299819 23.34419868 18.47651166 44.95812727\n",
            " 34.91387765 47.73255211 18.95035598 32.11352602 17.84528423 34.33476857\n",
            " 39.64445441 25.1852779  16.83999235 41.00880299 34.21622338 35.2365331\n",
            " 22.66189985 20.4189367  21.04031799 33.2292282  49.97869294 42.55449059\n",
            " 25.62573328 28.64867361 30.9893012  20.23403052 23.40766296 35.27534139\n",
            " 22.50526555 37.1144252  34.2303072  26.49945851 24.72957599 34.55561243\n",
            " 42.44143152 44.87887732 18.5725784  40.45070193 35.43560613 49.9795288\n",
            " 43.25748283 21.16629782 31.35774134 49.99660073 45.01766149 43.09682055\n",
            " 37.74877121 22.634797   32.90878807 34.63367474 31.9177151  16.6951941\n",
            " 32.10161089 38.01823348 40.66510979 35.25588661 20.69875482 28.51160315\n",
            " 22.55507473 38.16386045 21.87337345 33.35944413 47.83960971 29.97163683\n",
            " 46.70645571 23.40724574 36.32685497 28.40859651 19.62269197 32.50471758\n",
            " 49.97435998 19.23613059 35.90837369 18.9287746  27.11857726 17.11281013\n",
            " 49.99980911 23.68693357 41.6356082  35.96507524 27.52752145 35.21245341\n",
            " 22.34827427 41.37551871 16.88576343 16.80238676 33.90562813 17.91675863\n",
            " 18.00104748 18.49646991 24.56997548 38.80996397 24.57412869 17.58364777\n",
            " 41.77402179 23.65791416 19.77025013 24.94937012 20.92424794 17.55731274\n",
            " 18.32276322 36.08036246 39.01733155 14.34850332 29.76851819 18.61846973\n",
            " 28.31703604 22.32615947 47.19206402 21.15598205 29.23084859 28.14172581\n",
            " 16.41662946 40.0866404  20.03737281 43.71098857 17.53308611 34.48369664\n",
            " 29.92455324 49.68371524 17.76501278 26.71107663 28.18779252 35.9172394\n",
            " 31.13983762 40.44788839 44.54005289 48.34215273 31.45516121 20.55089924\n",
            " 20.03289721 36.7525412  49.81675408 42.93729246 24.83178817 36.64630876\n",
            " 19.78489762 23.89631145 40.07757449 25.82323119 17.915459   33.18848206\n",
            " 18.81507091 15.56081623 18.09779923 48.295416   22.30801513 35.75560768\n",
            " 43.0317048  32.75542949 39.44366098 25.56879484 28.03467085 44.52092109\n",
            " 24.13898262 35.53817644 26.62171777 38.90652811 25.42381301 31.06854328\n",
            " 39.30256877 21.6354542  18.94180723 27.6382614  29.72945672 16.82182583\n",
            " 29.50145551 20.8557581  36.22920891 39.36075108 31.78547513 37.57448191\n",
            " 31.57140323 15.73255396 31.47200451 49.66316389 22.03299962 19.40849854\n",
            " 36.3558816  26.22545017 23.71549251 38.64742367 39.85041624 40.98542685\n",
            " 22.35158369 20.87227381 47.32171831 39.53412351 33.85092464 30.98724865\n",
            " 49.95133193 24.1388278  15.45361122 35.31430035 21.57692011 29.68640563\n",
            " 32.82820187 24.05928878 49.99802917 43.69080037 41.62701821 45.73235627\n",
            " 34.56502403 34.85049735 47.40370797 14.63998643 37.76081532 29.63560489\n",
            " 49.24486034 20.04020432 19.53578799 23.36363682 27.74138355 35.42213648\n",
            " 20.792179   32.11494217 26.63941192 48.25585413 49.92320196 29.48296653\n",
            " 26.32506854 27.69060535 25.43369603 20.32588935 20.87223702 21.6364893\n",
            " 40.61444686 20.7866507  24.6716105  24.83268934 33.47513313 26.01789198\n",
            " 34.21622338 37.04202828 19.57606849 23.67210498 35.91665864 31.42286263\n",
            " 25.63420617 17.42288351 35.6603275  36.29449965 27.3740682  25.45852083\n",
            " 38.90063174 33.9064531  37.42175785 26.09521886 33.94936379 26.87632388\n",
            " 41.64909643 27.79949335 39.99737516 22.22460621 23.88266995 36.44621635\n",
            " 33.13901901 33.39732485 20.14705521 27.70528674 22.93383605 15.7580136\n",
            " 35.9054243  25.85997115 31.06348633 27.6314739  38.6045591  19.87295912\n",
            " 19.98054726 34.8542215  22.71147639 19.89567418 25.38220379 27.58462562\n",
            " 31.13849521 23.64131583 34.45198025 32.50625243 35.45676822 24.27092486\n",
            " 20.63812666 41.61358495 16.60235739 25.96085747 33.5382712  25.8720648\n",
            " 49.98704738 27.4324981  25.48963348 27.39505005 22.13004477 39.00018632\n",
            " 49.18104512 32.54691366 45.16771913 29.63732474 32.49528112 24.27082934\n",
            " 30.25078246 24.86963127 34.49279165 32.12596784 24.29279542 30.11591891\n",
            " 34.14739263 22.74792042 21.06646675 28.49521801 19.48144494 25.02463039\n",
            " 17.8284983  29.77093348 25.62130872 38.06762698 21.60368702 49.44058788\n",
            " 37.83901951 38.49074239 31.36916819 31.57102609 20.07879208 29.98146683\n",
            " 25.94899196 20.24024419 44.35521501 17.44317095 30.5166307  34.09299835\n",
            " 25.38180869 23.83331728 34.80646317 42.43070446 26.26558742 30.70385908\n",
            " 22.49634161 28.07248871 43.13719394 20.39888521 32.94501755 36.32963803\n",
            " 49.50918683 39.07691512 45.18447263 39.29887094 46.42503248 37.86018251\n",
            " 37.49533561 23.72374626 40.89867798 28.94222481 33.2493978  35.69954748\n",
            " 16.89629411 26.61831293 44.80875575 32.24086403 28.13616807 34.12497046\n",
            " 24.2860971  17.85548627 34.59559639 36.43222875 49.47767911 21.32337146\n",
            " 26.17814584 29.00657324 38.39032677 21.05095857 47.88848638 49.99926514\n",
            " 33.84194751 16.48489295 31.22279513 20.11998067 17.35309026 33.66939069\n",
            " 20.47524251 37.83567967 14.86387274 29.23052852 49.65647325 31.84218464\n",
            " 48.16817564 22.27375177 39.23823272 24.02760875 22.16465973 30.71658232\n",
            " 22.66756692 27.07174077 20.84871465 46.33658442 43.9975899  29.83582978\n",
            " 27.92845252 35.05039603 20.71515098 18.93800181 26.78697591 24.16040773\n",
            " 20.19217459 17.92373149 24.16327266 25.0603368  22.20783965 34.64834717\n",
            " 40.59993995 33.01880375 21.61223605 29.88796012 45.68174331 41.91701948\n",
            " 36.70000593 35.84897045 33.73720288 21.06412979 23.6821913  40.27509314\n",
            " 40.99229752 31.43752498 28.53670325 22.73854969 46.92428431 30.31176895\n",
            " 30.64329947 34.43710567 24.85242936 40.21032673 39.26920041 27.43487825\n",
            " 44.78765386 19.56035738 27.81233603 16.39388586 30.00573569 45.02229166\n",
            " 31.37324176 39.36668922 48.9524203  18.14452313 27.86476759 27.71051215\n",
            " 22.20783965 17.70444759 26.15469382 20.70836618 34.60434438 20.32322708\n",
            " 27.16831925 43.95997341 49.99936287 31.95221244 34.13723098 44.75560103\n",
            " 33.17521298 40.34331695 15.1885065  19.16817358 43.25234311 28.2517938\n",
            " 22.60596097 18.67460222 20.28379156 44.38264962 35.69358149 26.17479806\n",
            " 18.20229473 23.28957914 44.59476529 40.02845155 33.04959279 37.56935825\n",
            " 33.41987844 38.16167752 26.92994848 20.58869914 21.67669976 33.29753628\n",
            " 23.30152546 18.68825726 23.38424563 26.20091431 43.44610502 37.60684967\n",
            " 46.61585079 20.70310647 41.4675419  19.13399392 19.22158703 49.99959258\n",
            " 43.1992876  17.59225213 40.18477375 21.8723519  27.87988272 41.11898537\n",
            " 25.28017186 26.59952013 20.83241152 29.12476876 44.59838377 27.4922451\n",
            " 49.86750418 18.69555037 45.28511152 21.25085848 32.28478188 37.11203263\n",
            " 21.82957645 36.26930701 41.00874563 36.84285877 18.63300893 16.15042121\n",
            " 36.52590122 24.21495142 36.26705188 25.62501827 37.18688785 24.22412192\n",
            " 42.32857759 21.25717829 19.68020308 36.42800743 23.55866438 44.5762843\n",
            " 27.80124231 49.12495908 31.72398309 37.94323972 20.38246509 21.6364893\n",
            " 49.95119577 32.63680747 24.47743158 29.66719188 45.63134214 31.39483087\n",
            " 41.77776879 41.13355346 48.54469123 27.50359322 31.12609586 39.03035166\n",
            " 26.76285997 49.41359024 14.34850332 24.0881214  34.69314619 29.36530104\n",
            " 23.81136967 37.03695568 25.56896154 40.52515522 44.81081361 36.21782999\n",
            " 43.08161416 43.67434464 48.51567978 23.69725621 34.12073996 31.30987788\n",
            " 19.35829797 29.92151513 17.20738303 19.26289627 23.58562505 48.51574281\n",
            " 20.58648011 29.79968751 41.16099848 17.05548326 18.2127826  15.49187011\n",
            " 20.83652059 30.2038474  40.2815433  28.10898399 42.66681513 43.94683406\n",
            " 47.15666705 36.90490951 41.7064859  21.29906211 19.3722622  21.39517246\n",
            " 18.99544213 26.02283951 49.99808466 48.66386527 41.68492582 27.44370052\n",
            " 49.45994021 19.62175564 36.80956242 25.23623515 25.61362822 26.79746065\n",
            " 20.51764786 43.4108894  38.09294377 33.99760958 33.58284363 34.65279653\n",
            " 16.88096848 30.1478895  31.98223976 22.31624939 25.65408209 39.70450545\n",
            " 15.85094104 35.09828075 31.03571729 26.85904139 39.30283952 28.19387234\n",
            " 39.93362452 49.987792   32.22685496 17.44136634 28.55524649 23.29112189\n",
            " 38.03406697 40.75737929 25.6377108  30.05496981 45.60043303 34.00786319\n",
            " 47.62337461 23.56820072 31.54474768 46.90440381 23.6584048  19.69144031\n",
            " 30.87341754 34.55167467 34.06289244 43.08902805 28.79186649 35.50340056\n",
            " 49.67065969 25.77318514 20.45239135 39.10226152 14.66674235 31.62907301\n",
            " 24.64709091 32.08915863 27.62368467 39.07481516 28.56306845 26.78697591\n",
            " 42.12180049 35.06220129 24.66151472 38.1925433  22.54930066 36.83409945\n",
            " 20.85706358 43.2478023  32.8676738  37.87038791 45.85393236 49.11401475\n",
            " 21.27779798 26.89098586 49.77946962 22.12746278 23.35407819 34.64089826\n",
            " 47.44838098 35.19312792 27.1841009  31.94758947 28.01953246 40.84775364\n",
            " 15.58968872 49.77113659 20.16896595 47.1573579  26.52694964 40.23779932\n",
            " 31.02170867 20.81329532 39.44366098 46.93026463 21.05989993 45.05589738\n",
            " 23.61346546 42.9240411  36.64112369 44.09173499 33.30010867 49.73924293\n",
            " 27.50105045 25.10041926 14.67361639 33.08737408 32.48862938 23.0965776\n",
            " 17.12521132 26.95121856 24.58225443 35.21919323 40.40152879 25.93994271\n",
            " 34.74055008 46.95796653 45.71914166 24.93528706 41.21286057 47.31065777\n",
            " 43.93878633 32.29665091 20.16463285 23.92902543 38.11276067 19.35829797\n",
            " 34.67372632 29.55994684 34.56433893 37.60758941 28.03467085 30.31176895\n",
            " 27.23719427 27.71074134 31.13885794 44.55371205 24.50196931 19.39280388\n",
            " 20.23403052 22.81954957 14.8413543  19.52480043 19.79975856 41.09176379\n",
            " 33.06924086 23.42087701 29.99774851 17.72787757 39.48286623 34.0765836\n",
            " 26.06122485 23.95157923 31.4731312  17.97345199 27.04865842 24.84665432\n",
            " 17.38424669 21.52660531 33.4676197  35.03258113 41.76153207 26.74462606\n",
            " 42.93790065 29.56242465 32.02035857 32.28260058 38.26798958 41.58490973\n",
            " 35.45976814 43.50129007 34.571221   32.22674173 22.13126731 41.3418031\n",
            " 35.67334937 39.83167854 26.7391415  21.31030036 23.94151885 49.83779149\n",
            " 32.22076899 27.2600291  41.91294463 28.03228476 24.98795393 38.23222747\n",
            " 41.83877682 49.77961562 22.08396607 33.13613132 25.29811993 39.08941705\n",
            " 40.43651544 49.84269236 32.81951362 49.0200424  21.31020354 25.36166063\n",
            " 17.56106476 31.0248345  31.069404   27.68515682 21.52926623 31.60458174\n",
            " 17.55555081 22.52477536 30.61848502 23.17691805 49.99999504 24.47540586\n",
            " 45.8307925  27.73785222 27.33497189 46.93042705 24.36176066 21.95263763\n",
            " 16.41775792 49.94535358 30.550845   21.20277593]\n",
            "selection [596 153  23  16 255 694 746 782 428 506] (10,) [14.34850332 14.34850332 14.34850332 14.34850332 14.63998643 14.66674235\n",
            " 14.67361639 14.8413543  14.86387274 15.1885065 ]\n",
            "trainset before adding uncertain samples (440, 10) (440,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [189 261] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.797235 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.88       321\n",
            "           1       0.73      0.48      0.58       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "std (852,) [46.4889946  21.19099766 26.31814509 33.81290487 33.83124707 34.25731679\n",
            " 21.16378598 22.89528261 36.7663907  49.33958957 49.02104281 46.98815539\n",
            " 34.77103631 21.65075077 30.8265912  27.17702628 21.98909684 44.64969681\n",
            " 18.72350481 29.95588642 41.88030023 27.3916679  37.48869825 24.96673952\n",
            " 22.68035729 28.74598993 38.37953429 43.47540999 26.09130937 37.10767581\n",
            " 48.24238258 19.67357143 23.32329899 26.99395634 32.90518896 31.37189303\n",
            " 25.67476184 36.44541932 16.60354258 28.66426906 24.70708373 33.75783651\n",
            " 43.43068643 49.02933797 36.23172616 35.0890679  20.79526166 27.55103788\n",
            " 28.41885986 24.6464649  19.94639174 45.94018409 36.38737175 48.3049885\n",
            " 20.45873688 34.32993472 18.82462123 35.93828904 41.8192522  24.79422573\n",
            " 17.66064767 42.48879238 35.87947378 36.94959786 24.41888705 21.52060237\n",
            " 26.27513491 34.44049532 49.98935654 43.72449511 27.96718767 30.58433915\n",
            " 33.36262721 22.17032397 24.96383712 36.89002897 24.43142859 39.46490517\n",
            " 35.88783402 27.67965046 26.12233346 36.5499041  43.98172875 46.23611356\n",
            " 19.42278404 41.73476246 37.26068364 49.98940878 44.5195431  23.44699276\n",
            " 33.13504175 49.99841058 46.10304175 44.23859204 39.14529162 24.5466217\n",
            " 34.63131765 35.18994962 33.77754227 17.60402357 33.64867987 39.96958893\n",
            " 41.89318929 36.85694524 22.44008015 29.68807959 23.16799802 39.53057167\n",
            " 23.25143694 35.13306337 48.36498825 31.65536193 47.48122396 25.44649333\n",
            " 37.94189136 30.16860948 20.04008185 34.39671878 49.98400915 19.1528517\n",
            " 37.90179852 20.13579356 28.69844468 18.50732436 49.9999423  25.23323118\n",
            " 43.21950603 37.65049656 29.15068356 36.84449788 24.48438123 42.89329119\n",
            " 17.84176139 18.03356862 35.63522051 18.45198406 19.03022005 19.72970834\n",
            " 26.58097128 40.54487597 25.92723531 19.46830084 42.64903156 25.23973356\n",
            " 21.56507335 26.23632763 22.24028456 18.90630765 19.97256249 37.67227587\n",
            " 40.62413657 31.8775125  21.09266131 32.06452814 24.10887588 47.95704064\n",
            " 22.89069537 30.76601548 29.56320769 17.14907359 40.37641119 21.30455979\n",
            " 44.83048668 18.36526513 36.05694291 31.33425063 49.77206809 19.98976709\n",
            " 28.60640648 29.85933806 37.19126701 33.22191036 42.45334228 45.52672948\n",
            " 48.76842018 33.04662327 21.69995732 21.3247089  38.11500147 49.88661593\n",
            " 44.20346462 26.65711474 38.09139232 21.16475104 25.54814906 41.68938708\n",
            " 27.31170268 19.39741048 34.98748971 19.85055415 16.43486719 18.78435869\n",
            " 48.69781908 23.19267218 37.63906004 44.20823147 34.19654401 40.16699768\n",
            " 28.18684185 29.9849364  45.56668862 26.82279143 36.96201995 28.24266582\n",
            " 40.78220976 27.88546377 32.84759378 40.90238014 23.172946   20.24269028\n",
            " 29.05989861 31.58220673 18.07962918 30.87509995 22.86749641 37.83327378\n",
            " 41.32108398 33.83470967 39.31168979 33.05470015 16.98003889 33.38505314\n",
            " 49.78403296 23.52182264 20.81515352 37.94167491 28.14810283 25.07681425\n",
            " 40.10453115 41.34295409 42.92666103 23.58372433 22.04457633 48.11379394\n",
            " 40.93201969 36.03514191 32.74544785 49.97571452 25.43528049 16.58548036\n",
            " 36.7387863  22.48331121 30.97909678 34.58219803 26.27954244 49.99927664\n",
            " 44.68894317 42.97883585 46.86602802 36.18425059 36.89409735 48.0498012\n",
            " 42.58104461 31.48673899 49.52277682 21.42304062 20.98072849 25.43647053\n",
            " 29.32855149 36.35551776 23.19349396 33.18396796 28.308887   48.62706104\n",
            " 49.95379221 30.95563991 27.50117958 29.44492267 26.5802921  22.28284642\n",
            " 22.47729027 23.7405529  41.33180957 22.53012194 26.87796303 26.92697782\n",
            " 35.13787837 27.65843146 35.87947378 38.70723947 20.76880405 24.95346159\n",
            " 37.81869238 33.88639288 26.85256365 18.52053824 37.23498855 37.94177638\n",
            " 28.42142591 27.04400573 40.51686342 35.4861103  38.95448813 26.95503642\n",
            " 35.18105429 28.48309645 43.22070097 28.3520102  41.37992059 24.03915914\n",
            " 24.67363605 38.37013922 34.27225612 35.32374874 21.59454885 29.10730833\n",
            " 24.45367084 16.44964564 37.58826055 27.31544612 31.469475   29.24758299\n",
            " 39.9631779  22.03086228 21.80228221 36.56383121 23.97086326 21.89872178\n",
            " 27.26654901 29.20909491 32.87958039 25.28010711 36.00170731 34.48512742\n",
            " 36.8000837  24.57465825 21.53434857 43.02781822 18.29839147 26.76009329\n",
            " 35.02672574 27.10146406 49.99400312 29.46194823 26.94590231 28.50501654\n",
            " 23.71832812 40.05107959 49.44169786 34.32411302 46.10937025 31.2661396\n",
            " 34.18491808 25.78883405 31.34108439 27.32669331 36.00041973 33.44423516\n",
            " 26.61050937 31.77479489 35.35136321 25.58218322 22.44054562 31.06321996\n",
            " 21.19083723 25.91795093 18.42802083 31.58442585 27.6914325  39.62739905\n",
            " 22.82952209 49.63776563 39.50604163 40.30386233 33.18147717 32.78796508\n",
            " 22.1767471  31.94263752 26.81049561 21.74313556 45.48974502 18.02968517\n",
            " 32.19016323 34.5429065  26.61101864 25.29252527 36.81640064 43.69195806\n",
            " 28.01849132 31.76902731 23.60788617 29.95688448 44.19134785 20.91333309\n",
            " 34.19943095 36.80392255 49.66591893 40.7534975  46.34485438 41.15697857\n",
            " 47.14035647 39.44478866 39.06048845 24.91297667 42.37766656 30.34770428\n",
            " 34.73204902 37.3473274  18.56990785 28.51405914 45.85857328 33.90810878\n",
            " 29.60088295 35.94685777 26.70534042 16.93318711 36.75360276 38.3738527\n",
            " 49.62957331 22.71595477 28.87300742 31.08761459 39.95376074 22.00327874\n",
            " 48.49520658 49.9997273  34.68632553 18.04883056 32.44008791 21.28013933\n",
            " 18.88053052 36.07766834 22.12073972 39.55695089 30.97828115 49.7857829\n",
            " 33.70441357 48.78603503 23.76163827 41.00779244 25.78589994 23.58914724\n",
            " 33.0722495  24.30702238 28.54868086 21.97207122 47.19366689 45.04978896\n",
            " 32.27871119 29.94062948 36.71095666 22.68861058 20.20607556 28.98564258\n",
            " 26.17603235 21.87075216 19.19633449 25.5756436  26.90904843 23.89253261\n",
            " 36.16706006 42.30380873 34.9101096  22.19821539 31.15294734 46.60475371\n",
            " 43.15965532 38.06272075 37.67678367 35.65488884 22.65646349 25.33604356\n",
            " 41.84588077 41.2218422  33.05386698 30.50193561 24.05392482 47.69002266\n",
            " 32.10304826 32.05846677 36.45271921 27.44373883 41.57009674 40.49829209\n",
            " 29.40755369 45.62252371 21.40078068 30.20996908 18.0566129  31.68353409\n",
            " 46.93000117 32.9349331  40.86922005 49.28236562 18.84207152 29.94894965\n",
            " 29.62098399 23.89253261 18.02224324 27.82517416 21.562395   36.58890145\n",
            " 23.26453111 29.01127006 44.99193862 49.9997977  33.44798877 35.61769743\n",
            " 45.71009678 35.17058764 41.97227001 20.15582402 44.29010046 29.99319164\n",
            " 24.05130582 19.32045595 21.21685246 45.72868125 37.22094501 27.20890319\n",
            " 18.35863831 25.061737   45.59072916 41.5640253  34.2397202  38.78351723\n",
            " 35.23919258 39.78827171 28.31980759 21.9131518  23.19208371 35.10442955\n",
            " 24.81200217 19.38299815 24.2482875  27.47143873 44.6607809  38.98249835\n",
            " 47.29607782 23.20282164 42.80713117 20.59024411 20.52771628 49.99986985\n",
            " 44.47482158 18.57779196 41.55385644 23.31109062 28.94500278 42.47830472\n",
            " 26.73728644 28.79485802 22.24538245 30.20940262 45.59311277 29.45340085\n",
            " 49.91431484 20.25434448 46.24684129 23.82344418 33.81499555 39.69066549\n",
            " 23.61595001 37.76654443 42.21953517 38.4715459  19.26682564 17.62897538\n",
            " 38.25045806 25.97951213 38.10405855 27.47264952 39.02461206 25.58417423\n",
            " 43.73208393 22.50783033 20.85581823 37.94871513 24.94281805 45.65398399\n",
            " 29.53736945 49.45316563 34.41649503 39.84939492 21.06657559 23.7405529\n",
            " 49.9743508  34.17058846 26.08922735 31.25753106 46.54265718 33.88011709\n",
            " 42.98213073 42.55604985 49.01789629 29.42947343 28.22877643 40.36406761\n",
            " 27.0919893  49.60868217 25.24786143 36.63117069 30.86926203 25.66194757\n",
            " 38.48068475 26.84097    41.80960577 45.64482257 37.90534098 44.39806117\n",
            " 44.91025242 48.93484484 24.81019278 35.84553534 32.79119448 21.77865367\n",
            " 32.26943605 18.02396458 20.61610175 24.78577497 48.87784514 22.44622194\n",
            " 30.48878361 42.5250306  19.14356247 18.55937944 15.92214197 22.53332559\n",
            " 31.64121355 41.44749515 29.70409239 44.07216101 45.70552883 47.76982196\n",
            " 38.63221793 42.95066171 23.35070996 21.23835507 22.53543613 20.66136776\n",
            " 27.17994682 49.99926727 48.96081663 43.00312922 29.73661696 49.63432524\n",
            " 18.69186345 38.62820656 26.5838298  27.53184473 28.37458363 22.55323263\n",
            " 44.49482943 39.47936691 35.75736982 35.52658277 36.05384527 18.89625784\n",
            " 31.91588954 33.43146265 23.21276162 26.30868771 40.33234972 16.95994431\n",
            " 36.88014786 31.08612779 28.0418377  40.58368597 29.5057091  41.41711958\n",
            " 49.99378521 34.00375262 18.74366777 30.36299716 24.89923911 39.93584051\n",
            " 41.93891684 26.89069256 30.94957579 46.52624793 35.62531121 48.31050256\n",
            " 25.01614133 33.65830145 47.61361689 24.36338316 20.59200775 32.2112721\n",
            " 36.73093426 34.88841636 44.31903304 30.83148708 38.1184146  49.78279917\n",
            " 27.23732696 21.62090598 40.51941489 33.07619876 26.92282604 33.66255046\n",
            " 27.90389295 40.70818269 29.83580589 28.98564258 43.52262104 36.78454974\n",
            " 26.2362347  39.57701684 23.77623367 38.28270662 22.18127444 44.35809335\n",
            " 35.14832475 39.15371528 46.51609829 49.37433953 23.18649087 29.57361925\n",
            " 49.86716611 23.04995118 24.5955115  36.43165665 48.04399812 36.71557992\n",
            " 29.89702979 33.3852356  29.69551612 42.56851899 16.43541192 49.87141772\n",
            " 22.17652487 47.8196855  28.09308292 41.88904931 32.29809597 20.56219689\n",
            " 40.16699768 47.65702093 22.26034543 46.03784736 26.61353953 43.95241283\n",
            " 37.74396315 45.38600597 34.90319217 49.83429962 28.70302271 26.72950264\n",
            " 34.74864088 34.41098532 23.51905589 19.08143532 28.86627761 26.36568552\n",
            " 36.84398943 41.97958144 27.96381493 36.84376886 47.76312127 46.51959038\n",
            " 27.13177108 42.5251006  48.01722585 45.10950386 33.96286155 20.90005981\n",
            " 25.86436396 39.53973344 21.77865367 36.53631063 31.49744256 36.73104204\n",
            " 39.0982021  29.9849364  32.10304826 28.48576268 29.07535481 32.75520683\n",
            " 45.63801319 25.86548363 20.3057382  22.17032397 24.32475461 21.35092391\n",
            " 19.59853587 42.20762466 35.00159508 24.86719281 31.55708257 18.58860103\n",
            " 37.23189916 35.6426139  27.57796324 25.58146754 33.14755952 20.73937596\n",
            " 28.9498582  26.97316558 18.30733645 22.76701782 35.20965377 36.59424833\n",
            " 43.29137795 28.20718441 43.40975116 31.36436265 33.25332451 34.08176369\n",
            " 40.1895401  42.7726851  37.08517418 44.6321552  35.25749275 33.86412183\n",
            " 23.22407134 42.79951972 37.49974395 40.81609434 28.03671538 22.18759575\n",
            " 26.03707797 49.89362884 34.01747443 27.90926939 42.93420946 29.72765303\n",
            " 26.86916543 39.57197658 42.52626614 49.87020091 23.88709872 34.27308601\n",
            " 25.682909   40.38066296 41.92308834 49.90144854 35.18687745 49.29433562\n",
            " 22.32040332 26.8793086  17.48533938 32.77955805 32.56677545 29.37043657\n",
            " 23.0427606  32.92536889 18.69714383 23.85753545 32.82609428 23.83213372\n",
            " 49.99999857 25.36106445 46.7425808  29.21074557 29.88190933 47.60294431\n",
            " 27.09047896 23.89114383 17.2783632  49.96940672 32.27370964 21.86057675]\n",
            "selection [616 190 718 307 239  38 405 653 220 159] (10,) [15.92214197 16.43486719 16.43541192 16.44964564 16.58548036 16.60354258\n",
            " 16.93318711 16.95994431 16.98003889 17.14907359]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (460, 10) (460,)\n",
            "updated train set: (460, 10) (460,) unique(labels): [195 265] [0 1]\n",
            "val set: (842, 10) (842,)\n",
            "\n",
            "Train set: (460, 10)\n",
            "Validation set: (842, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.10869565217391304, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (842,) [45.99061403 19.31153952 24.53282192 30.49572144 32.87506277 32.58170996\n",
            " 19.57091776 21.65497224 37.37016666 49.10981206 48.83272596 45.67705722\n",
            " 32.53872034 18.62757156 29.85842249 26.20297349 21.60041217 43.84326206\n",
            " 18.101306   28.75590426 40.06012533 26.19793389 34.66748527 22.97889379\n",
            " 20.76507417 27.58015048 36.61834732 42.73570053 25.04346862 35.82614744\n",
            " 47.78299639 18.31283929 22.53024613 26.16800027 32.92263304 30.12205524\n",
            " 24.56171976 34.55689089 28.06840999 20.35282081 31.39124509 42.37846505\n",
            " 48.85746641 34.70335743 33.73054091 20.06076919 26.65004082 27.13036505\n",
            " 23.46299173 18.38078114 44.03955258 35.24383571 47.93917053 19.86230876\n",
            " 30.62353646 18.11363144 33.83067422 39.74662851 23.36499981 16.16969596\n",
            " 39.78052909 34.95490941 35.2141854  18.96709588 19.14760452 21.73822143\n",
            " 32.83178489 49.98436446 43.05829883 26.2121901  28.96682456 29.59137091\n",
            " 20.97310093 23.78202349 35.20197586 22.76773366 37.5972101  33.83366463\n",
            " 27.56435191 25.80801058 34.76393604 42.74696727 45.33088562 18.74777503\n",
            " 40.89829949 36.05282582 49.98610209 43.35667687 20.07072476 30.90287495\n",
            " 49.99776524 45.48216085 42.51049769 38.23440859 22.52638718 32.7795384\n",
            " 34.67573974 31.62602724 16.45026414 32.48324516 35.48166302 40.76076859\n",
            " 35.64351125 20.48569089 27.98640412 22.09118313 38.2530431  22.1706344\n",
            " 33.24485154 47.97873182 29.64644442 46.82491207 22.72512742 36.84863789\n",
            " 28.3666771  19.80819726 33.40253722 49.97815255 20.3401986  35.9222691\n",
            " 19.61523879 27.31384046 14.88063636 49.99991452 24.35301321 42.11518235\n",
            " 35.8178839  28.02239376 34.74893654 22.45815046 41.39438686 16.49777937\n",
            " 16.23126991 34.05625317 18.12535345 18.67021326 18.76411429 24.27259044\n",
            " 38.89188879 25.43458856 17.17562549 41.38032812 24.03619806 19.9674077\n",
            " 25.37776745 19.80837578 18.46141727 19.09768679 36.87543221 39.02347992\n",
            " 30.16646284 18.33583677 28.81698561 23.12170868 47.13252071 21.14507398\n",
            " 29.61639568 28.17721619 41.16784624 19.797406   43.9328062  16.7269325\n",
            " 35.09494117 31.48722265 49.4773126  18.06571757 26.00850211 28.76746009\n",
            " 36.35386496 31.24870542 40.6629095  44.69839525 48.53283373 31.10035129\n",
            " 20.90622849 20.24889737 36.71537514 49.83194451 43.55276091 25.53914138\n",
            " 36.60472786 20.21442023 23.7008693  40.79713317 26.03215813 18.77921389\n",
            " 32.61710546 18.87628421 16.446138   48.49940269 21.86347564 36.16653435\n",
            " 43.27650783 32.98954458 39.43503546 26.27442106 28.80872263 44.84785412\n",
            " 25.18358229 35.93491825 27.06846898 38.58357592 25.55539477 31.01381117\n",
            " 39.26365216 22.17991216 16.92063757 27.76604539 28.61620626 13.60911789\n",
            " 29.86867911 18.66994222 36.1884466  39.63414306 31.21697078 36.41922679\n",
            " 32.05967778 31.09117329 49.72151439 22.3823628  19.25132956 36.00615693\n",
            " 24.04733982 23.4186275  35.99976849 39.99928858 41.06176999 23.94279333\n",
            " 21.55563523 47.6813469  39.28870071 32.88285728 30.81929453 49.96904675\n",
            " 24.57207825 35.58744566 21.96486563 29.83991755 33.22861136 24.6323248\n",
            " 49.99892203 44.27480142 42.44143158 46.10765337 34.13098162 34.48034335\n",
            " 47.18930491 38.93558401 30.11439114 49.34632509 21.06446516 20.11337284\n",
            " 21.90441757 28.78393062 35.59883159 19.66645806 32.3303335  26.58640287\n",
            " 48.42401837 49.9364566  30.14227593 25.18494654 28.08130482 26.21701061\n",
            " 19.65071801 20.56542354 21.12054365 40.43318434 21.67252372 25.16731004\n",
            " 24.97292526 33.06708046 25.02664388 34.95490941 36.0508483  19.17004086\n",
            " 24.03848629 35.34563175 32.4756252  25.00545616 18.29953782 36.32498372\n",
            " 37.19889303 27.13280002 24.81808586 39.26889885 34.09454396 37.19011612\n",
            " 25.39903222 34.40323718 26.5375613  40.87165566 27.91339952 40.78767596\n",
            " 22.45225548 24.64615259 36.75324597 32.7310898  32.57375313 19.55138907\n",
            " 29.03180888 23.89608737 36.01001603 25.98313063 30.72688791 28.61568226\n",
            " 37.18340507 20.1487862  20.25524689 36.04542048 22.95682058 20.64195303\n",
            " 24.75237459 27.68490563 30.33447947 24.09899884 34.21887821 31.14429321\n",
            " 35.66467449 24.74361589 20.82746276 41.81750115 16.33864344 25.89388649\n",
            " 33.82526007 26.26958102 49.99088623 27.30968441 25.77629313 26.8661364\n",
            " 22.7606865  38.49244178 49.09654625 33.27906329 45.38616703 29.03486263\n",
            " 31.16786826 24.96666386 30.81135663 23.82039956 34.93650908 32.33468036\n",
            " 24.31393387 29.24002445 33.8294483  20.26606091 20.1983732  27.87736632\n",
            " 19.57907634 27.48169459 17.43034349 29.94738595 25.68037194 37.54354464\n",
            " 21.77942908 49.50783667 37.90856194 38.60135486 31.03923464 31.21731736\n",
            " 18.53274957 29.48717139 25.6450045  20.83150292 44.68175545 17.34131421\n",
            " 30.17260055 33.94600609 24.49375306 24.63301759 33.48354054 42.35032731\n",
            " 26.09300197 31.12733137 21.52884809 28.30603847 43.27569989 19.61573578\n",
            " 33.74846222 35.13098755 49.55103776 38.130338   44.98557133 37.50522581\n",
            " 46.87868001 38.53647272 37.84896363 24.3140051  40.25078592 29.40613612\n",
            " 31.79776871 36.06645389 16.25936185 26.66194721 45.06366314 32.67318816\n",
            " 28.06955127 33.95542184 23.813374   35.90275264 37.40024413 49.52607221\n",
            " 21.03297017 25.27654623 28.2836166  38.7363027  19.7508192  48.13646958\n",
            " 49.99956237 33.45716118 16.06910013 31.63392179 20.9205272  17.46450021\n",
            " 34.43311152 20.98229728 38.15128195 28.23628377 49.74945203 30.66226715\n",
            " 48.43171449 22.35052509 39.74444915 24.25111098 23.02456419 31.01857071\n",
            " 22.23245973 27.36214983 20.94920491 46.39854473 44.11702195 29.95215596\n",
            " 27.64267942 35.21726691 22.03944635 19.26174915 27.54073933 23.13213727\n",
            " 19.98670961 19.14250642 24.9589335  25.41413956 22.95193067 34.71302171\n",
            " 41.27396558 33.01774627 21.29605557 30.1451744  45.5512854  42.22541079\n",
            " 35.62621205 34.99340737 33.81953771 19.23921819 23.96859571 39.64032134\n",
            " 40.51062905 30.70716173 28.36426945 22.59966005 47.35303174 30.4178404\n",
            " 30.6199166  34.14993905 24.76210255 40.67181157 38.21425048 27.21453247\n",
            " 44.67703796 20.0716758  27.54178983 16.34869737 29.76173246 45.87418011\n",
            " 31.37717603 38.97549693 48.82268638 18.69367197 27.70592257 27.74703179\n",
            " 22.95193067 17.06765552 27.16406131 20.67063453 34.44767598 18.29024874\n",
            " 27.69078607 44.10986795 49.99967347 33.28245636 34.06759592 45.09745945\n",
            " 33.63536962 40.92261431 18.76351359 42.82553469 29.10099757 22.5673686\n",
            " 17.7997217  21.00012331 44.68916208 35.65811804 25.52014432 17.19706353\n",
            " 23.57401897 44.87133513 39.88569048 33.38312051 37.17873643 33.59682979\n",
            " 37.94824592 26.44698023 21.7217705  23.10622244 32.97233439 23.78765485\n",
            " 18.27383926 21.71295    25.22190638 42.19125415 37.63490047 47.28396358\n",
            " 21.81389198 41.98752919 20.29329259 20.36112995 49.999787   43.62963791\n",
            " 18.08992286 40.88628822 22.06726785 27.42008748 40.47669355 25.81543029\n",
            " 26.7367598  21.24490691 28.01605844 44.61360995 28.10725899 49.88147588\n",
            " 18.968245   45.53185743 18.13390597 32.68179333 36.9380542  22.18605943\n",
            " 36.39344169 41.31996404 36.90797547 17.84921413 16.22317099 36.87797478\n",
            " 23.21553573 36.01779587 25.91566254 35.93385021 24.16282783 42.4117565\n",
            " 21.70726431 19.83446281 35.54220456 23.96561529 43.52363157 27.51234136\n",
            " 49.25634173 31.82623475 38.62620239 20.33931089 21.12054365 49.9635087\n",
            " 32.98897649 24.23709027 29.60407704 45.52162856 31.69769169 42.02659909\n",
            " 40.41086143 48.74100517 27.73677103 30.13822113 37.89032486 26.93852085\n",
            " 49.50890309 24.93616186 34.91632282 29.72621497 24.36585306 38.18069206\n",
            " 26.05003843 40.47922031 44.87644718 36.1241377  43.38047256 43.33803169\n",
            " 48.70588845 25.94649924 34.58970571 31.13154686 19.7415659  31.38824376\n",
            " 15.962527   19.88645008 23.49282518 48.62967422 20.74149958 29.21457392\n",
            " 41.25052928 15.77057893 19.00206375 21.24855753 29.83623372 40.45880277\n",
            " 27.7593058  43.17248928 44.31040013 47.90226937 37.17512909 40.88592787\n",
            " 22.25653526 20.69323051 21.94783957 19.3891811  28.56911903 49.99876523\n",
            " 48.78267406 42.0290482  27.34465432 49.51171024 20.01022552 36.75260864\n",
            " 25.6615037  25.73508863 27.49330547 21.57346902 43.79271744 38.62558263\n",
            " 34.08556336 34.47151425 34.9355562  16.49612793 30.10065232 31.72992946\n",
            " 23.3212763  24.11945216 40.70466658 35.13726136 32.63794398 26.17647394\n",
            " 38.90702437 28.83122942 40.07765379 49.9910925  32.05867096 16.91146031\n",
            " 29.09587435 25.76409284 36.7608088  40.65162671 24.9904683  31.18384068\n",
            " 45.84491972 34.13849556 47.87685201 23.3852194  29.76756324 47.26121125\n",
            " 23.04628352 19.70830966 30.91693459 34.59483254 34.37476782 43.39232906\n",
            " 28.59480079 36.82785596 49.73569941 25.36663323 20.4418983  39.34994964\n",
            " 31.6557509  25.53474388 32.08301324 28.21912632 38.79725552 29.72991609\n",
            " 27.54073933 41.83122629 32.25252405 21.98295128 38.18067484 22.85416184\n",
            " 37.14020864 21.5977251  43.51198032 32.46988222 37.90570021 46.48560476\n",
            " 49.19420284 19.79169401 24.90431019 49.81853981 22.17886864 24.47775467\n",
            " 34.58986267 47.66402974 32.74095576 26.68191624 33.17246258 27.98578788\n",
            " 39.80646161 49.81153498 19.76118821 47.54474702 26.97142318 41.31847772\n",
            " 30.77359356 22.40378695 39.43503546 47.06364636 20.24086894 45.09486053\n",
            " 20.6068592  43.01373811 35.70676728 44.45655473 34.09182036 49.78199954\n",
            " 26.85343113 25.71383431 33.20749157 32.45531636 23.05467085 17.60599937\n",
            " 26.93558297 25.82484479 35.37995054 40.32087242 26.08872879 35.0155381\n",
            " 47.29775311 46.0322793  23.93869258 41.87411214 46.44150282 43.28478871\n",
            " 32.88844899 18.59965668 24.76956173 38.0474878  19.7415659  34.2784898\n",
            " 29.69282607 34.96648322 38.16378381 28.80872263 30.4178404  27.73407839\n",
            " 27.96027508 30.9669749  44.38862099 23.42340637 20.53391089 20.97310093\n",
            " 23.70511822 20.07885811 18.30901909 41.71761803 33.12855988 23.88854559\n",
            " 31.81668266 17.62835398 38.03864271 34.00123598 24.64844283 24.67202826\n",
            " 31.03645695 17.20420673 27.1659668  24.90512648 18.18372979 21.80671032\n",
            " 32.71384944 35.00016182 40.71976623 27.37390576 43.40205879 29.3236134\n",
            " 31.42209808 32.23944163 38.25940886 41.38131105 34.9507513  43.21238168\n",
            " 34.79966326 32.54569235 22.02781513 42.18630058 34.52719704 39.92872396\n",
            " 26.75785337 18.79937438 24.0981509  49.86285422 33.89401308 26.80277785\n",
            " 41.98222451 28.17681367 25.03473854 38.77268094 41.20410591 49.83638015\n",
            " 22.94762882 34.03247436 24.19454447 39.21526993 41.21306461 49.87041854\n",
            " 32.99525284 49.06762299 20.73204767 24.98427002 17.47385434 32.8658817\n",
            " 31.62383695 27.24239458 21.60579385 31.61364141 17.91653547 22.70166824\n",
            " 28.07264338 22.7369746  49.99996377 25.22403608 46.12100688 28.18344152\n",
            " 27.0705926  47.28228053 23.84801513 22.59772324 16.77244056 49.95678147\n",
            " 30.07425025 21.49575136]\n",
            "selection [209 122 607 600 410  59 550 132 392 322] (10,) [13.60911789 14.88063636 15.77057893 15.962527   16.06910013 16.16969596\n",
            " 16.22317099 16.23126991 16.25936185 16.33864344]\n",
            "trainset before adding uncertain samples (460, 10) (460,)\n",
            "trainset after adding uncertain samples (470, 10) (470,)\n",
            "updated train set: (470, 10) (470,) unique(labels): [199 271] [0 1]\n",
            "val set: (832, 10) (832,)\n",
            "\n",
            "Train set: (470, 10)\n",
            "Validation set: (832, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.10638297872340426, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.72      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1\n",
            " 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1\n",
            " 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (832,) [45.9961098  21.60510703 25.54965088 30.55333374 33.43300767 33.75370886\n",
            " 20.98138119 22.73233869 38.64005872 49.20965111 48.92011414 46.42502586\n",
            " 33.7688324  20.04133329 30.68486279 27.18415142 21.79200732 44.07991347\n",
            " 18.29063625 29.52931836 41.14352891 27.48772537 35.92827242 23.68421969\n",
            " 23.54445021 28.78905098 37.34540686 43.58634455 26.23644176 36.72063357\n",
            " 48.03885925 18.57897991 22.92268398 27.21726736 33.41325051 31.42042933\n",
            " 25.86950927 36.39324105 29.26986872 21.45504911 32.71437542 42.57153553\n",
            " 48.85936167 36.02719241 34.53374041 20.75935262 27.32364823 28.09226596\n",
            " 23.85781919 19.58549055 44.96468511 36.79533985 48.06631911 20.23729666\n",
            " 31.95786103 18.75536652 35.19821904 40.63598708 24.61452805 40.93187186\n",
            " 35.95871976 36.28711243 21.96381426 20.65942995 21.11460715 34.9487838\n",
            " 49.98497135 43.49627332 26.24539308 30.15080662 30.90586957 21.98261068\n",
            " 24.49301416 36.01403163 23.42234262 38.01614205 34.97889722 28.74087159\n",
            " 26.49930984 35.460443   43.00793461 45.50796886 19.25853628 41.74651544\n",
            " 36.28028205 49.98534343 43.92507519 21.49310415 32.06550432 49.99775626\n",
            " 45.81541342 43.39320195 38.83211878 23.73977527 34.11251036 35.44947619\n",
            " 33.08133414 17.27812903 33.21788251 36.85519123 41.42836669 36.67045977\n",
            " 20.95435307 28.51351464 24.0464601  39.07701544 22.52372701 34.11429291\n",
            " 48.09448305 30.49318474 47.08667992 23.28027835 37.89247047 29.42443078\n",
            " 21.03819082 32.91852576 49.97796049 21.16074383 36.78584915 20.84079594\n",
            " 28.30376631 49.99991227 24.75253798 42.29410109 37.14201928 29.16238994\n",
            " 35.54085683 23.5600186  42.15177778 17.7632002  35.2547236  17.9299916\n",
            " 16.86047369 19.06308101 25.42676606 39.57771039 25.79473992 18.3047423\n",
            " 42.26139675 24.98543383 21.09821446 25.17811855 20.62593862 18.83866942\n",
            " 19.60026789 37.81520283 39.93507887 31.36145017 18.56206626 29.93913108\n",
            " 23.86597442 47.70499969 22.22926161 30.48942448 29.71814901 42.7588674\n",
            " 20.47483949 44.12634068 18.15710528 35.79702549 32.4577802  49.64679247\n",
            " 19.03290218 27.53063366 29.25864161 37.15571131 31.47020279 41.41794107\n",
            " 45.0387344  48.67340301 32.34771661 21.99998463 21.03085759 37.5794805\n",
            " 49.8886317  43.77811414 26.40974508 37.66217942 20.76946466 25.23995463\n",
            " 41.0695071  26.56458062 19.07233206 33.95417615 19.47393662 18.27569438\n",
            " 48.69950099 23.46049818 37.30268115 43.53150755 34.14629399 39.21043424\n",
            " 27.05380938 29.77432775 45.21591609 24.85730069 37.1892741  28.03558776\n",
            " 39.915516   26.49995141 32.22909948 40.08528065 22.96628979 18.9481109\n",
            " 28.75181658 29.65111465 30.72030663 19.99125247 37.32752153 40.37299773\n",
            " 32.57250928 37.62082132 31.59315165 32.29352606 49.73544972 22.97992778\n",
            " 21.25503658 37.27050342 25.43270189 24.14154446 37.91073848 40.79774149\n",
            " 41.85859678 24.48923488 22.69282264 47.77000577 40.02362131 34.19854759\n",
            " 31.34887246 49.96978493 25.07149623 36.88633751 22.58056197 31.01046689\n",
            " 34.56607465 25.350216   49.99900514 44.18505294 42.96894944 46.21291625\n",
            " 35.3319538  35.83320192 47.3280847  38.56956698 31.38954436 49.38851183\n",
            " 21.54952903 20.25472864 23.13107511 29.69356642 36.63853508 21.0231755\n",
            " 33.78016162 27.0973256  48.67658631 49.93988483 30.88049525 26.88774199\n",
            " 28.63924647 27.21113017 21.36247233 21.49344185 22.49806205 40.10733023\n",
            " 22.30926635 26.63053339 26.19448665 34.23590341 27.06368925 35.95871976\n",
            " 37.13385735 21.28692212 24.73051639 35.92721616 33.77998513 25.73111808\n",
            " 18.71315549 37.18079255 37.48875433 27.24678626 26.01344775 39.37609802\n",
            " 35.10924698 38.30817229 26.54550523 35.20435513 27.14627629 42.07555504\n",
            " 29.62852491 41.88870353 23.76407599 24.80507895 37.46363975 34.41380611\n",
            " 33.72258712 20.44357753 30.62850295 24.74364192 37.0483842  26.96170495\n",
            " 32.0956857  29.46880557 38.79758102 20.09769276 21.33976333 36.16278598\n",
            " 23.10941736 21.21517722 25.6063363  28.80992351 31.96301668 24.79265609\n",
            " 35.56074315 33.00172246 37.40958258 25.75293808 22.20537582 42.85490421\n",
            " 27.10093363 34.17550665 27.20850574 49.99156521 28.45411665 26.78496126\n",
            " 26.90501805 23.83056444 39.54603929 49.37357576 34.24826362 45.31660827\n",
            " 30.11116022 32.48742163 25.93271215 31.39559177 25.11388951 36.13454673\n",
            " 33.39640606 25.19453824 30.23041113 34.62176787 22.88866259 21.00049023\n",
            " 29.09833091 19.91199837 28.08098797 17.45462296 31.01802227 26.7515057\n",
            " 38.70476487 22.15812396 49.54144809 39.05835325 39.2497268  31.81902429\n",
            " 31.84106285 19.44644263 30.71382175 25.82540128 21.344003   45.03135001\n",
            " 18.46873473 30.87007168 35.16301679 25.95513243 25.48866725 34.7321288\n",
            " 43.18707728 25.96159535 31.86179914 22.09190552 29.35101957 43.54910663\n",
            " 21.27873502 33.97897859 36.91030727 49.58484331 39.40048934 45.21725025\n",
            " 38.98526555 46.87801545 39.5206835  39.02800469 24.71123565 41.35470613\n",
            " 30.43327715 34.31748897 37.28078768 27.84253294 45.4140405  33.97935691\n",
            " 29.19181036 35.18629156 24.54586248 36.46919929 37.5067224  49.55803024\n",
            " 21.94735813 26.67971293 29.76147322 40.03231142 20.55241874 48.28333321\n",
            " 49.99960136 35.156563   32.83866026 21.98551696 18.55733898 35.26950054\n",
            " 21.93475867 39.16605193 29.71482248 49.74674478 31.0137719  48.59675766\n",
            " 23.79620039 40.32262916 25.20847777 23.54608555 31.80681495 22.65725336\n",
            " 27.74671787 21.31101442 46.73421082 44.69961598 30.20045156 28.98857464\n",
            " 36.36584545 22.35992726 19.46636003 28.8165282  24.0935523  20.72952652\n",
            " 20.19100376 25.09983791 26.21969297 24.06446108 35.76448567 41.45411501\n",
            " 34.05549686 22.44081803 31.410133   46.36353723 42.63603012 36.80631367\n",
            " 36.24594035 35.33162777 20.86366936 24.40832695 40.83155351 40.63941889\n",
            " 32.24332535 28.59768945 22.73727166 47.5568396  31.57081549 30.9641878\n",
            " 34.76297249 26.09103951 41.21680287 39.19776546 28.09756355 45.04865244\n",
            " 20.38224426 29.46276245 17.04691852 31.15474222 46.09207873 32.16053294\n",
            " 40.22634917 49.12057891 19.32697301 27.70974255 28.69174378 24.06446108\n",
            " 17.76412206 27.98212763 22.2221998  35.48236422 19.27917798 28.72751257\n",
            " 44.81824464 49.99970775 33.80102081 34.93797501 45.01798957 34.72044047\n",
            " 41.91092877 19.52146854 43.67888526 30.31175812 23.54231227 18.02527511\n",
            " 21.42918304 44.99056868 37.04092723 26.7547759  18.79750202 24.10528962\n",
            " 45.17488651 40.90561968 34.4294664  38.14298246 34.43881516 39.04768371\n",
            " 27.6036949  22.37396112 23.73187244 34.11305252 24.67107861 18.60797575\n",
            " 23.45329002 26.15685246 43.13743211 38.72297518 47.86358834 23.03728277\n",
            " 42.77455406 20.55295861 20.6865772  49.99978604 43.85207879 18.94402122\n",
            " 41.14158171 22.87224164 28.94279625 41.70728499 26.81365181 27.92900101\n",
            " 20.88269639 29.66525479 44.91767659 29.06902835 49.89621476 19.44566563\n",
            " 45.8968464  19.20147937 33.6944487  37.64181901 23.16579914 37.31600977\n",
            " 42.135828   37.88778776 19.21098944 38.00324999 24.65426658 37.28039588\n",
            " 26.7444422  37.19701838 24.91461533 43.65532898 21.9657551  20.46320685\n",
            " 36.64523918 24.77673744 44.49411614 28.80200127 49.30753331 32.98479454\n",
            " 39.16283421 21.46419886 22.49806205 49.96569709 34.0012128  24.87574261\n",
            " 30.72379253 46.30016876 33.14147273 42.25213111 41.45940086 48.8478608\n",
            " 28.66867146 31.26448567 39.10970836 28.01712636 49.50309971 25.7112203\n",
            " 36.14581059 30.64556568 25.38041007 38.22005205 26.9973105  40.83194077\n",
            " 45.10037181 37.05075187 43.74676073 44.23744379 48.75632757 26.2720815\n",
            " 35.46877552 32.04067494 20.88929432 31.83990799 20.51498219 23.42461085\n",
            " 49.02324871 21.78327589 30.73483282 41.58794273 19.60915511 21.96255551\n",
            " 30.14554593 41.4661751  29.00584357 43.46285324 44.45607726 48.09257483\n",
            " 38.45082887 41.9898998  23.24028965 21.71752926 22.4029222  21.21823104\n",
            " 27.65117458 49.99886056 48.76534225 42.92331837 28.25009644 49.59948847\n",
            " 20.83770977 38.03597987 26.82599569 26.85572429 27.73947503 22.37595739\n",
            " 44.51080481 39.48761037 35.12625534 35.80833993 36.79979895 17.22814342\n",
            " 30.75808337 33.10237993 24.16306244 25.65344727 41.75425436 36.28915534\n",
            " 33.92017808 27.0088306  39.79804321 29.83602067 41.03913397 49.99146101\n",
            " 34.98375928 18.10467261 29.91535087 27.22784699 37.61007215 41.04789336\n",
            " 25.30838086 31.6326655  46.30927792 35.28155262 47.99811809 24.1594733\n",
            " 31.32006926 47.32269712 24.42778102 20.66634893 31.99438651 35.99467905\n",
            " 35.03367568 44.18844016 30.09782334 36.82679981 49.74934079 26.29748018\n",
            " 21.3836944  39.7986483  32.82230103 26.4813588  33.0188568  29.58877025\n",
            " 39.94541378 30.47564655 28.8165282  42.81786557 34.55027508 23.75012821\n",
            " 38.8754286  23.78632198 37.89032502 22.45993156 44.06976036 33.55026135\n",
            " 38.71852427 46.18620001 49.21316374 22.80791561 26.49275585 49.83499278\n",
            " 23.29453559 24.96178147 36.02802202 47.73970484 34.09069965 28.13538497\n",
            " 34.30405245 29.03244798 41.21332328 49.82545145 19.75952666 47.45620161\n",
            " 27.79730089 41.5688757  31.54270091 23.1930796  39.21043424 47.07112567\n",
            " 22.472057   45.99583435 22.15467657 43.71904944 37.34968324 44.76832252\n",
            " 34.96092943 49.79483946 27.21333042 26.7264141  33.68167149 33.66088515\n",
            " 24.7184906  18.08951646 27.44161296 27.05924098 36.15913983 41.38410789\n",
            " 26.9983866  35.99313162 47.36762526 46.80532538 24.87391245 41.97941658\n",
            " 47.15152324 44.2301606  33.85865556 20.22337249 25.66766363 38.76835624\n",
            " 20.88929432 35.43304828 30.63466749 36.30871171 39.23720989 29.77432775\n",
            " 31.57081549 28.65765666 28.749094   31.6505234  45.17750963 25.9447695\n",
            " 21.36595205 21.98261068 24.50151712 20.86503381 19.66128955 42.4034996\n",
            " 34.09673762 24.58812334 32.95545111 18.58326938 40.4644153  35.12198741\n",
            " 25.67780268 25.25293533 32.9446029  18.29160744 27.96953226 25.80829067\n",
            " 18.81238458 22.43930966 33.94679498 36.27905568 41.94258825 28.48128211\n",
            " 44.31731659 30.54536938 31.93533909 33.39357107 39.06716427 42.53696244\n",
            " 35.88878914 44.11113689 36.23076886 33.26620304 22.08717119 42.56817524\n",
            " 35.75973625 40.16888125 27.26239283 20.15497139 25.36199987 49.85836908\n",
            " 34.51336676 28.2987983  42.73795443 29.27368296 26.1561993  38.70011619\n",
            " 41.91912656 49.84754885 24.01329999 34.5267947  25.83591032 39.95226076\n",
            " 42.09878453 49.86885068 33.34397385 49.17646009 21.36898699 25.83862663\n",
            " 18.51527267 33.68609719 32.63052214 28.38576373 22.4977999  32.20410921\n",
            " 18.5575508  23.40855928 30.83957865 24.07451229 49.99998726 24.32390386\n",
            " 46.49795024 29.29718594 28.38695823 47.38174162 24.99795744 23.55214107\n",
            " 16.7015828  49.95653435 31.58912491 22.08143935]\n",
            "selection [828 132 464 629  97 345 129 474 131 491] (10,) [16.7015828  16.86047369 17.04691852 17.22814342 17.27812903 17.45462296\n",
            " 17.7632002  17.76412206 17.9299916  18.02527511]\n",
            "trainset before adding uncertain samples (470, 10) (470,)\n",
            "trainset after adding uncertain samples (480, 10) (480,)\n",
            "updated train set: (480, 10) (480,) unique(labels): [204 276] [0 1]\n",
            "val set: (822, 10) (822,)\n",
            "\n",
            "Train set: (480, 10)\n",
            "Validation set: (822, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.797235 \n",
            "Classification report for LogisticRegression(C=0.10416666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.88       321\n",
            "           1       0.73      0.48      0.58       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 0]\n",
            "std (822,) [45.44836297 21.38465752 24.55273091 27.33971623 32.49991189 32.49730536\n",
            " 19.97682234 21.389764   38.12784649 48.96247698 48.78989659 46.18384903\n",
            " 33.27790479 20.11263985 29.70051195 25.32587455 20.62171461 43.15257754\n",
            " 17.43229346 29.22212047 40.57400134 27.29452081 35.81026377 23.12548311\n",
            " 21.43879462 28.40113838 35.89052684 42.95579326 25.93946984 35.91594831\n",
            " 47.62737106 18.14092619 21.83333626 25.57010416 32.93080642 30.49495684\n",
            " 25.14916035 36.25961252 28.61162817 20.87783299 32.11682607 41.15312735\n",
            " 48.65173191 35.74227365 34.24556974 20.09526425 26.77346392 27.45127733\n",
            " 23.53791995 19.48746844 44.66923526 36.07286318 47.74095771 18.96896137\n",
            " 31.12856125 17.46845915 34.49791887 39.1270091  25.47566818 40.66605968\n",
            " 35.00318606 35.66298513 21.61564036 19.3203567  18.70957896 34.30268736\n",
            " 49.97808209 43.07530271 25.45023427 30.34058894 30.87511733 20.92835565\n",
            " 23.6422687  34.15107725 22.98735107 36.80406375 34.41132178 28.45073271\n",
            " 26.00627916 33.31337707 42.2166127  44.7469021  18.7961566  41.29074049\n",
            " 35.541726   49.97939871 43.208033   19.69280802 31.32818792 49.99658295\n",
            " 45.45481687 43.17460795 38.97682545 22.76504108 33.2690068  35.92024789\n",
            " 31.82182635 32.66568589 36.54171309 40.22031209 35.63425459 19.80592151\n",
            " 27.4319811  23.93164086 38.06196181 22.24251503 33.82079849 47.86746641\n",
            " 30.34243944 46.31730264 22.03052836 36.91482266 28.50970711 20.83243948\n",
            " 31.52492765 49.96634722 21.48787611 35.04344445 20.28035329 27.01124976\n",
            " 49.99986251 24.69793792 41.07602284 36.26068449 28.15767457 33.77499051\n",
            " 23.25013785 41.00845042 34.4336903  18.28207381 24.03798191 38.44191183\n",
            " 25.43981759 17.96082978 40.74300225 24.42953432 19.81250319 25.04816701\n",
            " 20.83081811 18.07978702 18.77903325 36.7776546  38.43282599 30.1163575\n",
            " 17.34696434 28.00656199 22.70523885 47.73920415 21.31539099 29.52537547\n",
            " 28.51366659 42.52940485 19.39432303 43.40756562 18.01918578 35.23654499\n",
            " 31.25983539 49.66169575 18.21767274 26.82998912 27.75370229 36.49769656\n",
            " 29.88935893 40.1772518  44.11292316 48.4110744  32.12348935 20.92916643\n",
            " 20.4555338  36.67071185 49.85305792 43.35632136 25.18239068 36.98426118\n",
            " 20.40211613 24.72149922 40.44968764 25.15687561 17.94610972 32.92253915\n",
            " 18.86342125 17.63795696 48.59534178 23.19732415 36.83698921 42.70258727\n",
            " 33.45672289 37.98586307 26.03373127 28.59500636 44.90218515 24.19500975\n",
            " 36.29472819 27.22419806 38.76342028 25.47907914 31.27523686 39.02157276\n",
            " 21.94374451 19.10850473 28.17481899 29.35822513 30.09644754 19.3115431\n",
            " 36.55839889 39.29201213 31.65397045 36.97227238 30.1378134  31.53138605\n",
            " 49.65257883 21.71785899 20.03754821 36.57372941 25.03082553 23.7318268\n",
            " 37.99050972 39.65127072 40.48869509 24.1509144  22.01165493 47.27524518\n",
            " 39.92775494 33.12079088 29.56209106 49.96206198 24.49151172 36.93010263\n",
            " 21.76003681 30.75969721 33.4824181  24.60705584 49.99848907 43.48081935\n",
            " 42.48029793 45.30730622 34.26854506 35.25934167 46.56519558 35.01887543\n",
            " 30.42557065 49.20794513 21.52356746 19.7777947  22.36546662 28.46454172\n",
            " 36.51605139 19.22043754 33.04559424 25.90544675 48.60292928 49.91514223\n",
            " 30.10731865 26.04657716 27.97857629 26.2538606  20.91869206 20.89592089\n",
            " 21.54938232 38.82541968 21.59940759 26.10943018 25.23421229 33.54285761\n",
            " 26.86460137 35.00318606 36.77366517 20.5637155  23.5769756  34.27359162\n",
            " 32.85696282 25.63900038 17.97206333 36.47453102 36.96944087 25.26956465\n",
            " 25.62095386 38.67698999 34.22475461 37.7173053  26.13572936 34.96124702\n",
            " 27.09211037 41.44527013 29.51086548 40.9230736  23.33564826 23.82988055\n",
            " 36.35033501 33.97204444 32.69084971 20.45447217 29.95370021 24.17106705\n",
            " 35.82984632 25.73672026 31.53129727 28.68781464 38.53441647 16.93545478\n",
            " 20.31815187 34.95767013 21.58533264 20.0727772  25.06065704 28.02383664\n",
            " 31.78126473 23.95902997 35.01429546 32.50249359 36.98772911 25.92803697\n",
            " 21.45640087 42.25361405 26.47413594 33.82920789 26.3906953  49.98706914\n",
            " 27.16113864 25.89279061 25.410875   22.5194629  39.54249778 49.27256221\n",
            " 33.17061694 44.57480827 30.21662122 32.13909841 25.14654638 31.11617517\n",
            " 24.09290006 35.29250448 32.73334162 25.07452583 29.89661611 33.32935204\n",
            " 22.61180861 20.39684151 27.5395655  18.82116159 27.55818729 30.13115363\n",
            " 25.68525928 38.26601548 21.61880387 49.41900036 38.5160906  38.11381355\n",
            " 31.50273099 30.42723635 18.43868502 29.57812949 24.9207525  20.94243868\n",
            " 44.46357436 19.19083302 29.71397225 35.33053345 25.30778678 24.77901985\n",
            " 34.3116763  42.52354694 24.70188603 31.4608315  21.67982016 28.38535665\n",
            " 42.58003507 20.01162418 33.2384646  34.25997575 49.42659629 38.77680993\n",
            " 44.30252073 38.53851712 46.17771497 38.87700665 37.94960123 23.93028543\n",
            " 40.85794497 29.2419054  34.65613385 36.91542233 27.13444872 44.65665887\n",
            " 33.32931383 28.47641569 34.302271   24.45033471 35.85124479 37.10424562\n",
            " 49.42414291 20.98524543 25.64892825 28.38042534 39.2773213  20.33140185\n",
            " 47.92490133 49.99934419 34.80366185 32.33292273 20.66270046 17.43581024\n",
            " 34.19797956 21.29204128 38.20753036 28.34113268 49.69471509 29.8813322\n",
            " 48.37645734 22.16315191 39.85279047 23.96964189 23.0672762  29.89196097\n",
            " 21.17256048 25.96065441 20.44880278 45.99834005 44.20694501 29.02336856\n",
            " 28.68652    35.62934471 22.11613285 18.65949064 28.11740802 22.58177113\n",
            " 19.69576638 20.38972022 24.28218401 26.24618337 23.47700073 34.98976733\n",
            " 40.62429577 33.23076145 22.17010915 30.54999741 45.96437393 42.30835461\n",
            " 36.74803361 35.73596436 34.2983614  21.29186431 23.87044774 40.19395225\n",
            " 39.9169005  31.11800109 26.9938703  21.29345624 47.36292589 30.43950438\n",
            " 29.33635368 32.56637835 25.91812156 40.71863513 38.83989914 26.87363246\n",
            " 44.55707565 20.74823715 28.39713225 31.18407643 44.61065025 31.53830878\n",
            " 39.78627494 49.08694531 18.12983149 26.61817538 27.91901994 23.47700073\n",
            " 26.30221539 21.50553674 34.8471274  17.97069883 28.27735247 44.49407676\n",
            " 49.99954913 33.24063443 33.65704847 44.0636657  34.47035225 41.1825544\n",
            " 18.82166853 43.52091713 28.83783239 23.45213775 20.36890862 44.23910313\n",
            " 36.06141163 27.13143314 18.26104749 22.54324806 44.70127265 40.27282837\n",
            " 33.91678118 37.66868271 32.64417495 38.49034077 26.78951552 21.67627901\n",
            " 22.89668267 33.00388084 23.85186629 18.60712116 21.5041407  26.17119326\n",
            " 42.88238878 37.86582006 47.24186883 22.78503091 42.02477754 18.56896041\n",
            " 19.43545362 49.99965339 42.93784189 18.02771916 40.38909879 22.56891758\n",
            " 28.83322468 41.28030181 26.03501268 26.97568042 20.45518419 28.28293693\n",
            " 43.62190447 28.05818759 49.844696   19.14627008 45.41214065 18.72002956\n",
            " 32.52772511 34.83138228 21.95639415 35.98696418 41.15793978 37.08064919\n",
            " 17.95436125 37.59082986 23.06234612 36.57467185 25.19438054 36.82901416\n",
            " 24.78505308 42.61644932 21.02166127 19.93673882 36.16355741 24.57895328\n",
            " 44.38232929 27.71735855 49.1190031  31.81971428 38.84549939 19.21320164\n",
            " 21.54938232 49.95169123 33.11442099 24.533257   30.93627351 45.93728512\n",
            " 32.76824587 40.99471957 41.08945127 48.58767333 27.73346941 33.60071712\n",
            " 38.83960922 25.92689644 49.32313532 24.77997488 35.1398883  29.74483515\n",
            " 24.53245656 38.18429318 26.4973792  40.0875502  44.20180379 36.6702386\n",
            " 43.10970391 43.79531926 48.56069271 25.48132278 34.93479271 31.39905479\n",
            " 19.78339473 31.09538978 19.89942562 22.29442111 48.76491119 20.59587073\n",
            " 30.37519779 41.14718991 20.01617419 21.23331054 28.99691351 40.26041835\n",
            " 28.67414959 42.70198756 43.11216433 47.92244783 37.77987778 41.86645033\n",
            " 22.18442805 21.19442424 22.09037482 20.13970504 27.38000174 49.99810168\n",
            " 48.47883903 42.20271402 27.6597488  49.46757545 22.26759075 37.39264688\n",
            " 26.7113063  26.15278669 27.7335016  21.11460773 44.00152822 38.85256751\n",
            " 34.60914302 34.83916682 35.76366421 29.44966938 32.19737802 24.46170586\n",
            " 25.1810945  41.9185178  35.59900379 34.29349144 26.6745155  38.89261321\n",
            " 28.67156737 39.96242655 49.98701749 33.6019206  16.06521958 29.37205471\n",
            " 26.55453626 38.02502854 40.90612803 24.17454664 30.72564609 45.94061712\n",
            " 34.69312975 47.59630328 23.45809033 30.35757642 46.49593145 23.30076727\n",
            " 19.89736517 31.36587127 35.17221136 33.68220395 43.50393298 28.78330377\n",
            " 35.70943099 49.69534848 25.50052559 20.48909348 39.26874365 32.06283569\n",
            " 25.04631899 32.25660576 29.61763592 39.47308081 28.95096755 28.11740802\n",
            " 42.53868001 34.35582181 24.26770849 37.71651135 24.02089014 37.36698327\n",
            " 20.8408084  43.64787204 31.69258102 38.60719849 45.16927431 49.00228888\n",
            " 22.92402286 25.49874503 49.77578292 22.88215913 23.88559283 34.87765132\n",
            " 47.42626946 34.37171626 26.58180279 33.57758248 28.27728903 40.53944428\n",
            " 49.75307188 17.98770175 47.06150864 26.5692787  41.01222647 30.69433481\n",
            " 23.48401798 37.98586307 46.56166342 21.42660208 46.02283306 21.68088433\n",
            " 43.26022956 36.38162213 44.12607164 33.71011606 49.72728331 27.78449496\n",
            " 25.86089627 32.56697032 32.87284294 24.7887717  17.14298972 25.19339026\n",
            " 26.67833599 35.10579469 40.83377674 25.73280203 34.79389407 46.94918716\n",
            " 46.4374337  24.35282034 40.63082927 46.98952062 43.66984263 32.98222702\n",
            " 18.52612997 25.45751566 38.06317049 19.78339473 34.88413097 29.996041\n",
            " 35.20263407 38.23081106 28.59500636 30.43950438 27.49937927 27.87624186\n",
            " 31.14770196 44.7150521  25.755949   21.62194166 20.92835565 25.29168086\n",
            " 20.6300496  19.73173601 41.9988256  33.28619195 23.2952502  32.35134873\n",
            " 17.84722445 42.04281878 34.61617232 25.09476739 24.20091475 31.65592208\n",
            " 17.11558163 26.73624285 25.12042072 18.69328024 21.73223998 33.34253083\n",
            " 35.33834963 41.49640786 27.76699671 44.50107589 30.32880113 31.09003237\n",
            " 33.09326946 37.70225546 41.99489646 35.34704512 43.70178288 36.40804255\n",
            " 33.20489686 22.67046857 41.92140739 35.29073398 39.83955757 26.53251202\n",
            " 20.69943794 24.05684855 49.80584239 33.34590283 28.9763484  42.32132269\n",
            " 28.50137877 24.98557025 37.55711569 40.09263195 49.79581277 22.60567343\n",
            " 34.75705998 26.09746933 39.19887358 41.68468418 49.81732772 32.47972635\n",
            " 48.9454862  21.17585826 25.2583132  19.0409021  32.40064059 32.10109086\n",
            " 27.65387303 21.83512602 31.25473149 18.49202594 22.53956883 29.91882104\n",
            " 23.91800872 49.99999047 22.71790053 46.0938736  28.13044552 27.13761226\n",
            " 47.16844018 23.79893039 22.05462363 49.93929342 31.22639374 22.22314965]\n",
            "selection [634 299 756 712 144  18 401  55 181 750] (10,) [16.06521958 16.93545478 17.11558163 17.14298972 17.34696434 17.43229346\n",
            " 17.43581024 17.46845915 17.63795696 17.84722445]\n",
            "trainset before adding uncertain samples (480, 10) (480,)\n",
            "trainset after adding uncertain samples (490, 10) (490,)\n",
            "updated train set: (490, 10) (490,) unique(labels): [209 281] [0 1]\n",
            "val set: (812, 10) (812,)\n",
            "\n",
            "Train set: (490, 10)\n",
            "Validation set: (812, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 82.027650 \n",
            "Classification report for LogisticRegression(C=0.10204081632653061, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89       321\n",
            "           1       0.74      0.48      0.58       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0]\n",
            "std (812,) [45.71784702 20.25442975 24.5435397  30.2445436  32.85556139 32.29085753\n",
            " 19.11931045 21.38527185 39.64011036 48.95331657 48.71402675 46.0513641\n",
            " 32.56710678 18.94227015 29.43271021 25.73681694 20.69922751 42.96827443\n",
            " 29.52710994 40.19006995 26.14940545 34.65097543 22.70087898 20.81972535\n",
            " 26.94189806 35.46532627 42.94286536 25.65309069 35.52609493 47.72755849\n",
            " 18.63878385 22.15816086 25.49045755 32.09879909 29.74737644 23.66333834\n",
            " 35.02836282 28.07674921 20.99901446 31.33174315 40.90488266 48.65607017\n",
            " 34.62588684 34.18055072 19.36596921 26.93564225 27.17258781 23.71659056\n",
            " 18.04326461 44.22149243 35.95069108 47.68540065 18.75931959 31.04482449\n",
            " 33.59391793 40.21611657 21.50954741 40.18410902 34.60858793 35.47324408\n",
            " 20.55223174 19.16241302 25.96388194 33.46671933 49.97767151 42.48998894\n",
            " 26.9371771  29.3249302  30.46466082 21.10814518 23.9530716  34.36150086\n",
            " 22.67854259 38.27633442 33.88756786 27.91250772 25.03547364 34.33585088\n",
            " 42.59651532 45.55715618 18.38341398 40.71043603 35.64110029 49.97948127\n",
            " 43.27938856 21.67768601 31.11544366 49.99632605 45.43038913 42.77347084\n",
            " 38.67177197 22.63700551 32.70171517 34.79779962 31.77542354 32.48361006\n",
            " 36.14494783 40.1840505  35.14015471 19.92816995 27.06850881 21.9744274\n",
            " 37.73574777 22.00669483 33.30859858 47.80418092 30.11665284 46.53645231\n",
            " 23.67823257 36.53603414 28.19254728 18.48356867 32.32656643 49.96373059\n",
            " 20.6727195  35.67934659 19.56320206 26.9584431  49.9998622  23.89684525\n",
            " 41.86307034 35.6877805  27.74652224 33.70912201 22.57362645 41.28434985\n",
            " 33.93756539 17.94289745 24.38639319 38.86015506 24.99345088 17.67336948\n",
            " 40.1666054  24.11781182 19.96517711 25.05044453 20.00903348 18.79007716\n",
            " 19.28665677 36.51882348 38.61743095 30.41760588 31.93601788 23.36933937\n",
            " 47.49259424 21.36823949 29.09434248 27.66629435 40.88098461 19.16011141\n",
            " 43.76117232 17.18715362 34.70594677 31.07931823 49.46889535 18.33718327\n",
            " 25.72620904 28.05157904 35.99033056 30.85155832 41.11459107 44.11409208\n",
            " 48.30838303 31.43719495 20.33482035 20.20971434 36.23395354 49.83964941\n",
            " 43.55195418 25.96804516 36.41883542 20.19344108 24.27558948 40.34965128\n",
            " 25.05918382 18.86318318 32.62949    18.53195    48.41442237 22.17501287\n",
            " 36.52326132 42.74261372 32.96732863 37.60480326 26.6413407  29.39009362\n",
            " 44.87999469 26.71664085 35.87459181 27.30447406 38.69793455 27.30940783\n",
            " 31.14690625 39.11924475 22.05506872 17.63266945 27.60219561 28.52950215\n",
            " 29.81025008 19.15109277 36.33602173 40.04341551 31.8247972  36.605202\n",
            " 30.26607342 30.94060521 49.6535171  21.67130259 19.89336749 36.12251472\n",
            " 24.2337612  23.50112652 36.15491733 39.86124404 41.39941507 24.5141829\n",
            " 21.14336278 47.56942158 39.06417746 33.2733184  30.26637775 49.96112435\n",
            " 24.40722109 36.38506662 21.10234108 29.27182445 33.40568687 24.56255513\n",
            " 49.99848429 43.38079879 42.74004788 45.72901874 34.1183075  35.01660668\n",
            " 47.17893332 42.94879854 29.60641616 49.27489787 20.59261378 19.64494604\n",
            " 21.93401018 28.23219563 34.84425098 19.9150041  32.11514441 25.90496543\n",
            " 48.37504553 49.91214626 29.79556692 25.26268106 28.16513035 25.72052685\n",
            " 20.50093539 20.49986802 21.5633658  38.72291458 21.13603534 25.65574355\n",
            " 25.30121628 33.14459204 25.89263535 34.60858793 36.25742072 19.77113012\n",
            " 23.40866099 35.14149298 33.36028784 24.77457372 18.24836354 36.29149628\n",
            " 37.16494086 25.48540804 24.96600949 38.98550057 33.66504478 37.46174206\n",
            " 26.31624095 33.94561658 27.05055286 41.07503576 27.42410534 40.66033668\n",
            " 22.81450612 23.79545508 36.87927629 32.89341294 32.50642367 19.51160527\n",
            " 28.23382899 24.09110838 35.59811862 25.62543416 30.21364335 28.04403322\n",
            " 37.01270386 19.78092231 36.44399667 21.81685666 20.54124572 24.70730517\n",
            " 27.48655625 31.19392286 24.2922203  34.32218189 32.15067493 36.31934366\n",
            " 24.50458071 20.6336139  41.95660324 24.78862517 33.51436427 25.87827444\n",
            " 49.98765872 27.30914965 25.39569781 25.52594648 22.41890961 38.73386722\n",
            " 49.16986515 32.77801776 44.61632275 29.51991773 31.30322182 24.93184095\n",
            " 30.3852208  23.74911038 34.85888127 31.9198697  24.55818888 29.09682629\n",
            " 32.56939723 22.64630023 19.88111067 29.43360288 19.56900476 26.82632728\n",
            " 29.91890368 25.60420075 37.85885702 21.49146932 49.44484727 38.22506689\n",
            " 38.84700554 31.52724493 30.17456826 18.74050852 29.2845359  24.4133465\n",
            " 20.71279592 44.51512972 18.45899174 29.62772307 33.94575679 24.16931334\n",
            " 24.69256806 33.71636263 42.50368153 26.08503058 30.59633258 21.74513765\n",
            " 28.62609911 42.45690456 18.57552721 33.02600002 32.80664269 49.42526619\n",
            " 38.6685739  44.97785523 38.44374162 46.33977234 39.05897553 38.29469673\n",
            " 24.39633318 40.30352321 29.07307545 33.38481359 36.26855553 26.55765786\n",
            " 44.64102949 32.96168334 27.79512291 34.17340561 24.09939355 36.49208926\n",
            " 37.12604858 49.39087839 20.43568495 25.84713721 28.61625915 38.58038784\n",
            " 19.26625974 47.93113723 49.99932242 33.14744135 31.50377684 20.0780805\n",
            " 34.85333732 21.35983571 37.97874405 28.78673526 49.69238773 31.12357074\n",
            " 48.55089872 22.61177489 40.11057359 24.1110236  23.48067693 31.30099687\n",
            " 21.50590161 26.42424975 20.45358188 46.10769396 44.17019486 30.21524301\n",
            " 27.37348466 35.41604557 22.66870397 18.62438004 27.74942709 23.1445007\n",
            " 19.80777125 19.78465406 24.46243805 26.31686816 22.94147834 34.30436665\n",
            " 41.09355902 33.11732869 21.58120966 29.98485084 45.65530007 42.22346889\n",
            " 35.78389654 35.56575993 33.73550124 20.49852778 23.87465084 39.72658628\n",
            " 39.77369093 29.96156248 27.86905784 21.19933145 47.40041964 30.08736806\n",
            " 29.65085745 33.40091934 25.99478503 40.43816115 38.15192741 27.1494675\n",
            " 44.32729736 20.08292319 29.26347684 30.22958686 46.49369237 30.90523419\n",
            " 39.10745261 49.00284418 17.43100704 28.26694615 27.72032042 22.94147834\n",
            " 26.91327441 20.65280515 34.60179314 18.73637679 27.92378186 44.11995315\n",
            " 49.99953223 33.039991   33.22941482 44.16111332 33.71873568 40.96469946\n",
            " 18.2609659  42.98194162 28.69031589 22.82580674 19.90328954 44.64310345\n",
            " 35.33801626 26.34366897 17.26675302 22.99104383 44.56570994 39.87672614\n",
            " 33.59962224 37.20815503 32.79676139 38.3958638  25.84623838 21.79305943\n",
            " 22.9423533  33.00680366 23.76487773 18.00699829 20.82138981 25.02890143\n",
            " 42.30277618 37.17892488 47.23839106 22.8420822  41.95481577 19.43559333\n",
            " 19.73971191 49.99967915 43.15378896 17.40157102 40.07391859 21.68346907\n",
            " 27.60635239 40.65875453 25.49706673 27.45840416 20.49146437 27.58600651\n",
            " 43.74885322 28.62111702 49.83062582 19.37317839 45.29545107 18.57688646\n",
            " 32.41595998 37.24785588 21.90533141 35.73952513 41.06688338 36.70570982\n",
            " 17.11674537 37.48420808 23.6511478  36.45479001 25.82526501 36.30887097\n",
            " 24.10676333 42.51468911 20.79465974 19.50926323 35.65240619 23.93352314\n",
            " 43.89213435 27.26439937 49.19416442 32.11637604 39.3951856  20.19279861\n",
            " 21.5633658  49.95222515 32.79728858 24.39804495 30.43149493 45.59171896\n",
            " 32.68992077 40.94663877 40.78903279 48.70072043 28.10187929 26.84634265\n",
            " 37.9022675  25.76174856 49.3598486  24.51103547 35.2419739  29.37235123\n",
            " 24.89666503 37.42827148 26.1857437  39.77649636 44.26794713 36.74141867\n",
            " 43.18313151 43.41536623 48.51908821 25.89455231 34.52034239 30.78208848\n",
            " 19.96994728 32.08165878 19.79109487 22.6343876  48.56481442 20.59055238\n",
            " 29.13297476 41.31214503 19.44122012 21.28907165 28.63359322 39.71137276\n",
            " 27.97596246 42.97903791 44.53967109 47.8827706  37.40296364 41.08722255\n",
            " 21.34233308 20.61036595 22.15306369 20.58093858 28.34041578 49.99823757\n",
            " 48.44026985 41.94567679 27.77106322 49.42930922 20.00226801 37.15197786\n",
            " 25.78257972 26.21212758 27.5403596  21.34565099 43.83482119 38.59941239\n",
            " 34.3450124  34.71367901 35.04796091 29.39464927 31.52477962 22.53461006\n",
            " 24.39463507 39.99528751 35.40416729 32.32523298 26.33747123 38.56939548\n",
            " 28.25420893 39.73450789 49.98692507 32.38164482 29.44962831 26.9329871\n",
            " 37.58286688 40.90246915 23.64401497 31.28272464 45.94173016 34.49376028\n",
            " 47.71822692 22.9100533  29.54457393 46.73790506 23.11554307 18.67087841\n",
            " 30.90507385 34.33524546 33.20476783 43.26427538 28.59877897 37.87679044\n",
            " 49.67438887 25.38075415 19.97009681 39.39193282 31.44514024 25.79535529\n",
            " 31.76029893 27.6252282  39.08578327 28.27775818 27.74942709 42.41885204\n",
            " 32.099648   22.21643503 37.59046192 23.17780236 36.81205801 20.9201153\n",
            " 43.4787695  32.64251313 38.46859959 45.3647935  49.01442935 21.31773927\n",
            " 25.17293649 49.77649507 21.31205574 23.67065058 34.68811174 47.46475354\n",
            " 32.69351599 27.31056455 32.92995949 27.87992    40.01055826 49.78238206\n",
            " 19.28591762 47.11499948 26.48915752 41.16453585 29.91469189 22.37749094\n",
            " 37.60480326 46.77379159 20.71130482 45.43615996 21.69598143 42.85910794\n",
            " 35.86056176 44.38124646 33.70342003 49.72519323 26.73160305 25.28057862\n",
            " 32.99599365 32.75107167 22.80439202 25.60767098 26.40192814 34.99012418\n",
            " 40.60454711 25.66731857 35.06308353 47.10688159 46.24518453 24.32508567\n",
            " 41.09980938 46.5752763  43.49976762 32.68806106 18.07712419 24.91762559\n",
            " 38.23465224 19.96994728 34.2954376  30.0660179  34.98711757 37.90649706\n",
            " 29.39009362 30.08736806 27.03915304 27.44462443 30.60930437 44.49736541\n",
            " 24.42385628 21.52794957 21.10814518 24.69348109 20.43722015 18.25026911\n",
            " 41.36537387 33.17526124 23.62797326 32.49134702 33.8306466  34.33210365\n",
            " 24.33796441 23.97226206 31.86636396 26.96197253 25.76698731 17.92330689\n",
            " 21.21202821 33.00199173 34.62179009 41.19428995 27.22447596 43.34809147\n",
            " 29.74791988 30.79722784 32.50215663 38.4691204  41.24019195 34.77366727\n",
            " 43.23904928 34.67340983 32.52565565 21.59854993 42.04496459 34.96755093\n",
            " 38.75204101 28.36484104 17.09909482 24.20897511 49.80459873 33.52882783\n",
            " 25.34078476 41.59328138 27.50774872 25.01243869 37.91523301 39.42507829\n",
            " 49.79507969 22.62445831 33.52485672 24.66979154 38.77322224 41.45835806\n",
            " 49.82012939 34.09109645 48.90947007 20.39587006 25.02104356 17.29064743\n",
            " 32.6119464  32.57668636 26.84133395 21.37584211 30.96885366 18.04851468\n",
            " 22.14766309 29.43230592 23.30700737 49.9999726  23.38126044 45.87854745\n",
            " 27.87933628 27.44243846 46.99577693 23.75658398 22.55973949 49.94168579\n",
            " 31.39607697 21.50543005]\n",
            "selection [770 528 151 482 791 507 458 195 131 749] (10,) [17.09909482 17.11674537 17.18715362 17.26675302 17.29064743 17.40157102\n",
            " 17.43100704 17.63266945 17.67336948 17.92330689]\n",
            "trainset before adding uncertain samples (490, 10) (490,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [213 287] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.72      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.036866359447, 75.80645161290323, 77.64976958525345, 78.57142857142857, 78.80184331797236, 78.11059907834101, 80.18433179723502, 80.87557603686636, 80.87557603686636, 79.95391705069125, 80.64516129032258, 80.87557603686636, 78.3410138248848, 79.03225806451613, 79.95391705069125, 80.4147465437788, 79.72350230414746, 80.18433179723502, 79.95391705069125, 79.72350230414746, 80.4147465437788, 80.4147465437788, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.4147465437788, 80.87557603686636, 80.4147465437788, 81.10599078341014, 80.87557603686636, 80.87557603686636, 80.87557603686636, 80.87557603686636, 80.64516129032258, 80.64516129032258, 81.5668202764977, 80.87557603686636, 81.5668202764977, 81.5668202764977, 81.5668202764977, 81.79723502304147, 81.79723502304147, 81.33640552995391, 81.5668202764977, 81.79723502304147, 82.02764976958525, 81.5668202764977]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-20.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          76.49769585253456,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          74.65437788018433,\n",
            "          77.41935483870968,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          76.036866359447,\n",
            "          77.18894009216591,\n",
            "          77.41935483870968,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          81.33640552995391,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          81.5668202764977,\n",
            "          81.79723502304147,\n",
            "          81.10599078341014,\n",
            "          81.5668202764977,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          81.33640552995391,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          81.33640552995391,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.27188940092167,\n",
            "          71.6589861751152,\n",
            "          69.81566820276498,\n",
            "          74.65437788018433,\n",
            "          74.42396313364056,\n",
            "          75.57603686635944,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502,\n",
            "          81.33640552995391,\n",
            "          82.02764976958525,\n",
            "          82.25806451612904,\n",
            "          81.5668202764977,\n",
            "          81.33640552995391,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.57142857142857,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.49769585253456,\n",
            "          73.73271889400922,\n",
            "          77.88018433179722,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          60.13824884792627,\n",
            "          56.68202764976959,\n",
            "          60.59907834101382,\n",
            "          68.4331797235023,\n",
            "          70.27649769585254,\n",
            "          72.81105990783409,\n",
            "          71.19815668202764,\n",
            "          72.81105990783409,\n",
            "          72.11981566820278,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          72.58064516129032,\n",
            "          72.58064516129032,\n",
            "          72.11981566820278,\n",
            "          70.73732718894009,\n",
            "          72.58064516129032,\n",
            "          71.19815668202764,\n",
            "          73.04147465437788,\n",
            "          72.81105990783409,\n",
            "          72.58064516129032,\n",
            "          72.58064516129032,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          73.27188940092167,\n",
            "          73.50230414746544,\n",
            "          72.81105990783409,\n",
            "          73.04147465437788,\n",
            "          73.73271889400922,\n",
            "          74.19354838709677,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          73.50230414746544,\n",
            "          73.963133640553,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.87557603686636,\n",
            "          79.03225806451613,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          81.33640552995391\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MinStdSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.11059907834101,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.5668202764977,\n",
            "          80.87557603686636,\n",
            "          81.5668202764977,\n",
            "          81.5668202764977,\n",
            "          81.5668202764977,\n",
            "          81.79723502304147,\n",
            "          81.79723502304147,\n",
            "          81.33640552995391,\n",
            "          81.5668202764977,\n",
            "          81.79723502304147,\n",
            "          82.02764976958525,\n",
            "          81.5668202764977\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          70.96774193548387,\n",
            "          73.27188940092167,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          67.51152073732719,\n",
            "          74.65437788018433,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          81.33640552995391,\n",
            "          81.33640552995391,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          79.26267281105991,\n",
            "          73.73271889400922,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.87557603686636,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          78.11059907834101,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "{'LogModel': {'RandomSelection': {'250': [[77.88018433179722, 79.95391705069125]], '125': [[81.5668202764977, 81.10599078341014, 81.10599078341014, 79.49308755760369]], '50': [[77.18894009216591, 76.26728110599078, 77.64976958525345, 79.26267281105991, 79.03225806451613, 78.57142857142857, 80.18433179723502, 80.64516129032258, 80.87557603686636, 77.88018433179722]], '25': [[79.72350230414746, 79.95391705069125, 80.87557603686636, 78.57142857142857, 80.18433179723502, 80.18433179723502, 78.11059907834101, 79.95391705069125, 79.72350230414746, 79.72350230414746, 78.57142857142857, 78.57142857142857, 78.57142857142857, 79.49308755760369, 79.49308755760369, 78.80184331797236, 79.26267281105991, 79.26267281105991, 79.26267281105991, 79.03225806451613]], '10': [[76.72811059907833, 76.72811059907833, 79.26267281105991, 73.73271889400922, 78.3410138248848, 79.26267281105991, 80.87557603686636, 80.4147465437788, 80.18433179723502, 81.5668202764977, 81.10599078341014, 79.49308755760369, 80.64516129032258, 79.95391705069125, 79.26267281105991, 79.72350230414746, 79.49308755760369, 79.72350230414746, 78.80184331797236, 79.03225806451613, 79.26267281105991, 79.03225806451613, 79.03225806451613, 79.26267281105991, 79.49308755760369, 79.49308755760369, 79.26267281105991, 79.72350230414746, 79.72350230414746, 79.26267281105991, 80.4147465437788, 80.18433179723502, 80.4147465437788, 80.64516129032258, 80.87557603686636, 79.72350230414746, 79.26267281105991, 79.26267281105991, 79.03225806451613, 79.26267281105991, 79.72350230414746, 80.18433179723502, 79.49308755760369, 80.18433179723502, 79.72350230414746, 79.72350230414746, 80.18433179723502, 80.64516129032258, 79.95391705069125, 80.4147465437788]]}, 'MarginSamplingSelection': {'250': [[79.72350230414746, 81.33640552995391]], '125': [[79.72350230414746, 80.4147465437788, 81.10599078341014, 79.95391705069125]], '50': [[78.57142857142857, 80.18433179723502, 80.4147465437788, 80.18433179723502, 80.64516129032258, 80.4147465437788, 80.64516129032258, 80.87557603686636, 81.10599078341014, 80.87557603686636]], '25': [[76.72811059907833, 73.50230414746544, 73.963133640553, 80.4147465437788, 79.72350230414746, 79.49308755760369, 80.87557603686636, 79.03225806451613, 80.64516129032258, 79.95391705069125, 79.72350230414746, 80.18433179723502, 79.49308755760369, 79.95391705069125, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746]], '10': [[60.13824884792627, 56.68202764976959, 60.59907834101382, 68.4331797235023, 70.27649769585254, 72.81105990783409, 71.19815668202764, 72.81105990783409, 72.11981566820278, 71.42857142857143, 71.42857142857143, 72.58064516129032, 72.58064516129032, 72.11981566820278, 70.73732718894009, 72.58064516129032, 71.19815668202764, 73.04147465437788, 72.81105990783409, 72.58064516129032, 72.58064516129032, 72.58064516129032, 73.04147465437788, 73.27188940092167, 73.50230414746544, 72.81105990783409, 73.04147465437788, 73.73271889400922, 74.19354838709677, 79.26267281105991, 79.03225806451613, 77.64976958525345, 78.57142857142857, 78.11059907834101, 78.80184331797236, 78.3410138248848, 78.80184331797236, 79.03225806451613, 79.26267281105991, 79.03225806451613, 79.49308755760369, 79.26267281105991, 79.49308755760369, 78.80184331797236, 78.57142857142857, 78.57142857142857, 78.80184331797236, 79.26267281105991, 79.03225806451613, 78.80184331797236]]}, 'EntropySelection': {'250': [[78.57142857142857, 80.87557603686636]], '125': [[77.41935483870968, 81.10599078341014, 80.4147465437788, 80.64516129032258]], '50': [[76.49769585253456, 73.73271889400922, 77.88018433179722, 79.26267281105991, 79.72350230414746, 81.5668202764977, 81.10599078341014, 81.33640552995391, 80.87557603686636, 80.64516129032258]], '25': [[73.27188940092167, 71.6589861751152, 69.81566820276498, 74.65437788018433, 74.42396313364056, 75.57603686635944, 78.11059907834101, 79.49308755760369, 79.03225806451613, 80.18433179723502, 81.33640552995391, 82.02764976958525, 82.25806451612904, 81.5668202764977, 81.33640552995391, 80.64516129032258, 81.10599078341014, 81.10599078341014, 81.33640552995391, 80.87557603686636]], '10': [[76.49769585253456, 77.41935483870968, 76.72811059907833, 76.72811059907833, 74.65437788018433, 77.41935483870968, 79.72350230414746, 78.57142857142857, 78.11059907834101, 77.88018433179722, 76.95852534562212, 77.41935483870968, 77.64976958525345, 76.036866359447, 77.18894009216591, 77.41935483870968, 76.95852534562212, 79.03225806451613, 79.26267281105991, 79.03225806451613, 78.80184331797236, 79.49308755760369, 79.49308755760369, 79.03225806451613, 80.64516129032258, 80.18433179723502, 80.18433179723502, 80.18433179723502, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.4147465437788, 80.64516129032258, 81.33640552995391, 81.10599078341014, 81.10599078341014, 80.64516129032258, 81.5668202764977, 81.79723502304147, 81.10599078341014, 81.5668202764977, 80.18433179723502, 81.5668202764977, 81.33640552995391, 80.64516129032258, 81.10599078341014, 80.87557603686636, 81.33640552995391, 81.10599078341014, 81.33640552995391]]}, 'MinStdSelection': {'250': [[79.03225806451613, 79.49308755760369]], '125': [[70.96774193548387, 73.27188940092167, 79.03225806451613, 79.26267281105991]], '50': [[77.64976958525345, 78.57142857142857, 78.11059907834101, 78.80184331797236, 79.72350230414746, 80.4147465437788, 79.72350230414746, 79.49308755760369, 79.95391705069125, 79.72350230414746]], '25': [[67.51152073732719, 74.65437788018433, 76.95852534562212, 77.41935483870968, 79.03225806451613, 78.80184331797236, 78.80184331797236, 79.49308755760369, 80.18433179723502, 80.64516129032258, 80.87557603686636, 81.33640552995391, 81.33640552995391, 81.10599078341014, 80.64516129032258, 81.10599078341014, 80.18433179723502, 80.87557603686636, 80.64516129032258, 80.87557603686636]], '10': [[76.036866359447, 75.80645161290323, 77.64976958525345, 78.57142857142857, 78.80184331797236, 78.11059907834101, 80.18433179723502, 80.87557603686636, 80.87557603686636, 79.95391705069125, 80.64516129032258, 80.87557603686636, 78.3410138248848, 79.03225806451613, 79.95391705069125, 80.4147465437788, 79.72350230414746, 80.18433179723502, 79.95391705069125, 79.72350230414746, 80.4147465437788, 80.4147465437788, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.4147465437788, 80.87557603686636, 80.4147465437788, 81.10599078341014, 80.87557603686636, 80.87557603686636, 80.87557603686636, 80.87557603686636, 80.64516129032258, 80.64516129032258, 81.5668202764977, 80.87557603686636, 81.5668202764977, 81.5668202764977, 81.5668202764977, 81.79723502304147, 81.79723502304147, 81.33640552995391, 81.5668202764977, 81.79723502304147, 82.02764976958525, 81.5668202764977]]}}}\n",
            "{'LogModel': {'EntropySelection': {'10': [[76.49769585253456, 77.41935483870968, 76.72811059907833, 76.72811059907833, 74.65437788018433, 77.41935483870968, 79.72350230414746, 78.57142857142857, 78.11059907834101, 77.88018433179722, 76.95852534562212, 77.41935483870968, 77.64976958525345, 76.036866359447, 77.18894009216591, 77.41935483870968, 76.95852534562212, 79.03225806451613, 79.26267281105991, 79.03225806451613, 78.80184331797236, 79.49308755760369, 79.49308755760369, 79.03225806451613, 80.64516129032258, 80.18433179723502, 80.18433179723502, 80.18433179723502, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.4147465437788, 80.64516129032258, 81.33640552995391, 81.10599078341014, 81.10599078341014, 80.64516129032258, 81.5668202764977, 81.79723502304147, 81.10599078341014, 81.5668202764977, 80.18433179723502, 81.5668202764977, 81.33640552995391, 80.64516129032258, 81.10599078341014, 80.87557603686636, 81.33640552995391, 81.10599078341014, 81.33640552995391]], '125': [[77.41935483870968, 81.10599078341014, 80.4147465437788, 80.64516129032258]], '25': [[73.27188940092167, 71.6589861751152, 69.81566820276498, 74.65437788018433, 74.42396313364056, 75.57603686635944, 78.11059907834101, 79.49308755760369, 79.03225806451613, 80.18433179723502, 81.33640552995391, 82.02764976958525, 82.25806451612904, 81.5668202764977, 81.33640552995391, 80.64516129032258, 81.10599078341014, 81.10599078341014, 81.33640552995391, 80.87557603686636]], '250': [[78.57142857142857, 80.87557603686636]], '50': [[76.49769585253456, 73.73271889400922, 77.88018433179722, 79.26267281105991, 79.72350230414746, 81.5668202764977, 81.10599078341014, 81.33640552995391, 80.87557603686636, 80.64516129032258]]}, 'MarginSamplingSelection': {'10': [[60.13824884792627, 56.68202764976959, 60.59907834101382, 68.4331797235023, 70.27649769585254, 72.81105990783409, 71.19815668202764, 72.81105990783409, 72.11981566820278, 71.42857142857143, 71.42857142857143, 72.58064516129032, 72.58064516129032, 72.11981566820278, 70.73732718894009, 72.58064516129032, 71.19815668202764, 73.04147465437788, 72.81105990783409, 72.58064516129032, 72.58064516129032, 72.58064516129032, 73.04147465437788, 73.27188940092167, 73.50230414746544, 72.81105990783409, 73.04147465437788, 73.73271889400922, 74.19354838709677, 79.26267281105991, 79.03225806451613, 77.64976958525345, 78.57142857142857, 78.11059907834101, 78.80184331797236, 78.3410138248848, 78.80184331797236, 79.03225806451613, 79.26267281105991, 79.03225806451613, 79.49308755760369, 79.26267281105991, 79.49308755760369, 78.80184331797236, 78.57142857142857, 78.57142857142857, 78.80184331797236, 79.26267281105991, 79.03225806451613, 78.80184331797236]], '125': [[79.72350230414746, 80.4147465437788, 81.10599078341014, 79.95391705069125]], '25': [[76.72811059907833, 73.50230414746544, 73.963133640553, 80.4147465437788, 79.72350230414746, 79.49308755760369, 80.87557603686636, 79.03225806451613, 80.64516129032258, 79.95391705069125, 79.72350230414746, 80.18433179723502, 79.49308755760369, 79.95391705069125, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746]], '250': [[79.72350230414746, 81.33640552995391]], '50': [[78.57142857142857, 80.18433179723502, 80.4147465437788, 80.18433179723502, 80.64516129032258, 80.4147465437788, 80.64516129032258, 80.87557603686636, 81.10599078341014, 80.87557603686636]]}, 'MinStdSelection': {'10': [[76.036866359447, 75.80645161290323, 77.64976958525345, 78.57142857142857, 78.80184331797236, 78.11059907834101, 80.18433179723502, 80.87557603686636, 80.87557603686636, 79.95391705069125, 80.64516129032258, 80.87557603686636, 78.3410138248848, 79.03225806451613, 79.95391705069125, 80.4147465437788, 79.72350230414746, 80.18433179723502, 79.95391705069125, 79.72350230414746, 80.4147465437788, 80.4147465437788, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.4147465437788, 80.87557603686636, 80.4147465437788, 81.10599078341014, 80.87557603686636, 80.87557603686636, 80.87557603686636, 80.87557603686636, 80.64516129032258, 80.64516129032258, 81.5668202764977, 80.87557603686636, 81.5668202764977, 81.5668202764977, 81.5668202764977, 81.79723502304147, 81.79723502304147, 81.33640552995391, 81.5668202764977, 81.79723502304147, 82.02764976958525, 81.5668202764977]], '125': [[70.96774193548387, 73.27188940092167, 79.03225806451613, 79.26267281105991]], '25': [[67.51152073732719, 74.65437788018433, 76.95852534562212, 77.41935483870968, 79.03225806451613, 78.80184331797236, 78.80184331797236, 79.49308755760369, 80.18433179723502, 80.64516129032258, 80.87557603686636, 81.33640552995391, 81.33640552995391, 81.10599078341014, 80.64516129032258, 81.10599078341014, 80.18433179723502, 80.87557603686636, 80.64516129032258, 80.87557603686636]], '250': [[79.03225806451613, 79.49308755760369]], '50': [[77.64976958525345, 78.57142857142857, 78.11059907834101, 78.80184331797236, 79.72350230414746, 80.4147465437788, 79.72350230414746, 79.49308755760369, 79.95391705069125, 79.72350230414746]]}, 'RandomSelection': {'10': [[76.72811059907833, 76.72811059907833, 79.26267281105991, 73.73271889400922, 78.3410138248848, 79.26267281105991, 80.87557603686636, 80.4147465437788, 80.18433179723502, 81.5668202764977, 81.10599078341014, 79.49308755760369, 80.64516129032258, 79.95391705069125, 79.26267281105991, 79.72350230414746, 79.49308755760369, 79.72350230414746, 78.80184331797236, 79.03225806451613, 79.26267281105991, 79.03225806451613, 79.03225806451613, 79.26267281105991, 79.49308755760369, 79.49308755760369, 79.26267281105991, 79.72350230414746, 79.72350230414746, 79.26267281105991, 80.4147465437788, 80.18433179723502, 80.4147465437788, 80.64516129032258, 80.87557603686636, 79.72350230414746, 79.26267281105991, 79.26267281105991, 79.03225806451613, 79.26267281105991, 79.72350230414746, 80.18433179723502, 79.49308755760369, 80.18433179723502, 79.72350230414746, 79.72350230414746, 80.18433179723502, 80.64516129032258, 79.95391705069125, 80.4147465437788]], '125': [[81.5668202764977, 81.10599078341014, 81.10599078341014, 79.49308755760369]], '25': [[79.72350230414746, 79.95391705069125, 80.87557603686636, 78.57142857142857, 80.18433179723502, 80.18433179723502, 78.11059907834101, 79.95391705069125, 79.72350230414746, 79.72350230414746, 78.57142857142857, 78.57142857142857, 78.57142857142857, 79.49308755760369, 79.49308755760369, 78.80184331797236, 79.26267281105991, 79.26267281105991, 79.26267281105991, 79.03225806451613]], '250': [[77.88018433179722, 79.95391705069125]], '50': [[77.18894009216591, 76.26728110599078, 77.64976958525345, 79.26267281105991, 79.03225806451613, 78.57142857142857, 80.18433179723502, 80.64516129032258, 80.87557603686636, 77.88018433179722]]}}}\n"
          ]
        }
      ],
      "source": [
        "(X, y) = data_prep()\n",
        "(X_train_full, y_train_full, X_test, y_test) = split(trainset_size)\n",
        "print ('train:', X_train_full.shape, y_train_full.shape)\n",
        "print ('test :', X_test.shape, y_test.shape)\n",
        "classes = len(np.unique(y))\n",
        "print ('unique classes', classes)\n",
        "\n",
        "def pickle_save(fname, data):\n",
        "  filehandler = open(fname,\"wb\")\n",
        "  pickle.dump(data,filehandler)\n",
        "  filehandler.close() \n",
        "  print('saved', fname, os.getcwd(), os.listdir())\n",
        "\n",
        "def pickle_load(fname):\n",
        "  print(os.getcwd(), os.listdir())\n",
        "  file = open(fname,'rb')\n",
        "  data = pickle.load(file)\n",
        "  file.close()\n",
        "  print(data)\n",
        "  return data\n",
        "  \n",
        "def experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
        "    algos_temp = []\n",
        "    print ('stopping at:', max_queried)\n",
        "    count = 0\n",
        "    for model_object in models:\n",
        "      if model_object.__name__ not in d:\n",
        "          d[model_object.__name__] = {}\n",
        "      \n",
        "      for selection_function in selection_functions:\n",
        "        if selection_function.__name__ not in d[model_object.__name__]:\n",
        "            d[model_object.__name__][selection_function.__name__] = {}\n",
        "        \n",
        "        for k in Ks:\n",
        "            d[model_object.__name__][selection_function.__name__][str(k)] = []           \n",
        "            \n",
        "            for i in range(0, repeats):\n",
        "                count+=1\n",
        "                if count >= contfrom:\n",
        "                    print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
        "                    alg = TheAlgorithm(k, \n",
        "                                       model_object, \n",
        "                                       selection_function\n",
        "                                       )\n",
        "                    alg.run(X_train_full, y_train_full, X_test, y_test)\n",
        "                    d[model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
        "                    fname = '/Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-' + str(count) + '.pkl'\n",
        "                    pickle_save(fname, d)\n",
        "                    if count % 5 == 0:\n",
        "                        print(json.dumps(d, indent=2, sort_keys=True))\n",
        "                    print ()\n",
        "                    print ('---------------------------- FINISHED ---------------------------')\n",
        "                    print ()\n",
        "    return d\n",
        "\n",
        "\n",
        "max_queried = 500 \n",
        "\n",
        "repeats = 1\n",
        "\n",
        "models = [LogModel] \n",
        "# models = [SvmModel, RfModel, LogModel, GDBCModel, KnnModel] \n",
        "\n",
        "# selection_functions = [RandomSelection] \n",
        "selection_functions = [RandomSelection, MarginSamplingSelection, EntropySelection, MinStdSelection] \n",
        "\n",
        "Ks = [250,125,50,25,10] \n",
        "\n",
        "d = {}\n",
        "stopped_at = -1 \n",
        "\n",
        "# print('directory dump including pickle files:', os.getcwd(), np.sort(os.listdir()))  \n",
        "# d = pickle_load('Active-learning-experiment-' + str(stopped_at) + '.pkl')  \n",
        "# print(json.dumps(d, indent=2, sort_keys=True))\n",
        "\n",
        "d = experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
        "print (d)\n",
        "results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So which is the better model? under the stopping condition and hyper parameters\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ea9920facb548e5bf04306e568fcef9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%matplotlib widget\n",
        "# %matplotlib inline\n",
        "def performance_plot(fully_supervised_accuracy, dic, models, selection_functions, Ks, repeats):  \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot([0,500],[fully_supervised_accuracy, fully_supervised_accuracy],label = 'upper-bound')\n",
        "    for model_object in models:\n",
        "      for selection_function in selection_functions:\n",
        "        for idx, k in enumerate(Ks):\n",
        "            x = np.arange(float(Ks[idx]), 500 + float(Ks[idx]), float(Ks[idx]))            \n",
        "            Sum = np.array(dic[model_object][selection_function][k][0])\n",
        "            for i in range(1, repeats):\n",
        "                Sum = Sum + np.array(dic[model_object][selection_function][k][i])\n",
        "            mean = Sum / repeats\n",
        "            ax.plot(x, mean, label = model_object[0:3] + '-' + selection_function[0:3] + '-' + str(k))\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.set_xlim([50,500])\n",
        "    ax.set_ylim([70,83])\n",
        "    ax.grid(True)\n",
        "    mplcursors.cursor()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# models_str = ['SvmModel', 'RfModel', 'LogModel','GDBCModel','KnnModel']\n",
        "models_str = ['LogModel']\n",
        "selection_functions_str = ['RandomSelection', 'MarginSamplingSelection', 'EntropySelection', 'MinStdSelection']\n",
        "# selection_functions_str = ['RandomSelection']\n",
        "Ks_str = ['250','125','50','25','10'] \n",
        "repeats = 10\n",
        "random_forest_upper_bound = 89.\n",
        "svm_upper_bound = 87.\n",
        "log_upper_bound = 87.\n",
        "gdbc_upper_bound = 86.\n",
        "knn_upper_bound = 86.\n",
        "total_experiments = len(models_str) * len(selection_functions_str) * len(Ks_str) * repeats\n",
        "\n",
        "print('So which is the better model? under the stopping condition and hyper parameters')\n",
        "# performance_plot(random_forest_upper_bound, d, ['RfModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# performance_plot(svm_upper_bound, d, ['SvmModel'] , selection_functions_str    , Ks_str, 1)\n",
        "performance_plot(log_upper_bound, d, ['LogModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# performance_plot(_upper_bound, d, ['GDBCModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# performance_plot(log_upper_bound, d, ['KnnModel'] , selection_functions_str    , Ks_str, 1)"
      ]
    }
  ]
}