{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 183,
      "source": [
        "print(__doc__)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from scipy.special import expit\n",
        "from scipy import stats\n",
        "from pylab import rcParams\n",
        "import mplcursors\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import neighbors\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "# pd.options.display.max_rows = 20\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "source": [
        "def fetch_data_corona():\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/ML-for-COVID-19-dataset/main/all_training.csv\", sep=',')\n",
        "    # Column selection\n",
        "    df = data.iloc[:,np.r_[3:34]].copy()\n",
        "    # define row and column index\n",
        "    col = df.columns\n",
        "    row = [i for i in range(df.shape[0])]\n",
        "    # define imputer\n",
        "    imputer = IterativeImputer(estimator=linear_model.BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
        "    # fit on the dataset\n",
        "    imputer.fit(df)\n",
        "    # transform the dataset\n",
        "    df_imputed = imputer.transform(df)\n",
        "    # convert back to pandas dataframe and rename back to df_normalized\n",
        "    df = pd.DataFrame(data=df_imputed, index=row, columns=col)\n",
        "    X = df\n",
        "    y = data.target    \n",
        "    # # Recursive feature elimination\n",
        "    # rdmreg = RandomForestClassifier(n_estimators=100)\n",
        "    # # Define the method\n",
        "    # rfe = RFE(estimator=rdmreg, n_features_to_select=10)\n",
        "    # # Fit the model\n",
        "    # rfe = rfe.fit(X, y.values.ravel())\n",
        "    # print(rfe.support_)\n",
        "    # # Drop columns that failed RFE test\n",
        "    # col = df.columns[rfe.support_]\n",
        "    # X = X[col]\n",
        "    X = X.to_numpy()\n",
        "    print ('df:', X.shape, y.shape)\n",
        "    return (X, y)\n",
        "\n",
        "\n",
        "# def split(train_size):\n",
        "#     X_train_full = X[:train_size]\n",
        "#     y_train_full = y[:train_size]\n",
        "#     X_test = X[train_size:]\n",
        "#     y_test = y[train_size:]   \n",
        "#     return (X_train_full, y_train_full, X_test, y_test)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "source": [
        "def fetch_data_iris():\n",
        "    iris = load_iris()\n",
        "    X = iris.data.astype('float64')\n",
        "    y = iris.target\n",
        "    print ('Dataset : ', X.shape, y.shape)\n",
        "    return (X, y)\n",
        "\n",
        "def fetch_data_mnist():\n",
        "    mnist = load_digits()\n",
        "    X = mnist.data.astype('float64')\n",
        "    y = mnist.target\n",
        "    print ('Dataset : ', X.shape, y.shape)\n",
        "    return (X, y)\n",
        "\n",
        "def fetch_data_wine():\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/winequality-white.csv\", sep=';')\n",
        "    df = data.iloc[:,np.r_[0:10]].copy()\n",
        "    X = df.to_numpy()\n",
        "    y = data.quality\n",
        "    print ('df:', X.shape, y.shape)\n",
        "    return (X, y)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SvmModel(BaseModel):\n",
        "\n",
        "    model_type = 'Support Vector Machine with linear Kernel'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test):\n",
        "        print ('training svm...')\n",
        "        self.classifier = SVC(\n",
        "            C=1, \n",
        "            kernel='linear', \n",
        "            probability=True\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    model_type = 'Logistic Regression' \n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test):\n",
        "        # print ('training logistic regression...')\n",
        "        # train_samples = X_train.shape[0]\n",
        "        self.classifier = LogisticRegression(\n",
        "            # C=50. / train_samples,\n",
        "            penalty='l2',\n",
        "            solver='liblinear',\n",
        "            tol=0.1\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class RfModel(BaseModel):\n",
        "\n",
        "    model_type = 'Random Forest'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test):\n",
        "        print ('training random forest...')\n",
        "        self.classifier = RandomForestClassifier(\n",
        "            n_estimators=100, \n",
        "            n_jobs=-1\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
        "\n",
        "class GDBCModel(BaseModel):\n",
        "\n",
        "    model_type = 'Gradient Boost classifier'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test):\n",
        "        print ('training GDBC...')\n",
        "        self.classifier = GradientBoostingClassifier(\n",
        "            n_estimators=1200,\n",
        "            max_depth=3,\n",
        "            subsample=0.5,\n",
        "            learning_rate=0.01,\n",
        "            min_samples_leaf=1,\n",
        "            random_state=3\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
        "\n",
        "class KnnModel(BaseModel):\n",
        "\n",
        "    model_type = 'Nearest Neighbour classifier'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test):\n",
        "        print ('training KNN...')\n",
        "        self.classifier = neighbors.KNeighborsClassifier(\n",
        "            n_neighbors = 10,\n",
        "            n_jobs = -1\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    # we train normally and use the probabilities to select the most uncertain samples\n",
        "    def train(self, X_train, y_train, X_val, X_test):\n",
        "        t0 = time.time()\n",
        "        (X_train, X_val, X_test, self.val_y_predicted,\n",
        "         self.test_y_predicted) = \\\n",
        "            self.model_object.fit_predict(X_train, y_train, X_val, X_test)\n",
        "        self.run_time = time.time() - t0\n",
        "        return (X_train, X_val, X_test)\n",
        "\n",
        "    # we want accuracy only for the test set\n",
        "    def get_test_accuracy(self, i, y_test):\n",
        "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        # print('--------------------------------')\n",
        "        # print('y-test set:',y_test.shape)\n",
        "        # print('Training run in %.3f s' % self.run_time,'\\n')\n",
        "        print(\"Accuracy rate is %f \" % (classif_rate))    \n",
        "        # print(\"Classification report for %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
        "        # print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
        "        print('--------------------------------')\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "source": [
        "class QueryFunction(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # def stream_select(self):\n",
        "    #     pass\n",
        "\n",
        "    def pool_select(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomSelection(QueryFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        random_state = check_random_state(0)\n",
        "        # probas_val.shape[0] is the size of validation set\n",
        "        selection = np.random.choice(probas_val.shape[0], batch_size, replace=False)\n",
        "        # print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',batch_size)\n",
        "        return selection\n",
        "\n",
        "\n",
        "class EntropySelection(QueryFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "        selection = (np.argsort(e)[::-1])[:batch_size]\n",
        "        return selection\n",
        "\n",
        "class MinStdSelection(QueryFunction):\n",
        "\n",
        "    # select the samples where the std is smallest. There is uncertainty regarding the relevant class\n",
        "    # and then train on these \"hard\" to classify samples.\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        std = np.std(probas_val * 100, axis=1) \n",
        "        selection = std.argsort()[:batch_size]\n",
        "        selection = selection.astype('int64')\n",
        "        print('std',std.shape,std)\n",
        "        print('selection',selection, selection.shape, std[selection])\n",
        "        return selection\n",
        "\n",
        "class LeastConfidenceSelection(QueryFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        sort_prob = -np.sort(-probas_val, axis=1)\n",
        "        values = sort_prob[:, 0]\n",
        "        selection = np.argsort(values)[:batch_size]\n",
        "        return selection\n",
        "      \n",
        "      \n",
        "class MarginSelection(QueryFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        sort_prob = -np.sort(-probas_val, axis=1)\n",
        "        values = sort_prob[:, 0] - sort_prob[:, 1]\n",
        "        selection = np.argsort(values)[:batch_size]\n",
        "        return selection\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "source": [
        "class Normalize(object):\n",
        "    \n",
        "    def normalize(self, X_train, X_val, X_test):\n",
        "        self.scaler = RobustScaler()\n",
        "        X_train = self.scaler.fit_transform(X_train)\n",
        "        X_val   = self.scaler.transform(X_val)\n",
        "        X_test  = self.scaler.transform(X_test)\n",
        "        return (X_train, X_val, X_test) \n",
        "    \n",
        "    def inverse(self, X_train, X_val, X_test):\n",
        "        X_train = self.scaler.inverse_transform(X_train)\n",
        "        X_val   = self.scaler.inverse_transform(X_val)\n",
        "        X_test  = self.scaler.inverse_transform(X_test)\n",
        "        return (X_train, X_val, X_test) "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "source": [
        "def get_random_samples(initial_samples, X_train_full,\n",
        "                         y_train_full):\n",
        "\n",
        "    random_state = check_random_state(0)\n",
        "\n",
        "    permutation = np.random.choice(len(X_train_full),initial_samples,replace=False)\n",
        "    \n",
        "    # print ()\n",
        "    # print(type(permutation))\n",
        "    # print ('initial random chosen samples', permutation)\n",
        "\n",
        "    X_train = X_train_full[permutation]\n",
        "    y_train = y_train_full[permutation]\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "\n",
        "    # bin_count = np.bincount(y_train.astype('int64'))\n",
        "    # unique = np.unique(y_train.astype('int64'))\n",
        "    # print (\n",
        "    #     'initial train set:',\n",
        "    #     X_train.shape,\n",
        "    #     y_train.shape,\n",
        "    #     'unique(labels):',\n",
        "    #     bin_count,\n",
        "    #     unique,\n",
        "    #     )\n",
        "    return (permutation, X_train, y_train)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "source": [
        "class TheAlgorithm(object):\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    def __init__(self, step, model_object, selection_function):\n",
        "        self.step = step\n",
        "        self.model_object = model_object\n",
        "        self.sample_selection_function = selection_function\n",
        "        \n",
        "# To-do: Move initiation selections as arguments\n",
        "    def run(self, X_train_full, y_train_full, X_test, y_test, initial_queried, max_queried):\n",
        "\n",
        "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
        "        (permutation, X_train, y_train) = \\\n",
        "            get_random_samples(initial_queried, X_train_full, y_train_full)\n",
        "        self.queried = initial_queried\n",
        "        # self.samplecount = [self.initiation_selections]\n",
        "\n",
        "        # assign the val set the rest of the 'unlabelled' training data\n",
        "        X_val = np.array([])\n",
        "        y_val = np.array([])\n",
        "        X_val = np.copy(X_train_full)\n",
        "        X_val = np.delete(X_val, permutation, axis=0)\n",
        "        y_val = np.copy(y_train_full)\n",
        "        y_val = np.delete(y_val, permutation, axis=0)\n",
        "        # print ('Val set:', X_val.shape, y_val.shape, permutation.shape)\n",
        "        # print ()\n",
        "\n",
        "        # normalize data\n",
        "        normalizer = Normalize()\n",
        "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)\n",
        "           \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test)\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(1, y_test)\n",
        "\n",
        "        # queried_num = [self.queried]\n",
        "\n",
        "        while self.queried <= max_queried-self.step:\n",
        "\n",
        "            active_iteration += 1\n",
        "            self.queried += self.step\n",
        "            # queried_num.append(self.queried)\n",
        "            # get validation probabilities\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
        "            # print('Classifier class:', self.clf_model.model_object.classifier.classes_)\n",
        "            # print('Probas_val:', probas_val)\n",
        "            # pred_val = \\\n",
        "            #     self.clf_model.val_y_predicted\n",
        "            # model_val = \\\n",
        "            #     self.clf_model\n",
        "            # print ('val predicted:',\n",
        "            #         self.clf_model.val_y_predicted.shape,\n",
        "            #         self.clf_model.val_y_predicted)\n",
        "            # display probability of binary value predictions of the validation set\n",
        "            # print ('probas_val value', probas_val)\n",
        "            # display which binary value has the highest probabilities of the validation set\n",
        "            # print ('probabilities:', probas_val.shape, '\\n',\n",
        "            #        np.argmax(probas_val, axis=1))\n",
        "\n",
        "            # select samples using a selection function\n",
        "            uncertain_samples = self.sample_selection_function.pool_select(probas_val, self.step)\n",
        "\n",
        "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
        "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
        "\n",
        "            # get the uncertain samples from the validation set\n",
        "            # print ('trainset before adding uncertain samples', X_train.shape, y_train.shape)\n",
        "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
        "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
        "            # print ('trainset after adding uncertain samples', X_train.shape, y_train.shape)\n",
        "            # self.samplecount.append(X_train.shape[0])\n",
        "\n",
        "            \n",
        "\n",
        "            # bin_count = np.bincount(y_train.astype('int64'))\n",
        "            # unique = np.unique(y_train.astype('int64'))\n",
        "            # print (\n",
        "            #     'updated train set:',\n",
        "            #     X_train.shape,\n",
        "            #     y_train.shape,\n",
        "            #     'unique(labels):',\n",
        "            #     bin_count,\n",
        "            #     unique,\n",
        "            #     )\n",
        "\n",
        "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
        "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
        "            # print ('val set:', X_val.shape, y_val.shape)\n",
        "            # print ()\n",
        "\n",
        "            # normalize again after creating the 'new' train/test sets\n",
        "            normalizer = Normalize()\n",
        "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
        "\n",
        "            # self.queried += self.step\n",
        "\n",
        "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test)\n",
        "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
        "\n",
        "        # print('Queried numbers', queried_num)\n",
        "        return self.clf_model.accuracies\n",
        "        # print ('final active learning accuracies',\n",
        "        #        self.clf_model.accuracies)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "source": [
        "def pool_experiment(model,sampling_method,max_queried,initial_queried,step):\n",
        "    (X, y) = fetch_data_mnist()\n",
        "    # (X_train_full, X_test,y_train_full,  y_test) = train_test_split(X, y, test_size=0.25)\n",
        "    # print(type(X_train_full))\n",
        "\n",
        "    kf = KFold(n_splits=4)\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train_full, X_test = X[train_index], X[test_index]\n",
        "        y_train_full, y_test = y[train_index], y[test_index]\n",
        "        \n",
        "    act_alg = TheAlgorithm(step, model , sampling_method)\n",
        "    accuracies = act_alg.run(X_train_full,y_train_full,X_test,y_test,initial_queried,max_queried)\n",
        "\n",
        "    (permutation, X_train_selected, y_train_selected) = get_random_samples(initial_queried, X_train_full, y_train_full)\n",
        "    original_accuracies=[]\n",
        "    classifier_original = LogisticRegression(\n",
        "            # C=50. / train_samples,\n",
        "            penalty='l2',\n",
        "            solver='liblinear',\n",
        "            tol=0.1\n",
        "            )\n",
        "    x_axis = []\n",
        "    for i in range(initial_queried-1,max_queried,step):\n",
        "        classifier_original.fit(X_train_selected[:i+1], y_train_selected[:i+1])\n",
        "        y_pred_original = classifier_original.predict(X_test)\n",
        "        original_accuracies.append(accuracy_score(y_test, y_pred_original)*100)\n",
        "        x_axis.append(i+1)\n",
        "    print(\"accuracies\",accuracies)\n",
        "    print(\"nonactive_accuracies\",original_accuracies)\n",
        "    # print(\"x-axis:\",x_axis)\n",
        "    # print(\"x-axis length:\",len(x_axis))\n",
        "    # x_axis = np.linspace(initial_queried,max_queried,num=(max_queried - initial_queried)//step +1,endpoint=True)\n",
        "    plt.plot(x_axis, accuracies, 'r',label='active')\n",
        "    plt.plot(x_axis, original_accuracies, 'blue',label='non-active')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Sample Size')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "source": [
        "# print ('train:', X_train_full.shape, y_train_full.shape)\n",
        "# print ('test :', X_test.shape, y_test.shape)\n",
        "# classes = len(np.unique(y))\n",
        "# print ('unique classes', classes)\n",
        "\n",
        "# def pickle_save(fname, data):\n",
        "#   filehandler = open(fname,\"wb\")\n",
        "#   pickle.dump(data,filehandler)\n",
        "#   filehandler.close()\n",
        "#   print('saved', fname, os.getcwd(), os.listdir())\n",
        "\n",
        "# def pickle_load(fname):\n",
        "#   print(os.getcwd(), os.listdir())\n",
        "#   file = open(fname,'rb')\n",
        "#   data = pickle.load(file)\n",
        "#   file.close()\n",
        "#   print(data)\n",
        "#   return data\n",
        "  \n",
        "# def batch_experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
        "\n",
        "#     (X, y) = data_prep()\n",
        "#     # (X_train_full, X_test,y_train_full,  y_test) = train_test_split(X, y, test_size=0.25)\n",
        "#     # print(type(X_train_full))\n",
        "\n",
        "#     from sklearn.model_selection import KFold\n",
        "#     kf = KFold(n_splits=4)\n",
        "\n",
        "#     for train_index, test_index in kf.split(X):\n",
        "#         X_train_full, X_test = X[train_index], X[test_index]\n",
        "#         y_train_full, y_test = y[train_index], y[test_index]\n",
        "\n",
        "#     algos_temp = []\n",
        "#     print ('stopping at:', max_queried)\n",
        "#     count = 0\n",
        "#     for model_object in models:\n",
        "#       if model_object.__name__ not in d:\n",
        "#           d[model_object.__name__] = {}\n",
        "      \n",
        "#       for selection_function in selection_functions:\n",
        "#         if selection_function.__name__ not in d[model_object.__name__]:\n",
        "#             d[model_object.__name__][selection_function.__name__] = {}\n",
        "        \n",
        "#         for k in Ks:\n",
        "#             d[model_object.__name__][selection_function.__name__][str(k)] = []           \n",
        "            \n",
        "#             for i in range(0, repeats):\n",
        "#                 count+=1\n",
        "#                 if count >= contfrom:\n",
        "#                     # print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
        "#                     alg = TheAlgorithm(k, \n",
        "#                                        model_object, \n",
        "#                                        selection_function\n",
        "#                                        )\n",
        "#                     alg.run(X_train_full, y_train_full, X_test, y_test)\n",
        "#                     d[model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
        "#                     # fname = '/Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-' + str(count) + '.pkl'\n",
        "#                     # pickle_save(fname, d)\n",
        "#                     # if count % 5 == 0:\n",
        "#                     #     print(json.dumps(d, indent=2, sort_keys=True))\n",
        "#                     # print ()\n",
        "#                     # print ('---------------------------- FINISHED ---------------------------')\n",
        "#                     # print ()\n",
        "#     return d\n",
        "\n",
        "\n",
        "# max_queried = 500 \n",
        "# repeats = 1\n",
        "# models = [LogModel] \n",
        "# # models = [SvmModel, RfModel, LogModel, GDBCModel, KnnModel] \n",
        "# selection_functions = [LeastConfidenceSelection] \n",
        "# # selection_functions = [RandomSelection, MarginSamplingSelection, EntropySelection, MinStdSelection, LeastConfidenceSelection] \n",
        "# Ks = [250,125,50,25,10] \n",
        "# d = {}\n",
        "# stopped_at = -1 \n",
        "# # print('directory dump including pickle files:', os.getcwd(), np.sort(os.listdir()))  \n",
        "# # d = pickle_load('Active-learning-experiment-' + str(stopped_at) + '.pkl')  \n",
        "# # print(json.dumps(d, indent=2, sort_keys=True))\n",
        "# d = batch_experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
        "# # print (d)\n",
        "# # results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
        "# # print(results)\n"
      ],
      "outputs": [],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "source": [
        "# %matplotlib widget\n",
        "# # %matplotlib inline\n",
        "# def performance_plot(fully_supervised_accuracy, dic, models, selection_functions, Ks, repeats):  \n",
        "#     fig, ax = plt.subplots()\n",
        "#     ax.plot([0,500],[fully_supervised_accuracy, fully_supervised_accuracy],label = 'upper-bound')\n",
        "#     for model_object in models:\n",
        "#       for selection_function in selection_functions:\n",
        "#         for idx, k in enumerate(Ks):\n",
        "#             x = np.arange(float(Ks[idx]), 500 + float(Ks[idx]), float(Ks[idx]))            \n",
        "#             Sum = np.array(dic[model_object][selection_function][k][0])\n",
        "#             for i in range(1, repeats):\n",
        "#                 Sum = Sum + np.array(dic[model_object][selection_function][k][i])\n",
        "#             mean = Sum / repeats\n",
        "#             ax.plot(x, mean, label = model_object[0:3] + '-' + selection_function[0:3] + '-' + str(k))\n",
        "#     ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "#     ax.set_xlim([50,500])\n",
        "#     ax.set_ylim([20,83])\n",
        "#     ax.grid(True)\n",
        "#     mplcursors.cursor()\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # models_str = ['SvmModel', 'RfModel', 'LogModel','GDBCModel','KnnModel']\n",
        "# models_str = ['LogModel']\n",
        "# # selection_functions_str = ['RandomSelection', 'MarginSamplingSelection', 'EntropySelection', 'MinStdSelection','LeastConfidenceSelection']\n",
        "# selection_functions_str = ['LeastConfidenceSelection']\n",
        "# Ks_str = ['250','125','50','25','10'] \n",
        "# repeats = 10\n",
        "# # random_forest_upper_bound = 89.\n",
        "# # svm_upper_bound = 87.\n",
        "# log_upper_bound = 87.\n",
        "# # gdbc_upper_bound = 86.\n",
        "# # knn_upper_bound = 86.\n",
        "# total_experiments = len(models_str) * len(selection_functions_str) * len(Ks_str) * repeats\n",
        "\n",
        "# print('So which is the better model? under the stopping condition and hyper parameters')\n",
        "# # performance_plot(random_forest_upper_bound, d, ['RfModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# # performance_plot(svm_upper_bound, d, ['SvmModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# performance_plot(log_upper_bound, d, ['LogModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# # performance_plot(gdbc_upper_bound, d, ['GDBCModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# # performance_plot(log_upper_bound, d, ['KnnModel'] , selection_functions_str    , Ks_str, 1)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "source": [
        "pool_experiment(LogModel,RandomSelection,500,25,5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset :  (1797, 64) (1797,)\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "Accuracy rate is 46.325167 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "Accuracy rate is 56.124722 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "Accuracy rate is 62.583519 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "Accuracy rate is 65.478842 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "Accuracy rate is 71.269488 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "Accuracy rate is 72.383073 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "Accuracy rate is 72.605791 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "Accuracy rate is 71.714922 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "Accuracy rate is 76.391982 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "Accuracy rate is 75.946548 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "Accuracy rate is 75.946548 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "Accuracy rate is 75.501114 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "Accuracy rate is 75.278396 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "Accuracy rate is 75.946548 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "Accuracy rate is 79.287305 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "Accuracy rate is 80.400891 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "Accuracy rate is 82.182628 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "Accuracy rate is 83.073497 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "Accuracy rate is 83.518931 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "Accuracy rate is 83.296214 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "Accuracy rate is 83.073497 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "Accuracy rate is 84.855234 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "Accuracy rate is 84.632517 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "Accuracy rate is 85.968820 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "Accuracy rate is 85.077951 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "Accuracy rate is 84.632517 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "Accuracy rate is 84.855234 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "Accuracy rate is 84.409800 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "Accuracy rate is 85.077951 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "Accuracy rate is 84.855234 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "Accuracy rate is 84.855234 \n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/wenxuanhuang/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Iteration: 32\n",
            "Accuracy rate is 84.409800 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "Accuracy rate is 84.855234 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "Accuracy rate is 84.409800 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "Accuracy rate is 84.187082 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "Accuracy rate is 83.964365 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "Accuracy rate is 84.632517 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "Accuracy rate is 85.746102 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "Accuracy rate is 85.746102 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "Accuracy rate is 85.300668 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "Accuracy rate is 85.746102 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "Accuracy rate is 85.523385 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "Accuracy rate is 86.636971 \n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/wenxuanhuang/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Iteration: 44\n",
            "Accuracy rate is 86.414254 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "Accuracy rate is 86.636971 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "Accuracy rate is 86.414254 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "Accuracy rate is 86.414254 \n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/wenxuanhuang/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Iteration: 48\n",
            "Accuracy rate is 86.636971 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "Accuracy rate is 86.414254 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "Accuracy rate is 86.636971 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 51\n",
            "Accuracy rate is 86.191537 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 52\n",
            "Accuracy rate is 85.968820 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 53\n",
            "Accuracy rate is 85.968820 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 54\n",
            "Accuracy rate is 86.414254 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 55\n",
            "Accuracy rate is 86.191537 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 56\n",
            "Accuracy rate is 86.414254 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 57\n",
            "Accuracy rate is 86.191537 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 58\n",
            "Accuracy rate is 86.414254 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 59\n",
            "Accuracy rate is 87.973274 \n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/wenxuanhuang/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Iteration: 60\n",
            "Accuracy rate is 87.750557 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 61\n",
            "Accuracy rate is 87.750557 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 62\n",
            "Accuracy rate is 87.973274 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 63\n",
            "Accuracy rate is 87.973274 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 64\n",
            "Accuracy rate is 87.082405 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 65\n",
            "Accuracy rate is 87.305122 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 66\n",
            "Accuracy rate is 87.527840 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 67\n",
            "Accuracy rate is 87.973274 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 68\n",
            "Accuracy rate is 87.973274 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 69\n",
            "Accuracy rate is 87.750557 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 70\n",
            "Accuracy rate is 87.750557 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 71\n",
            "Accuracy rate is 87.750557 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 72\n",
            "Accuracy rate is 86.859688 \n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/wenxuanhuang/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Iteration: 73\n",
            "Accuracy rate is 88.195991 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 74\n",
            "Accuracy rate is 88.195991 \n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/wenxuanhuang/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/wenxuanhuang/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Iteration: 75\n",
            "Accuracy rate is 88.195991 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 76\n",
            "Accuracy rate is 88.418708 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 77\n",
            "Accuracy rate is 88.195991 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 78\n",
            "Accuracy rate is 87.527840 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 79\n",
            "Accuracy rate is 87.973274 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 80\n",
            "Accuracy rate is 88.195991 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 81\n",
            "Accuracy rate is 88.418708 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 82\n",
            "Accuracy rate is 88.195991 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 83\n",
            "Accuracy rate is 88.641425 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 84\n",
            "Accuracy rate is 88.418708 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 85\n",
            "Accuracy rate is 88.195991 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 86\n",
            "Accuracy rate is 88.195991 \n",
            "--------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/wenxuanhuang/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Iteration: 87\n",
            "Accuracy rate is 88.641425 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 88\n",
            "Accuracy rate is 88.864143 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 89\n",
            "Accuracy rate is 88.864143 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 90\n",
            "Accuracy rate is 89.309577 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 91\n",
            "Accuracy rate is 88.641425 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 92\n",
            "Accuracy rate is 88.864143 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 93\n",
            "Accuracy rate is 89.086860 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 94\n",
            "Accuracy rate is 89.309577 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 95\n",
            "Accuracy rate is 89.309577 \n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 96\n",
            "Accuracy rate is 89.309577 \n",
            "--------------------------------\n",
            "accuracies [46.32516703786192, 56.12472160356348, 62.58351893095768, 65.47884187082406, 71.26948775055679, 72.38307349665925, 72.60579064587974, 71.71492204899778, 76.39198218262806, 75.94654788418708, 75.94654788418708, 75.50111358574611, 75.27839643652561, 75.94654788418708, 79.28730512249443, 80.40089086859689, 82.18262806236079, 83.07349665924276, 83.51893095768375, 83.29621380846325, 83.07349665924276, 84.85523385300668, 84.63251670378618, 85.96881959910914, 85.07795100222717, 84.63251670378618, 84.85523385300668, 84.4097995545657, 85.07795100222717, 84.85523385300668, 84.85523385300668, 84.4097995545657, 84.85523385300668, 84.4097995545657, 84.18708240534521, 83.96436525612472, 84.63251670378618, 85.74610244988864, 85.74610244988864, 85.30066815144765, 85.74610244988864, 85.52338530066815, 86.63697104677061, 86.41425389755011, 86.63697104677061, 86.41425389755011, 86.41425389755011, 86.63697104677061, 86.41425389755011, 86.63697104677061, 86.19153674832963, 85.96881959910914, 85.96881959910914, 86.41425389755011, 86.19153674832963, 86.41425389755011, 86.19153674832963, 86.41425389755011, 87.97327394209354, 87.75055679287304, 87.75055679287304, 87.97327394209354, 87.97327394209354, 87.08240534521158, 87.30512249443207, 87.52783964365256, 87.97327394209354, 87.97327394209354, 87.75055679287304, 87.75055679287304, 87.75055679287304, 86.8596881959911, 88.19599109131403, 88.19599109131403, 88.19599109131403, 88.41870824053451, 88.19599109131403, 87.52783964365256, 87.97327394209354, 88.19599109131403, 88.41870824053451, 88.19599109131403, 88.64142538975501, 88.41870824053451, 88.19599109131403, 88.19599109131403, 88.64142538975501, 88.8641425389755, 88.8641425389755, 89.30957683741649, 88.64142538975501, 88.8641425389755, 89.086859688196, 89.30957683741649, 89.30957683741649, 89.30957683741649]\n",
            "nonactive_accuracies [58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354, 58.797327394209354]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d",
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "name": "Logistic.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}