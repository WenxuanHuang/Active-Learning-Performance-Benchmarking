{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic.ipynb",
      "provenance": [],
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.2 64-bit"
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    },
    "interpreter": {
      "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from scipy.special import expit\n",
        "from scipy import stats\n",
        "from pylab import rcParams\n",
        "import mplcursors\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, \\\n",
        "    GradientBoostingClassifier\n",
        "from sklearn import neighbors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "# pd.options.display.max_rows = 20\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "max_queried = 500\n",
        "trainset_size = 1302"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        " def data_prep():\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/ML-for-COVID-19-dataset/main/all_training.csv\", sep=',')\n",
        "    # Column selection\n",
        "    df = data.iloc[:,np.r_[3:34]].copy()\n",
        "    # define row and column index\n",
        "    col = df.columns\n",
        "    row = [i for i in range(df.shape[0])]\n",
        "    # define imputer\n",
        "    imputer = IterativeImputer(estimator=linear_model.BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
        "    # fit on the dataset\n",
        "    imputer.fit(df)\n",
        "    # transform the dataset\n",
        "    df_imputed = imputer.transform(df)\n",
        "    # convert back to pandas dataframe and rename back to df_normalized\n",
        "    df = pd.DataFrame(data=df_imputed, index=row, columns=col)\n",
        "    X = df\n",
        "    y = data.target    \n",
        "    # Recursive feature elimination\n",
        "    rdmreg = RandomForestClassifier(n_estimators=100)\n",
        "    # Define the method\n",
        "    rfe = RFE(estimator=rdmreg, n_features_to_select=10)\n",
        "    # Fit the model\n",
        "    rfe = rfe.fit(X, y.values.ravel())\n",
        "    print(rfe.support_)\n",
        "    # Drop columns that failed RFE test\n",
        "    col = df.columns[rfe.support_]\n",
        "    X = X[col]\n",
        "    X = X.to_numpy()\n",
        "    print ('df:', X.shape, y.shape)\n",
        "    return (X, y)\n",
        "\n",
        "\n",
        "def split(train_size):\n",
        "    X_train_full = X[:train_size]\n",
        "    y_train_full = y[:train_size]\n",
        "    X_test = X[train_size:]\n",
        "    y_test = y[train_size:]   \n",
        "    return (X_train_full, y_train_full, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SvmModel(BaseModel):\n",
        "\n",
        "    model_type = 'Support Vector Machine with linear Kernel'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training svm...')\n",
        "        self.classifier = SVC(\n",
        "            C=1, \n",
        "            kernel='linear', \n",
        "            probability=True,\n",
        "            class_weight=c_weight\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    model_type = 'Logistic Regression' \n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training logistic regression...')\n",
        "        train_samples = X_train.shape[0]\n",
        "        self.classifier = LogisticRegression(\n",
        "            C=50. / train_samples,\n",
        "            penalty='l1',\n",
        "            solver='liblinear',\n",
        "            tol=0.1,\n",
        "            class_weight=c_weight\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class RfModel(BaseModel):\n",
        "\n",
        "    model_type = 'Random Forest'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training random forest...')\n",
        "        self.classifier = RandomForestClassifier(\n",
        "            n_estimators=500, \n",
        "            class_weight=c_weight, \n",
        "            n_jobs=-1\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
        "\n",
        "class GDBCModel(BaseModel):\n",
        "\n",
        "    model_type = 'Gradient Boost classifier'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training GDBC...')\n",
        "        self.classifier = GradientBoostingClassifier(\n",
        "            n_estimators=1200,\n",
        "            max_depth=3,\n",
        "            subsample=0.5,\n",
        "            learning_rate=0.01,\n",
        "            min_samples_leaf=1,\n",
        "            random_state=3\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
        "\n",
        "class KnnModel(BaseModel):\n",
        "\n",
        "    model_type = 'Nearest Neighbour classifier'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training KNN...')\n",
        "        self.classifier = neighbors.KNeighborsClassifier(\n",
        "            n_neighbors = 10,\n",
        "            n_jobs = -1\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    # we train normally and use the probabilities to select the most uncertain samples\n",
        "    def train(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('Train set:', X_train.shape)\n",
        "        print ('Validation set:', X_val.shape)\n",
        "        print ('Test set:', X_test.shape)\n",
        "        t0 = time.time()\n",
        "        (X_train, X_val, X_test, self.val_y_predicted,\n",
        "         self.test_y_predicted) = \\\n",
        "            self.model_object.fit_predict(X_train, y_train, X_val, X_test, c_weight)\n",
        "        self.run_time = time.time() - t0\n",
        "        return (X_train, X_val, X_test)\n",
        "\n",
        "    # we want accuracy only for the test set\n",
        "    def get_test_accuracy(self, i, y_test):\n",
        "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        print('--------------------------------')\n",
        "        print('y-test set:',y_test.shape)\n",
        "        print('Training run in %.3f s' % self.run_time,'\\n')\n",
        "        print(\"Accuracy rate is %f \" % (classif_rate))    \n",
        "        print(\"Classification report for %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
        "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
        "        print('--------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseSelectionFunction(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def select(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomSelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        random_state = check_random_state(0)\n",
        "        selection = np.random.choice(probas_val.shape[0], initial_labeled_samples, replace=False)\n",
        "        print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',initial_labeled_samples)\n",
        "        return selection\n",
        "\n",
        "\n",
        "class EntropySelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "        selection = (np.argsort(e)[::-1])[:initial_labeled_samples]\n",
        "        return selection\n",
        "\n",
        "class MinStdSelection(BaseSelectionFunction):\n",
        "\n",
        "    # select the samples where the std is smallest. There is uncertainty regarding the relevant class\n",
        "    # and then train on these \"hard\" to classify samples.\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        std = np.std(probas_val * 100, axis=1)\n",
        "        selection = std.argsort()[:initial_labeled_samples]\n",
        "        selection = selection.astype('int64')\n",
        "        print('std',std.shape,std)\n",
        "        print('selection',selection, selection.shape, std[selection])\n",
        "        return selection\n",
        "      \n",
        "      \n",
        "class MarginSamplingSelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        rev = np.sort(probas_val, axis=1)[:, ::-1]\n",
        "        values = rev[:, 0] - rev[:, 1]\n",
        "        selection = np.argsort(values)[:initial_labeled_samples]\n",
        "        return selection\n",
        "\n",
        "# class MonteCarloSelection(BaseSelectionFunction):\n",
        "\n",
        "# # not working yet\n",
        "#      @staticmethod\n",
        "#      def select(initial_labeled_samples):\n",
        "#         for i in X_val:\n",
        "#             X_train = np.concatenate(X_val, i, axis=0)\n",
        "#             probas_train_with_i[i] = (self.clf_model.model_object.classifier.predict_proba(X_train))\n",
        "#         selection = (np.argsort(probas_train_with_i)[::-1])[:initial_labeled_samples]\n",
        "#         return selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Normalize(object):\n",
        "    \n",
        "    def normalize(self, X_train, X_val, X_test):\n",
        "        self.scaler = RobustScaler()\n",
        "        X_train = self.scaler.fit_transform(X_train)\n",
        "        X_val   = self.scaler.transform(X_val)\n",
        "        X_test  = self.scaler.transform(X_test)\n",
        "        return (X_train, X_val, X_test) \n",
        "    \n",
        "    def inverse(self, X_train, X_val, X_test):\n",
        "        X_train = self.scaler.inverse_transform(X_train)\n",
        "        X_val   = self.scaler.inverse_transform(X_val)\n",
        "        X_test  = self.scaler.inverse_transform(X_test)\n",
        "        return (X_train, X_val, X_test) "
      ]
    },
    {
      "source": [
        "def get_k_random_samples(initial_labeled_samples, X_train_full,\n",
        "                         y_train_full):\n",
        "    random_state = check_random_state(0)\n",
        "    permutation = np.random.choice(trainset_size,\n",
        "                                   initial_labeled_samples,\n",
        "                                   replace=False)\n",
        "    print ()\n",
        "    print ('initial random chosen samples', permutation.shape),\n",
        "    X_train = X_train_full[permutation]\n",
        "    y_train = y_train_full[permutation]\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "    bin_count = np.bincount(y_train.astype('int64'))\n",
        "    unique = np.unique(y_train.astype('int64'))\n",
        "    print (\n",
        "        'initial train set:',\n",
        "        X_train.shape,\n",
        "        y_train.shape,\n",
        "        'unique(labels):',\n",
        "        bin_count,\n",
        "        unique,\n",
        "        )\n",
        "    return (permutation, X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {},
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TheAlgorithm(object):\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    def __init__(self, initial_labeled_samples, model_object, selection_function):\n",
        "        self.initial_labeled_samples = initial_labeled_samples\n",
        "        self.model_object = model_object\n",
        "        self.sample_selection_function = selection_function\n",
        "\n",
        "    def run(self, X_train_full, y_train_full, X_test, y_test):\n",
        "\n",
        "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
        "        (permutation, X_train, y_train) = \\\n",
        "            get_k_random_samples(self.initial_labeled_samples,\n",
        "                                 X_train_full, y_train_full)\n",
        "        self.queried = self.initial_labeled_samples\n",
        "        self.samplecount = [self.initial_labeled_samples]\n",
        "\n",
        "        # assign the val set the rest of the 'unlabelled' training data\n",
        "        X_val = np.array([])\n",
        "        y_val = np.array([])\n",
        "        X_val = np.copy(X_train_full)\n",
        "        X_val = np.delete(X_val, permutation, axis=0)\n",
        "        y_val = np.copy(y_train_full)\n",
        "        y_val = np.delete(y_val, permutation, axis=0)\n",
        "        print ('Val set:', X_val.shape, y_val.shape, permutation.shape)\n",
        "        print ()\n",
        "\n",
        "        # normalize data\n",
        "        normalizer = Normalize()\n",
        "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)   \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(1, y_test)\n",
        "\n",
        "        while self.queried < max_queried:\n",
        "\n",
        "            active_iteration += 1\n",
        "\n",
        "            # get validation probabilities\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
        "            print ('val predicted:',\n",
        "                   self.clf_model.val_y_predicted.shape,\n",
        "                   self.clf_model.val_y_predicted)\n",
        "            print ('probabilities:', probas_val.shape, '\\n',\n",
        "                   np.argmax(probas_val, axis=1))\n",
        "\n",
        "            # select samples using a selection function\n",
        "            uncertain_samples = \\\n",
        "                self.sample_selection_function.select(probas_val, self.initial_labeled_samples)\n",
        "\n",
        "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
        "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
        "\n",
        "            # get the uncertain samples from the validation set\n",
        "            print ('trainset before adding uncertain samples', X_train.shape, y_train.shape)\n",
        "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
        "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
        "            print ('trainset after adding uncertain samples', X_train.shape, y_train.shape)\n",
        "            self.samplecount.append(X_train.shape[0])\n",
        "\n",
        "            bin_count = np.bincount(y_train.astype('int64'))\n",
        "            unique = np.unique(y_train.astype('int64'))\n",
        "            print (\n",
        "                'updated train set:',\n",
        "                X_train.shape,\n",
        "                y_train.shape,\n",
        "                'unique(labels):',\n",
        "                bin_count,\n",
        "                unique,\n",
        "                )\n",
        "\n",
        "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
        "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
        "            print ('val set:', X_val.shape, y_val.shape)\n",
        "            print ()\n",
        "\n",
        "            # normalize again after creating the 'new' train/test sets\n",
        "            normalizer = Normalize()\n",
        "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
        "\n",
        "            self.queried += self.initial_labeled_samples\n",
        "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
        "\n",
        "        print ('final active learning accuracies',\n",
        "               self.clf_model.accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True False False False False False  True False  True  True False False\n",
            " False  True  True False False False False False False False False False\n",
            "  True False  True  True False  True False]\n",
            "df: (1736, 10) (1736,)\n",
            "train: (1302, 10) (1302,)\n",
            "test : (434, 10) (434,)\n",
            "unique classes 2\n",
            "stopping at: 500\n",
            "Count = 1, using model = LogModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial labeled samples size 250\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 10) (250,) unique(labels): [111 139] [0 1]\n",
            "Val set: (1052, 10) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "uniques chosen: 250 <= should be equal to: 250\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [230 270] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "final active learning accuracies [80.4147465437788, 79.95391705069125]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-1.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 2, using model = LogModel, selection_function = RandomSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial labeled samples size 125\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 10) (125,) unique(labels): [57 68] [0 1]\n",
            "Val set: (1177, 10) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.64      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.67      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "uniques chosen: 125 <= should be equal to: 125\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [115 135] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "uniques chosen: 125 <= should be equal to: 125\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [172 203] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0]\n",
            "uniques chosen: 125 <= should be equal to: 125\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [242 258] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [79.03225806451613, 79.03225806451613, 79.72350230414746, 79.03225806451613]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-2.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 3, using model = LogModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial labeled samples size 50\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 10) (50,) unique(labels): [15 35] [0 1]\n",
            "Val set: (1252, 10) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.958525 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [38 62] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 71.428571 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.76      0.80       321\n",
            "           1       0.46      0.59      0.52       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.67      0.66       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[243  78]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [68 82] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 91 109] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [110 140] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [136 164] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [161 189] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [184 216] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 1 0 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [199 251] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0]\n",
            "uniques chosen: 50 <= should be equal to: 50\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [221 279] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.95852534562212, 71.42857142857143, 78.3410138248848, 78.3410138248848, 78.80184331797236, 79.26267281105991, 79.72350230414746, 79.26267281105991, 79.72350230414746, 79.26267281105991]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-3.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 4, using model = LogModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial labeled samples size 25\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 10) (25,) unique(labels): [10 15] [0 1]\n",
            "Val set: (1277, 10) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 10)\n",
            "Validation set: (1277, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=2.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (25, 10) (25,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [22 28] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (75, 10) (75,)\n",
            "updated train set: (75, 10) (75,) unique(labels): [31 44] [0 1]\n",
            "val set: (1227, 10) (1227,)\n",
            "\n",
            "Train set: (75, 10)\n",
            "Validation set: (1227, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.6666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (75, 10) (75,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [41 59] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (125, 10) (125,)\n",
            "updated train set: (125, 10) (125,) unique(labels): [56 69] [0 1]\n",
            "val set: (1177, 10) (1177,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.61      0.44      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.67      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [70 80] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.649770 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.59      0.45      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.68       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (175, 10) (175,)\n",
            "updated train set: (175, 10) (175,) unique(labels): [82 93] [0 1]\n",
            "val set: (1127, 10) (1127,)\n",
            "\n",
            "Train set: (175, 10)\n",
            "Validation set: (1127, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.958525 \n",
            "Classification report for LogisticRegression(C=0.2857142857142857, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.58      0.43      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (175, 10) (175,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 98 102] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (225, 10) (225,)\n",
            "updated train set: (225, 10) (225,) unique(labels): [105 120] [0 1]\n",
            "val set: (1077, 10) (1077,)\n",
            "\n",
            "Train set: (225, 10)\n",
            "Validation set: (1077, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.2222222222222222, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (225, 10) (225,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [119 131] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (275, 10) (275,)\n",
            "updated train set: (275, 10) (275,) unique(labels): [131 144] [0 1]\n",
            "val set: (1027, 10) (1027,)\n",
            "\n",
            "Train set: (275, 10)\n",
            "Validation set: (1027, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.18181818181818182, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (275, 10) (275,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [143 157] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (325, 10) (325,)\n",
            "updated train set: (325, 10) (325,) unique(labels): [159 166] [0 1]\n",
            "val set: (977, 10) (977,)\n",
            "\n",
            "Train set: (325, 10)\n",
            "Validation set: (977, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.15384615384615385, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.63      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (325, 10) (325,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [174 176] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [184 191] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0\n",
            " 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [199 201] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (425, 10) (425,)\n",
            "updated train set: (425, 10) (425,) unique(labels): [209 216] [0 1]\n",
            "val set: (877, 10) (877,)\n",
            "\n",
            "Train set: (425, 10)\n",
            "Validation set: (877, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.11764705882352941, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (425, 10) (425,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [220 230] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (475, 10) (475,)\n",
            "updated train set: (475, 10) (475,) unique(labels): [231 244] [0 1]\n",
            "val set: (827, 10) (827,)\n",
            "\n",
            "Train set: (475, 10)\n",
            "Validation set: (827, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.10526315789473684, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0]\n",
            "uniques chosen: 25 <= should be equal to: 25\n",
            "trainset before adding uncertain samples (475, 10) (475,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [241 259] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "final active learning accuracies [80.4147465437788, 79.95391705069125, 79.49308755760369, 80.64516129032258, 78.11059907834101, 77.64976958525345, 76.95852534562212, 79.95391705069125, 80.18433179723502, 79.26267281105991, 79.49308755760369, 79.49308755760369, 79.26267281105991, 79.03225806451613, 79.72350230414746, 79.95391705069125, 79.03225806451613, 79.03225806451613, 79.49308755760369, 79.03225806451613]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-4.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 5, using model = LogModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial labeled samples size 10\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 10) (10,) unique(labels): [7 3] [0 1]\n",
            "Val set: (1292, 10) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 10)\n",
            "Validation set: (1292, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 77.188940 \n",
            "Classification report for LogisticRegression(C=5.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.56      0.57      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (10, 10) (10,)\n",
            "trainset after adding uncertain samples (20, 10) (20,)\n",
            "updated train set: (20, 10) (20,) unique(labels): [12  8] [0 1]\n",
            "val set: (1282, 10) (1282,)\n",
            "\n",
            "Train set: (20, 10)\n",
            "Validation set: (1282, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=2.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (20, 10) (20,)\n",
            "trainset after adding uncertain samples (30, 10) (30,)\n",
            "updated train set: (30, 10) (30,) unique(labels): [16 14] [0 1]\n",
            "val set: (1272, 10) (1272,)\n",
            "\n",
            "Train set: (30, 10)\n",
            "Validation set: (1272, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=1.6666666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (30, 10) (30,)\n",
            "trainset after adding uncertain samples (40, 10) (40,)\n",
            "updated train set: (40, 10) (40,) unique(labels): [20 20] [0 1]\n",
            "val set: (1262, 10) (1262,)\n",
            "\n",
            "Train set: (40, 10)\n",
            "Validation set: (1262, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=1.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (40, 10) (40,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [22 28] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (60, 10) (60,)\n",
            "updated train set: (60, 10) (60,) unique(labels): [27 33] [0 1]\n",
            "val set: (1242, 10) (1242,)\n",
            "\n",
            "Train set: (60, 10)\n",
            "Validation set: (1242, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.8333333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87       321\n",
            "           1       0.62      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.73      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (60, 10) (60,)\n",
            "trainset after adding uncertain samples (70, 10) (70,)\n",
            "updated train set: (70, 10) (70,) unique(labels): [30 40] [0 1]\n",
            "val set: (1232, 10) (1232,)\n",
            "\n",
            "Train set: (70, 10)\n",
            "Validation set: (1232, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.7142857142857143, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (70, 10) (70,)\n",
            "trainset after adding uncertain samples (80, 10) (80,)\n",
            "updated train set: (80, 10) (80,) unique(labels): [34 46] [0 1]\n",
            "val set: (1222, 10) (1222,)\n",
            "\n",
            "Train set: (80, 10)\n",
            "Validation set: (1222, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (80, 10) (80,)\n",
            "trainset after adding uncertain samples (90, 10) (90,)\n",
            "updated train set: (90, 10) (90,) unique(labels): [41 49] [0 1]\n",
            "val set: (1212, 10) (1212,)\n",
            "\n",
            "Train set: (90, 10)\n",
            "Validation set: (1212, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.5555555555555556, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.69      0.54      0.60       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.77      0.73      0.74       434\n",
            "weighted avg       0.81      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (90, 10) (90,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [48 52] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (110, 10) (110,)\n",
            "updated train set: (110, 10) (110,) unique(labels): [51 59] [0 1]\n",
            "val set: (1192, 10) (1192,)\n",
            "\n",
            "Train set: (110, 10)\n",
            "Validation set: (1192, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.45454545454545453, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.67      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (110, 10) (110,)\n",
            "trainset after adding uncertain samples (120, 10) (120,)\n",
            "updated train set: (120, 10) (120,) unique(labels): [54 66] [0 1]\n",
            "val set: (1182, 10) (1182,)\n",
            "\n",
            "Train set: (120, 10)\n",
            "Validation set: (1182, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.4166666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (120, 10) (120,)\n",
            "trainset after adding uncertain samples (130, 10) (130,)\n",
            "updated train set: (130, 10) (130,) unique(labels): [58 72] [0 1]\n",
            "val set: (1172, 10) (1172,)\n",
            "\n",
            "Train set: (130, 10)\n",
            "Validation set: (1172, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.38461538461538464, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (130, 10) (130,)\n",
            "trainset after adding uncertain samples (140, 10) (140,)\n",
            "updated train set: (140, 10) (140,) unique(labels): [62 78] [0 1]\n",
            "val set: (1162, 10) (1162,)\n",
            "\n",
            "Train set: (140, 10)\n",
            "Validation set: (1162, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.35714285714285715, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (140, 10) (140,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [65 85] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (160, 10) (160,)\n",
            "updated train set: (160, 10) (160,) unique(labels): [71 89] [0 1]\n",
            "val set: (1142, 10) (1142,)\n",
            "\n",
            "Train set: (160, 10)\n",
            "Validation set: (1142, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.3125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (160, 10) (160,)\n",
            "trainset after adding uncertain samples (170, 10) (170,)\n",
            "updated train set: (170, 10) (170,) unique(labels): [77 93] [0 1]\n",
            "val set: (1132, 10) (1132,)\n",
            "\n",
            "Train set: (170, 10)\n",
            "Validation set: (1132, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.29411764705882354, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (170, 10) (170,)\n",
            "trainset after adding uncertain samples (180, 10) (180,)\n",
            "updated train set: (180, 10) (180,) unique(labels): [83 97] [0 1]\n",
            "val set: (1122, 10) (1122,)\n",
            "\n",
            "Train set: (180, 10)\n",
            "Validation set: (1122, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.2777777777777778, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (180, 10) (180,)\n",
            "trainset after adding uncertain samples (190, 10) (190,)\n",
            "updated train set: (190, 10) (190,) unique(labels): [ 86 104] [0 1]\n",
            "val set: (1112, 10) (1112,)\n",
            "\n",
            "Train set: (190, 10)\n",
            "Validation set: (1112, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.2631578947368421, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (190, 10) (190,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 91 109] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (210, 10) (210,)\n",
            "updated train set: (210, 10) (210,) unique(labels): [ 98 112] [0 1]\n",
            "val set: (1092, 10) (1092,)\n",
            "\n",
            "Train set: (210, 10)\n",
            "Validation set: (1092, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.23809523809523808, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (210, 10) (210,)\n",
            "trainset after adding uncertain samples (220, 10) (220,)\n",
            "updated train set: (220, 10) (220,) unique(labels): [104 116] [0 1]\n",
            "val set: (1082, 10) (1082,)\n",
            "\n",
            "Train set: (220, 10)\n",
            "Validation set: (1082, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.22727272727272727, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (220, 10) (220,)\n",
            "trainset after adding uncertain samples (230, 10) (230,)\n",
            "updated train set: (230, 10) (230,) unique(labels): [108 122] [0 1]\n",
            "val set: (1072, 10) (1072,)\n",
            "\n",
            "Train set: (230, 10)\n",
            "Validation set: (1072, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.21739130434782608, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.88       321\n",
            "           1       0.67      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (230, 10) (230,)\n",
            "trainset after adding uncertain samples (240, 10) (240,)\n",
            "updated train set: (240, 10) (240,) unique(labels): [113 127] [0 1]\n",
            "val set: (1062, 10) (1062,)\n",
            "\n",
            "Train set: (240, 10)\n",
            "Validation set: (1062, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.20833333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (240, 10) (240,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [119 131] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.88       321\n",
            "           1       0.67      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (260, 10) (260,)\n",
            "updated train set: (260, 10) (260,) unique(labels): [124 136] [0 1]\n",
            "val set: (1042, 10) (1042,)\n",
            "\n",
            "Train set: (260, 10)\n",
            "Validation set: (1042, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.19230769230769232, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (260, 10) (260,)\n",
            "trainset after adding uncertain samples (270, 10) (270,)\n",
            "updated train set: (270, 10) (270,) unique(labels): [131 139] [0 1]\n",
            "val set: (1032, 10) (1032,)\n",
            "\n",
            "Train set: (270, 10)\n",
            "Validation set: (1032, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.18518518518518517, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (270, 10) (270,)\n",
            "trainset after adding uncertain samples (280, 10) (280,)\n",
            "updated train set: (280, 10) (280,) unique(labels): [137 143] [0 1]\n",
            "val set: (1022, 10) (1022,)\n",
            "\n",
            "Train set: (280, 10)\n",
            "Validation set: (1022, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.17857142857142858, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.65      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (280, 10) (280,)\n",
            "trainset after adding uncertain samples (290, 10) (290,)\n",
            "updated train set: (290, 10) (290,) unique(labels): [142 148] [0 1]\n",
            "val set: (1012, 10) (1012,)\n",
            "\n",
            "Train set: (290, 10)\n",
            "Validation set: (1012, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.1724137931034483, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (290, 10) (290,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [147 153] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (310, 10) (310,)\n",
            "updated train set: (310, 10) (310,) unique(labels): [152 158] [0 1]\n",
            "val set: (992, 10) (992,)\n",
            "\n",
            "Train set: (310, 10)\n",
            "Validation set: (992, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.16129032258064516, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (310, 10) (310,)\n",
            "trainset after adding uncertain samples (320, 10) (320,)\n",
            "updated train set: (320, 10) (320,) unique(labels): [160 160] [0 1]\n",
            "val set: (982, 10) (982,)\n",
            "\n",
            "Train set: (320, 10)\n",
            "Validation set: (982, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.15625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (320, 10) (320,)\n",
            "trainset after adding uncertain samples (330, 10) (330,)\n",
            "updated train set: (330, 10) (330,) unique(labels): [166 164] [0 1]\n",
            "val set: (972, 10) (972,)\n",
            "\n",
            "Train set: (330, 10)\n",
            "Validation set: (972, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.15151515151515152, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (330, 10) (330,)\n",
            "trainset after adding uncertain samples (340, 10) (340,)\n",
            "updated train set: (340, 10) (340,) unique(labels): [171 169] [0 1]\n",
            "val set: (962, 10) (962,)\n",
            "\n",
            "Train set: (340, 10)\n",
            "Validation set: (962, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.14705882352941177, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (340, 10) (340,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [177 173] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (360, 10) (360,)\n",
            "updated train set: (360, 10) (360,) unique(labels): [180 180] [0 1]\n",
            "val set: (942, 10) (942,)\n",
            "\n",
            "Train set: (360, 10)\n",
            "Validation set: (942, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.1388888888888889, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (360, 10) (360,)\n",
            "trainset after adding uncertain samples (370, 10) (370,)\n",
            "updated train set: (370, 10) (370,) unique(labels): [182 188] [0 1]\n",
            "val set: (932, 10) (932,)\n",
            "\n",
            "Train set: (370, 10)\n",
            "Validation set: (932, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.13513513513513514, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 1 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (370, 10) (370,)\n",
            "trainset after adding uncertain samples (380, 10) (380,)\n",
            "updated train set: (380, 10) (380,) unique(labels): [187 193] [0 1]\n",
            "val set: (922, 10) (922,)\n",
            "\n",
            "Train set: (380, 10)\n",
            "Validation set: (922, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.13157894736842105, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0\n",
            " 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0\n",
            " 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (380, 10) (380,)\n",
            "trainset after adding uncertain samples (390, 10) (390,)\n",
            "updated train set: (390, 10) (390,) unique(labels): [192 198] [0 1]\n",
            "val set: (912, 10) (912,)\n",
            "\n",
            "Train set: (390, 10)\n",
            "Validation set: (912, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.1282051282051282, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.88       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (390, 10) (390,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [195 205] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.69      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 1 1 0 1 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (410, 10) (410,)\n",
            "updated train set: (410, 10) (410,) unique(labels): [198 212] [0 1]\n",
            "val set: (892, 10) (892,)\n",
            "\n",
            "Train set: (410, 10)\n",
            "Validation set: (892, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(C=0.12195121951219512, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.70      0.50      0.59       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (410, 10) (410,)\n",
            "trainset after adding uncertain samples (420, 10) (420,)\n",
            "updated train set: (420, 10) (420,) unique(labels): [202 218] [0 1]\n",
            "val set: (882, 10) (882,)\n",
            "\n",
            "Train set: (420, 10)\n",
            "Validation set: (882, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.11904761904761904, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.70      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (420, 10) (420,)\n",
            "trainset after adding uncertain samples (430, 10) (430,)\n",
            "updated train set: (430, 10) (430,) unique(labels): [204 226] [0 1]\n",
            "val set: (872, 10) (872,)\n",
            "\n",
            "Train set: (430, 10)\n",
            "Validation set: (872, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.11627906976744186, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (430, 10) (430,)\n",
            "trainset after adding uncertain samples (440, 10) (440,)\n",
            "updated train set: (440, 10) (440,) unique(labels): [208 232] [0 1]\n",
            "val set: (862, 10) (862,)\n",
            "\n",
            "Train set: (440, 10)\n",
            "Validation set: (862, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.11363636363636363, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 1 1 0 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (440, 10) (440,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [216 234] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.88       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (460, 10) (460,)\n",
            "updated train set: (460, 10) (460,) unique(labels): [220 240] [0 1]\n",
            "val set: (842, 10) (842,)\n",
            "\n",
            "Train set: (460, 10)\n",
            "Validation set: (842, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.10869565217391304, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (460, 10) (460,)\n",
            "trainset after adding uncertain samples (470, 10) (470,)\n",
            "updated train set: (470, 10) (470,) unique(labels): [226 244] [0 1]\n",
            "val set: (832, 10) (832,)\n",
            "\n",
            "Train set: (470, 10)\n",
            "Validation set: (832, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.10638297872340426, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (470, 10) (470,)\n",
            "trainset after adding uncertain samples (480, 10) (480,)\n",
            "updated train set: (480, 10) (480,) unique(labels): [231 249] [0 1]\n",
            "val set: (822, 10) (822,)\n",
            "\n",
            "Train set: (480, 10)\n",
            "Validation set: (822, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.10416666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (480, 10) (480,)\n",
            "trainset after adding uncertain samples (490, 10) (490,)\n",
            "updated train set: (490, 10) (490,) unique(labels): [236 254] [0 1]\n",
            "val set: (812, 10) (812,)\n",
            "\n",
            "Train set: (490, 10)\n",
            "Validation set: (812, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.10204081632653061, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0]\n",
            "uniques chosen: 10 <= should be equal to: 10\n",
            "trainset before adding uncertain samples (490, 10) (490,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [242 258] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.65      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.18894009216591, 79.26267281105991, 80.64516129032258, 79.95391705069125, 78.80184331797236, 79.95391705069125, 80.64516129032258, 80.18433179723502, 81.5668202764977, 80.18433179723502, 80.87557603686636, 80.64516129032258, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.72350230414746, 80.18433179723502, 80.4147465437788, 79.72350230414746, 80.4147465437788, 81.10599078341014, 79.72350230414746, 80.87557603686636, 79.49308755760369, 80.87557603686636, 80.4147465437788, 79.95391705069125, 79.95391705069125, 79.49308755760369, 79.72350230414746, 80.64516129032258, 78.80184331797236, 79.95391705069125, 79.26267281105991, 80.64516129032258, 79.49308755760369, 80.18433179723502, 80.18433179723502, 80.64516129032258, 81.10599078341014, 81.5668202764977, 81.33640552995391, 80.87557603686636, 80.4147465437788, 80.64516129032258, 80.64516129032258, 79.49308755760369, 79.95391705069125, 80.64516129032258, 79.95391705069125]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-5.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.26267281105991,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          79.72350230414746,\n",
            "          80.87557603686636,\n",
            "          79.49308755760369,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          81.5668202764977,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          80.4147465437788,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          71.42857142857143,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 6, using model = LogModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial labeled samples size 250\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 10) (250,) unique(labels): [109 141] [0 1]\n",
            "Val set: (1052, 10) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.66      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [192 308] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [79.95391705069125, 80.4147465437788]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-6.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 7, using model = LogModel, selection_function = MarginSamplingSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial labeled samples size 125\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 10) (125,) unique(labels): [60 65] [0 1]\n",
            "Val set: (1177, 10) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [ 94 156] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.63      0.44      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [143 232] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [199 301] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "final active learning accuracies [80.64516129032258, 78.80184331797236, 80.4147465437788, 80.64516129032258]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-7.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 8, using model = LogModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial labeled samples size 50\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 10) (50,) unique(labels): [18 32] [0 1]\n",
            "Val set: (1252, 10) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 69.124424 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.71      0.77       321\n",
            "           1       0.44      0.65      0.52       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.64      0.68      0.65       434\n",
            "weighted avg       0.74      0.69      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[227  94]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [40 60] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 75.345622 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.83       321\n",
            "           1       0.53      0.54      0.53       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.68      0.68       434\n",
            "weighted avg       0.76      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [75 75] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.267281 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.55      0.48      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 95 105] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.728111 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       321\n",
            "           1       0.56      0.47      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [115 135] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 77.419355 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.59      0.45      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [131 169] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.60      0.46      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [151 199] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [170 230] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [193 257] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.66      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [206 294] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [69.12442396313364, 75.34562211981567, 76.26728110599078, 76.72811059907833, 77.41935483870968, 78.11059907834101, 79.26267281105991, 79.49308755760369, 79.95391705069125, 80.4147465437788]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-8.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 9, using model = LogModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial labeled samples size 25\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 10) (25,) unique(labels): [15 10] [0 1]\n",
            "Val set: (1277, 10) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 10)\n",
            "Validation set: (1277, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.267281 \n",
            "Classification report for LogisticRegression(C=2.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.55      0.53      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (25, 10) (25,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [29 21] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (75, 10) (75,)\n",
            "updated train set: (75, 10) (75,) unique(labels): [50 25] [0 1]\n",
            "val set: (1227, 10) (1227,)\n",
            "\n",
            "Train set: (75, 10)\n",
            "Validation set: (1227, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.6666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (75, 10) (75,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [64 36] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.57      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (125, 10) (125,)\n",
            "updated train set: (125, 10) (125,) unique(labels): [82 43] [0 1]\n",
            "val set: (1177, 10) (1177,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [94 56] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.497696 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (175, 10) (175,)\n",
            "updated train set: (175, 10) (175,) unique(labels): [111  64] [0 1]\n",
            "val set: (1127, 10) (1127,)\n",
            "\n",
            "Train set: (175, 10)\n",
            "Validation set: (1127, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.2857142857142857, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (175, 10) (175,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [121  79] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (225, 10) (225,)\n",
            "updated train set: (225, 10) (225,) unique(labels): [133  92] [0 1]\n",
            "val set: (1077, 10) (1077,)\n",
            "\n",
            "Train set: (225, 10)\n",
            "Validation set: (1077, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.2222222222222222, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (225, 10) (225,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [143 107] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (275, 10) (275,)\n",
            "updated train set: (275, 10) (275,) unique(labels): [152 123] [0 1]\n",
            "val set: (1027, 10) (1027,)\n",
            "\n",
            "Train set: (275, 10)\n",
            "Validation set: (1027, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.18181818181818182, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (275, 10) (275,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [160 140] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.70      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (325, 10) (325,)\n",
            "updated train set: (325, 10) (325,) unique(labels): [167 158] [0 1]\n",
            "val set: (977, 10) (977,)\n",
            "\n",
            "Train set: (325, 10)\n",
            "Validation set: (977, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.15384615384615385, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (325, 10) (325,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [175 175] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [181 194] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.87       321\n",
            "           1       0.67      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [191 209] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (425, 10) (425,)\n",
            "updated train set: (425, 10) (425,) unique(labels): [202 223] [0 1]\n",
            "val set: (877, 10) (877,)\n",
            "\n",
            "Train set: (425, 10)\n",
            "Validation set: (877, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.11764705882352941, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (425, 10) (425,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [211 239] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (475, 10) (475,)\n",
            "updated train set: (475, 10) (475,) unique(labels): [216 259] [0 1]\n",
            "val set: (827, 10) (827,)\n",
            "\n",
            "Train set: (475, 10)\n",
            "Validation set: (827, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.10526315789473684, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "trainset before adding uncertain samples (475, 10) (475,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [230 270] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.26728110599078, 79.49308755760369, 78.80184331797236, 79.49308755760369, 78.57142857142857, 76.49769585253456, 79.26267281105991, 79.26267281105991, 80.4147465437788, 79.95391705069125, 80.64516129032258, 81.33640552995391, 80.18433179723502, 80.18433179723502, 80.4147465437788, 79.03225806451613, 79.49308755760369, 80.18433179723502, 79.95391705069125, 80.18433179723502]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-9.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 10, using model = LogModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial labeled samples size 10\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 10) (10,) unique(labels): [6 4] [0 1]\n",
            "Val set: (1292, 10) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 10)\n",
            "Validation set: (1292, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 72.580645 \n",
            "Classification report for LogisticRegression(C=5.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.82      0.82       321\n",
            "           1       0.47      0.45      0.46       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.64      0.64      0.64       434\n",
            "weighted avg       0.72      0.73      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[264  57]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1292, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before adding uncertain samples (10, 10) (10,)\n",
            "trainset after adding uncertain samples (20, 10) (20,)\n",
            "updated train set: (20, 10) (20,) unique(labels): [ 9 11] [0 1]\n",
            "val set: (1282, 10) (1282,)\n",
            "\n",
            "Train set: (20, 10)\n",
            "Validation set: (1282, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 74.193548 \n",
            "Classification report for LogisticRegression(C=2.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.86      0.83       321\n",
            "           1       0.51      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.65      0.63      0.64       434\n",
            "weighted avg       0.73      0.74      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1282, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before adding uncertain samples (20, 10) (20,)\n",
            "trainset after adding uncertain samples (30, 10) (30,)\n",
            "updated train set: (30, 10) (30,) unique(labels): [11 19] [0 1]\n",
            "val set: (1272, 10) (1272,)\n",
            "\n",
            "Train set: (30, 10)\n",
            "Validation set: (1272, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 76.036866 \n",
            "Classification report for LogisticRegression(C=1.6666666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.44      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (30, 10) (30,)\n",
            "trainset after adding uncertain samples (40, 10) (40,)\n",
            "updated train set: (40, 10) (40,) unique(labels): [15 25] [0 1]\n",
            "val set: (1262, 10) (1262,)\n",
            "\n",
            "Train set: (40, 10)\n",
            "Validation set: (1262, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.267281 \n",
            "Classification report for LogisticRegression(C=1.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (40, 10) (40,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [19 31] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.267281 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (60, 10) (60,)\n",
            "updated train set: (60, 10) (60,) unique(labels): [22 38] [0 1]\n",
            "val set: (1242, 10) (1242,)\n",
            "\n",
            "Train set: (60, 10)\n",
            "Validation set: (1242, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 77.649770 \n",
            "Classification report for LogisticRegression(C=0.8333333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.59      0.45      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.68       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (60, 10) (60,)\n",
            "trainset after adding uncertain samples (70, 10) (70,)\n",
            "updated train set: (70, 10) (70,) unique(labels): [27 43] [0 1]\n",
            "val set: (1232, 10) (1232,)\n",
            "\n",
            "Train set: (70, 10)\n",
            "Validation set: (1232, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.7142857142857143, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.62      0.45      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (70, 10) (70,)\n",
            "trainset after adding uncertain samples (80, 10) (80,)\n",
            "updated train set: (80, 10) (80,) unique(labels): [30 50] [0 1]\n",
            "val set: (1222, 10) (1222,)\n",
            "\n",
            "Train set: (80, 10)\n",
            "Validation set: (1222, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.63      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (80, 10) (80,)\n",
            "trainset after adding uncertain samples (90, 10) (90,)\n",
            "updated train set: (90, 10) (90,) unique(labels): [32 58] [0 1]\n",
            "val set: (1212, 10) (1212,)\n",
            "\n",
            "Train set: (90, 10)\n",
            "Validation set: (1212, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.5555555555555556, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.45      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (90, 10) (90,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [34 66] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.66      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (110, 10) (110,)\n",
            "updated train set: (110, 10) (110,) unique(labels): [37 73] [0 1]\n",
            "val set: (1192, 10) (1192,)\n",
            "\n",
            "Train set: (110, 10)\n",
            "Validation set: (1192, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.45454545454545453, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (110, 10) (110,)\n",
            "trainset after adding uncertain samples (120, 10) (120,)\n",
            "updated train set: (120, 10) (120,) unique(labels): [41 79] [0 1]\n",
            "val set: (1182, 10) (1182,)\n",
            "\n",
            "Train set: (120, 10)\n",
            "Validation set: (1182, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.4166666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.70      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (120, 10) (120,)\n",
            "trainset after adding uncertain samples (130, 10) (130,)\n",
            "updated train set: (130, 10) (130,) unique(labels): [47 83] [0 1]\n",
            "val set: (1172, 10) (1172,)\n",
            "\n",
            "Train set: (130, 10)\n",
            "Validation set: (1172, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.38461538461538464, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.70      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (130, 10) (130,)\n",
            "trainset after adding uncertain samples (140, 10) (140,)\n",
            "updated train set: (140, 10) (140,) unique(labels): [49 91] [0 1]\n",
            "val set: (1162, 10) (1162,)\n",
            "\n",
            "Train set: (140, 10)\n",
            "Validation set: (1162, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.35714285714285715, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.69      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (140, 10) (140,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [52 98] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.69      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (160, 10) (160,)\n",
            "updated train set: (160, 10) (160,) unique(labels): [ 57 103] [0 1]\n",
            "val set: (1142, 10) (1142,)\n",
            "\n",
            "Train set: (160, 10)\n",
            "Validation set: (1142, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.3125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.70      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (160, 10) (160,)\n",
            "trainset after adding uncertain samples (170, 10) (170,)\n",
            "updated train set: (170, 10) (170,) unique(labels): [ 61 109] [0 1]\n",
            "val set: (1132, 10) (1132,)\n",
            "\n",
            "Train set: (170, 10)\n",
            "Validation set: (1132, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.29411764705882354, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.69      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (170, 10) (170,)\n",
            "trainset after adding uncertain samples (180, 10) (180,)\n",
            "updated train set: (180, 10) (180,) unique(labels): [ 63 117] [0 1]\n",
            "val set: (1122, 10) (1122,)\n",
            "\n",
            "Train set: (180, 10)\n",
            "Validation set: (1122, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.2777777777777778, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.69      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (180, 10) (180,)\n",
            "trainset after adding uncertain samples (190, 10) (190,)\n",
            "updated train set: (190, 10) (190,) unique(labels): [ 66 124] [0 1]\n",
            "val set: (1112, 10) (1112,)\n",
            "\n",
            "Train set: (190, 10)\n",
            "Validation set: (1112, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.2631578947368421, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.87       321\n",
            "           1       0.70      0.41      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (190, 10) (190,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 67 133] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.87       321\n",
            "           1       0.69      0.40      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (210, 10) (210,)\n",
            "updated train set: (210, 10) (210,) unique(labels): [ 69 141] [0 1]\n",
            "val set: (1092, 10) (1092,)\n",
            "\n",
            "Train set: (210, 10)\n",
            "Validation set: (1092, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.23809523809523808, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (210, 10) (210,)\n",
            "trainset after adding uncertain samples (220, 10) (220,)\n",
            "updated train set: (220, 10) (220,) unique(labels): [ 73 147] [0 1]\n",
            "val set: (1082, 10) (1082,)\n",
            "\n",
            "Train set: (220, 10)\n",
            "Validation set: (1082, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.22727272727272727, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87       321\n",
            "           1       0.71      0.39      0.50       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (220, 10) (220,)\n",
            "trainset after adding uncertain samples (230, 10) (230,)\n",
            "updated train set: (230, 10) (230,) unique(labels): [ 74 156] [0 1]\n",
            "val set: (1072, 10) (1072,)\n",
            "\n",
            "Train set: (230, 10)\n",
            "Validation set: (1072, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.21739130434782608, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.40      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.67      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (230, 10) (230,)\n",
            "trainset after adding uncertain samples (240, 10) (240,)\n",
            "updated train set: (240, 10) (240,) unique(labels): [ 76 164] [0 1]\n",
            "val set: (1062, 10) (1062,)\n",
            "\n",
            "Train set: (240, 10)\n",
            "Validation set: (1062, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.20833333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.40      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (240, 10) (240,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [ 82 168] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.88       321\n",
            "           1       0.76      0.39      0.51       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.79      0.67      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[307  14]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (260, 10) (260,)\n",
            "updated train set: (260, 10) (260,) unique(labels): [ 89 171] [0 1]\n",
            "val set: (1042, 10) (1042,)\n",
            "\n",
            "Train set: (260, 10)\n",
            "Validation set: (1042, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.19230769230769232, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (260, 10) (260,)\n",
            "trainset after adding uncertain samples (270, 10) (270,)\n",
            "updated train set: (270, 10) (270,) unique(labels): [ 90 180] [0 1]\n",
            "val set: (1032, 10) (1032,)\n",
            "\n",
            "Train set: (270, 10)\n",
            "Validation set: (1032, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.18518518518518517, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (270, 10) (270,)\n",
            "trainset after adding uncertain samples (280, 10) (280,)\n",
            "updated train set: (280, 10) (280,) unique(labels): [ 90 190] [0 1]\n",
            "val set: (1022, 10) (1022,)\n",
            "\n",
            "Train set: (280, 10)\n",
            "Validation set: (1022, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.17857142857142858, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (280, 10) (280,)\n",
            "trainset after adding uncertain samples (290, 10) (290,)\n",
            "updated train set: (290, 10) (290,) unique(labels): [ 91 199] [0 1]\n",
            "val set: (1012, 10) (1012,)\n",
            "\n",
            "Train set: (290, 10)\n",
            "Validation set: (1012, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1724137931034483, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.75      0.40      0.52       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[306  15]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (290, 10) (290,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [ 93 207] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.40      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.67      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (310, 10) (310,)\n",
            "updated train set: (310, 10) (310,) unique(labels): [ 97 213] [0 1]\n",
            "val set: (992, 10) (992,)\n",
            "\n",
            "Train set: (310, 10)\n",
            "Validation set: (992, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.16129032258064516, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.39      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
            "trainset before adding uncertain samples (310, 10) (310,)\n",
            "trainset after adding uncertain samples (320, 10) (320,)\n",
            "updated train set: (320, 10) (320,) unique(labels): [ 98 222] [0 1]\n",
            "val set: (982, 10) (982,)\n",
            "\n",
            "Train set: (320, 10)\n",
            "Validation set: (982, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.15625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87       321\n",
            "           1       0.71      0.39      0.50       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
            "trainset before adding uncertain samples (320, 10) (320,)\n",
            "trainset after adding uncertain samples (330, 10) (330,)\n",
            "updated train set: (330, 10) (330,) unique(labels): [100 230] [0 1]\n",
            "val set: (972, 10) (972,)\n",
            "\n",
            "Train set: (330, 10)\n",
            "Validation set: (972, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.15151515151515152, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.72      0.39      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0]\n",
            "trainset before adding uncertain samples (330, 10) (330,)\n",
            "trainset after adding uncertain samples (340, 10) (340,)\n",
            "updated train set: (340, 10) (340,) unique(labels): [101 239] [0 1]\n",
            "val set: (962, 10) (962,)\n",
            "\n",
            "Train set: (340, 10)\n",
            "Validation set: (962, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.14705882352941177, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87       321\n",
            "           1       0.71      0.39      0.50       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
            "trainset before adding uncertain samples (340, 10) (340,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [103 247] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (360, 10) (360,)\n",
            "updated train set: (360, 10) (360,) unique(labels): [106 254] [0 1]\n",
            "val set: (942, 10) (942,)\n",
            "\n",
            "Train set: (360, 10)\n",
            "Validation set: (942, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.1388888888888889, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
            "trainset before adding uncertain samples (360, 10) (360,)\n",
            "trainset after adding uncertain samples (370, 10) (370,)\n",
            "updated train set: (370, 10) (370,) unique(labels): [108 262] [0 1]\n",
            "val set: (932, 10) (932,)\n",
            "\n",
            "Train set: (370, 10)\n",
            "Validation set: (932, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.13513513513513514, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0]\n",
            "trainset before adding uncertain samples (370, 10) (370,)\n",
            "trainset after adding uncertain samples (380, 10) (380,)\n",
            "updated train set: (380, 10) (380,) unique(labels): [109 271] [0 1]\n",
            "val set: (922, 10) (922,)\n",
            "\n",
            "Train set: (380, 10)\n",
            "Validation set: (922, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.13157894736842105, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
            "trainset before adding uncertain samples (380, 10) (380,)\n",
            "trainset after adding uncertain samples (390, 10) (390,)\n",
            "updated train set: (390, 10) (390,) unique(labels): [112 278] [0 1]\n",
            "val set: (912, 10) (912,)\n",
            "\n",
            "Train set: (390, 10)\n",
            "Validation set: (912, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1282051282051282, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (390, 10) (390,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [114 286] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (410, 10) (410,)\n",
            "updated train set: (410, 10) (410,) unique(labels): [117 293] [0 1]\n",
            "val set: (892, 10) (892,)\n",
            "\n",
            "Train set: (410, 10)\n",
            "Validation set: (892, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.12195121951219512, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0]\n",
            "trainset before adding uncertain samples (410, 10) (410,)\n",
            "trainset after adding uncertain samples (420, 10) (420,)\n",
            "updated train set: (420, 10) (420,) unique(labels): [121 299] [0 1]\n",
            "val set: (882, 10) (882,)\n",
            "\n",
            "Train set: (420, 10)\n",
            "Validation set: (882, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.11904761904761904, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (420, 10) (420,)\n",
            "trainset after adding uncertain samples (430, 10) (430,)\n",
            "updated train set: (430, 10) (430,) unique(labels): [123 307] [0 1]\n",
            "val set: (872, 10) (872,)\n",
            "\n",
            "Train set: (430, 10)\n",
            "Validation set: (872, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.11627906976744186, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.75      0.40      0.52       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[306  15]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (430, 10) (430,)\n",
            "trainset after adding uncertain samples (440, 10) (440,)\n",
            "updated train set: (440, 10) (440,) unique(labels): [128 312] [0 1]\n",
            "val set: (862, 10) (862,)\n",
            "\n",
            "Train set: (440, 10)\n",
            "Validation set: (862, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.11363636363636363, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.75      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.79      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[306  15]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (440, 10) (440,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [130 320] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.75      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.79      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[306  15]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (460, 10) (460,)\n",
            "updated train set: (460, 10) (460,) unique(labels): [134 326] [0 1]\n",
            "val set: (842, 10) (842,)\n",
            "\n",
            "Train set: (460, 10)\n",
            "Validation set: (842, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.10869565217391304, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.88       321\n",
            "           1       0.76      0.40      0.52       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.79      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[307  14]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (460, 10) (460,)\n",
            "trainset after adding uncertain samples (470, 10) (470,)\n",
            "updated train set: (470, 10) (470,) unique(labels): [138 332] [0 1]\n",
            "val set: (832, 10) (832,)\n",
            "\n",
            "Train set: (470, 10)\n",
            "Validation set: (832, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.10638297872340426, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (470, 10) (470,)\n",
            "trainset after adding uncertain samples (480, 10) (480,)\n",
            "updated train set: (480, 10) (480,) unique(labels): [140 340] [0 1]\n",
            "val set: (822, 10) (822,)\n",
            "\n",
            "Train set: (480, 10)\n",
            "Validation set: (822, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.10416666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (480, 10) (480,)\n",
            "trainset after adding uncertain samples (490, 10) (490,)\n",
            "updated train set: (490, 10) (490,) unique(labels): [141 349] [0 1]\n",
            "val set: (812, 10) (812,)\n",
            "\n",
            "Train set: (490, 10)\n",
            "Validation set: (812, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.10204081632653061, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
            "trainset before adding uncertain samples (490, 10) (490,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [142 358] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.41      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "final active learning accuracies [72.58064516129032, 74.19354838709677, 76.036866359447, 76.26728110599078, 76.26728110599078, 77.64976958525345, 78.57142857142857, 78.80184331797236, 79.95391705069125, 79.49308755760369, 79.95391705069125, 80.18433179723502, 80.18433179723502, 79.95391705069125, 79.95391705069125, 80.18433179723502, 79.95391705069125, 79.95391705069125, 79.95391705069125, 79.72350230414746, 80.4147465437788, 79.95391705069125, 80.4147465437788, 80.18433179723502, 80.87557603686636, 80.87557603686636, 80.64516129032258, 80.87557603686636, 80.87557603686636, 80.4147465437788, 80.4147465437788, 79.95391705069125, 80.18433179723502, 79.95391705069125, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.87557603686636, 80.64516129032258, 80.87557603686636, 80.64516129032258, 80.87557603686636, 81.10599078341014, 81.10599078341014, 81.10599078341014, 80.87557603686636, 80.87557603686636, 80.87557603686636, 80.87557603686636]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-10.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.58064516129032,\n",
            "          74.19354838709677,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          80.64516129032258,\n",
            "          78.80184331797236,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          76.49769585253456,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          81.33640552995391,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.12442396313364,\n",
            "          75.34562211981567,\n",
            "          76.26728110599078,\n",
            "          76.72811059907833,\n",
            "          77.41935483870968,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.26267281105991,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          79.72350230414746,\n",
            "          80.87557603686636,\n",
            "          79.49308755760369,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          81.5668202764977,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          80.4147465437788,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          71.42857142857143,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 11, using model = LogModel, selection_function = EntropySelection, k = 250, iteration = 0.\n",
            "\n",
            "initial labeled samples size 250\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 10) (250,) unique(labels): [106 144] [0 1]\n",
            "Val set: (1052, 10) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.60      0.49      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [222 278] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.65      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "final active learning accuracies [78.11059907834101, 79.95391705069125]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-11.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 12, using model = LogModel, selection_function = EntropySelection, k = 125, iteration = 0.\n",
            "\n",
            "initial labeled samples size 125\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 10) (125,) unique(labels): [52 73] [0 1]\n",
            "Val set: (1177, 10) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [113 137] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [171 204] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0\n",
            " 0 0]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [210 290] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.66      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [79.72350230414746, 80.18433179723502, 79.72350230414746, 79.95391705069125]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-12.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 13, using model = LogModel, selection_function = EntropySelection, k = 50, iteration = 0.\n",
            "\n",
            "initial labeled samples size 50\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 10) (50,) unique(labels): [20 30] [0 1]\n",
            "Val set: (1252, 10) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [56 44] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.188940 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [78 72] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 98 102] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [118 132] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.65      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [138 162] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [157 193] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [168 232] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "<ipython-input-27-c6897e11c81d>:24: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-27-c6897e11c81d>:24: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.70      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 1 0 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [187 263] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [210 290] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [78.57142857142857, 77.18894009216591, 78.3410138248848, 78.3410138248848, 79.95391705069125, 80.64516129032258, 80.87557603686636, 81.10599078341014, 81.10599078341014, 81.10599078341014]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-13.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 14, using model = LogModel, selection_function = EntropySelection, k = 25, iteration = 0.\n",
            "\n",
            "initial labeled samples size 25\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 10) (25,) unique(labels): [13 12] [0 1]\n",
            "Val set: (1277, 10) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 10)\n",
            "Validation set: (1277, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.419355 \n",
            "Classification report for LogisticRegression(C=2.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.86       321\n",
            "           1       0.60      0.41      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (25, 10) (25,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [22 28] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 75.115207 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83       321\n",
            "           1       0.52      0.48      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.67       434\n",
            "weighted avg       0.74      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (75, 10) (75,)\n",
            "updated train set: (75, 10) (75,) unique(labels): [31 44] [0 1]\n",
            "val set: (1227, 10) (1227,)\n",
            "\n",
            "Train set: (75, 10)\n",
            "Validation set: (1227, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 77.188940 \n",
            "Classification report for LogisticRegression(C=0.6666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.59      0.42      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (75, 10) (75,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [43 57] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.62      0.43      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.67      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (125, 10) (125,)\n",
            "updated train set: (125, 10) (125,) unique(labels): [53 72] [0 1]\n",
            "val set: (1177, 10) (1177,)\n",
            "\n",
            "Train set:<ipython-input-27-c6897e11c81d>:24: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-27-c6897e11c81d>:24: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            " (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [60 90] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (175, 10) (175,)\n",
            "updated train set: (175, 10) (175,) unique(labels): [ 67 108] [0 1]\n",
            "val set: (1127, 10) (1127,)\n",
            "\n",
            "Train set: (175, 10)\n",
            "Validation set: (1127, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.2857142857142857, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (175, 10) (175,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 76 124] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (225, 10) (225,)\n",
            "updated train set: (225, 10) (225,) unique(labels): [ 84 141] [0 1]\n",
            "val set: (1077, 10) (1077,)\n",
            "\n",
            "Train set: (225, 10)\n",
            "Validation set: (1077, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.2222222222222222, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (225, 10) (225,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [ 92 158] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (275, 10) (275,)\n",
            "updated train set: (275, 10) (275,) unique(labels): [105 170] [0 1]\n",
            "val set: (1027, 10) (1027,)\n",
            "\n",
            "Train set: (275, 10)\n",
            "Validation set: (1027, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.18181818181818182, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (275, 10) (275,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [113 187] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.45      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (325, 10) (325,)\n",
            "updated train set: (325, 10) (325,) unique(labels): [124 201] [0 1]\n",
            "val set: (977, 10) (977,)\n",
            "\n",
            "Train set: (325, 10)\n",
            "Validation set: (977, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.15384615384615385, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.65      0.38      0.48       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.65      0.67       434\n",
            "weighted avg       0.77      0.79      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (325, 10) (325,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [135 215] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.66      0.38      0.48       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.66      0.67       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [141 234] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.67      0.40      0.50       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.66      0.68       434\n",
            "weighted avg       0.78      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [146 254] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.68      0.39      0.49       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.66      0.68       434\n",
            "weighted avg       0.78      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1\n",
            " 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (425, 10) (425,)\n",
            "updated train set: (425, 10) (425,) unique(labels): [155 270] [0 1]\n",
            "val set: (877, 10) (877,)\n",
            "\n",
            "Train set: (425, 10)\n",
            "Validation set: (877, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.11764705882352941, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.69      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (425, 10) (425,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [162 288] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (475, 10) (475,)\n",
            "updated train set: (475, 10) (475,) unique(labels): [168 307] [0 1]\n",
            "val set: (827, 10) (827,)\n",
            "\n",
            "Train set: (475, 10)\n",
            "Validation set: (827, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.10526315789473684, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.70      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (475, 10) (475,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [170 330] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.70      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.41935483870968, 75.11520737327189, 77.18894009216591, 78.3410138248848, 79.49308755760369, 80.87557603686636, 80.4147465437788, 79.26267281105991, 80.18433179723502, 80.64516129032258, 80.64516129032258, 80.64516129032258, 78.57142857142857, 78.80184331797236, 79.26267281105991, 79.26267281105991, 79.95391705069125, 80.4147465437788, 80.18433179723502, 80.18433179723502]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-14.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 15, using model = LogModel, selection_function = EntropySelection, k = 10, iteration = 0.\n",
            "\n",
            "initial labeled samples size 10\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 10) (10,) unique(labels): [6 4] [0 1]\n",
            "Val set: (1292, 10) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 10)\n",
            "Validation set: (1292, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 76.728111 \n",
            "Classification report for LogisticRegression(C=5.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (10, 10) (10,)\n",
            "trainset after adding uncertain samples (20, 10) (20,)\n",
            "updated train set: (20, 10) (20,) unique(labels): [ 9 11] [0 1]\n",
            "val set: (1282, 10) (1282,)\n",
            "\n",
            "Train set: (20, 10)\n",
            "Validation set: (1282, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=2.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (20, 10) (20,)\n",
            "trainset after adding uncertain samples (30, 10) (30,)\n",
            "updated train set: (30, 10) (30,) unique(labels): [14 16] [0 1]\n",
            "val set: (1272, 10) (1272,)\n",
            "\n",
            "Train set: (30, 10)\n",
            "Validation set: (1272, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=1.6666666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (30, 10) (30,)\n",
            "trainset after adding uncertain samples (40, 10) (40,)\n",
            "updated train set: (40, 10) (40,) unique(labels): [20 20] [0 1]\n",
            "val set: (1262, 10) (1262,)\n",
            "\n",
            "Train set: (40, 10)\n",
            "Validation set: (1262, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=1.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (40, 10) (40,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [22 28] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (60, 10) (60,)\n",
            "updated train set: (60, 10) (60,) unique(labels): [28 32] [0 1]\n",
            "val set: (1242, 10) (1242,)\n",
            "\n",
            "Train set: (60, 10)\n",
            "Validation set: (1242, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.8333333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "<ipython-input-27-c6897e11c81d>:24: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-27-c6897e11c81d>:24: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-27-c6897e11c81d>:24: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-27-c6897e11c81d>:24: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (60, 10) (60,)\n",
            "trainset after adding uncertain samples (70, 10) (70,)\n",
            "updated train set: (70, 10) (70,) unique(labels): [34 36] [0 1]\n",
            "val set: (1232, 10) (1232,)\n",
            "\n",
            "Train set: (70, 10)\n",
            "Validation set: (1232, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.7142857142857143, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (70, 10) (70,)\n",
            "trainset after adding uncertain samples (80, 10) (80,)\n",
            "updated train set: (80, 10) (80,) unique(labels): [38 42] [0 1]\n",
            "val set: (1222, 10) (1222,)\n",
            "\n",
            "Train set: (80, 10)\n",
            "Validation set: (1222, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (80, 10) (80,)\n",
            "trainset after adding uncertain samples (90, 10) (90,)\n",
            "updated train set: (90, 10) (90,) unique(labels): [42 48] [0 1]\n",
            "val set: (1212, 10) (1212,)\n",
            "\n",
            "Train set: (90, 10)\n",
            "Validation set: (1212, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.5555555555555556, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (90, 10) (90,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [43 57] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.45      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (110, 10) (110,)\n",
            "updated train set: (110, 10) (110,) unique(labels): [49 61] [0 1]\n",
            "val set: (1192, 10) (1192,)\n",
            "\n",
            "Train set: (110, 10)\n",
            "Validation set: (1192, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.45454545454545453, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (110, 10) (110,)\n",
            "trainset after adding uncertain samples (120, 10) (120,)\n",
            "updated train set: (120, 10) (120,) unique(labels): [54 66] [0 1]\n",
            "val set: (1182, 10) (1182,)\n",
            "\n",
            "Train set: (120, 10)\n",
            "Validation set: (1182, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.4166666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (120, 10) (120,)\n",
            "trainset after adding uncertain samples (130, 10) (130,)\n",
            "updated train set: (130, 10) (130,) unique(labels): [58 72] [0 1]\n",
            "val set: (1172, 10) (1172,)\n",
            "\n",
            "Train set: (130, 10)\n",
            "Validation set: (1172, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.38461538461538464, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (130, 10) (130,)\n",
            "trainset after adding uncertain samples (140, 10) (140,)\n",
            "updated train set: (140, 10) (140,) unique(labels): [60 80] [0 1]\n",
            "val set: (1162, 10) (1162,)\n",
            "\n",
            "Train set: (140, 10)\n",
            "Validation set: (1162, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.35714285714285715, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (140, 10) (140,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [66 84] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.45      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (160, 10) (160,)\n",
            "updated train set: (160, 10) (160,) unique(labels): [69 91] [0 1]\n",
            "val set: (1142, 10) (1142,)\n",
            "\n",
            "Train set: (160, 10)\n",
            "Validation set: (1142, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.3125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.45      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (160, 10) (160,)\n",
            "trainset after adding uncertain samples (170, 10) (170,)\n",
            "updated train set: (170, 10) (170,) unique(labels): [71 99] [0 1]\n",
            "val set: (1132, 10) (1132,)\n",
            "\n",
            "Train set: (170, 10)\n",
            "Validation set: (1132, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.29411764705882354, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (170, 10) (170,)\n",
            "trainset after adding uncertain samples (180, 10) (180,)\n",
            "updated train set: (180, 10) (180,) unique(labels): [ 73 107] [0 1]\n",
            "val set: (1122, 10) (1122,)\n",
            "\n",
            "Train set: (180, 10)\n",
            "Validation set: (1122, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.2777777777777778, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (180, 10) (180,)\n",
            "trainset after adding uncertain samples (190, 10) (190,)\n",
            "updated train set: (190, 10) (190,) unique(labels): [ 76 114] [0 1]\n",
            "val set: (1112, 10) (1112,)\n",
            "\n",
            "Train set: (190, 10)\n",
            "Validation set: (1112, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.2631578947368421, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.70      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (190, 10) (190,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 78 122] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.70      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (210, 10) (210,)\n",
            "updated train set: (210, 10) (210,) unique(labels): [ 79 131] [0 1]\n",
            "val set: (1092, 10) (1092,)\n",
            "\n",
            "Train set: (210, 10)\n",
            "Validation set: (1092, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.23809523809523808, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.88       321\n",
            "           1       0.70      0.43      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (210, 10) (210,)\n",
            "trainset after adding uncertain samples (220, 10) (220,)\n",
            "updated train set: (220, 10) (220,) unique(labels): [ 82 138] [0 1]\n",
            "val set: (1082, 10) (1082,)\n",
            "\n",
            "Train set: (220, 10)\n",
            "Validation set: (1082, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.22727272727272727, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (220, 10) (220,)\n",
            "trainset after adding uncertain samples (230, 10) (230,)\n",
            "updated train set: (230, 10) (230,) unique(labels): [ 87 143] [0 1]\n",
            "val set: (1072, 10) (1072,)\n",
            "\n",
            "Train set: (230, 10)\n",
            "Validation set: (1072, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.21739130434782608, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.69      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (230, 10) (230,)\n",
            "trainset after adding uncertain samples (240, 10) (240,)\n",
            "updated train set: (240, 10) (240,) unique(labels): [ 92 148] [0 1]\n",
            "val set: (1062, 10) (1062,)\n",
            "\n",
            "Train set: (240, 10)\n",
            "Validation set: (1062, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.20833333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.87       321\n",
            "           1       0.69      0.40      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (240, 10) (240,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [ 95 155] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (260, 10) (260,)\n",
            "updated train set: (260, 10) (260,) unique(labels): [ 99 161] [0 1]\n",
            "val set: (1042, 10) (1042,)\n",
            "\n",
            "Train set: (260, 10)\n",
            "Validation set: (1042, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.19230769230769232, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.67      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (260, 10) (260,)\n",
            "trainset after adding uncertain samples (270, 10) (270,)\n",
            "updated train set: (270, 10) (270,) unique(labels): [100 170] [0 1]\n",
            "val set: (1032, 10) (1032,)\n",
            "\n",
            "Train set: (270, 10)\n",
            "Validation set: (1032, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.18518518518518517, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (270, 10) (270,)\n",
            "trainset after adding uncertain samples (280, 10) (280,)\n",
            "updated train set: (280, 10) (280,) unique(labels): [103 177] [0 1]\n",
            "val set: (1022, 10) (1022,)\n",
            "\n",
            "Train set: (280, 10)\n",
            "Validation set: (1022, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.17857142857142858, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.67      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (280, 10) (280,)\n",
            "trainset after adding uncertain samples (290, 10) (290,)\n",
            "updated train set: (290, 10) (290,) unique(labels): [105 185] [0 1]\n",
            "val set: (1012, 10) (1012,)\n",
            "\n",
            "Train set: (290, 10)\n",
            "Validation set: (1012, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.1724137931034483, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (290, 10) (290,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [107 193] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.70      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (310, 10) (310,)\n",
            "updated train set: (310, 10) (310,) unique(labels): [108 202] [0 1]\n",
            "val set: (992, 10) (992,)\n",
            "\n",
            "Train set: (310, 10)\n",
            "Validation set: (992, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.16129032258064516, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0]\n",
            "trainset before adding uncertain samples (310, 10) (310,)\n",
            "trainset after adding uncertain samples (320, 10) (320,)\n",
            "updated train set: (320, 10) (320,) unique(labels): [113 207] [0 1]\n",
            "val set: (982, 10) (982,)\n",
            "\n",
            "Train set: (320, 10)\n",
            "Validation set: (982, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.15625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.67      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
            "trainset before adding uncertain samples (320, 10) (320,)\n",
            "trainset after adding uncertain samples (330, 10) (330,)\n",
            "updated train set: (330, 10) (330,) unique(labels): [113 217] [0 1]\n",
            "val set: (972, 10) (972,)\n",
            "\n",
            "Train set: (330, 10)\n",
            "Validation set: (972, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.15151515151515152, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.87       321\n",
            "           1       0.70      0.41      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0]\n",
            "trainset before adding uncertain samples (330, 10) (330,)\n",
            "trainset after adding uncertain samples (340, 10) (340,)\n",
            "updated train set: (340, 10) (340,) unique(labels): [114 226] [0 1]\n",
            "val set: (962, 10) (962,)\n",
            "\n",
            "Train set: (340, 10)\n",
            "Validation set: (962, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.14705882352941177, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
            "trainset before adding uncertain samples (340, 10) (340,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [116 234] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.70      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (360, 10) (360,)\n",
            "updated train set: (360, 10) (360,) unique(labels): [119 241] [0 1]\n",
            "val set: (942, 10) (942,)\n",
            "\n",
            "Train set: (360, 10)\n",
            "Validation set: (942, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.1388888888888889, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (360, 10) (360,)\n",
            "trainset after adding uncertain samples (370, 10) (370,)\n",
            "updated train set: (370, 10) (370,) unique(labels): [120 250] [0 1]\n",
            "val set: (932, 10) (932,)\n",
            "\n",
            "Train set: (370, 10)\n",
            "Validation set: (932, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.13513513513513514, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (370, 10) (370,)\n",
            "trainset after adding uncertain samples (380, 10) (380,)\n",
            "updated train set: (380, 10) (380,) unique(labels): [122 258] [0 1]\n",
            "val set: (922, 10) (922,)\n",
            "\n",
            "Train set: (380, 10)\n",
            "Validation set: (922, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(C=0.13157894736842105, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.73      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (380, 10) (380,)\n",
            "trainset after adding uncertain samples (390, 10) (390,)\n",
            "updated train set: (390, 10) (390,) unique(labels): [127 263] [0 1]\n",
            "val set: (912, 10) (912,)\n",
            "\n",
            "Train set: (390, 10)\n",
            "Validation set: (912, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.1282051282051282, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (390, 10) (390,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [129 271] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (410, 10) (410,)\n",
            "updated train set: (410, 10) (410,) unique(labels): [130 280] [0 1]\n",
            "val set: (892, 10) (892,)\n",
            "\n",
            "Train set: (410, 10)\n",
            "Validation set: (892, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.12195121951219512, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0]\n",
            "trainset before adding uncertain samples (410, 10) (410,)\n",
            "trainset after adding uncertain samples (420, 10) (420,)\n",
            "updated train set: (420, 10) (420,) unique(labels): [134 286] [0 1]\n",
            "val set: (882, 10) (882,)\n",
            "\n",
            "Train set: (420, 10)\n",
            "Validation set: (882, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.11904761904761904, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (420, 10) (420,)\n",
            "trainset after adding uncertain samples (430, 10) (430,)\n",
            "updated train set: (430, 10) (430,) unique(labels): [138 292] [0 1]\n",
            "val set: (872, 10) (872,)\n",
            "\n",
            "Train set: (430, 10)\n",
            "Validation set: (872, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.11627906976744186, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (430, 10) (430,)\n",
            "trainset after adding uncertain samples (440, 10) (440,)\n",
            "updated train set: (440, 10) (440,) unique(labels): [138 302] [0 1]\n",
            "val set: (862, 10) (862,)\n",
            "\n",
            "Train set: (440, 10)\n",
            "Validation set: (862, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.11363636363636363, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (440, 10) (440,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [141 309] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.74      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (460, 10) (460,)\n",
            "updated train set: (460, 10) (460,) unique(labels): [142 318] [0 1]\n",
            "val set: (842, 10) (842,)\n",
            "\n",
            "Train set: (460, 10)\n",
            "Validation set: (842, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.10869565217391304, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (460, 10) (460,)\n",
            "trainset after adding uncertain samples (470, 10) (470,)\n",
            "updated train set: (470, 10) (470,) unique(labels): [144 326] [0 1]\n",
            "val set: (832, 10) (832,)\n",
            "\n",
            "Train set: (470, 10)\n",
            "Validation set: (832, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.10638297872340426, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (470, 10) (470,)\n",
            "trainset after adding uncertain samples (480, 10) (480,)\n",
            "updated train set: (480, 10) (480,) unique(labels): [148 332] [0 1]\n",
            "val set: (822, 10) (822,)\n",
            "\n",
            "Train set: (480, 10)\n",
            "Validation set: (822, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(C=0.10416666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (480, 10) (480,)\n",
            "trainset after adding uncertain samples (490, 10) (490,)\n",
            "updated train set: (490, 10) (490,) unique(labels): [154 336] [0 1]\n",
            "val set: (812, 10) (812,)\n",
            "\n",
            "Train set: (490, 10)\n",
            "Validation set: (812, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.10204081632653061, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.69      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before adding uncertain samples (490, 10) (490,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [158 342] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.72811059907833, 78.3410138248848, 79.95391705069125, 79.49308755760369, 79.03225806451613, 79.49308755760369, 79.03225806451613, 80.87557603686636, 80.64516129032258, 79.95391705069125, 80.87557603686636, 80.18433179723502, 81.10599078341014, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.4147465437788, 80.4147465437788, 80.18433179723502, 80.18433179723502, 80.4147465437788, 79.72350230414746, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.49308755760369, 79.72350230414746, 79.49308755760369, 79.72350230414746, 80.18433179723502, 80.4147465437788, 80.18433179723502, 79.95391705069125, 80.64516129032258, 80.18433179723502, 81.10599078341014, 80.87557603686636, 80.87557603686636, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.4147465437788, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.64516129032258, 80.4147465437788, 80.64516129032258, 79.95391705069125, 80.4147465437788]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-15.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.87557603686636,\n",
            "          80.18433179723502,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          75.11520737327189,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.57142857142857,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.58064516129032,\n",
            "          74.19354838709677,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          80.64516129032258,\n",
            "          78.80184331797236,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          76.49769585253456,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          81.33640552995391,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.12442396313364,\n",
            "          75.34562211981567,\n",
            "          76.26728110599078,\n",
            "          76.72811059907833,\n",
            "          77.41935483870968,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.26267281105991,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          79.72350230414746,\n",
            "          80.87557603686636,\n",
            "          79.49308755760369,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          81.5668202764977,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          80.4147465437788,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          71.42857142857143,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 16, using model = LogModel, selection_function = MinStdSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial labeled samples size 250\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 10) (250,) unique(labels): [115 135] [0 1]\n",
            "Val set: (1052, 10) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "std (1052,) [35.16939004 21.7019211   2.20592255 ...  7.55685231 37.53781143\n",
            " 13.30138144]\n",
            "selection [ 415  764 1023  241  870  891  215  866  283  539  208  124   90   52\n",
            "  705   50 1038  499  194  701 1005  657  942  388 1015  618  353  615\n",
            "  789  854  931  515  625  140  222  385  947  666    2  147  280  571\n",
            "  641  372  686  619  556  590  677  651  726  966  117  778 1046  421\n",
            "  410  536  273  175  106  285    9  425  520  681 1044  414  626  518\n",
            "  116  997  661  406  759  883  588   24  431  274  276  871  730  835\n",
            "  914  434  462   73 1018  830  363  344  881  328    7   22  724  255\n",
            "  199  393  815  176  611  924  770  447  783  979  501  723  302  510\n",
            "  334  161 1036   31  722  190  356  411  788  223  928  972  543  713\n",
            "  814  409  304   97  708  605  769  432  697  460  937  664  475  482\n",
            "   14  970  887  954  508  996  227  798  331  862  365  591  271  968\n",
            "  775  744  218  457 1040  169   62  640  224  630  136  721  544  160\n",
            "  569  192  427  378  183  645  878  631  247  839  574  187  865  586\n",
            "  188  951  678 1049  967   39  513   27  889  867   67  148  799  269\n",
            "  821  542  141  244  467  196  216  929  742  154   43  572  399  121\n",
            "  552  231  832  485  320  965  977  725  869  886  993  200   68 1021\n",
            "  450  926  364  962  593  532  184  461  305  949  516  943  986  712\n",
            "  933  163  908  898  248  794  786  606  740  182  445  739] (250,) [ 0.02976034  0.09350389  0.12090419  0.13563614  0.28922061  0.3085691\n",
            "  0.37321967  0.38136278  0.39052635  0.62533178  0.62923993  0.71253949\n",
            "  0.72339913  0.73354008  0.73668379  0.75162976  0.78107985  0.85983775\n",
            "  0.87910124  0.94301624  0.99278431  1.07472358  1.08305655  1.08446406\n",
            "  1.15163159  1.41489773  1.43475144  1.45766693  1.47812946  1.49831657\n",
            "  1.51816825  1.53269207  1.567931    1.78566658  1.80130073  1.99837059\n",
            "  2.16485762  2.16763006  2.20592255  2.23894232  2.24928613  2.30328654\n",
            "  2.31891163  2.32587554  2.32676208  2.43539493  2.50075058  2.59765911\n",
            "  2.61598174  2.61913055  2.6349601   2.74000273  2.7532696   2.8115412\n",
            "  2.81257752  2.8229804   2.87275948  2.90552077  2.93852648  3.0424552\n",
            "  3.19222966  3.20539672  3.32730676  3.35897607  3.36898399  3.40394121\n",
            "  3.46560719  3.47673927  3.48729316  3.51941868  3.52437064  3.61589056\n",
            "  3.76062137  3.76901842  3.79748969  3.80305558  3.80487932  3.81246915\n",
            "  3.83316594  3.8665089   3.87552461  3.90090604  3.95941582  4.02322896\n",
            "  4.09078168  4.09819863  4.09819863  4.17798325  4.20224049  4.23044139\n",
            "  4.24971622  4.2573005   4.28015575  4.29797712  4.32811928  4.35164705\n",
            "  4.43844219  4.48821628  4.57677456  4.58685776  4.66076967  4.66831669\n",
            "  4.70961116  4.7205812   4.74874257  4.80430006  4.80430006  4.80430006\n",
            "  4.80430006  4.80430006  4.80430006  4.80430006  4.80430006  4.80430006\n",
            "  4.80430006  4.80430006  4.80430006  4.80430006  4.80430006  4.80430006\n",
            "  4.80430006  4.80430006  4.80430006  4.80430006  4.80430006  4.80430006\n",
            "  4.80430006  4.80430006  4.80430006  4.80430006  5.15987473  5.1897307\n",
            "  5.21437463  5.26561177  5.32085736  5.32595524  5.35374173  5.38109273\n",
            "  5.4265951   5.43532849  5.50404726  5.50914584  5.53782019  5.53878105\n",
            "  5.56240144  5.56790662  5.61914641  5.6244831   5.65407045  5.66226617\n",
            "  5.83630402  5.85951868  5.86136813  6.19611892  6.2695143   6.29448179\n",
            "  6.30154855  6.32470022  6.37423705  6.40686955  6.41379378  6.43571536\n",
            "  6.54181166  6.58840859  6.60793451  6.63877278  6.68044776  6.78394923\n",
            "  6.83234081  6.876999    6.90220478  7.01197892  7.11286945  7.12662264\n",
            "  7.26081606  7.26929756  7.28768545  7.28768545  7.30873067  7.34240741\n",
            "  7.34609241  7.39202712  7.44537237  7.46636906  7.49924964  7.55685231\n",
            "  7.55701747  7.64615213  7.67374991  7.70392742  7.74395017  7.74479236\n",
            "  7.7765501   7.79753523  7.83852755  7.89839517  7.92394044  7.93412051\n",
            "  7.93755477  7.97657958  8.09562102  8.15048497  8.31871872  8.33256681\n",
            "  8.45788136  8.57274656  8.61343812  8.66909548  8.76912082  8.80789589\n",
            "  8.84233876  8.8653519   8.87932928  8.88552664  8.89100799  8.90295943\n",
            "  8.96039897  8.96267686  8.97742727  9.03381647  9.07857992  9.12892176\n",
            "  9.1763784   9.23756214  9.25590641  9.28381355  9.29010792  9.32043901\n",
            "  9.34243438  9.36509273  9.43883855  9.48391934  9.59346802  9.64019511\n",
            "  9.73156089  9.73355895  9.7418583  10.07437453 10.08016808 10.12067468\n",
            " 10.25264211 10.2974278  10.2974278  10.38212692 10.40327349 10.40926616\n",
            " 10.45287278 10.49560127 10.56493148 10.60029794]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [222 278] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [78.11059907834101, 79.03225806451613]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-16.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 17, using model = LogModel, selection_function = MinStdSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial labeled samples size 125\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 10) (125,) unique(labels): [61 64] [0 1]\n",
            "Val set: (1177, 10) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.020 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.86       321\n",
            "           1       0.61      0.63      0.62       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.74      0.74       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1177,) [ 3.2417523  34.80437487 26.77507112 ... 24.76310958 38.89595483\n",
            " 17.6253141 ]\n",
            "selection [  24 1119   62  669   59  386  609  705  177   88  419  985  520  200\n",
            "  477  514  238 1100  188  160  405 1079  614  164   30  708   70 1138\n",
            "  255  727  308  833  581 1022  336  333  335 1090 1084 1160  579  144\n",
            "  397  568  503  372 1030 1154  888  209   34  802  619  178  618  882\n",
            " 1024  966 1052  624   86  312  204  768 1140  564 1089 1018  303  539\n",
            "  970  506  671 1058  737   14  777 1114  673   64 1173 1078  686 1037\n",
            "  115  594  444 1103  698  653  648  154  426 1135  133    0  488  910\n",
            " 1168  788  817  787  987 1023  186  192  183  738  455  230  357 1006\n",
            "  960  815  450  118    7  122  242  522  487   50  354  221  449] (125,) [0.08550693 0.15958561 0.18044876 0.2582639  0.27821064 0.29167144\n",
            " 0.33866177 0.3790036  0.39248785 0.39397411 0.5920428  0.60229507\n",
            " 0.63200254 0.6490182  0.72002374 0.79800083 0.83073988 0.9381751\n",
            " 0.98344377 0.98668819 1.15892372 1.19165011 1.27378154 1.31329937\n",
            " 1.34214448 1.39010444 1.43565994 1.46107716 1.46671627 1.49462462\n",
            " 1.64087101 1.69424063 1.70489985 1.71420998 1.72887747 1.73056667\n",
            " 1.73056667 1.73056667 1.73056667 1.73056667 1.73056667 1.73056667\n",
            " 1.73056667 1.73056667 1.73056667 1.73056667 1.73056667 1.73056667\n",
            " 1.73056667 1.73056667 1.73056667 1.73056667 1.73056667 1.73056667\n",
            " 1.73056667 1.73056667 1.73056667 1.84814914 1.84836648 1.85561801\n",
            " 1.86588911 1.90620941 1.92781001 1.96069114 2.00307684 2.02885854\n",
            " 2.04547771 2.0519189  2.07356649 2.10068122 2.14034813 2.24774971\n",
            " 2.27573526 2.27884778 2.28330946 2.31204454 2.31818606 2.37155632\n",
            " 2.49514118 2.52151877 2.52374711 2.53282739 2.54672049 2.57611444\n",
            " 2.70815954 2.71359598 2.73754152 2.77183417 2.79378142 2.91792559\n",
            " 3.02811763 3.0693351  3.11230823 3.13884289 3.16119607 3.2417523\n",
            " 3.28714422 3.35483185 3.43189026 3.48037465 3.56946336 3.64870375\n",
            " 3.6620881  3.73129756 3.84939706 4.06866914 4.07924971 4.09199305\n",
            " 4.14164252 4.18283235 4.20014203 4.20865805 4.23465366 4.24179628\n",
            " 4.29980898 4.35469801 4.36779842 4.37700251 4.37820343 4.41511457\n",
            " 4.41511457 4.41753339 4.42430177 4.45259918 4.49429561]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [133 117] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.62      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "std (1052,) [35.45474144 24.56388699  0.29137495 ... 34.59444506 32.91167426\n",
            " 16.12957307]\n",
            "selection [ 290  345    2  741  753  103  637  526  276  541  223  171   82 1003\n",
            "   44  511  401 1006   19  213  525  670   98  650  515  357   33  963\n",
            "  443  131  144  568  560  313  537  839  454  910  452  500  609   21\n",
            "  374  342  735  873  876  985  728  895  626  901  288  468  307  111\n",
            "  186  556  758  612  772  465  535  287  577  667  460  749 1038  177\n",
            "  954  752  212  423  450  764  885  968  720  125 1027  521  922  199\n",
            "  981  181  202  817  834  664  296  692  167   62   41  929   18  816\n",
            " 1046   85  884  864   22  349  783  579  781  881  561  948 1032  681\n",
            "  770  386    7  384  680  713  868  893  388  630  449  531  245] (125,) [0.05949417 0.22533239 0.29137495 0.42474577 0.43129971 0.5961481\n",
            " 0.884461   1.0848243  1.13685366 1.14018286 1.26060339 1.65973017\n",
            " 1.75902436 1.8817648  1.88356546 1.90427013 1.9945018  2.00449811\n",
            " 2.01065784 2.10461894 2.10480356 2.1694744  2.33140223 2.44565417\n",
            " 2.44649976 2.50305841 2.58038573 2.58715148 2.76074038 3.06129673\n",
            " 3.21886184 3.24919537 3.42783727 3.44074205 3.48407594 3.54262947\n",
            " 3.63870587 3.91949524 3.96775466 4.09796137 4.19527789 4.30441691\n",
            " 4.33882831 4.39061836 4.58245173 4.80675353 4.98836036 5.02402418\n",
            " 5.05256765 5.12404706 5.13339196 5.23156905 5.27056799 5.33093821\n",
            " 5.36696492 5.38025368 5.47419506 5.48192251 5.50735642 5.54035179\n",
            " 5.56213076 5.56771248 5.62700715 5.67828183 5.70412054 5.71125356\n",
            " 5.73382216 5.78314485 5.93864464 6.02498044 6.06891372 6.190483\n",
            " 6.20330246 6.28994396 6.29038803 6.41906979 6.49889049 6.54945086\n",
            " 6.57791536 6.62459611 6.63193066 6.87475312 6.88688631 6.91580911\n",
            " 7.19146032 7.34027224 7.42122114 7.49779544 7.50113425 7.53445632\n",
            " 7.53843934 7.57141865 7.5879229  7.64297822 7.66635812 7.67788682\n",
            " 7.67788682 7.70082754 7.7426814  7.75502911 7.82459312 7.88275698\n",
            " 7.9036768  7.98725352 7.99860424 8.00646792 8.05569083 8.08372586\n",
            " 8.13436783 8.26872669 8.36373826 8.38472836 8.43022779 8.52431743\n",
            " 8.62610537 8.63162582 8.69455798 8.71140875 8.7181976  8.74414011\n",
            " 8.83794592 8.88030284 8.9471808  9.02553474 9.05227861]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [190 185] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1\n",
            " 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1\n",
            " 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "std (927,) [2.84740973e+01 2.02053791e+01 1.95268367e+01 3.02368916e+01\n",
            " 1.10167601e+01 2.56930111e+01 1.41757989e+01 1.22092749e+01\n",
            " 1.15159517e+01 3.69961044e+01 3.51177282e+01 4.40825925e+01\n",
            " 4.09095301e+00 3.02541185e+01 2.60746100e+01 1.88367996e+01\n",
            " 2.10918904e+01 1.55720766e+01 3.27058052e+01 2.32960121e+01\n",
            " 1.57958477e+01 3.78823347e+01 2.56390191e+01 3.49570938e+01\n",
            " 1.85198855e+01 2.76769764e+01 2.86592956e+01 1.69630398e+01\n",
            " 3.70062702e+01 2.07779273e+01 9.11337830e+00 2.54418391e+01\n",
            " 3.76712061e+01 1.43129978e+01 4.46564227e+00 1.61093464e+01\n",
            " 1.82407000e+01 1.51488740e+01 2.21796213e+01 1.83860409e+01\n",
            " 3.96029536e+01 1.09035627e+01 1.39802095e+01 2.02819134e+01\n",
            " 1.50748253e+01 2.92527920e+01 1.80030930e+01 2.85099845e+01\n",
            " 3.07035204e+01 1.21244163e+01 1.72857384e+01 1.49740011e+01\n",
            " 1.20478879e+01 2.68285260e+01 4.19248249e+01 3.30928566e+01\n",
            " 3.15892866e+01 4.52272771e+00 1.57818496e+01 8.80979205e+00\n",
            " 2.93652666e+01 9.99673926e+00 1.06126609e+01 2.81995446e+01\n",
            " 1.87111829e+01 1.72805804e+01 3.66126511e+01 2.34878839e+01\n",
            " 8.17389619e+00 3.06337088e+01 3.65308530e+01 1.55427934e+01\n",
            " 1.50308544e+01 3.02326183e+01 2.86609833e+01 2.80599076e+01\n",
            " 3.09387382e+01 3.26031532e+01 1.75200788e+01 1.61601997e+01\n",
            " 1.70789750e+01 2.35103667e+01 2.38670705e+01 1.34531303e+01\n",
            " 2.77191280e+01 2.28020519e+01 1.33194399e+01 2.32583173e+01\n",
            " 2.57499395e+01 3.32316424e+01 3.84818368e+01 1.48832662e+01\n",
            " 2.64472319e+01 3.19402629e+01 3.99826479e+01 2.99764612e+01\n",
            " 1.98859885e+01 1.56689067e+01 2.98283798e+01 3.12952431e+01\n",
            " 1.24628267e+01 2.58624545e+01 2.16048457e+01 3.36458466e+01\n",
            " 2.44081231e+01 8.01078774e+00 2.51646726e+01 1.56346903e+01\n",
            " 2.39570501e+01 2.46579710e+01 1.94384954e+01 3.18841357e+01\n",
            " 5.07795739e+00 4.48074867e+00 3.19643386e+01 2.41145695e+01\n",
            " 1.44249169e+01 2.82514321e+01 2.22641590e+01 1.89546965e+00\n",
            " 2.74496418e+01 2.42522373e+01 9.24272319e+00 1.74548123e+01\n",
            " 3.73733586e+01 1.53817034e+01 8.62184119e+00 1.63344578e+01\n",
            " 1.92048060e+01 1.94798693e+01 1.62171591e+01 1.29443059e+01\n",
            " 4.77499077e+01 2.47335935e+01 1.19425665e+01 3.31055005e+01\n",
            " 2.35947868e+01 1.55305226e+01 2.72739303e+01 8.74826641e+00\n",
            " 2.89153311e+01 1.60105701e+01 1.23860552e+01 3.12873770e+01\n",
            " 1.18863480e+01 1.85133951e+01 1.19590823e+01 2.53800823e+01\n",
            " 2.68368557e+00 1.86282664e+01 2.44794174e+01 2.09383396e+01\n",
            " 1.63128924e+01 1.70612060e+01 1.01232635e+01 1.08393253e+01\n",
            " 1.32632933e+01 1.13941084e+01 2.29234937e+01 2.62806976e+01\n",
            " 2.59956958e+01 5.20796305e-01 1.55737764e+01 4.79001147e+01\n",
            " 1.72450676e+01 1.84680899e+01 1.22469264e+01 3.93853291e+01\n",
            " 7.34571049e+00 2.06400961e+01 1.83424694e+01 2.40614725e+01\n",
            " 1.89817294e+01 4.94834055e+01 1.93525175e+01 2.66456998e+01\n",
            " 2.30270507e+01 1.32338697e+01 2.95151579e+01 2.40787686e+01\n",
            " 3.57784538e+01 2.78416499e+01 1.02044999e+01 1.26895736e+01\n",
            " 1.43370779e+01 2.33702078e+01 4.92609903e+01 2.61732449e+01\n",
            " 1.12601077e+01 4.52205768e+00 2.64534590e+01 1.59101327e+01\n",
            " 2.58627442e+01 2.22079783e+01 1.00848082e+01 6.02869783e+00\n",
            " 8.59241155e+00 8.22351259e+00 1.48223487e+01 4.28549002e+01\n",
            " 2.09726113e+01 2.75170775e+01 9.63815498e+00 3.54547217e+01\n",
            " 2.33413076e+01 1.39390662e+01 5.68462115e+00 7.77734158e+00\n",
            " 2.74270841e+01 7.51477152e+00 1.46566708e+01 1.20222385e+01\n",
            " 2.82778310e+01 1.18796156e+01 2.08073072e+01 1.20012825e+01\n",
            " 3.58614631e+01 2.85232492e+01 2.03514015e+01 3.65308309e+01\n",
            " 2.36101007e+01 2.49335273e+01 2.65359093e+01 1.60041853e+01\n",
            " 2.60466379e+01 2.23538098e+01 2.78703436e+01 3.01895978e+01\n",
            " 1.91304985e+01 1.56087853e+01 2.92028182e+01 2.91145106e+01\n",
            " 2.68450938e+01 3.35032107e+01 1.76242270e-02 2.98683712e+01\n",
            " 9.98615747e+00 3.60061194e+01 4.92858553e+00 1.49969494e+01\n",
            " 1.05038019e+01 3.12815617e+01 8.12231619e+00 1.44275753e+01\n",
            " 2.32280564e+01 7.37246044e+00 1.58172496e+01 3.03285148e+00\n",
            " 2.71423784e+01 2.86982665e+01 1.23668394e+01 1.82229948e+01\n",
            " 2.74714330e+01 3.98416032e+01 2.86386221e+01 1.30778864e+01\n",
            " 4.26428402e+01 3.45383037e+01 1.25509038e+01 2.82617732e+01\n",
            " 2.74586523e+01 1.49154241e+01 4.73007584e+01 2.99957118e+01\n",
            " 2.09856192e+01 2.60353026e+01 3.34673278e+01 2.00832559e+01\n",
            " 8.70092610e+00 2.39923851e+01 2.17513136e+01 3.52451961e+01\n",
            " 2.30965936e+01 2.13791824e+01 6.15624017e+00 2.49358102e+01\n",
            " 1.67373273e+01 2.95975734e+01 4.60313020e+01 4.00607051e+01\n",
            " 1.97376114e+01 2.21423318e+01 1.52735240e+01 2.45809810e+01\n",
            " 1.78915684e+01 1.90126391e+01 9.29431257e+00 2.99664154e+00\n",
            " 2.24687806e+01 3.25264805e+01 1.30562571e+00 2.32273693e+01\n",
            " 2.04791124e+01 4.89094650e+00 2.73235114e+01 2.97599560e+01\n",
            " 2.34878839e+01 3.35894676e+01 2.05083881e+01 1.45017221e+01\n",
            " 1.79798252e+01 3.04028848e+01 9.52416998e+00 1.00171844e+01\n",
            " 2.78081069e+01 2.28918796e+01 1.32106180e+01 2.14195245e+01\n",
            " 1.34237834e+01 2.43561182e+01 1.43616289e+01 2.46559284e+01\n",
            " 2.12349165e+01 1.48765310e+01 4.00691752e+01 2.84625641e+01\n",
            " 3.32653005e+01 2.08759268e+01 1.18587083e+01 2.38076308e+01\n",
            " 3.18196075e+01 2.80920495e+01 1.92108934e+01 2.63978473e+01\n",
            " 1.98443933e+01 1.07258884e+01 1.15760230e+01 2.36398708e+01\n",
            " 5.37819473e+00 2.28245618e+01 1.66478116e+01 3.69001971e+01\n",
            " 2.04647211e+01 4.82365930e+00 1.83169603e+01 2.29654552e+01\n",
            " 2.35431648e+01 1.15539896e+01 3.08046642e+01 8.32535049e+00\n",
            " 3.01220007e+01 3.41891081e+01 3.35909962e+01 1.44678276e+01\n",
            " 4.02821912e+01 1.91562710e+01 2.23233415e+01 4.48165910e+01\n",
            " 2.22473214e+01 8.49361990e+00 1.65585443e+01 1.72633498e+01\n",
            " 1.31420915e+01 2.39719822e+01 4.83509691e+01 2.68967871e+01\n",
            " 2.93445181e+01 1.83112789e+01 2.66952030e+01 1.58617298e+01\n",
            " 2.61004757e+01 5.59575139e+00 2.73931413e+01 5.67893762e+00\n",
            " 3.05274819e+01 2.72865419e+01 1.51142702e+01 3.67124445e+01\n",
            " 1.45408725e+01 2.30436682e+01 1.11381678e+01 1.11905060e+01\n",
            " 3.29271490e+00 2.65232867e+01 2.27467983e+01 7.23750414e+00\n",
            " 3.02924705e+00 3.34860279e+01 1.43482851e+01 3.60446933e+01\n",
            " 6.63181721e+00 1.40081529e+01 2.31090137e+01 2.67343262e+01\n",
            " 1.29354613e+01 1.52260745e+01 2.63356539e+01 1.84856831e+01\n",
            " 1.38103169e+01 2.96611387e+01 1.92431435e+01 1.55976251e+01\n",
            " 2.88387598e+01 1.92495102e+01 1.79560174e+01 3.46868756e+01\n",
            " 3.04855461e+01 1.23569425e+01 2.34559931e+01 5.96914469e+00\n",
            " 2.21941052e+01 1.67449041e+01 1.58986324e+01 3.36975055e+01\n",
            " 3.42186278e+01 1.95552357e+01 8.87373304e+00 2.19197253e+01\n",
            " 1.69914063e+01 5.21764263e+00 3.35654759e+01 3.24310795e+01\n",
            " 6.75470951e+00 1.98342360e+01 3.78368123e+01 3.34105525e+01\n",
            " 1.54823298e+01 2.50447979e+01 2.65773679e+01 2.83936780e+01\n",
            " 2.05989861e+01 2.98750320e+01 2.91377904e+01 1.72677643e+01\n",
            " 2.31597083e+01 2.70263071e+01 1.00357645e+01 3.52835288e+01\n",
            " 1.39590914e+01 1.51897092e+01 9.44075502e+00 2.60727942e+01\n",
            " 2.24473050e+01 3.75220769e+01 1.43263424e+01 4.80797096e+01\n",
            " 3.25146141e+01 1.91753497e+01 9.35981690e+00 1.84108235e+01\n",
            " 1.09184826e+00 2.55206797e+01 1.50704054e+01 1.71544494e+01\n",
            " 2.85484023e+01 4.24195909e+00 2.33190387e+01 1.43095633e+01\n",
            " 8.40157324e+00 3.00149290e+00 1.71654648e+01 1.87333038e+01\n",
            " 3.09492217e+01 1.75053438e+01 1.59278448e+01 8.92302267e+00\n",
            " 1.23252703e+01 7.31938075e+00 6.30105588e+00 1.14662756e+01\n",
            " 2.58783416e+01 2.51173487e+00 2.97355345e+01 1.01450023e+01\n",
            " 1.46469417e+01 3.81574288e+01 3.21877289e+01 3.04236223e+01\n",
            " 9.37979129e+00 1.24692523e+01 2.84268832e+01 1.49657352e+01\n",
            " 1.51727729e+01 1.58929492e+01 2.25442369e+01 2.63720256e+01\n",
            " 2.09202967e+01 2.91066305e+01 1.83979694e+01 1.38423079e+01\n",
            " 2.14881970e+01 3.23991803e+01 3.22512848e+01 3.18730257e+01\n",
            " 6.03255935e+00 1.05413639e+01 3.31956066e+01 2.54196226e+01\n",
            " 8.97909738e+00 1.29850363e+01 3.85679819e+01 2.77809879e+01\n",
            " 8.05893005e+00 3.79278208e+01 2.64961331e+01 1.31493092e+01\n",
            " 3.02601173e+01 3.03730796e+01 3.24621858e+01 2.13272542e+01\n",
            " 2.36793317e+01 1.15760230e+01 2.86815594e+01 1.23787718e+01\n",
            " 3.25236656e+01 2.63329889e+01 2.17582964e+01 3.67531576e+01\n",
            " 4.78505291e+01 9.31646655e+00 2.45112108e+01 2.00761422e+01\n",
            " 1.72329828e+01 1.08889007e+01 1.60182717e+01 2.01452844e+01\n",
            " 3.33917878e+01 2.06864273e+01 2.64166877e+01 4.16467995e+01\n",
            " 4.86742661e+01 2.01674083e+01 1.62788962e+01 9.39759809e+00\n",
            " 2.09003191e+01 3.63412467e+01 1.44037242e+01 3.95272303e+01\n",
            " 1.38130479e+01 4.07977243e+01 2.27052068e+01 1.92723093e+01\n",
            " 1.21030337e+01 2.79034906e+01 2.86881972e+01 2.47859505e+01\n",
            " 1.05612506e+01 1.52345732e+01 2.60131549e+01 3.82994546e+01\n",
            " 2.47159133e+01 2.74279348e+01 9.13704273e+00 1.68201443e+01\n",
            " 4.10782937e+00 3.40584266e+01 1.84011008e+01 1.55137858e+01\n",
            " 1.26456512e+01 1.82752362e+01 2.06476535e+01 2.57078958e+01\n",
            " 3.73407430e+00 5.94758907e+00 1.75748339e+01 1.38314236e+01\n",
            " 2.10550630e+01 1.33319653e+01 4.09959178e+01 4.02204165e+01\n",
            " 2.42888349e+01 3.18799324e+01 6.52946059e+00 1.16585379e+01\n",
            " 4.70207621e+01 2.19949706e+01 1.20941542e+01 1.81666545e+01\n",
            " 1.79741400e+01 2.08772484e+01 2.75333981e+01 2.14918428e+01\n",
            " 9.08596164e+00 2.17016886e+01 1.70634118e+01 1.22587738e+01\n",
            " 2.24875826e+01 2.01387457e+01 2.12599711e+01 1.88766012e+01\n",
            " 3.66885408e+01 1.55973211e+01 1.62013458e+01 3.33505490e+01\n",
            " 2.27075812e+01 2.18010387e+01 2.40184298e+01 1.26728169e+01\n",
            " 1.82500461e+01 2.16012493e+01 2.18808623e+01 2.80557662e+01\n",
            " 2.57579432e+01 1.42341083e+01 2.08169429e+01 3.22840624e+01\n",
            " 1.96490743e+01 3.32605185e+01 1.60838207e+01 2.04070899e+01\n",
            " 3.86405679e+01 1.10964113e+01 3.02180517e+01 2.02813954e+01\n",
            " 1.25879118e+01 2.30902790e+01 4.12628141e+00 2.28333145e+01\n",
            " 3.67342562e+01 3.45671855e+01 3.02506250e+01 1.70016935e+01\n",
            " 1.90126391e+01 4.28144865e+01 2.21027345e+01 2.36926037e+00\n",
            " 1.89982093e+01 2.77415723e+01 4.43035064e+01 3.36001145e+01\n",
            " 1.53511143e+01 3.73821874e+01 1.77747659e+01 5.66473739e+00\n",
            " 1.13178969e+01 3.21246297e+01 2.16177349e+01 2.54693905e+01\n",
            " 3.49150004e+01 7.71700684e+00 2.45805763e+01 1.33119438e+01\n",
            " 1.85526569e+01 1.88587329e+01 1.92641312e+01 1.94301323e+01\n",
            " 1.89460852e+01 1.36333802e+01 9.53384440e+00 2.09539251e+01\n",
            " 3.13485213e+01 2.49436171e+01 4.27407341e+01 2.90178776e+01\n",
            " 1.70010341e+01 1.34725597e+01 2.36957667e+01 2.14370201e+01\n",
            " 8.82870512e+00 4.09095301e+00 2.25927415e+01 1.88446274e+01\n",
            " 6.90410217e+00 1.51224122e+01 2.35305344e+00 4.61671928e+01\n",
            " 1.83357234e+01 2.43713888e+01 2.50639574e+01 1.46594922e+01\n",
            " 5.49871600e+00 8.83803096e+00 1.56123954e+01 1.66757458e+01\n",
            " 2.56597287e+01 2.47421771e+01 2.41330637e+01 2.38817545e+01\n",
            " 2.15173132e+01 3.74138328e+01 1.05423132e+01 1.18538891e+01\n",
            " 3.38671198e+01 2.10554988e+01 3.93513544e+01 2.58556107e+01\n",
            " 1.16291705e+01 2.08614566e+01 2.04574977e+01 6.03734268e+00\n",
            " 2.23953538e+01 3.33409271e+01 2.81491402e+01 4.31198911e+01\n",
            " 2.00927544e+01 3.42896698e+01 2.34497932e+01 2.25641366e+01\n",
            " 1.17748922e+01 7.64854000e+00 1.95445885e+01 3.65750418e+01\n",
            " 2.85851111e+01 3.15934245e+01 3.19885008e+01 2.01798205e+01\n",
            " 3.33812297e+01 6.01839303e+00 1.50699554e+01 9.02843833e+00\n",
            " 2.32684265e+01 2.52102385e+01 1.78504684e+01 3.63184639e+01\n",
            " 3.24255137e+01 1.19159867e+01 3.09124784e+01 1.21369185e+01\n",
            " 4.80271949e+00 1.41972211e+01 2.67529558e+01 2.01760512e+01\n",
            " 2.83614106e+01 4.33773969e+01 1.33194369e+01 1.05096799e+01\n",
            " 2.43970261e+01 1.77961625e+01 3.86723668e+01 2.77665425e+01\n",
            " 2.68439845e+01 3.05965395e+01 1.39020186e+01 2.99123510e+01\n",
            " 1.65357764e+01 1.22968804e+01 1.52540983e+01 1.07807780e+01\n",
            " 1.47158118e+01 2.27828780e+01 3.79469946e+01 3.61926768e+01\n",
            " 2.54464959e+01 2.07464530e+01 3.89176763e+01 1.74548815e+01\n",
            " 1.06826125e+01 5.47523085e+00 2.35495564e+01 1.51973793e+01\n",
            " 2.33651606e+01 1.97789460e+01 1.49945525e+01 7.51477152e+00\n",
            " 2.75394395e+01 3.67831780e+01 1.02893773e+01 2.84268832e+01\n",
            " 3.84155403e+01 3.67974656e+01 3.22870620e+01 2.19249954e+01\n",
            " 1.94329241e+01 1.77113936e+01 2.71758470e+01 1.22522005e+01\n",
            " 3.48854699e+01 8.08849478e+00 3.53385713e+01 3.14665493e+01\n",
            " 8.01430947e+00 3.01590309e+01 1.07985394e+01 3.96537522e+01\n",
            " 6.81416967e+00 1.36483454e+01 7.43778435e+00 1.99281502e-01\n",
            " 2.68595751e+01 7.54053449e+00 1.37830688e+01 1.77508474e+01\n",
            " 5.16996949e+00 2.99872741e+01 3.44208218e+01 9.29667283e+00\n",
            " 2.62745052e+01 2.36461338e+01 4.03374196e+01 1.47164325e+01\n",
            " 3.89922317e+01 9.89536165e+00 1.96733884e+01 2.12578159e+01\n",
            " 1.72971377e+01 1.59999776e+01 1.86270364e+01 2.12458012e+01\n",
            " 2.62799286e+01 4.54525916e+01 2.67234911e+01 3.44579435e+01\n",
            " 1.78730491e+01 2.93341104e+01 1.92692141e+01 2.37646846e+01\n",
            " 1.53095738e+01 2.93435925e+01 2.45638187e+01 1.55821383e+01\n",
            " 2.38936277e+01 2.16100550e+01 3.85295330e+01 2.29725506e+01\n",
            " 2.37928518e+01 2.77130884e+01 4.46602886e+01 1.33954382e+01\n",
            " 1.17398572e+01 4.66663430e+01 3.93790508e+01 2.36771863e+01\n",
            " 1.44463142e+01 1.06152424e+01 2.64304092e+01 2.14757476e+01\n",
            " 3.13329564e+01 2.25927415e+01 3.35565877e+01 2.54903900e+01\n",
            " 3.39453800e+01 1.01082651e+01 2.73235543e+01 2.08073072e+01\n",
            " 1.84844583e+01 1.69928322e+01 1.98428230e+01 8.94611389e+00\n",
            " 4.94341173e+00 4.20462279e+01 3.25657355e+01 2.13111584e+01\n",
            " 1.75200788e+01 2.32929055e+01 1.52217584e+01 9.69321300e+00\n",
            " 3.29009066e+01 3.01339072e+01 1.30454159e+01 1.99196604e+01\n",
            " 1.06647044e+01 8.15623234e+00 4.06995004e+01 2.80359275e+01\n",
            " 1.97970122e+01 1.32965637e+01 2.56273369e+01 2.22851648e+01\n",
            " 2.15819846e+01 1.87023209e+01 1.90396720e+01 1.42143345e+01\n",
            " 1.21085035e+01 2.81776363e+01 1.12272402e+01 2.73000180e+01\n",
            " 3.90049696e+01 2.29173982e+01 4.28895039e+01 2.76644964e+01\n",
            " 1.53117375e+01 3.02821315e+01 2.52812624e+01 3.86143181e+01\n",
            " 4.05477831e+01 3.46795575e+01 2.36002738e+01 2.95625550e+01\n",
            " 3.09409107e+01 1.15435085e+01 2.48465851e+01 2.08529745e+01\n",
            " 3.11717730e+01 9.03170095e+00 2.06479113e+01 3.25105161e+01\n",
            " 3.24537040e+01 7.10338546e+00 8.66723675e-01 2.41216449e+01\n",
            " 2.74766399e+01 2.00509330e+01 1.36232686e+01 1.76696079e+01\n",
            " 3.95750272e+01 1.95229618e+01 2.51843503e+01 2.01154322e+01\n",
            " 1.15247102e+01 2.33112780e+01 2.74184021e+01 1.30180297e+00\n",
            " 3.88970674e+01 9.70525943e+00 3.36825280e+01 1.90895612e+01\n",
            " 1.83530047e+01 1.12121898e+01 3.65859923e+01 1.84705473e+01\n",
            " 4.36271788e+00 1.61957817e+01 1.94553639e+01 1.14479853e+01\n",
            " 2.68746734e+01 2.63530185e+01 2.50597158e+01 1.17030724e+01\n",
            " 1.57922434e+01 1.33674495e+01 1.80626076e+01 9.11709097e+00\n",
            " 4.28095387e+00 3.78544935e+01 1.39339412e+01 4.12762244e-01\n",
            " 4.99999935e+01 6.41747476e+00 3.52624220e+01 1.64518997e+01\n",
            " 2.68371186e+01 2.94824986e+01 2.77460155e+01 1.68011819e+01\n",
            " 3.92260686e+01 2.98461168e+01 1.12011762e+01]\n",
            "selection [234 767 915 161 878 440 891 290 119 654 615 461 148 287 449 376 247 372\n",
            " 552 649  12 544 606 445 912 900  34 113 189  57 708 333 293 238 828 112\n",
            " 772 409 328 737 660 361 623 363 206 553 399 697 195 484 679 274 458 917\n",
            " 562 380 412 764 652 877 375 457 168 245 766 209 743 769 689 629 207 105\n",
            " 760 492 757 242 841  68 197 339 448 349 196 126 268 139  59 648 661 406\n",
            " 455 827 488 699 873 572  30 911 542 122 286 775 509 438 468 523 430 302\n",
            " 638 202 835 893 781 236  61 303 426 194 821 154 463 182 746 240 715] (125,) [ 0.01762423  0.1992815   0.41276224  0.52079631  0.86672368  1.09184826\n",
            "  1.30180297  1.30562571  1.89546965  2.35305344  2.36926037  2.51173487\n",
            "  2.68368557  2.99664154  3.0014929   3.02924705  3.03285148  3.2927149\n",
            "  3.7340743   4.09095301  4.09095301  4.10782937  4.12628141  4.24195909\n",
            "  4.28095387  4.36271788  4.46564227  4.48074867  4.52205768  4.52272771\n",
            "  4.80271949  4.8236593   4.8909465   4.92858553  4.94341173  5.07795739\n",
            "  5.16996949  5.21764263  5.37819473  5.47523085  5.498716    5.59575139\n",
            "  5.66473739  5.67893762  5.68462115  5.94758907  5.96914469  6.01839303\n",
            "  6.02869783  6.03255935  6.03734268  6.15624017  6.30105588  6.41747476\n",
            "  6.52946059  6.63181721  6.75470951  6.81416967  6.90410217  7.10338546\n",
            "  7.23750414  7.31938075  7.34571049  7.37246044  7.43778435  7.51477152\n",
            "  7.51477152  7.54053449  7.64854     7.71700684  7.77734158  8.01078774\n",
            "  8.01430947  8.05893005  8.08849478  8.12231619  8.15623234  8.17389619\n",
            "  8.22351259  8.32535049  8.40157324  8.4936199   8.59241155  8.62184119\n",
            "  8.7009261   8.74826641  8.80979205  8.82870512  8.83803096  8.87373304\n",
            "  8.92302267  8.94611389  8.97909738  9.02843833  9.03170095  9.08596164\n",
            "  9.1133783   9.11709097  9.13704273  9.24272319  9.29431257  9.29667283\n",
            "  9.31646655  9.3598169   9.37979129  9.39759809  9.44075502  9.52416998\n",
            "  9.5338444   9.63815498  9.693213    9.70525943  9.89536165  9.98615747\n",
            "  9.99673926 10.01718443 10.03576446 10.08480823 10.10826512 10.12326347\n",
            " 10.14500235 10.20449995 10.28937728 10.50380189 10.50967985]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [230 270] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "final active learning accuracies [79.72350230414746, 79.72350230414746, 79.26267281105991, 80.18433179723502]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-17.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 18, using model = LogModel, selection_function = MinStdSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial labeled samples size 50\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 10) (50,) unique(labels): [22 28] [0 1]\n",
            "Val set: (1252, 10) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 71.658986 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.80       321\n",
            "           1       0.47      0.64      0.54       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[239  82]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 1 ... 1 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 1 ... 1 0 0]\n",
            "std (1252,) [11.74402237 24.12435403 22.26799048 ... 30.98461015 35.07704184\n",
            " 12.05430096]\n",
            "selection [ 165  339 1094  540 1178  873  251  822 1239  374  454   17  761  677\n",
            "  244 1061  109  196  476  168 1015  989  751  356  377 1100  704 1045\n",
            "  336  472  861 1099  952  122  267 1160  670 1105  400  671  946  198\n",
            "  546  429  163  630   24  492  621  357] (50,) [0.06320241 0.06649871 0.10804426 0.13356281 0.14518993 0.16015225\n",
            " 0.17262522 0.20153389 0.2925088  0.29386802 0.31260817 0.32717726\n",
            " 0.33066543 0.37587623 0.39385401 0.41152168 0.42318478 0.43768088\n",
            " 0.45344273 0.47744312 0.48027082 0.52121756 0.53878883 0.55799344\n",
            " 0.58356826 0.59853989 0.62395444 0.72370336 0.73404586 0.76789967\n",
            " 0.86345356 0.86345356 0.86345356 0.86345356 0.86345356 0.86345356\n",
            " 0.86345356 0.86345356 0.86345356 0.86345356 0.86345356 0.86345356\n",
            " 0.86345356 0.86345356 0.86345356 0.86345356 0.86345356 0.86345356\n",
            " 0.86345356 0.86345356]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [52 48] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 77.419355 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.84       321\n",
            "           1       0.56      0.63      0.59       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.73      0.72       434\n",
            "weighted avg       0.78      0.77      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[265  56]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1202,) [ 9.77889309 35.41849321 25.65714964 ...  7.1202096  36.99855047\n",
            " 12.62407006]\n",
            "selection [ 731  559  462  865  505  533 1017   96  106  993  209  234 1009  199\n",
            "  854   64   66  740  871   16 1146 1168  758  844 1118  175  996  573\n",
            "   27 1190  883 1080 1086  320  323  436  352  212 1054  396  140  181\n",
            " 1019  517  176  366  147 1108  835  836] (50,) [0.0103612  0.01346223 0.02025395 0.069945   0.08599754 0.15743702\n",
            " 0.17958385 0.19316623 0.21819473 0.28405135 0.33437135 0.35037524\n",
            " 0.47662479 0.5445651  0.70834757 0.72319399 0.80871113 0.84669094\n",
            " 0.88948411 0.90496185 0.91790633 0.99469179 1.00861932 1.03806964\n",
            " 1.08937894 1.11212323 1.11814056 1.22431515 1.33104275 1.33594853\n",
            " 1.35685477 1.38730681 1.39722733 1.39747334 1.65520407 1.68860766\n",
            " 1.70130214 1.71673825 1.78259244 1.81981106 1.87503199 1.8857663\n",
            " 1.89142927 1.90013885 1.9010568  1.90225524 1.91479648 1.92754123\n",
            " 1.96354165 1.96354165]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [78 72] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 70.737327 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.73      0.79       321\n",
            "           1       0.46      0.64      0.53       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.75      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[235  86]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "std (1152,) [ 4.54366559 35.3491784  21.30640231 ... 23.70988316 28.80633463\n",
            "  5.37615928]\n",
            "selection [ 964   63  123  222  137  438  729    4  982  102  845  385 1133 1140\n",
            "  903  326   34  332  207  448  576  924  583  722  501 1092  544  823\n",
            "   66  260  422  597    7  780  716  952  149  724  362  629  867 1047\n",
            " 1100  168  228  829   22 1020  116  705] (50,) [0.04153742 0.1261787  0.13904694 0.15598206 0.16443444 0.17610562\n",
            " 0.20674957 0.26332766 0.30884594 0.31634866 0.32005669 0.37062975\n",
            " 0.3906608  0.3906608  0.3906608  0.3906608  0.3906608  0.3906608\n",
            " 0.3906608  0.3906608  0.41771343 0.43172629 0.44206402 0.44959785\n",
            " 0.48565306 0.53855942 0.55802772 0.56573705 0.59728179 0.66407944\n",
            " 0.71570843 0.75942253 0.84007171 0.8943545  0.9175681  0.9232303\n",
            " 0.99148297 0.99367729 1.01284433 1.03368338 1.05667708 1.10843719\n",
            " 1.11734639 1.1348405  1.2978659  1.30105239 1.31936401 1.31936401\n",
            " 1.35528465 1.49711426]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [111  89] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.041475 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.49      0.63      0.55       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[246  75]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "std (1102,) [12.11973228 31.37232161 21.05080585 ... 36.61305817 26.45160433\n",
            "  1.2687514 ]\n",
            "selection [ 226  986    6  688 1037  972  576  633  595  355  153  965   41  755\n",
            "  278  969 1101  484  742  788  915  766 1069 1030  864  596   88  617\n",
            "  186  672  796  422  192  721  881  569  616  180  395  344  776  577\n",
            "  557 1091  564  195  237  590  497  825] (50,) [0.01449547 0.07225209 0.07927054 0.09072954 0.1613451  0.22184054\n",
            " 0.22279839 0.47837924 0.48302619 0.6816004  0.78420065 0.92729258\n",
            " 1.10074798 1.11155149 1.1943292  1.20736639 1.2687514  1.36788418\n",
            " 1.39593254 1.47626127 1.55025991 1.60308639 1.62772271 1.62911549\n",
            " 1.67115215 1.70802929 1.72336474 1.80791694 1.82988731 1.85774266\n",
            " 1.88525019 1.96278166 1.99119244 2.00093697 2.02065318 2.06297756\n",
            " 2.06874177 2.15695122 2.17670032 2.18343558 2.20743757 2.21503132\n",
            " 2.21676434 2.24392165 2.25713214 2.27641101 2.27850601 2.44148283\n",
            " 2.44568126 2.48923743]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [136 114] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 73.732719 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.78      0.81       321\n",
            "           1       0.50      0.63      0.55       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.68      0.70      0.68       434\n",
            "weighted avg       0.76      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[249  72]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1052,) [11.12658787 23.98203585 15.47235099 ... 18.44851413 31.67607558\n",
            " 18.61550243]\n",
            "selection [1009  762  197  681  465  749  980 1015  638  888  111  593   34  694\n",
            "  498  843  310 1025  281  476  446    3  300   21  672  823  918  613\n",
            "  229   88  573  250  800 1047  883  170   89  845  915  631  720  608\n",
            "  999   12  950  315  898  242  879  536] (50,) [0.11742079 0.13169161 0.17969752 0.22932812 0.24680948 0.34084001\n",
            " 0.42836875 0.46723263 0.59663277 0.71503619 0.72869371 0.73069552\n",
            " 0.74709836 0.91537122 0.93490558 0.95650016 0.99334629 1.01448025\n",
            " 1.04859029 1.05989974 1.05989974 1.07110568 1.26273179 1.26380838\n",
            " 1.33166381 1.36526708 1.40206645 1.41522072 1.55513643 1.61112661\n",
            " 1.65128568 1.70865825 1.71733142 1.84948178 1.90531037 1.9553735\n",
            " 1.95582097 1.97048056 1.98634431 1.99411437 2.02253239 2.03017075\n",
            " 2.05840537 2.06752953 2.20740505 2.28521301 2.30187431 2.30187431\n",
            " 2.33219018 2.33718332]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [159 141] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 74.654378 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.78      0.82       321\n",
            "           1       0.51      0.65      0.57       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.69      0.72      0.70       434\n",
            "weighted avg       0.77      0.75      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[250  71]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1002,) [ 7.09855925 19.17739755 12.81803225 ... 16.14082904 31.3356236\n",
            " 14.17596171]\n",
            "selection [ 76 374 869 524  51 769 300 676 890 338 460 137 449 600 334 778  61 551\n",
            " 841 279  17 717  12 908  77 926 756 879 391 168 964 573  73 946 661 356\n",
            " 737 221 703 396 419 498 103 974 822 369 851 744 992 590] (50,) [1.40985522e-03 6.39708749e-02 1.09824905e-01 1.43349878e-01\n",
            " 2.10843620e-01 2.31470971e-01 2.57040917e-01 2.57076373e-01\n",
            " 2.75550310e-01 3.37002416e-01 4.96864028e-01 5.99107777e-01\n",
            " 6.00873989e-01 6.10011067e-01 6.67302090e-01 8.04056751e-01\n",
            " 8.33947148e-01 8.85304886e-01 9.09698581e-01 1.00140339e+00\n",
            " 1.03025829e+00 1.06668865e+00 1.06668865e+00 1.22084285e+00\n",
            " 1.26364606e+00 1.31869474e+00 1.41817245e+00 1.43927362e+00\n",
            " 1.54681621e+00 1.56097539e+00 1.67948447e+00 1.69787677e+00\n",
            " 1.73188109e+00 1.82955589e+00 1.85019565e+00 1.85766448e+00\n",
            " 1.90316190e+00 1.93315467e+00 1.96665061e+00 1.97106301e+00\n",
            " 2.06782490e+00 2.09693920e+00 2.21990315e+00 2.32359185e+00\n",
            " 2.34117926e+00 2.41711774e+00 2.48039706e+00 2.52517702e+00\n",
            " 2.58778018e+00 2.63032501e+00]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [190 160] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86       321\n",
            "           1       0.61      0.60      0.60       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.73      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0]\n",
            "std (952,) [ 9.62258437 13.64960257 10.15212598 18.42426203  3.20383707 14.62307287\n",
            "  5.69117885  3.07206637  5.80272683 31.3700576  21.86585971 21.06920106\n",
            " 14.26290171 10.07575623  9.82329698 17.30708134 15.16662295  7.9659812\n",
            " 13.01021876 19.20558654 10.84788676  5.62922869 14.0285803  19.26033997\n",
            " 12.06143323 12.85143794  7.91103766 18.35167964 18.25791057  6.6382544\n",
            " 22.50633178  5.64171584 11.40644351 22.36377603  4.34971199  4.71643216\n",
            "  7.70103159  5.23536092  5.09065694 14.25177893  5.56115756  9.66904951\n",
            "  5.58857016 23.06787728 20.5761994  11.7697187   3.90416969 10.99449416\n",
            " 13.43033788 12.87880393 20.04650137 15.68772196 15.67278694 11.68696933\n",
            " 13.04148634  4.91290026  9.81690778  6.52553846  8.72780719 14.75139577\n",
            " 14.31831185 15.60420977 17.16951422  4.66407561  4.77118947 14.17687592\n",
            "  7.65901148 10.24670446  7.44559378 14.67463653 14.47417157 10.54740887\n",
            "  4.36175965 14.04822373 11.41515429 16.01769554 16.46953852  8.63699545\n",
            " 32.40995987 13.4347911  15.60697183  1.75187776  4.85779505 11.26515552\n",
            " 11.00454216  8.35353494 11.20005906 13.41081329 10.41229923 11.36657793\n",
            " 15.5935733  11.07256231 17.51567456  5.2717834  10.0694207  11.66710753\n",
            " 16.0965766   5.40259036 30.58626846 11.17362204  3.91068619 10.87071534\n",
            " 36.56757597 13.23910654 15.92185885  9.99276431 11.32292086  6.26041148\n",
            "  9.77216215 15.69339138  5.77220627 12.00264923  8.19657495  7.38992878\n",
            " 12.66627077 18.64499383 13.43441287  9.61775377  2.4677029   7.47174315\n",
            " 11.42314204  9.15150511 17.18314957 15.08016439 12.7682309  10.91483525\n",
            "  9.95869511 18.30740582  8.59597968  9.31659348 14.72210983 13.04617072\n",
            " 10.73384439  8.45432964 35.1607109   4.10879276 13.69170835  8.28484142\n",
            " 10.70756287  7.32187526  4.54706146 39.73910249 16.06871328  5.16274984\n",
            " 17.95254589 13.9534216   6.40126661 16.42318914  5.22850584 14.25711482\n",
            "  6.18154309  9.57013174  6.37546024 17.93879533 13.53045661  9.62962711\n",
            "  4.47447635 11.73903797  3.75041632  8.90511223 13.92212916 11.10247367\n",
            "  9.78009249  6.00046183  5.63409465 10.2997196  10.31461793 15.26444919\n",
            "  7.42577559 10.03275169 11.75750298 13.91182978  3.57102033 10.5890523\n",
            " 34.54076874  2.90180435  8.85981076  8.57884329  9.88274074 22.22873996\n",
            "  9.16999005  4.7073163  19.33713172 14.84978511 13.59649924 12.40664263\n",
            "  9.40045135 14.97238416 15.59917119 21.97782155  5.11011336 11.03811874\n",
            "  4.74089485  6.8197583   5.95933132  5.83262527  9.44618694 39.01169529\n",
            "  7.32852423 12.42786619 10.53911781  5.39091013  5.34083131 10.27117575\n",
            "  9.28367388 11.22487997 10.2106502   3.86849672  7.25537376 12.50820626\n",
            "  4.13095948 21.79293439  5.89836126 13.88862949 21.92386258 14.25182878\n",
            "  5.07824979  4.14679854  2.66844538 13.17850151  6.05562032 13.35443472\n",
            "  7.81255025 19.38523419  5.95255713 12.18807785  8.27185893 17.06837788\n",
            " 12.92795345 10.65888274 21.00182564 11.67796275 13.14041064 11.61759777\n",
            "  9.69725744  8.10625454 10.27579901 10.15206247 11.0971916   7.314086\n",
            "  3.10375445 13.42376957 15.54715702 11.22654923 12.43629189  5.94093159\n",
            " 15.23658839 26.42822222  8.45411078  9.21627802 10.53958316  4.64985499\n",
            "  8.94282212 14.89934728  3.88912097  9.19172077  5.33078091  6.53974842\n",
            "  6.30971352 14.2863846  13.34116361 11.1985499  10.89687355 16.82701785\n",
            " 22.02999826 13.55429587  9.91032182 25.57948283 20.27690809 15.95530083\n",
            "  8.15702858 15.65310123 14.64407035 36.90366307 13.94293075 13.09942623\n",
            " 17.35695053 11.21670409 17.99322442  9.77028517 19.56657442 10.53631951\n",
            " 21.22465295 15.32004395 10.968447    8.82739223  7.28778091 15.64053675\n",
            "  8.75745031 28.13507095 31.05136353  9.23776687  5.56745113  6.60286991\n",
            " 10.92328183  8.56811056  8.58269774  8.9023098   6.06499326  6.89139756\n",
            " 15.4742545  19.59781702 16.28425624 13.56044884 13.40301522 11.93507691\n",
            " 11.41515429 12.91804484  6.91928086  9.09672141  8.74116548 17.01060519\n",
            "  3.95115843  6.63986969 14.05363303 11.08956458  8.9835661  11.55856021\n",
            " 10.71860298  6.33932115 15.54976853 11.41782201 14.17582279  2.72888313\n",
            " 10.11819342  7.38357935  2.83865093 20.74362704 12.30115947 20.30688451\n",
            "  9.24315307  5.91357905  3.0426632   7.03941247  4.62792495 11.9535843\n",
            " 13.10852173 13.05277901  7.04519724 19.042824   11.59523686  5.00295159\n",
            "  4.37495881 10.70027793  9.15331718  2.31518545  4.52443361  7.2828432\n",
            " 12.65917563 14.09975248 13.43519637 10.77070406 12.84309893 14.89047523\n",
            " 11.9748464   9.31655065 12.41508843 15.07885926 20.03750855  8.08057912\n",
            "  2.73777363  2.13198944 24.93013487  9.26080396  7.95231362  6.71969407\n",
            " 33.99693263  4.8503194   7.46380279 10.8833623   5.94382235 35.30247319\n",
            " 21.07161251  8.22524517  8.56736318  3.17186277 12.47437682 14.26835162\n",
            "  5.63002424 12.23767433  3.90987065 13.7835461   4.71664921 17.6183206\n",
            " 10.5413931   4.31658255 17.28486841  5.46835743  8.82058122  5.95010326\n",
            "  7.31643427  3.74214365  1.64507619 14.27929003  6.56233161 14.17787235\n",
            " 14.32594177  5.24129556 14.13056648 12.3972214   7.1463674  24.13017814\n",
            " 19.12261819  5.02786075  2.2733875   7.40795305 11.19095708  7.24373798\n",
            " 13.37554886  3.61661138 11.19326675  6.05877861  3.79507722  6.58271006\n",
            "  7.87477969  5.39546631  9.64423356  8.28624209  6.47385103 12.700102\n",
            "  4.10136058 12.12887509 13.94451675  7.68980818  3.7704679   3.1379273\n",
            "  4.37451042 23.71342434 11.96422904  2.49234128  5.46059024  8.16196984\n",
            " 17.98008599 12.81317313  9.36458676  6.85311272 18.4143486  17.21891562\n",
            " 19.32598386 11.29058031 13.58780288 14.40797672 18.85258341  4.61472784\n",
            " 15.30925654 12.66158738 13.2850596   8.29443872 13.81879719 16.42827341\n",
            "  4.3015304   9.79580849 16.17133441  3.44345037  9.3604262  24.98684583\n",
            "  8.44009792  5.68493611 14.28744067  9.39311126 23.11215067 10.69886866\n",
            " 19.18898456 40.85212072 13.35813252 10.90604866  5.708611    7.19575551\n",
            "  5.03414634 11.01683012 10.71955551  5.70080489 13.0284723   9.71209447\n",
            " 13.77940241  3.57947068  8.16870328 22.54443273  6.08471018  5.22866311\n",
            "  4.53318209 25.38667897  9.52830903 13.92254726 13.07481975 10.95307112\n",
            "  8.11483514  8.08053963  6.48055955 10.5078044   5.47358924  5.4853193\n",
            "  5.26722745  6.40534252 12.56745551 24.61955595 19.03433654  3.23066644\n",
            "  4.69307659 16.11339912  8.61177919 17.39347419  5.84407139 11.05686368\n",
            " 11.67674656 10.33866869  4.97097426 12.13763948 11.58646976 16.57778561\n",
            "  2.95566422  4.20413379  9.05703045 27.15667041 13.58658695  1.9801899\n",
            "  8.1823788  11.54771807  2.21251716 12.98828389 18.86693191  7.06893567\n",
            "  6.14325377 18.93507367 17.55159861 13.76091309 18.88430508 15.35601556\n",
            "  4.76384276  7.04606844 15.46035829 14.87281754  8.74841132 11.47619151\n",
            "  5.08597846 15.96688222  4.37495881 12.13574164  6.73335185 16.41138705\n",
            " 22.14663006  8.40098115 18.28477839  7.06503342  5.5009856  14.26651275\n",
            " 10.33866869  8.79685471 12.19519021 10.3880046  19.39491763  8.17454797\n",
            " 15.00897822 26.95084752 38.12047335 11.22927085  7.93887896 11.44423208\n",
            " 13.28905073 23.86474202  5.88809012  6.39623084 29.37050512  3.9159099\n",
            " 17.07749667 15.06194865  9.27660199  0.7005723   9.07505845 13.01521679\n",
            " 13.32469413  4.9664123  10.76795551  7.35097212  4.59652797 21.73125469\n",
            "  8.66253966  6.37518636  8.52715032 10.0832893  15.29327398  6.40277075\n",
            "  9.85789829  9.8607144  14.13960402  7.87333302 13.05630084  3.58113135\n",
            "  4.98300982 10.32614061  3.61436389  3.93706838  6.7319531  13.54202711\n",
            " 31.64290843 15.35302068 17.1378001  10.73614679 37.47882964  8.03808249\n",
            "  7.4956235  10.6570362   9.82222694 17.34886191 12.5876532  13.30246305\n",
            "  6.02833032  8.22026395 12.49392548 12.96955869 13.86735711  7.86489305\n",
            "  6.69276817 18.9775986  10.4435983   5.55764299 13.77208109  7.53342475\n",
            "  5.44894437 11.30799447 11.4041304   3.62845131  6.96772449 14.84750729\n",
            "  8.91889574  6.56758885 10.43711145  8.99008729 16.03518044 10.40970796\n",
            "  7.79072464 23.55901793  3.85048468 10.74065737  8.30010901  6.61084041\n",
            "  7.32774418 14.20111352  1.66843659 11.79426601  8.94658164 23.43845337\n",
            " 23.20126091 14.65591303 13.73968657  5.45837172 12.35076062  8.9023098\n",
            " 32.90184757  9.56621866  8.75138474 10.18519285 27.77950313  5.78350525\n",
            " 17.9099709   9.40967551 15.00583305  3.9981197   4.82486798 14.2848012\n",
            " 11.82247562  3.47718559 11.33127597  2.82799889 23.34347257  6.8677001\n",
            "  6.73443492 18.04832585  7.93975538 11.14730063 16.55861735  7.14816511\n",
            "  2.88676056  9.6805756  14.484238   12.85621097  9.42615788  9.83588001\n",
            " 24.54800441  8.81721767  3.02622982  8.16437974  8.03944925  7.80021606\n",
            "  5.16753678 14.68814277 11.30483519  9.16123283  3.73616341 30.50526077\n",
            " 11.61011463  5.79278425  6.41411339  8.58364166  7.99279464  2.80152232\n",
            "  9.49636853 11.8619447  10.20351672 20.0538539  12.69691987  4.04844476\n",
            "  9.85533542 22.22334691 19.78773357  6.2790349  16.29843274 19.68150152\n",
            "  4.96834018 12.49740388 10.13520869  2.72529025  6.09120849 22.41395165\n",
            " 17.23787374 15.22077321 28.8469436   2.51469522 15.32239582 10.27002303\n",
            " 11.18638773  6.15360662 16.59598692 18.92068288 13.45619445 16.07572697\n",
            " 19.50482722  6.74864156 17.51636259  3.96373914  7.3983388   8.12334958\n",
            " 15.40722455 17.09083835  6.18473784 16.61111424  9.58457693  5.6184504\n",
            " 15.05549088  5.50413625  8.23922353 11.49643985 12.50389967 12.4332105\n",
            " 35.6372058   9.99328563 22.23433432  5.91523315 14.41027222 15.60957937\n",
            "  5.07456946  6.83114621 17.39121973 15.04847981 16.58498057  4.7294913\n",
            " 11.90133304 19.02336665 10.01690648  4.85883298  7.13159751  6.03190978\n",
            " 24.8287459  11.56701814 20.22906707 14.01035981 12.32724653 25.77479903\n",
            "  5.9842676   3.92041426  2.43147548  9.81702137 16.78941283  7.65575675\n",
            "  6.05562032  8.17106434 13.41547385 19.16717064  6.77188453 17.39347419\n",
            " 19.81313419  8.76749069  9.37765503  5.66258812 12.33902839  9.45756302\n",
            " 16.41895022  6.21459998 10.08339844 15.70387787  6.11305731 12.87299039\n",
            " 22.00967046 11.19950254 22.59841091  2.13045843  4.95148544 10.42287249\n",
            "  2.51412949 13.35048339  7.19001406  7.79255958  7.56335901  7.78795573\n",
            "  3.51162339 15.35324368 20.54309922  4.96070919 13.90967789 12.30451488\n",
            " 20.60437994  1.57872463 25.8344808  17.94673146 12.12262997  7.89476237\n",
            "  7.78186729 11.0974229  12.2052004  14.76372983 31.71391064  7.6184123\n",
            " 15.72648929 11.78160793  4.17531026 15.38055244 28.77728771  3.93276524\n",
            "  5.37016282 15.51430858  8.54942672 11.15362353 13.92703713  9.75338842\n",
            " 22.28528744 14.59877217 15.50684736 29.17655801 11.98269646 19.76993753\n",
            " 16.663213   12.10217893  6.66534375 15.06232584  3.91511395 10.27584896\n",
            " 14.68814277 16.84781324 14.19834066 24.76095547 14.44045596 12.18807785\n",
            " 15.35601556 11.21597814  7.15176922  4.64126301  5.01998002 24.7167939\n",
            " 12.69905675  8.97084515 11.00454216 11.85907148  4.46567743 11.37034175\n",
            " 15.73928587 17.43041549  8.49813284 19.47437352  3.88413795  3.76227372\n",
            "  8.91018066 12.79892703  5.2074015   7.65204966  8.06753454 13.44389744\n",
            "  1.572383   12.58806912 11.32627212 11.52223057  6.65427223  3.69446697\n",
            "  5.14542096  8.36588113  5.37146377 12.40919011 14.45751203 13.01287391\n",
            " 22.58766784 11.61577699  6.23863001 17.39222412 13.17766971 15.63055381\n",
            " 21.15741652 14.89661899  7.08066909  1.4000673   1.21060985 16.93000684\n",
            "  7.81804451 12.25841694 29.6435506   7.36452171 14.06393657 15.29986558\n",
            " 11.2257235  11.01553886 14.1450896  17.12786357 10.98020611  8.13831811\n",
            " 12.72286566 11.42653854 28.30184968 14.05096722  8.1751156   2.23765772\n",
            " 23.34378806 12.75948824 27.22686723  2.73959842 30.51576168 15.14759137\n",
            "  3.09245415  7.12559298 22.3247056   7.31421989  4.05391005  8.35162514\n",
            "  6.63873102 12.14687919  5.03968678 12.40351186 10.71427304  2.40562266\n",
            "  7.4624799   8.01074656  4.26369842 11.06200057  4.19079316  1.18990618\n",
            " 17.24810366  5.80978097 17.02890075  7.26110209 16.24658499 12.14144582\n",
            " 18.2713304  12.41440398 32.07364307 10.97204092]\n",
            "selection [567 941 898 897 876 811 392 638  81 515 795 361 518 917 404 345 935 770\n",
            " 118 429 798 717 218 711 323 360 921 695 663 326 672 175 510 680 332   7\n",
            " 924 240 425 375   4 497 453 661 804 172 475 587 590 409] (50,) [0.7005723  1.18990618 1.21060985 1.4000673  1.572383   1.57872463\n",
            " 1.64507619 1.66843659 1.75187776 1.9801899  2.13045843 2.13198944\n",
            " 2.21251716 2.23765772 2.2733875  2.31518545 2.40562266 2.43147548\n",
            " 2.4677029  2.49234128 2.51412949 2.51469522 2.66844538 2.72529025\n",
            " 2.72888313 2.73777363 2.73959842 2.80152232 2.82799889 2.83865093\n",
            " 2.88676056 2.90180435 2.95566422 3.02622982 3.0426632  3.07206637\n",
            " 3.09245415 3.10375445 3.1379273  3.17186277 3.20383707 3.23066644\n",
            " 3.44345037 3.47718559 3.51162339 3.57102033 3.57947068 3.58113135\n",
            " 3.61436389 3.61661138]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [213 187] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "std (902,) [1.44340006e+01 2.70077564e+00 4.69812942e+00 8.24234617e+00\n",
            " 7.56542068e+00 5.23134743e+00 1.21108597e+00 3.48073375e+01\n",
            " 1.79815357e+01 1.14789142e+01 9.38927771e+00 5.89665372e+00\n",
            " 6.52812657e+00 7.21927319e+00 1.11659254e+01 1.67580973e+00\n",
            " 1.47503884e+00 1.75552406e+01 7.83435987e+00 1.95076847e+00\n",
            " 5.46688492e+00 1.04750955e+01 5.47788306e+00 6.95734248e+00\n",
            " 5.19474258e+00 8.92690539e-01 1.02464150e+01 4.57967006e+00\n",
            " 1.18468509e+01 2.81187059e+00 8.13620463e+00 1.57125874e+01\n",
            " 3.03569217e+00 1.19175862e+00 3.14838295e+00 3.24278691e+00\n",
            " 3.10371961e+00 6.51272430e+00 7.81394583e+00 3.24117495e+00\n",
            " 3.73513497e+00 1.04882897e+01 1.12433317e+01 3.11556608e+00\n",
            " 3.23025562e-01 2.76344163e+00 8.83380415e+00 6.65269060e+00\n",
            " 1.99533828e+01 1.18261583e+01 1.10522452e+01 6.29558668e+00\n",
            " 1.29906532e+00 5.08509104e+00 6.98286469e+00 1.64927217e-02\n",
            " 2.32802208e+00 7.06244388e+00 9.68867331e+00 6.73967769e+00\n",
            " 1.63262782e+01 2.60238180e+00 4.78989839e+00 8.13111373e+00\n",
            " 4.36585173e+00 4.33531542e+00 3.69963734e+00 7.78335933e+00\n",
            " 7.82271088e+00 6.71207532e+00 3.74013762e+00 1.41560968e+01\n",
            " 6.07341309e+00 1.06360672e+01 3.65560370e+00 1.30355856e+00\n",
            " 3.31114912e+01 1.17263613e+01 1.06083570e+01 1.59998351e+00\n",
            " 6.99104738e+00 5.42175977e+00 4.73652728e+00 5.64868484e+00\n",
            " 6.78662867e+00 1.08042858e+01 7.87888558e+00 1.33069352e+01\n",
            " 2.94031905e+00 8.45996134e+00 2.49210779e+00 1.16330261e+01\n",
            " 1.44148859e+01 1.14177215e+01 3.42565159e+00 3.28122693e+01\n",
            " 8.70532203e+00 9.52385684e-01 5.52891500e+00 3.77255050e+01\n",
            " 1.62909135e+01 1.15757527e+01 1.10338764e+01 3.71277541e+00\n",
            " 4.45351915e+00 3.08605924e+00 5.73669475e+00 1.54285222e+00\n",
            " 2.68370309e+00 7.57652147e+00 9.17731001e+00 8.90382383e+00\n",
            " 9.37490823e+00 7.82414276e+00 5.89343773e+00 2.05827187e+00\n",
            " 7.55065555e+00 7.27757546e+00 1.20072482e+01 1.12102361e+01\n",
            " 1.52826399e+01 1.08117536e+01 5.59417862e+00 1.26457866e+01\n",
            " 1.63207557e+00 2.49703413e-02 8.22524162e+00 6.29459702e+00\n",
            " 3.88957884e+00 2.39860364e+00 3.30927746e+01 5.76905895e+00\n",
            " 6.35803779e+00 1.95033735e+00 4.83282575e+00 2.11435233e-01\n",
            " 2.51361343e+00 4.37522823e+01 1.07926915e+01 1.01505284e+01\n",
            " 8.17822028e+00 5.92210299e+00 1.04212900e+00 5.92806547e+00\n",
            " 4.40747738e+00 9.61917877e+00 1.19412448e+00 4.43227314e+00\n",
            " 3.61492295e+00 8.89757153e+00 4.81098440e+00 3.05893727e+00\n",
            " 1.55325175e+00 9.54689654e+00 5.24859761e+00 1.56571913e+00\n",
            " 3.21248554e+00 5.00724878e+00 1.34960375e+00 3.72725919e+00\n",
            " 3.47238874e+00 2.32975722e-02 6.72797425e+00 8.76961107e+00\n",
            " 3.93585965e+00 5.16472077e+00 6.58688723e+00 8.28559639e+00\n",
            " 5.82494352e+00 2.47548106e+01 1.94350461e+00 5.43718855e+00\n",
            " 1.96655188e+00 5.59497093e+00 1.03256849e+01 2.75774250e+00\n",
            " 1.82141994e+01 5.83987003e+00 4.48297828e+00 7.30148540e-01\n",
            " 7.01056710e+00 1.02784182e+01 1.02242379e+01 1.67745480e+01\n",
            " 1.87144723e+00 6.83004291e+00 5.54772552e+00 2.31505445e+00\n",
            " 2.56050744e+00 3.17014769e+00 5.83640825e+00 2.05314502e+01\n",
            " 1.63594076e+00 1.21694580e+01 5.74962292e+00 1.35287471e+00\n",
            " 2.71350982e+00 5.28665531e+00 6.95195310e+00 4.27760296e+00\n",
            " 1.13757191e+01 3.26640890e+00 4.26306896e+00 6.05413217e+00\n",
            " 1.42182554e+00 1.97932350e+01 4.29667179e+00 1.34649585e+01\n",
            " 1.19925449e+01 1.14144834e+01 3.50213260e+00 2.43417759e+00\n",
            " 6.24209630e+00 3.09611648e+00 5.98170926e+00 6.33197466e+00\n",
            " 7.16782102e+00 3.56616452e+00 5.49540790e+00 7.61731434e-01\n",
            " 1.76844118e+01 3.79815839e+00 5.39181185e+00 6.82728648e+00\n",
            " 9.85913236e+00 6.86448594e+00 8.37982815e+00 4.65654620e+00\n",
            " 3.26126174e+00 5.91876378e+00 7.86133639e+00 4.29864894e-01\n",
            " 4.99488214e+00 9.41860653e+00 1.07461883e+01 4.10745904e+00\n",
            " 7.70970492e+00 1.26171595e+00 7.47286033e+00 2.41687121e+01\n",
            " 3.78104099e+00 1.99959408e+00 6.64137743e+00 2.92267737e+00\n",
            " 4.23440147e-01 8.04114913e+00 3.37627978e+00 4.02321575e+00\n",
            " 2.45103780e+00 6.00770428e+00 7.47175995e+00 7.24587325e+00\n",
            " 8.79696656e+00 1.09915690e+01 3.41799964e+00 1.66313132e+01\n",
            " 1.46745193e+01 8.44297830e+00 5.29563821e+00 3.22401486e+01\n",
            " 1.67292737e+01 4.39287484e+00 6.47623695e+00 9.81199567e+00\n",
            " 5.31504331e+00 3.88444801e+01 1.32616461e+01 1.55253046e+01\n",
            " 1.38004904e+01 6.50302668e+00 9.48237121e+00 1.17624484e+01\n",
            " 2.01719704e+01 4.06710881e+00 2.08063484e+01 7.63386655e+00\n",
            " 5.85460999e+00 2.66637861e+00 1.53360797e+00 1.00643678e+01\n",
            " 4.33435258e+00 2.41178503e+01 2.87426312e+01 6.36918908e+00\n",
            " 3.50884833e+00 5.03039201e+00 4.87462375e+00 4.26634077e+00\n",
            " 4.64406956e+00 2.35702667e+00 2.64622564e+00 1.06809903e+01\n",
            " 6.30158416e+00 8.24736139e+00 5.56305780e+00 3.91095417e+00\n",
            " 8.62539767e+00 4.90873950e+00 6.07341309e+00 1.00902567e+01\n",
            " 8.26336931e-01 6.23851299e+00 6.94084039e+00 7.42016941e+00\n",
            " 3.15515206e+00 4.12388852e+00 9.65234154e+00 1.44291111e+01\n",
            " 6.08308833e+00 5.69449852e+00 5.70234926e+00 3.53058784e+00\n",
            " 1.09568163e+01 7.47679213e+00 8.40585011e+00 7.79290277e+00\n",
            " 8.12000628e+00 1.01300861e+01 6.70339792e+00 7.17770473e+00\n",
            " 4.03942939e+00 2.89065264e+00 5.32725534e+00 8.78206785e-01\n",
            " 8.99731193e+00 6.79239978e+00 7.82441910e+00 4.67811286e+00\n",
            " 1.12689190e+01 5.86084326e+00 1.55770760e+00 3.05493921e+00\n",
            " 4.94925647e+00 4.73604640e+00 3.36185003e+00 4.92688236e+00\n",
            " 5.42794557e+00 4.63925210e+00 6.27220789e+00 6.96235699e+00\n",
            " 6.76742759e+00 5.23081715e+00 9.04702174e+00 5.31722668e+00\n",
            " 6.46635887e+00 6.53401253e+00 1.33506133e+01 1.02171544e+01\n",
            " 1.17052828e+01 3.49120079e+00 1.07094252e+01 3.66004511e+00\n",
            " 3.32851080e+01 4.53027066e+00 4.20003162e+00 2.71290853e+00\n",
            " 4.79347969e+00 2.34785749e+01 9.46108323e+00 6.96595495e+00\n",
            " 4.96356036e+00 8.09871811e+00 4.03165764e+00 6.19178393e-01\n",
            " 6.16587895e+00 4.20291720e-01 6.83448299e+00 4.75583898e+00\n",
            " 6.67846503e+00 7.24128335e+00 4.23644186e+00 2.80570418e+00\n",
            " 4.32841726e+00 7.18937254e+00 5.12138560e+00 5.54669240e+00\n",
            " 2.95489909e+00 6.49988185e+00 2.66442636e-01 7.24807744e+00\n",
            " 1.59469257e+00 4.33676059e-01 1.02576769e+01 1.88843980e+00\n",
            " 6.27117421e+00 2.25155011e+01 9.75927237e+00 3.39045299e+00\n",
            " 6.56439518e+00 8.96944623e+00 3.80765077e+00 5.73683504e+00\n",
            " 6.93522337e+00 4.16250350e+00 3.18919233e+00 4.81440559e+00\n",
            " 1.04559031e+01 1.76412916e+00 5.55814500e+00 7.47489045e+00\n",
            " 9.78915416e-01 1.03471563e+01 3.17844649e+00 6.64801236e+00\n",
            " 1.07432441e+01 2.38689736e+00 6.15996703e+00 2.26987598e+00\n",
            " 1.98522606e+01 6.51864674e+00 7.88432093e-01 3.41331540e+00\n",
            " 1.45243236e+01 3.19091038e+00 2.12647521e+00 3.58035242e+00\n",
            " 8.43645651e+00 1.11333729e+01 1.13910255e+01 5.64567710e+00\n",
            " 8.58690229e+00 2.39371090e+00 1.22657271e+01 1.08435967e+00\n",
            " 9.15547446e+00 1.17045608e+01 5.88474452e+00 5.00211241e+00\n",
            " 5.65614653e+00 8.42521536e+00 1.30713367e-01 5.85137060e+00\n",
            " 1.26412157e+01 1.06202072e+01 2.11806798e+01 5.11958634e+00\n",
            " 3.86650866e+00 6.17124512e+00 1.74073466e+00 9.29070885e+00\n",
            " 2.05666751e+00 1.70401305e+01 4.03135823e+01 7.08036593e+00\n",
            " 4.04737172e+00 5.59782746e-01 3.81203922e+00 7.35644239e+00\n",
            " 5.40008418e+00 2.19488983e+00 4.45143709e+00 8.81762738e+00\n",
            " 4.68223283e+00 8.11511970e+00 4.49114383e+00 2.66428093e+01\n",
            " 4.99274612e+00 2.99529943e+00 2.28324357e+00 2.32311282e+01\n",
            " 5.29145752e+00 5.71695721e+00 1.33319566e+01 5.66063727e+00\n",
            " 5.26369246e+00 1.17452908e+00 4.07010763e+00 8.63321748e-01\n",
            " 3.40090916e+00 4.51729716e+00 2.44641751e+00 3.36802486e+00\n",
            " 1.12468811e+01 5.38767104e+00 7.60927080e+00 8.19685799e-01\n",
            " 8.66131759e+00 6.64721877e+00 7.47150560e+00 3.29287872e+00\n",
            " 8.50551625e+00 9.73311934e+00 5.39596934e+00 5.40122047e+00\n",
            " 7.87440073e+00 1.13024910e+01 9.91205935e+00 5.89939687e-01\n",
            " 3.08943307e+00 1.28911918e+01 1.59204843e+01 7.53058119e+00\n",
            " 6.84120083e+00 6.99552386e+00 4.63718289e+00 4.54741557e+00\n",
            " 2.12175323e+00 7.89366722e+00 9.21398254e+00 4.29801389e+00\n",
            " 2.13470600e+01 6.30182125e+00 5.20276074e+00 6.68731147e+00\n",
            " 3.36784880e+00 1.45806182e+01 9.06901793e+00 6.06875632e+00\n",
            " 9.37714970e+00 9.83397086e+00 3.05493921e+00 3.28281280e+00\n",
            " 2.89653776e+00 1.18425562e+01 1.47657882e+01 7.75113319e+00\n",
            " 1.14161889e+01 3.58078732e+00 1.07586934e+01 9.13798479e+00\n",
            " 5.39596934e+00 4.73570045e+00 6.37228199e+00 3.79389202e+00\n",
            " 1.08523510e+01 1.53350973e+00 7.41599422e+00 1.84278051e+01\n",
            " 4.12425370e+01 9.56232023e+00 4.30070193e+00 5.69612706e+00\n",
            " 1.37039015e+01 1.30082066e+01 3.50091894e+00 3.43542525e+00\n",
            " 1.53279368e+01 1.66833870e+00 1.07561087e+01 5.37701691e+00\n",
            " 7.17049625e+00 6.34516259e+00 1.30282896e+01 4.98780837e+00\n",
            " 7.31865653e+00 7.49618855e+00 1.33246038e+01 7.49732376e-01\n",
            " 1.11339351e+01 4.14090681e+00 5.89611921e+00 3.09746595e+00\n",
            " 6.07466570e+00 8.82576476e+00 3.61792605e+00 5.07458180e+00\n",
            " 6.34104389e+00 7.40520194e+00 1.00007412e+01 8.18467907e+00\n",
            " 6.93155799e-01 5.66167186e+00 1.94726457e+00 2.01658444e+00\n",
            " 6.89001422e+00 7.42589700e+00 8.64818995e+00 1.13430890e+01\n",
            " 6.62614033e+00 4.22652455e+01 3.40216454e+00 4.21307435e+00\n",
            " 5.74329584e+00 8.43393128e+00 8.59877108e+00 5.84392531e+00\n",
            " 7.13060445e+00 3.09913787e+00 3.01257648e+00 6.48993200e+00\n",
            " 7.17889055e+00 2.60120165e+01 4.69726193e+00 2.35701127e+00\n",
            " 1.34283945e+01 6.96868159e+00 1.01954047e+00 7.75602156e+00\n",
            " 5.81915231e+00 5.02280460e+00 4.48457294e+00 6.06952964e+00\n",
            " 4.22154660e+00 6.73523158e+00 6.92450992e+00 6.26969420e+00\n",
            " 3.14331752e+00 3.10786562e+00 4.62326488e+00 7.41264256e+00\n",
            " 6.18640711e+00 5.74079658e+00 5.64776327e+00 3.58237633e+00\n",
            " 9.12106918e+00 5.44239448e+00 1.10205593e-01 7.36308655e-02\n",
            " 8.94311778e+00 4.74626157e+00 5.60412033e+00 2.15952798e+01\n",
            " 7.75219403e+00 2.73201128e+00 1.26114917e+01 2.38792673e+00\n",
            " 3.33501867e+00 2.35702667e+00 3.09507454e+01 6.57916815e+00\n",
            " 8.00466110e+00 6.82383956e+00 1.39884937e+01 4.52089099e+00\n",
            " 1.01830448e+01 8.11173731e+00 1.06126124e+01 1.34236228e+00\n",
            " 2.30087622e+00 1.40978245e+01 7.08473143e+00 9.02646782e+00\n",
            " 2.26547501e+01 9.83984577e-01 3.13330339e+00 9.75470023e+00\n",
            " 4.72098459e+00 5.56868642e+00 7.93671031e+00 3.37875150e+00\n",
            " 1.99405555e+00 1.04704087e+01 9.16301877e+00 1.02974301e+01\n",
            " 6.26620936e+00 1.26320704e+01 6.26547155e+00 2.67130610e+00\n",
            " 8.42711435e+00 6.70820338e+00 1.27185875e+00 3.09445378e+00\n",
            " 3.63977586e-02 5.47378286e+00 7.11191576e-01 4.21062618e+00\n",
            " 2.41597845e+00 5.13192355e+00 2.27013049e+00 3.38558067e+00\n",
            " 4.69876537e+00 6.15011817e+00 4.92252835e+00 6.21446344e+00\n",
            " 1.13621400e+01 1.13804076e+01 2.48678234e+00 1.10348266e+01\n",
            " 2.31643062e+01 1.10681148e+01 4.57669214e+00 1.25434101e+01\n",
            " 7.05018747e+00 1.92976343e+00 6.18869466e+00 1.97617792e+00\n",
            " 1.26418943e+01 2.01256008e+01 9.54257517e+00 7.23024296e+00\n",
            " 1.72241543e+01 6.77325071e+00 4.92439468e+00 5.37610962e+00\n",
            " 1.98275854e+00 7.07625105e+00 1.11176578e+01 8.70724516e+00\n",
            " 8.92131078e+00 7.28850886e+00 3.75209885e+00 5.56025031e+00\n",
            " 8.70430405e-02 6.20210329e+00 3.28860725e+00 7.89680397e+00\n",
            " 1.00685141e+01 4.57016022e+00 7.50145459e+00 5.24309903e+00\n",
            " 3.10707634e-01 1.09297246e+01 4.29340803e+00 3.16104873e+00\n",
            " 5.85870196e+00 6.18505569e+00 6.76614402e+00 3.43876042e+01\n",
            " 5.24279265e+00 1.21259137e+00 2.17371174e+00 9.40849760e+00\n",
            " 1.43470642e+01 1.36093611e+01 6.03190977e+00 1.08143610e+01\n",
            " 1.11690713e+01 1.67460869e+01 3.76652474e+00 4.32335486e+00\n",
            " 1.34538360e+01 5.14092396e+00 1.36816273e+00 4.37352954e+00\n",
            " 9.40529400e-01 9.62458093e+00 4.53953462e+00 1.23987787e+01\n",
            " 3.50219701e+00 1.58368637e+01 2.50262540e+01 3.45260030e+00\n",
            " 2.29867607e+00 5.26505604e+00 9.06154505e+00 1.86465936e+00\n",
            " 3.09611648e+00 7.77452595e+00 8.62197833e+00 1.19330051e+01\n",
            " 5.38006423e+00 7.47150560e+00 1.62866879e+01 2.37571269e+00\n",
            " 7.91967186e+00 4.25885751e+00 1.06999810e+01 3.10608661e+00\n",
            " 1.39975787e+01 2.25255537e+00 7.00279865e+00 1.62424921e+01\n",
            " 9.57682636e+00 1.37048701e+01 2.13292283e+01 3.82143965e+00\n",
            " 6.92645887e+00 3.46638780e+00 2.29778345e+00 6.00354557e+00\n",
            " 2.34322844e+00 3.78438708e+00 4.57275970e-01 4.09915301e+00\n",
            " 4.75375553e+00 1.70633245e+01 4.21316178e+00 1.38362998e+00\n",
            " 6.65668791e+00 8.15095777e+00 2.43381126e+01 1.81914126e+01\n",
            " 6.19777573e+00 1.13228742e+01 5.30657559e+00 6.10931151e+00\n",
            " 1.58664365e+01 5.89753251e+00 2.08958447e+01 1.45038222e+00\n",
            " 1.13639267e+01 2.64725570e+00 3.80416967e-01 1.28593525e+01\n",
            " 2.51943035e+01 5.66949245e+00 6.22285388e+00 7.21345933e+00\n",
            " 4.35904573e+00 4.93969985e+00 6.81684112e+00 7.23892903e+00\n",
            " 1.15688451e+01 6.89281771e+00 1.73505149e+01 1.56951317e+01\n",
            " 1.05975451e+01 2.44407441e+00 1.05743082e+01 7.95671766e+00\n",
            " 4.79044344e-01 7.31520075e+00 8.29767236e+00 2.77128292e+00\n",
            " 3.09445378e+00 9.64320887e+00 9.28581632e+00 1.25730048e+01\n",
            " 6.38777557e+00 5.49540790e+00 6.30182125e+00 6.56272186e+00\n",
            " 5.26828327e+00 3.39365700e+00 3.79514542e+00 1.46325621e+01\n",
            " 3.66594458e+00 6.60429799e+00 5.42175977e+00 1.01442716e+01\n",
            " 3.34589508e+00 7.83142768e+00 1.32807200e+01 1.06155749e+01\n",
            " 5.55767729e+00 1.65161054e+01 9.03539850e-01 4.27657712e+00\n",
            " 6.29321116e-01 8.59704633e+00 4.11195297e+00 6.24621569e+00\n",
            " 2.60096708e+00 1.69107456e+00 6.25108706e+00 8.71080421e+00\n",
            " 5.62307845e+00 4.36097533e+00 2.18629252e+00 3.77811159e+00\n",
            " 4.41339036e+00 4.14007529e+00 6.06172031e+00 8.65451214e+00\n",
            " 6.28102411e+00 1.61717402e+01 7.45155928e+00 4.70866169e+00\n",
            " 1.12928986e+01 7.16759096e+00 1.17312842e+01 1.42837024e+01\n",
            " 1.12075228e+01 8.32130393e+00 1.56986824e+01 2.56008998e+00\n",
            " 3.74598084e+00 2.87817916e+01 2.44195847e+00 8.78839244e+00\n",
            " 5.38694654e+00 1.05760579e+01 1.48070666e+00 1.58345517e+00\n",
            " 8.05873382e+00 4.22977253e+00 4.77301613e+00 5.92229032e+00\n",
            " 1.77517296e+00 2.71721320e+01 3.64698904e+00 1.01394700e+01\n",
            " 1.25378357e+01 1.00739574e+01 1.75553219e+01 2.85923972e+01\n",
            " 7.27724923e+00 8.67492370e+00 1.67059395e+01 6.07695146e+00\n",
            " 7.08361671e+00 1.26297595e+00 3.93896397e+00 1.23589543e+01\n",
            " 5.89760031e+00 6.22111961e+00 7.91124311e+00 5.43431491e+00\n",
            " 6.35740390e+00 2.90162148e+00 4.68390448e+00 2.96458425e+00\n",
            " 1.45519505e+00 2.03448985e-01 1.42668266e+01 3.26983776e+00\n",
            " 5.83326358e+00 1.70205337e+01 6.39242951e+00 4.50656105e+00\n",
            " 3.17088807e+01 6.00828967e+00]\n",
            "selection [ 55 161 125 652 607 692 606 430 893 135 378 700  44 782 365 244 231 381\n",
            " 762 800 445 487 363 828 560 654 179 547 219 410 475 300 467 323  25 826\n",
            " 724  97 400 633 586 142 423 465  33 146   6 709 237 881] (50,) [0.01649272 0.02329757 0.02497034 0.03639776 0.07363087 0.08704304\n",
            " 0.11020559 0.13071337 0.20344898 0.21143523 0.26644264 0.31070763\n",
            " 0.32302556 0.38041697 0.42029172 0.42344015 0.42986489 0.43367606\n",
            " 0.45727597 0.47904434 0.55978275 0.58993969 0.61917839 0.62932112\n",
            " 0.6931558  0.71119158 0.73014854 0.74973238 0.76173143 0.78843209\n",
            " 0.8196858  0.82633693 0.86332175 0.87820678 0.89269054 0.90353985\n",
            " 0.9405294  0.95238568 0.97891542 0.98398458 1.01954047 1.042129\n",
            " 1.08435967 1.17452908 1.19175862 1.19412448 1.21108597 1.21259137\n",
            " 1.26171595 1.26297595]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [236 214] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0]\n",
            "std (852,) [1.11816250e+01 1.61388801e+01 1.01806471e+01 1.55473224e+01\n",
            " 1.42941526e+01 2.37238385e+00 3.79387211e+01 2.27429385e+01\n",
            " 2.65615906e+01 1.39056798e+01 8.58940098e+00 1.02187279e+01\n",
            " 1.43942252e+01 1.32948661e+01 6.53314894e+00 1.24694488e+01\n",
            " 2.30987174e+01 1.14983808e+01 5.87761385e+00 1.02347508e+01\n",
            " 1.95230065e+01 1.34446801e+01 1.38834533e+01 7.05480973e+00\n",
            " 1.59136254e+01 7.96229558e+00 2.41382610e+01 5.98255186e+00\n",
            " 1.26401841e+01 2.42056314e+01 1.04466248e+00 8.70373479e+00\n",
            " 6.38994738e+00 1.77327983e+00 1.27548490e+01 7.80174952e+00\n",
            " 1.25249419e+01 7.08702176e+00 2.41859531e+01 1.07625745e+01\n",
            " 9.39938370e+00 6.35422751e+00 1.55465184e+01 1.33795514e+01\n",
            " 1.87981781e+01 1.73906244e+01 1.47607547e+01 9.30124605e+00\n",
            " 9.43302291e+00 7.38101691e+00 1.07179703e+01 2.41176761e+00\n",
            " 1.09532851e+01 1.88049993e+01 1.85401236e+01 1.87842541e+01\n",
            " 5.27497651e+00 4.99815578e+00 8.89079264e+00 6.20044954e+00\n",
            " 1.09464102e+01 6.09803355e+00 1.57327701e+01 1.58444442e+01\n",
            " 8.77061759e+00 7.38683555e+00 1.68060610e+01 1.41666537e+01\n",
            " 1.62714665e+01 1.59863408e+01 1.66943533e+01 3.18657694e+01\n",
            " 1.55616012e+01 1.52260855e+01 2.90450770e+00 1.13641212e+01\n",
            " 1.02280193e+01 8.79484676e+00 1.10725853e+01 1.03647732e+01\n",
            " 9.39532271e+00 8.00828023e+00 1.64522695e+01 1.48505539e+01\n",
            " 1.40288389e+01 7.26408372e+00 1.12029215e+01 1.30410913e+01\n",
            " 1.81813647e+01 5.00038583e+00 2.88704432e+01 1.52369016e+01\n",
            " 1.20184888e+01 3.54173999e+01 1.61276303e+01 1.83469978e+01\n",
            " 1.31599635e+01 1.21522978e+01 4.87276546e+00 1.36549263e+01\n",
            " 1.69340832e+01 5.92849320e+00 1.49783835e+01 9.83134000e+00\n",
            " 1.05623802e+01 1.33846429e+01 6.62354579e+00 1.44880990e+01\n",
            " 6.22540172e+00 1.12832492e+01 1.31768564e+01 7.30618679e+00\n",
            " 1.45980433e+01 1.68121817e+01 1.65706925e+01 1.15793050e+01\n",
            " 8.04533258e+00 1.85441818e+01 9.25345014e+00 1.60134580e+01\n",
            " 1.24420953e+01 8.22199872e+00 9.96692969e+00 2.93315399e+01\n",
            " 5.76724503e+00 1.33318772e+01 1.06261863e+01 1.07141733e+01\n",
            " 6.14876231e+00 4.15906390e+01 1.12177208e+01 7.52708172e+00\n",
            " 1.79072172e+01 1.34400660e+01 1.42560094e+01 5.07472748e+00\n",
            " 1.52996695e+01 9.02360875e+00 4.65006878e+00 1.70162442e+01\n",
            " 1.18358199e+01 1.04366455e+01 5.02495547e+00 1.21747374e+01\n",
            " 5.46084382e+00 1.07882419e+01 1.51796314e+01 1.11435385e+01\n",
            " 1.06045100e+01 6.41637577e+00 4.95148342e+00 9.09029417e+00\n",
            " 1.62577073e+01 6.99569383e+00 1.30947597e+01 6.08860415e+00\n",
            " 1.29297996e+01 1.00648139e+01 3.59906095e+01 1.05823510e+01\n",
            " 9.53761794e+00 1.29319805e+01 2.95385063e+01 1.14783727e+01\n",
            " 6.31390895e+00 2.94241983e+01 1.38003107e+01 1.43707766e+01\n",
            " 1.25455786e+01 1.57401688e+01 1.60229080e+01 2.39705794e+01\n",
            " 3.16563160e+00 1.37027530e+01 3.81984503e+00 8.60675653e+00\n",
            " 4.71117804e+00 6.97394240e+00 1.21390088e+01 4.28903317e+01\n",
            " 4.11715978e+00 1.48703695e+01 1.17660826e+01 5.73595526e+00\n",
            " 3.92594407e+00 1.41818480e+01 7.88721889e+00 1.21621744e+01\n",
            " 1.11397726e+01 5.39860224e+00 5.99808887e+00 1.38908036e+01\n",
            " 6.12394589e+00 2.57283697e+01 8.47287679e+00 1.27363899e+01\n",
            " 2.13429322e+01 1.33824734e+01 5.80384342e+00 4.20233079e+00\n",
            " 1.47481282e+01 4.52627112e+00 1.03114822e+01 5.13846100e+00\n",
            " 1.53659537e+01 6.09393261e+00 1.32972171e+01 1.66401198e+01\n",
            " 1.65304248e+01 1.18175300e+01 2.14384613e+01 9.30446666e+00\n",
            " 1.45930276e+01 1.34972170e+01 9.04955385e+00 9.71178348e+00\n",
            " 1.08935846e+01 8.61827601e+00 9.92141367e+00 1.56444204e+01\n",
            " 1.56929796e+01 1.42061774e+01 1.33694529e+01 1.39423999e+01\n",
            " 2.63200506e+01 5.03774940e+00 7.02753611e+00 7.80705836e+00\n",
            " 5.34803634e+00 1.72751223e+01 5.46283649e+00 8.48364517e+00\n",
            " 7.64487264e+00 7.83805755e+00 5.99321854e+00 1.59172523e+01\n",
            " 1.50534504e+01 1.27620585e+01 1.16627440e+01 1.75227032e+01\n",
            " 1.73988298e+01 1.40707738e+01 8.63946584e+00 3.01034588e+01\n",
            " 2.21305332e+01 9.84225816e+00 7.43180032e+00 1.44037952e+01\n",
            " 1.67661950e+01 4.02853776e+01 1.16122777e+01 1.50584580e+01\n",
            " 1.51124827e+01 1.31700256e+01 1.91014495e+01 1.09956991e+01\n",
            " 8.10673806e+00 1.30401148e+01 2.22626615e+01 1.15185807e+01\n",
            " 1.14477214e+01 6.14945416e+00 8.74992719e+00 1.50229785e+01\n",
            " 9.15881052e+00 2.84067419e+01 2.98573290e+01 9.56875479e+00\n",
            " 8.22237864e+00 8.32484753e+00 1.17233928e+01 9.86484825e+00\n",
            " 8.11743640e+00 1.10706805e+01 6.83393637e+00 2.24024367e+00\n",
            " 1.17130673e+01 1.70134027e+01 6.20412984e+00 1.43496906e+01\n",
            " 1.51464339e+01 1.49092622e+01 1.41666537e+01 1.29171234e+01\n",
            " 7.94096511e+00 7.81788238e+00 1.82658841e+01 1.00521434e+00\n",
            " 6.60910605e+00 1.44111721e+01 9.38195868e+00 6.12637146e+00\n",
            " 8.32675686e+00 9.26499654e+00 8.34912781e+00 9.03077313e+00\n",
            " 1.25399947e+01 1.62573865e+01 1.21218043e+01 7.26359522e+00\n",
            " 2.18068242e+01 1.37030174e+01 2.32144411e+01 1.15698588e+01\n",
            " 7.15339230e+00 6.14778147e+00 1.24604039e+01 1.56285754e+01\n",
            " 1.22978171e+01 6.99615311e+00 1.93925264e+01 1.11408474e+01\n",
            " 6.03169245e+00 5.46120195e+00 1.33283778e+01 9.86664929e+00\n",
            " 8.94857825e+00 9.55578604e+00 1.71755039e+01 1.23054828e+01\n",
            " 9.87343383e+00 8.85326064e+00 1.25785448e+01 1.10210915e+01\n",
            " 1.45329142e+01 8.98042938e+00 1.53594076e+01 1.73813087e+01\n",
            " 2.42519398e+01 7.30707079e+00 2.49048873e+01 8.33966145e+00\n",
            " 6.40727134e+00 8.90270043e+00 3.51875006e+01 4.68917236e+00\n",
            " 9.14225379e+00 1.12364442e+01 6.00422659e+00 4.02500359e+01\n",
            " 1.76251598e+01 9.92346013e+00 1.02118336e+01 1.25952351e+01\n",
            " 1.24234148e+01 1.55491676e+01 1.38288084e+01 2.05512019e+00\n",
            " 1.39383014e+01 9.43867472e+00 6.84978928e+00 1.74500017e+01\n",
            " 5.84134104e+00 8.73516482e+00 4.58122519e+00 1.04744794e+01\n",
            " 4.00113381e+00 1.33669898e+01 1.28285775e+01 1.01477060e+01\n",
            " 1.63860363e+01 1.06340474e+01 5.79241375e+00 2.45610462e+01\n",
            " 1.93694814e+01 3.78714810e+00 1.04780185e+01 1.08941545e+01\n",
            " 7.82616461e+00 1.24252452e+01 8.61656809e+00 6.59608025e+00\n",
            " 6.89027998e+00 7.43586827e+00 1.06249853e+01 8.75420115e+00\n",
            " 1.02956033e+01 1.50688812e+01 1.18951379e+01 1.88753306e-01\n",
            " 1.26137843e+01 1.28395664e+01 8.37249711e+00 6.86557290e+00\n",
            " 9.90073088e+00 2.30438508e+01 1.60441591e+01 1.48019704e+01\n",
            " 1.54484566e+01 7.08022540e+00 6.82226707e+00 4.50370235e+00\n",
            " 2.03846400e+01 1.74266056e+01 1.87970739e+01 1.21720306e+01\n",
            " 2.08164957e+01 1.01255360e+01 2.01134851e+01 1.36708394e+01\n",
            " 1.44220217e+01 1.58036992e+01 9.96639997e+00 1.51924116e+01\n",
            " 1.22310664e+01 1.08049951e+01 1.50672900e+01 9.60811658e+00\n",
            " 2.40173181e+01 7.83891196e+00 5.08335225e+00 1.40039845e+01\n",
            " 1.25015473e+01 2.32129981e+01 1.01300097e+01 2.02654318e+01\n",
            " 4.19196740e+01 1.58313934e+01 8.30340782e+00 8.12090453e+00\n",
            " 5.14073208e+00 1.38093367e+01 1.08616191e+01 6.28146253e+00\n",
            " 1.31213567e+01 1.02392785e+01 1.58422566e+01 9.62392654e+00\n",
            " 2.36454229e+01 5.88898569e+00 5.52786564e+00 3.72567248e+00\n",
            " 2.50739938e+01 9.57051896e+00 9.00318424e+00 1.32381577e+01\n",
            " 1.04997760e+01 8.42340684e+00 8.81069934e+00 5.84918442e+00\n",
            " 4.57123996e+00 4.52069765e+00 6.92333235e+00 1.68864871e+01\n",
            " 2.76856490e+01 1.69652201e+01 1.71259052e+01 5.64879622e+00\n",
            " 1.64490670e+01 6.08641671e+00 1.33556613e+01 1.17108515e+01\n",
            " 1.11425658e+01 1.44084874e+00 1.30430445e+01 1.16513418e+01\n",
            " 1.54692340e+01 1.26641108e+01 2.94019529e+01 1.24218384e+01\n",
            " 1.15278383e+01 1.40264792e+01 6.62093522e+00 1.94911164e+01\n",
            " 9.38417972e+00 2.51433033e+00 1.97322511e+01 1.42545171e+01\n",
            " 1.43769545e+01 2.02958104e+01 1.43631792e+01 1.87029325e+00\n",
            " 5.88762712e+00 1.60426293e+01 1.37549086e+01 1.06508081e+01\n",
            " 1.01994932e+01 1.09028086e+01 1.20486200e+01 5.46120195e+00\n",
            " 1.53573437e+01 6.25140586e+00 1.57857497e+01 2.03254611e+01\n",
            " 8.40727849e+00 2.01438734e+01 6.19755316e+00 1.49659254e+00\n",
            " 1.21650297e+01 1.11425658e+01 8.15253811e+00 1.11444869e+01\n",
            " 1.19437412e+01 1.75256802e+01 6.29031228e+00 1.45187826e+01\n",
            " 2.60745157e+01 4.18594130e+01 1.17308823e+01 3.29213214e+00\n",
            " 1.13480610e+01 9.46921046e+00 2.04326606e+01 3.88711592e+00\n",
            " 7.79698157e+00 2.77808698e+01 5.24534235e+00 1.94640572e+01\n",
            " 1.49256531e+01 1.00653435e+01 6.44095315e+00 1.42167047e+01\n",
            " 1.63112119e+01 6.99740134e+00 6.77263081e+00 1.08141373e+01\n",
            " 2.04146276e+01 1.27179238e+01 1.05396617e+01 8.02378487e+00\n",
            " 1.01285202e+01 1.70582535e+01 8.25111652e+00 9.54344375e+00\n",
            " 8.26396907e+00 1.21454733e+01 8.87930740e+00 1.35886780e+01\n",
            " 1.04950975e+01 2.26130186e+00 4.21348636e+00 1.49990430e+01\n",
            " 3.65315864e+01 1.52733993e+01 1.92194745e+01 7.98710914e+00\n",
            " 3.94549530e+01 8.00544063e+00 7.51076375e+00 9.39075924e+00\n",
            " 1.02146274e+01 2.07963418e+01 1.20840455e+01 1.48931228e+01\n",
            " 5.68213664e+00 1.00161656e+01 1.10389568e+01 1.34944332e+01\n",
            " 1.73976882e+01 7.43130092e+00 6.05853673e+00 1.99500910e+01\n",
            " 1.05888031e+01 1.35949555e+01 7.67244800e+00 3.43748605e+00\n",
            " 1.01209512e+01 1.30853216e+01 1.16936656e+00 6.84765978e+00\n",
            " 1.84225254e+01 1.26441317e+01 6.49820641e+00 1.12091909e+01\n",
            " 8.63498686e+00 1.75730195e+01 9.54594974e+00 7.98323833e+00\n",
            " 2.70536032e+01 4.96225822e+00 1.22915990e+01 8.82703652e+00\n",
            " 1.89422649e+01 1.25059704e+01 5.95598191e+00 2.38525429e+01\n",
            " 1.97955267e+01 1.23891905e+01 1.45039047e+01 5.58825381e+00\n",
            " 8.78628782e+00 1.10706805e+01 3.29808087e+01 1.16505766e+01\n",
            " 7.89469359e+00 1.27624478e+01 2.94918338e+01 4.12257325e+00\n",
            " 1.77871641e+01 8.70063843e+00 1.74966473e+01 5.34775859e+00\n",
            " 4.60139217e+00 2.06575609e+01 1.17009329e+01 1.37181128e+01\n",
            " 1.92090754e+01 8.61052132e+00 1.84823628e+01 9.93537681e+00\n",
            " 1.22179743e+01 1.26526714e+01 9.78269938e+00 7.16751230e+00\n",
            " 1.41214049e+01 1.44501864e+01 1.28644971e+01 1.21538582e+01\n",
            " 2.40051805e+01 1.00828657e+01 9.29461471e+00 1.06650635e+01\n",
            " 9.06287572e+00 6.25887006e+00 1.31254384e+01 8.05249334e+00\n",
            " 3.80408635e+01 1.15951895e+01 9.47558262e+00 5.11713582e+00\n",
            " 6.83943626e+00 5.62099382e+00 8.51000476e+00 1.64029715e+01\n",
            " 1.16368872e+01 1.17226190e+01 1.42294052e+01 2.41713049e+00\n",
            " 8.54926326e+00 2.47933343e+01 2.14074929e+01 8.02075397e+00\n",
            " 1.87689827e+01 1.50665327e+01 3.39610530e+00 1.18175275e+01\n",
            " 1.24820515e+01 2.53531902e+00 1.78315337e+01 2.06057192e+01\n",
            " 1.27543880e+01 3.21594295e+01 1.77920180e+01 1.20714837e+01\n",
            " 1.16569983e+01 7.09330572e+00 1.38556007e+01 2.19800960e+01\n",
            " 1.62368940e+01 1.55119622e+01 2.00547025e+01 9.60861902e+00\n",
            " 2.05971014e+01 7.24978987e+00 1.19768441e+01 1.31325724e+01\n",
            " 2.19308484e+01 4.18661862e+00 1.70329641e+01 1.03868596e+01\n",
            " 1.62321088e+01 5.96342056e+00 3.54938014e+00 1.42134366e+01\n",
            " 1.24850256e+01 1.56679422e+01 3.48435983e+01 8.59770312e+00\n",
            " 5.81766153e+00 1.37939578e+01 1.46326071e+01 6.91243253e+00\n",
            " 9.17217951e+00 2.23851366e+01 1.76218793e+01 1.80319992e+01\n",
            " 5.91989529e+00 1.18356279e+01 1.65796929e+01 9.02631751e+00\n",
            " 8.34203035e+00 5.92890196e+00 2.16434450e+01 1.25484114e+01\n",
            " 2.15792530e+01 1.52238067e+01 8.62946981e+00 2.71331673e+01\n",
            " 7.78476349e+00 6.01900674e+00 1.22588434e+01 1.54564494e+01\n",
            " 7.24104462e+00 4.52627112e+00 9.35759242e+00 1.33356402e+01\n",
            " 1.98425948e+01 8.38588472e+00 1.64490670e+01 2.26319435e+01\n",
            " 1.54521735e+01 1.05456890e+01 8.63945313e+00 1.26644357e+01\n",
            " 9.21468702e+00 1.74973203e+01 5.64967951e+00 9.41602310e+00\n",
            " 1.33553696e+01 1.16956368e+01 7.04586962e+00 1.92942358e+01\n",
            " 7.20289786e+00 2.10341659e+01 4.85908093e+00 9.12721172e+00\n",
            " 1.32436247e+01 5.69832165e+00 7.17798262e+00 9.10690940e+00\n",
            " 1.73693173e+01 1.99105403e+01 4.78215643e+00 1.44210774e+01\n",
            " 1.20996040e+01 2.21070673e+01 2.64477103e+01 1.44260064e+01\n",
            " 1.05833216e+01 1.05677945e+01 8.72444768e+00 9.89923830e+00\n",
            " 1.09311303e+01 1.50744061e+01 3.51031556e+01 8.40131865e+00\n",
            " 1.83147852e+01 1.65536520e+01 1.61369247e+01 2.78657037e+01\n",
            " 2.28697868e-02 6.67406901e+00 1.61430815e+01 1.03310461e+01\n",
            " 8.75980029e+00 1.52231541e+01 1.09698338e+01 2.29575802e+01\n",
            " 1.16971724e+01 1.54286087e+01 3.12412028e+01 1.06597313e+01\n",
            " 1.54078972e+01 1.98637282e+01 1.27937240e+01 1.33417593e+01\n",
            " 8.29251818e+00 1.32536775e+01 1.31254384e+01 1.53620904e+01\n",
            " 1.36561850e+01 2.28011364e+01 1.78375339e+01 1.32972171e+01\n",
            " 1.43631792e+01 1.06261389e+01 7.90988535e+00 4.26726477e+00\n",
            " 3.39262901e+00 2.51943089e+01 1.57534974e+01 8.77164408e+00\n",
            " 1.02280193e+01 1.29998050e+01 5.72030977e+00 1.06901741e+01\n",
            " 1.68662351e+01 1.49474750e+01 8.34399009e+00 2.25510850e+01\n",
            " 4.94677547e+00 1.51635314e+01 5.87342588e+00 6.72332384e+00\n",
            " 1.35804287e+01 1.15489395e+01 1.05631533e+01 1.23700094e+01\n",
            " 7.50043088e+00 6.87066536e+00 3.81413772e+00 5.74974830e+00\n",
            " 1.13857345e+01 4.47025739e+00 1.47028001e+01 1.84064341e+01\n",
            " 1.30791297e+01 2.45740260e+01 1.27589228e+01 7.50225788e+00\n",
            " 1.77630920e+01 1.41689051e+01 1.37618683e+01 2.21746107e+01\n",
            " 1.62249545e+01 9.24784309e+00 1.51531420e+01 7.69930505e+00\n",
            " 1.24752382e+01 2.37082910e+01 4.39391087e+00 1.31733707e+01\n",
            " 1.48373554e+01 1.44453835e+01 7.51310397e+00 1.22556829e+01\n",
            " 1.45486112e+01 1.14231890e+01 8.16539852e+00 1.13265959e+01\n",
            " 5.06212861e+00 2.89516570e+01 1.30245167e+01 8.71814002e+00\n",
            " 1.50234234e+01 1.34704738e+01 2.64286947e+01 2.56176047e+01\n",
            " 1.23438672e+01 7.88818933e+00 2.43736544e+01 7.64133358e+00\n",
            " 4.47937336e+00 8.96846327e+00 1.13462323e+01 7.11249873e+00\n",
            " 1.13115018e+01 9.95149879e+00 8.55931859e+00 8.68818481e+00\n",
            " 6.10255561e+00 7.60107194e+00 3.57724029e+00 1.96095078e+01\n",
            " 1.94438566e+01 9.99516281e+00 1.52003674e+01 1.49013258e+01\n",
            " 1.45615211e+01 1.17744615e+01 2.90426494e+01 1.36680200e+01]\n",
            "selection [740 379 287  30 558 453 487  33 471 347 275 529   5  51 627 465 637  74\n",
            " 172 499 768 634 555 662 842 431 365 790 174 503 184 356 180 587 657 199\n",
            " 530 767 810 793 832 391 441 201 693 440 354 592 138 335] (50,) [0.02286979 0.18875331 1.00521434 1.04466248 1.16936656 1.44084874\n",
            " 1.49659254 1.77327983 1.87029325 2.05512019 2.24024367 2.26130186\n",
            " 2.37238385 2.41176761 2.41713049 2.51433033 2.53531902 2.9045077\n",
            " 3.1656316  3.29213214 3.39262901 3.3961053  3.43748605 3.54938014\n",
            " 3.57724029 3.72567248 3.7871481  3.81413772 3.81984503 3.88711592\n",
            " 3.92594407 4.00113381 4.11715978 4.12257325 4.18661862 4.20233079\n",
            " 4.21348636 4.26726477 4.39391087 4.47025739 4.47937336 4.50370235\n",
            " 4.52069765 4.52627112 4.52627112 4.57123996 4.58122519 4.60139217\n",
            " 4.65006878 4.68917236]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [247 253] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "final active learning accuracies [71.6589861751152, 77.41935483870968, 70.73732718894009, 73.04147465437788, 73.73271889400922, 74.65437788018433, 79.49308755760369, 78.57142857142857, 78.80184331797236, 79.95391705069125]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-18.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 19, using model = LogModel, selection_function = MinStdSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial labeled samples size 25\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 10) (25,) unique(labels): [14 11] [0 1]\n",
            "Val set: (1277, 10) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 10)\n",
            "Validation set: (1277, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.119816 \n",
            "Classification report for LogisticRegression(C=2.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.81      0.81       321\n",
            "           1       0.46      0.47      0.47       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.64      0.64      0.64       434\n",
            "weighted avg       0.72      0.72      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[260  61]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1277,) [49.9662873  28.8536205  36.33237823 ... 50.         31.1057179\n",
            "  5.67789955]\n",
            "selection [ 472 1243 1059  761  104 1103  758  338 1207  852  584  441 1267  569\n",
            "  532  305 1098  647   34  642  856  556  618  195  885] (25,) [0.06808826 0.22410244 0.49260674 0.61380191 0.65821882 0.74323007\n",
            " 0.81352372 0.83189341 0.88404875 0.93152581 0.98410404 0.98902771\n",
            " 1.03073722 1.03297393 1.03297393 1.06988947 1.18556729 1.25549151\n",
            " 1.31157807 1.39622995 1.41486548 1.4806693  1.50861979 1.56012424\n",
            " 1.6963123 ]\n",
            "trainset before adding uncertain samples (25, 10) (25,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [26 24] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 69.815668 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.74      0.78       321\n",
            "           1       0.44      0.58      0.50       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.64      0.66      0.64       434\n",
            "weighted avg       0.73      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[238  83]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1252,) [47.34906634 43.7574771  34.63708956 ... 49.99889271 36.07935017\n",
            "  3.91074275]\n",
            "selection [  66   16 1072   49 1001  293  238  383  168  697  424  719  495  107\n",
            "  698  361  509  576  884   25 1107  305    6 1053  231] (25,) [0.14954878 0.22702412 0.26320942 0.32013555 0.32099696 0.32099696\n",
            " 0.32855381 0.33503165 0.36788401 0.37058392 0.38574569 0.49895843\n",
            " 0.58404259 0.66137339 0.77455954 0.82445162 0.89078665 0.89390931\n",
            " 0.98116737 0.98162873 0.98162873 1.01672023 1.11268808 1.21537369\n",
            " 1.22836394]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (75, 10) (75,)\n",
            "updated train set: (75, 10) (75,) unique(labels): [39 36] [0 1]\n",
            "val set: (1227, 10) (1227,)\n",
            "\n",
            "Train set: (75, 10)\n",
            "Validation set: (1227, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 69.354839 \n",
            "Classification report for LogisticRegression(C=0.6666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.73      0.78       321\n",
            "           1       0.44      0.60      0.51       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.64      0.66      0.64       434\n",
            "weighted avg       0.73      0.69      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[233  88]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1227,) [39.85026496 44.76710032 39.00070022 ... 49.99678991 37.96901317\n",
            " 10.93389105]\n",
            "selection [ 538 1033 1182  867  617 1223  152  911  141 1178  545  122  963 1112\n",
            "  673  801  453  260  432  934  834 1018  218  589  531] (25,) [0.07704152 0.30417574 0.30492539 0.31846886 0.33674243 0.5705953\n",
            " 0.69688677 0.72415813 0.76359683 0.88219789 1.01765525 1.0829346\n",
            " 1.15217761 1.16494303 1.19844409 1.22792714 1.3132565  1.53427031\n",
            " 1.65434779 1.7768524  1.83159503 1.926364   1.93418856 2.1582698\n",
            " 2.15914095]\n",
            "trainset before adding uncertain samples (75, 10) (75,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [53 47] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 70.737327 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79       321\n",
            "           1       0.45      0.62      0.52       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[237  84]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1202,) [34.22043954 43.11663568 38.44595958 ... 49.98786329 39.20486739\n",
            "  0.97794892]\n",
            "selection [1170 1021  747  230   23  999  421   50  622 1201 1063  883  462  152\n",
            " 1124 1067  983  871 1050  996  881  217 1133  971   57] (25,) [0.18914394 0.23230804 0.32113967 0.3583666  0.41422208 0.51242521\n",
            " 0.62648153 0.72262309 0.89869515 0.97794892 1.15110462 1.16716544\n",
            " 1.19097796 1.19235197 1.24663012 1.25254872 1.72630551 1.74542188\n",
            " 1.78642505 1.97112878 2.15379063 2.40583697 2.42066893 2.43665391\n",
            " 2.47130292]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (125, 10) (125,)\n",
            "updated train set: (125, 10) (125,) unique(labels): [69 56] [0 1]\n",
            "val set: (1177, 10) (1177,)\n",
            "\n",
            "Train set: (125, 10)\n",
            "Validation set: (1177, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.580645 \n",
            "Classification report for LogisticRegression(C=0.4, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.76      0.80       321\n",
            "           1       0.48      0.63      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.67       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[244  77]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1177,) [40.79401187 36.01342652 32.95279467 ... 26.96766658 49.99282708\n",
            " 30.82126307]\n",
            "selection [ 895  159    7  688  110  458  209 1032  921 1038  374 1097  246  572\n",
            "  460 1090  342  337  894  335  809  888  818  819  111] (25,) [0.0836472  0.20656854 0.32551492 0.38616171 0.65042571 0.75498619\n",
            " 0.75498619 0.75498619 0.75498619 0.75498619 0.75498619 0.75498619\n",
            " 0.75498619 0.75498619 0.75498619 0.75498619 0.75498619 0.75498619\n",
            " 0.75498619 0.75498619 0.75498619 0.75498619 0.75498619 0.75498619\n",
            " 0.75498619]\n",
            "trainset before adding uncertain samples (125, 10) (125,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [90 60] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 74.884793 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.83       321\n",
            "           1       0.52      0.58      0.54       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.69      0.69       434\n",
            "weighted avg       0.76      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[260  61]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1152,) [46.25832368 28.99166035 33.56481831 ... 29.1616806  49.99837183\n",
            " 20.97459722]\n",
            "selection [ 463  763  908  102  748  486  122  128  388    9  392  609  570  610\n",
            "  175 1131  497 1138  144   21  528  187  899  707  370] (25,) [0.10548273 0.17077031 0.22325229 0.22374847 0.2564101  0.34705338\n",
            " 0.38562564 0.57402986 0.64496803 0.66370592 0.7279122  0.7279122\n",
            " 0.7279122  0.7279122  0.7279122  0.7279122  0.7279122  0.7279122\n",
            " 0.7279122  0.7279122  0.74333305 0.80242507 0.83333482 0.85368192\n",
            " 0.94774888]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (175, 10) (175,)\n",
            "updated train set: (175, 10) (175,) unique(labels): [113  62] [0 1]\n",
            "val set: (1127, 10) (1127,)\n",
            "\n",
            "Train set: (175, 10)\n",
            "Validation set: (1127, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 71.428571 \n",
            "Classification report for LogisticRegression(C=0.2857142857142857, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.75      0.79       321\n",
            "           1       0.46      0.62      0.53       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.68      0.66       434\n",
            "weighted avg       0.75      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[240  81]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1127,) [44.82394693 33.33652652 34.55923684 ... 30.75395151 49.99801489\n",
            " 27.93285631]\n",
            "selection [ 545  813 1079 1101  903  292  832  907  705  833 1113  563  596  702\n",
            "  765   12  524  605  812  917 1011  500 1015   51  372] (25,) [0.11031753 0.24466097 0.25546987 0.27904613 0.29120488 0.42041807\n",
            " 0.46369008 0.52066629 0.65912423 0.67555803 0.73863082 0.82459132\n",
            " 1.02343371 1.08406726 1.08425061 1.25501637 1.25642515 1.34593579\n",
            " 1.36991591 1.44069384 1.45388082 1.51216152 1.54650482 1.54770104\n",
            " 1.65836362]\n",
            "trainset before adding uncertain samples (175, 10) (175,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [130  70] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 71.889401 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.75      0.80       321\n",
            "           1       0.47      0.63      0.54       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[241  80]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1102,) [38.8004607  34.21262119 31.98584619 ... 24.68688582 49.94730638\n",
            " 26.72799671]\n",
            "selection [ 526  855   75  487  353  399  905  325 1080  179  107  311  485  529\n",
            "  449  783  801  489 1069  652  364  145   97  174  845] (25,) [0.08194225 0.29982002 0.43222097 0.46893813 0.49595381 0.61582982\n",
            " 0.62780348 0.71504953 0.88218144 1.07421885 1.14943491 1.15825886\n",
            " 1.21163032 1.24753742 1.30629642 1.31486734 1.69901601 1.80120243\n",
            " 1.80599076 1.8302678  1.84666052 1.88640474 2.05940012 2.29440714\n",
            " 2.42424433]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (225, 10) (225,)\n",
            "updated train set: (225, 10) (225,) unique(labels): [141  84] [0 1]\n",
            "val set: (1077, 10) (1077,)\n",
            "\n",
            "Train set: (225, 10)\n",
            "Validation set: (1077, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 71.889401 \n",
            "Classification report for LogisticRegression(C=0.2222222222222222, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.76      0.80       321\n",
            "           1       0.47      0.60      0.53       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.68      0.66       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[244  77]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1077,) [39.29182926 32.1418471  29.92461257 ... 22.34352377 49.92837054\n",
            " 28.35008021]\n",
            "selection [  64  170 1044  707  564  711  973  788  554  122  777  535  906   84\n",
            "  673  387  207  950  558  805  465  994  596  760  285] (25,) [0.3855173  0.49737557 1.28645226 1.32971229 1.6246366  1.91298518\n",
            " 2.64162328 2.66163929 2.69991566 2.73701392 2.74935649 2.76609422\n",
            " 2.92187979 3.28559582 3.51262831 3.5812572  3.67558582 3.70367778\n",
            " 3.78171943 4.08253742 4.20099206 4.29801479 4.36649392 4.58050522\n",
            " 4.74294804]\n",
            "trainset before adding uncertain samples (225, 10) (225,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [155  95] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 76.958525 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.56      0.56      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1052,) [37.91736127 27.47162168 26.19575852 ... 19.76063848 49.87364022\n",
            " 25.16215567]\n",
            "selection [ 68 287 893 838 276  20 886 881 666 970 697  33 541 501 144 841 480 634\n",
            " 475  40 244 718 312 979 374] (25,) [2.06909274 2.98904597 4.11103099 4.18191904 4.65451739 4.74561391\n",
            " 4.76424481 4.83022522 4.9558113  4.95871641 5.07417461 5.17942166\n",
            " 5.21050844 5.21845904 5.24090141 5.31334903 5.46766423 5.49770717\n",
            " 5.54286961 5.66854125 5.74048224 5.7983613  5.81506517 5.93654649\n",
            " 6.38453793]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (275, 10) (275,)\n",
            "updated train set: (275, 10) (275,) unique(labels): [164 111] [0 1]\n",
            "val set: (1027, 10) (1027,)\n",
            "\n",
            "Train set: (275, 10)\n",
            "Validation set: (1027, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 77.880184 \n",
            "Classification report for LogisticRegression(C=0.18181818181818182, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1027,) [40.46024269 24.31093208 24.21601202 ... 18.46136213 49.94898926\n",
            " 22.85459086]\n",
            "selection [ 895 1013  843  128  654  431  269  385  844 1001  501  718   59  482\n",
            "  715  325  625  996  850  182  987  731  724  717  620] (25,) [0.39683916 1.41546689 1.4616191  2.7892159  2.82938855 3.38047834\n",
            " 3.46020524 3.54409274 3.56078198 3.79350679 3.87883418 4.23000121\n",
            " 4.2423685  4.6786176  4.74488782 4.76673054 4.93176812 5.05504222\n",
            " 5.38772815 5.44768076 5.62979583 5.6423064  5.70179864 5.73644082\n",
            " 5.74222971]\n",
            "trainset before adding uncertain samples (275, 10) (275,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [175 125] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 77.880184 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "std (1002,) [35.8663854  26.78050952 24.05820085 ... 13.74414724 49.58890588\n",
            " 26.26455732]\n",
            "selection [987 274 562 128  40 141 102 272 190   4 371 500 504 748 377 203 826  64\n",
            "  66 832 567 609 215 363 305] (25,) [2.34760254 2.40537917 2.99452329 3.51677653 3.72572859 3.91187052\n",
            " 3.97399462 4.47365142 4.54362275 4.99516525 5.1144771  5.14283966\n",
            " 5.19526246 5.33297495 5.41849987 5.67694262 5.80621675 6.11930491\n",
            " 6.26253296 6.30759604 6.43936421 6.47798882 6.65688682 6.7693441\n",
            " 6.83307563]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (325, 10) (325,)\n",
            "updated train set: (325, 10) (325,) unique(labels): [183 142] [0 1]\n",
            "val set: (977, 10) (977,)\n",
            "\n",
            "Train set: (325, 10)\n",
            "Validation set: (977, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=0.15384615384615385, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "std (977,) [37.5652069  26.54804504 24.39300177 15.82355828 26.15226413 14.58709757\n",
            " 29.1489304  14.82449389 11.13839272 27.34797681 45.93979772 45.73489821\n",
            " 47.60606683 12.07556231 37.38605535 32.36302777 19.57293876 20.0014925\n",
            " 39.91811072  7.87734909  8.02967785 17.98305551 39.62961711 26.14435096\n",
            " 15.96522208 42.13879224 29.35426352 43.86785489 28.28601837 17.12520377\n",
            " 34.08607463 30.17694545 33.18426014 23.76396607  9.16198393 28.41644442\n",
            " 25.98427408 18.56793008 12.93943035 17.99916057 22.67506429 16.23392426\n",
            " 41.74607747 31.87646939  9.48746074 23.94943193 32.85673264 14.67510329\n",
            " 36.62987959 11.74835085 47.11873302 37.33640141 23.57837296 34.58540075\n",
            " 22.35949487 20.81655936 14.77888345 14.06407634 29.09005133 47.33251599\n",
            " 21.54314249 44.75390097 20.00901953 39.66986941 13.63969855 37.50384621\n",
            " 30.0890633  34.7603963  14.77163127 21.009819   18.51446227 16.15876731\n",
            " 46.05544456 23.18206036 35.89807089 40.17617563 13.40732146  7.68464818\n",
            " 49.87598269 14.69422224 27.61885979 38.72834133 16.00008539 31.15257471\n",
            " 36.18168032 41.88089275 17.45850543 19.93477629 25.04057352 30.69703896\n",
            " 24.7697593  38.47718471 21.22143717 29.36534951 12.88974184 37.34166018\n",
            " 32.73033811 34.11781284 25.73200647  9.93101336 49.88183861 36.57846412\n",
            " 32.92732471 49.97095907 36.92289111 45.22349319 27.65151651 15.18781248\n",
            " 35.46592733 25.15085908 30.05363992 24.3002577  46.69592303 28.59781371\n",
            " 10.81308488 27.52311476 24.43460702 14.91423576 18.13226591 29.98027402\n",
            " 17.81113289 39.22860856 45.60251574 34.81870668 14.79526255 39.37182748\n",
            " 15.83777227 27.16400641 30.42053323 22.4091335  26.10358727 49.89689901\n",
            " 14.82005097 15.31728861 28.78953406 16.32394818 22.18353836 29.04901399\n",
            " 21.97666783 14.92873064 49.99552641 32.42563213 24.79391647 36.71405743\n",
            " 25.15820131 24.48280861 35.32794729  9.58200988 32.48026039 24.58264406\n",
            "  9.75789929 22.39204607 34.71093807 13.70676591 24.99493111 30.33498499\n",
            " 10.42278994 29.54182013 32.73065339 25.25989629 20.42647476 20.78822599\n",
            " 28.96444924 10.59390912 15.54470943  8.99389082 18.45822504 24.45422879\n",
            "  8.975869    9.14036384 25.62758204 28.24885853 28.08515236 16.61167348\n",
            " 49.32410677 23.87243751 20.94970334 20.51869003  9.73742974 17.74408488\n",
            "  7.64307196 36.71042418 14.10143985 26.29109043 49.97461578 30.24689463\n",
            " 36.41020935 15.17749492 28.47224928 26.27137614 14.10364297 20.64463143\n",
            " 30.93382722 38.75766333 45.16133823 35.06395567  7.30415511 12.01468632\n",
            " 10.81559269 17.79058338 30.49946998 48.52697708 14.25120406 11.15288179\n",
            " 36.98165236 18.09749844 32.63650152 21.65033379 24.97258426 34.22419631\n",
            " 15.96524563 10.41201556  8.49900284 34.10460143 31.54811208 11.50504292\n",
            "  8.28341613 13.03428277 44.44107255 14.00326567 29.68167401 12.95729894\n",
            " 39.37199312 39.10772808 23.18083733 10.38403852 29.33895905 32.20274511\n",
            " 19.02709654 17.83519462 33.65421007 15.59531717 20.06913708  9.45905495\n",
            " 38.90799487 24.57616663 23.18021344 40.55536063 30.85007444 32.05022065\n",
            " 17.95058646 31.12655509 26.86572179 37.85818597 31.18361362 21.07663256\n",
            " 32.73961814 32.75355923 32.21296615 34.18904956 42.16379904 22.8883843\n",
            " 14.29219964 36.57763671 10.65383193 48.75171127 19.49835464 10.06838015\n",
            " 37.60933359 11.59338296 16.590103   38.16581173 26.05600398 47.82559314\n",
            " 31.03949548 29.79712742 15.41720969 19.9837688  40.35138738 45.13272915\n",
            " 38.43739459 25.64751218 49.60765428 14.31960815 45.3121283  37.66312973\n",
            " 38.54310528 17.55755811 33.80894399 26.71404721 19.60228298 49.96235179\n",
            " 40.6302357  21.40407339 40.77279247 31.64274538 40.73171634 42.05992297\n",
            " 47.06734302 28.63647223 15.54711894 33.44818681 14.46450165 28.93219261\n",
            " 26.21964626 20.87289651 20.20573297  8.11721156 13.33726211 44.87971854\n",
            " 49.65956813 19.68943111 18.92621578 23.59489893 17.90827316 23.54726362\n",
            " 25.67052939 27.5678069  36.32312414 27.02356752 31.70437886 29.97959848\n",
            " 27.55222935 36.43002551 32.48528719 23.18206036 42.16480944 17.64213841\n",
            " 31.01658191 26.19968707 19.12911759 11.06441915 25.95942706 24.71210887\n",
            " 20.26693327 12.63604676 12.85328033 28.32130312 13.22421051 31.88248673\n",
            " 27.85421594 36.7064725   7.99715325 30.02647374 30.1864517  20.70154995\n",
            " 25.63494634 24.74615644 15.52994107 11.64801951 14.43223187 10.21626493\n",
            " 28.28103657 37.03635484 28.80106813 25.18906522 19.63233401 17.43071454\n",
            " 27.77632248 20.68020984 16.7886367  44.90305157 27.17959982 17.69652267\n",
            " 22.0295949  33.0736225  28.6729965  10.8392978  35.06070248 11.48899557\n",
            " 20.48499517 34.30566123 40.13969358 33.16048071 12.18251062 41.18322209\n",
            " 28.30615355 26.17700558 28.23549648 16.26446084 49.85825585 28.22769161\n",
            " 11.31602566 18.29468434 24.46774322 17.97839363 18.15434373 28.86668213\n",
            " 49.36292089 32.65766352 42.29032871 34.76686232 38.77287496 16.23603569\n",
            " 22.37327422 19.01018645 37.77674985 20.29380511 24.59334859 30.43872441\n",
            " 10.52776116 38.51812419 36.79872091 24.34939236 40.75520152 26.4076181\n",
            " 17.50921114 16.77815293 12.92164037 30.14193803 29.95470747  4.2239129\n",
            " 40.11865725 16.28497282 23.02454432 20.80502884 48.31119478 38.42400365\n",
            " 10.91417503 26.51147508 35.24690893 26.31503421 29.30527613 33.22209688\n",
            " 22.61123683 23.44240657 10.86105882 40.15112754 27.51699914 21.94270496\n",
            " 24.62281789 18.03610526 43.69140927 32.55686333 19.13639769 28.5492173\n",
            " 13.49696228 26.31923955 38.75294462 12.01971568 20.69083851 18.04559325\n",
            " 47.7987615  41.68340608 35.5931494   9.77846855 45.37433342 38.61148248\n",
            "  9.22302125 18.80874669 19.89047676 44.50014783 20.54918757 39.37389195\n",
            " 21.69912741 37.05923384 31.96442487 27.81459828 23.85829662 33.78078024\n",
            " 39.92464852 25.74338206 34.23132435 48.35070702 13.24498798 23.22098241\n",
            " 13.93500077 37.3582906  29.75172692 38.03613383 26.96161736 43.39932152\n",
            " 49.98857796 28.73453725 26.68274965 19.67439012 25.57813411 14.76872057\n",
            " 25.23151172 20.3102415  30.27828792  9.64253986  6.60890309 20.97311232\n",
            " 48.64628723 34.65447215 43.25619901 32.18793441 22.17938199 16.85083089\n",
            " 13.36011422  8.50019981 15.48910639  8.51625601 15.33799732 14.67494475\n",
            " 16.84121854 40.62324191 11.25880779 36.06690477 22.94398598 40.48907057\n",
            "  9.8078213  34.89249722 33.57368505 19.81364896 30.71075977 15.83713251\n",
            " 24.95262758 23.01270902 20.41667795 14.74267128 29.5975259  20.54837411\n",
            " 30.16744737 34.31861428 35.33626457 10.00975849 12.56322193 20.0876823\n",
            " 46.08263162 35.71749857 40.87697268 39.56144879 24.04602372 15.56635021\n",
            " 35.72722331 33.71581973 20.16346568 17.92072054 43.98781307 41.936216\n",
            " 35.30401903  9.30200591 23.15336096 14.13186975 39.74789181 30.85722987\n",
            " 15.98348077 12.79692228 21.21291583 37.50188826 33.39358782 41.33903389\n",
            " 28.59445915 39.90789293 35.74349141 17.43071454 21.45042646 18.05060177\n",
            " 36.34405516 27.67827907 42.15000411 49.71140916 10.49185751 31.33785236\n",
            " 20.54837411 15.57226216 12.28560501 17.75403965 40.5578165  35.67406368\n",
            " 30.94359501 44.53084652 49.98410239 19.59371343 15.90416212 23.6230018\n",
            " 29.18569897  6.82223842 40.36136175 41.21786089 18.59340579 18.61905773\n",
            " 42.11597851 12.31356314 19.10421102 45.09270593 20.56198528 27.49787093\n",
            " 19.16288322 13.75544889 37.86413944 21.79188723 18.28106975 35.13217596\n",
            " 10.60401933 42.22394501 21.96846329 31.19439886  9.0779245  21.57264955\n",
            " 11.30386886 37.99055877 24.5686772  14.47477426 12.94200511 18.56707218\n",
            " 32.360192   16.63612162 21.21698931 15.5681541  12.76707289 32.86575914\n",
            " 29.13892657 47.57986631 31.42236195 17.05379063 27.2287533  29.87800193\n",
            " 11.0144447  12.63652926 49.99151674 34.48565096 14.43396216 10.6853421\n",
            " 30.747744   27.23817284 22.80095196  9.18700511 44.16435485  7.26360774\n",
            " 24.36797832 29.65938707 34.53948675 20.52185131 34.91488206 24.01162092\n",
            " 11.67555157 30.30292084 44.73691528 22.61016492 42.76646959 27.50189876\n",
            " 29.12043844 39.96843771 24.6176639  14.53136537 10.83218972 21.22246802\n",
            " 26.96758797 14.96085073 32.16555489 29.73726991 11.49722226 30.1177748\n",
            " 18.93076493 35.26963437 17.599645   37.77242107 18.90478654 43.90134933\n",
            " 27.26935824 37.34332219 10.82353206 14.26625808 39.19985175 23.50792912\n",
            " 48.1249854  22.07267889 29.42710234 13.82381278 47.21632265 40.74713609\n",
            " 20.98101551 32.66551063 14.08897088 12.96630747 27.5678069  49.7694281\n",
            " 22.1464909  28.58702465 33.37174302 46.52583686 11.60488934 35.73922495\n",
            " 43.94185973 30.93987429 42.77949239 26.1456034   7.37191053 24.25126932\n",
            " 43.15489764 12.43803511 47.7001132  15.06433625 33.36324171 20.74131728\n",
            " 22.27058544 16.47377367 31.21378431 19.53131717 30.0905242  39.55344912\n",
            " 36.0938278  37.08722474 13.83780653 46.30511173 46.19355344 11.62745641\n",
            " 17.42075214 25.40744959 27.70216629 12.07556231 16.42185699 17.67778119\n",
            " 18.01760577 14.74764703 10.59782563 41.46114639 25.19516192 18.5954611\n",
            " 34.00166899 29.0488357  13.02781617 24.37542664 20.98797248 25.04256648\n",
            " 25.52320528 31.40369002 28.32183287 35.8940326   9.77716662 28.84053555\n",
            " 12.43712914 36.97919501 13.67521792 22.29794616 45.78728908 30.44107235\n",
            " 14.89630073 19.35561728 49.97102044 47.70205157 33.79336918 35.50493077\n",
            " 47.44955516 36.3896944  26.11625967 26.67410824 10.97953749 12.71923422\n",
            " 24.44158547 34.85610357 28.59101735 35.64368528 28.7528785  12.51262402\n",
            " 21.42024312 10.74191408 24.38678513 25.04852119 29.46688968 16.81425223\n",
            " 32.82963341 22.10303445 35.98989104 16.85449241 11.93315214 20.01780357\n",
            " 20.4982681  36.69613659 20.39445427 29.50940539 49.9062473  13.046111\n",
            " 30.43198173 29.87221982 12.98178567 47.31287336 34.07015655 24.16700204\n",
            " 40.82118137 13.53835617 34.04335565 43.12179687 20.85252395 41.061039\n",
            " 37.29208853 14.40100898 12.33068794 20.90154434 42.20092235 24.19438366\n",
            " 35.89007056 30.71177181 16.65520332 48.93997938 24.41849401 17.62528336\n",
            " 12.873174   28.77235844  8.547713   26.09531075 25.21079839 25.77032962\n",
            " 18.22321043 41.9347948  30.71075977 44.13195165 46.40799527 41.41539903\n",
            " 28.2260412  23.16277475 32.03434663 11.51898833 37.33548051 22.19520624\n",
            " 31.88240728 33.45098093 34.60146329 47.90809713 14.98802625 37.62112032\n",
            " 23.5557338  11.02241098 41.74331689 18.75410091 15.20157162 20.65686657\n",
            " 48.84924649 22.86749149 31.65066539 46.29557036 45.54474106 11.46245782\n",
            " 33.05281622 27.95527385 30.01406302 46.47220048 14.66648299 48.53048293\n",
            " 24.72771624 45.608768   22.16030418 26.03500609 29.00852499 32.20274511\n",
            " 21.08393324 44.15518147 21.76992536 48.34364469 38.17595177  9.48464419\n",
            " 31.11308999 39.5532713  19.42240367 49.10120605 26.86051043 26.71662874\n",
            " 22.60062757 34.83693285 15.87372435 22.26437747 22.2487893  27.44455051\n",
            "  9.21734835 42.92952995 28.87989872 26.83177016 42.7290794  35.92091094\n",
            " 35.72164605 10.00737881 26.88771164 25.01991418 49.52247642 44.79576147\n",
            "  8.52032175 23.31357137 15.54832662 29.9220074  24.90792625 27.53282643\n",
            " 27.88951375 39.71518519 32.39453376 38.50095858 26.30222038 20.06913708\n",
            " 30.85722987 19.40853557 19.17319824 28.29053019  7.47699034 10.08955165\n",
            " 45.28650148 28.09035389  9.33290981 17.45850543 29.61370675 15.76924077\n",
            " 25.56022205  7.31930813 31.0577831  35.51672725 15.44810445 22.1811702\n",
            " 12.01086935 10.35791241 47.25924571 33.81316586 32.10736714 14.94064732\n",
            " 14.09910793 34.22507224  9.10724784 26.93797602 28.89718452 19.68312401\n",
            " 15.03453697  8.49813826 15.74938838 35.00643123 14.30978559 29.79630592\n",
            " 45.55688548 23.95469252 36.29196    33.80395881 29.48790121 25.59139014\n",
            " 38.96520687 31.37007377 41.91470664 39.84941817 44.47858156 27.92299468\n",
            " 29.82855568 21.87967249 15.96553447 30.03634362 41.89023754 38.15502123\n",
            " 41.41444209 24.7614456  49.58181919 17.25004011 40.23659009 35.02667881\n",
            "  7.20074039  8.61123818 30.46437015 27.66228054 34.05405862 25.14609646\n",
            " 15.46485755 34.31647923 32.13034847 34.7437256  48.44772845 20.3818831\n",
            " 26.70701641 16.10262235 20.05341176 20.98263166 39.90532881 49.53058597\n",
            " 19.51434931 27.10842712 46.76820314 20.11946819 26.22463105 17.14108436\n",
            " 29.61374395 13.47880738 32.83791198 25.12419778 26.54426586 21.96664288\n",
            " 16.12954728 26.8396222   9.5575227  41.64178488 17.03773984 10.95898654\n",
            " 50.         13.91886538 14.27014961 34.66214107 43.10719672 12.4908601\n",
            " 38.24676357 19.56404171 15.89122797 49.82868001 27.27383861]\n",
            "selection [401 472 559 930 611 196 883 670 874 180  77  19 332  20 297 216 901 212\n",
            " 481 483 858 782 931 168 165] (25,) [4.2239129  6.60890309 6.82223842 7.20074039 7.26360774 7.30415511\n",
            " 7.31930813 7.37191053 7.47699034 7.64307196 7.68464818 7.87734909\n",
            " 7.99715325 8.02967785 8.11721156 8.28341613 8.49813826 8.49900284\n",
            " 8.50019981 8.51625601 8.52032175 8.547713   8.61123818 8.975869\n",
            " 8.99389082]\n",
            "trainset before adding uncertain samples (325, 10) (325,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [194 156] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "std (952,) [37.91083475 25.3414836  23.31064555 18.72591889 25.03805283 16.40057437\n",
            " 29.72038262 14.17291697 12.70338702 24.61877802 46.38000159 45.66107893\n",
            " 46.64191017 10.74279066 37.42957712 31.57849787 21.34899022 22.35768087\n",
            " 41.70224563 22.15019867 41.46302841 24.20916215 18.14881964 41.70723069\n",
            " 28.40972621 42.59537942 26.42645356 23.58674755 34.37900164 30.28150553\n",
            " 36.22266416 21.98528794  8.61943795 28.91972658 22.81698496 16.80300404\n",
            " 14.12777803 20.09733504 24.02106172 14.77566832 40.52851487 31.62935154\n",
            " 12.41251777 23.78734809 29.56100928 12.40980342 36.32872029 12.81040942\n",
            " 47.40755642 37.54821679 23.91991939 32.60564997 23.07065871 20.39421233\n",
            " 13.71532716 12.3371314  28.50974623 46.67939115 22.51807942 45.01704283\n",
            " 21.27053196 38.56816758 14.76890784 37.17219422 31.00472083 31.35967942\n",
            " 11.43110406 21.66290621 19.27627288 15.70120273 45.30127738 24.26343435\n",
            " 34.99782515 38.58746755 14.11076027 49.90834802 17.94762218 26.83065249\n",
            " 39.7219322  13.96800638 31.79026038 34.89242892 40.84016756 19.28784505\n",
            " 18.24316477 28.20914211 29.0637729  22.67578307 37.14873279 18.50211722\n",
            " 28.09275203 12.57296921 36.80996955 32.86489071 34.90923961 26.74298992\n",
            " 10.17447611 49.91361836 36.65964675 31.79114737 49.97943323 37.54159827\n",
            " 43.9492096  26.99850815 13.36412809 34.40906666 21.51479758 29.41959768\n",
            " 23.21238435 45.68977801 29.18037165 10.66465134 29.79911467 23.70966511\n",
            " 16.24502421 18.13622623 30.51764083 17.18466518 37.93383051 45.42704081\n",
            " 32.43062773 15.83230785 40.39335832 14.05877741 29.50470022 29.67446888\n",
            " 23.27737208 28.76682091 49.93653067 12.37718516 13.11834365 31.05928191\n",
            " 16.94691764 21.8496109  27.17337393 21.01650277 13.01081013 49.9965046\n",
            " 30.90943615 25.90218278 37.33536838 26.04551057 24.94564528 34.79984421\n",
            "  9.27146018 33.23551042 23.98749442  9.44542602 22.50590416 34.9743431\n",
            " 14.6099479  24.50497169 30.27835962  7.385005   27.21940231 35.53022223\n",
            " 24.4192303  20.42811825 19.50349118 26.98721773  8.90052383 15.12795949\n",
            " 17.76459168 26.9159145  10.04735206 25.93453715 28.34152043 27.70485625\n",
            " 16.82747716 49.12013233 22.16241953 22.22557096 22.90476197  9.38534297\n",
            " 19.92086965 36.53132763 14.67913014 28.07616616 49.95490776 29.16599273\n",
            " 35.62517237 16.18373131 27.57186395 25.66833384 13.47752723 20.72582842\n",
            " 31.02159099 40.2354178  45.68831025 33.33984079 14.06580956  9.89245508\n",
            " 16.68371659 30.40446544 48.83015128 12.45867533  9.93583807 36.45854367\n",
            " 16.59349119 33.50653024 21.45938589 23.82805402 34.60966068 16.45721886\n",
            "  9.97129526 33.88385172 34.64903146 12.38824565 12.3756046  43.86776252\n",
            " 13.78361613 30.02602548 10.66553274 38.43746934 40.20909144 20.88009009\n",
            " 10.05512205 28.4490106  33.43673136 19.56170897 17.54291092 32.19636158\n",
            " 14.02821179 19.05882338  7.7796336  38.38803049 25.4209651  22.09774907\n",
            " 40.56060136 29.60079066 32.16106336 18.50879479 29.99448215 25.46733587\n",
            " 36.76165232 30.89798455 19.85634643 31.36189744 31.32667959 32.79262405\n",
            " 32.48855598 41.0451579  24.82282817 15.34219787 36.0488016  13.18291257\n",
            " 49.019267   20.1366871   7.78674541 36.51080792 10.57469674 17.63844246\n",
            " 36.63928012 23.1712576  47.10628792 31.9629453  29.94665423 13.28574791\n",
            " 20.83733523 40.47686803 43.91432182 38.00232678 27.84070626 49.60703815\n",
            " 14.26831999 44.27690753 34.99898568 36.99362461 17.38461541 33.92364874\n",
            " 27.0375978  21.86688786 49.97184987 42.49995578 21.86753444 42.87659858\n",
            " 31.02001553 39.84726387 43.23741101 47.20533785 27.55144097 16.62080926\n",
            " 32.68479935 17.28586506 27.60440808 25.43414082 23.45835625 20.47422556\n",
            " 12.69873949 44.75091108 49.7415532  20.36025733 18.67971128 23.63703285\n",
            " 18.17472463 21.73741783 25.79624878 27.10105017 36.9056558  26.25937129\n",
            " 29.59126915 28.32935747 30.42820405 34.61846293 30.88598351 24.26343435\n",
            " 41.09765654 17.90625884 31.35133456 25.38687801 19.97090916 10.57813211\n",
            " 26.80584653 24.66842905 23.32903465 10.21331038 12.97553539 27.38062991\n",
            " 13.95762117 31.67197717 29.21943447 35.57198012 29.23431382 27.72701068\n",
            " 21.8648263  28.75382149 23.88579461 16.13663633 10.43248674 13.60999182\n",
            " 11.80746236 28.66687376 36.27109739 28.15462584 28.14816642 20.34261503\n",
            " 16.25764896 29.0911503  19.78700591 18.22138594 44.24715358 27.81828183\n",
            " 21.18254591 22.38618183 32.01483849 28.79344615 13.8611215  33.42797107\n",
            "  9.8739136  19.29337478 33.21030412 38.74530034 32.15458896 12.87698424\n",
            " 41.38520439 25.90757065 25.76117738 27.08157022 16.72058822 49.88353456\n",
            " 28.35382097 10.1108675  19.49103541 26.06788552 18.18690112 17.16693999\n",
            " 26.54209475 49.37812493 32.38508631 43.24415411 32.79833532 37.75578075\n",
            " 13.69764829 22.62813897 17.03723052 37.01993321 16.43283929 25.53846742\n",
            " 29.65359529 10.82964976 36.95553383 35.51515925 23.77465793 39.29149022\n",
            " 24.89830879 17.12684491 14.95288924 12.34423612 29.71404282 29.49081182\n",
            " 39.00897054 18.11847956 21.91544657 18.98104332 48.41252375 38.21810365\n",
            " 11.89392135 22.92960268 32.45378879 27.6472035  27.75265754 33.25914923\n",
            " 23.22441811 23.12071768  8.98804303 40.58144231 28.27785803 20.57826923\n",
            " 25.07566901 17.26839786 42.4595705  31.84909888 18.44508947 26.74701328\n",
            " 12.60015361 25.75993008 40.31573777 14.32812274 21.23193816 15.84678338\n",
            " 48.20363679 40.32486377 34.76434467  7.90330863 43.96602616 40.4331464\n",
            " 10.94794883 18.16072531 20.1493945  44.18916026 20.28228492 36.77510246\n",
            " 19.9364969  35.95174898 33.10204644 27.33938666 24.66518737 32.68055084\n",
            " 38.35038504 26.65229187 34.74868421 48.66659239 15.05838885 23.78293503\n",
            " 13.87825176 36.63496162 28.82503428 38.65872417 23.81658753 44.13674276\n",
            " 49.99213466 28.81494978 25.61922049 18.6808017  24.48841214 16.00694395\n",
            " 27.39674095 20.91786103 31.74640422 10.91721094 19.56257269 48.78929552\n",
            " 35.60530532 42.64398984 31.0850629  22.41721056 14.75791238 17.72186861\n",
            " 17.37446084 17.11717566 16.11917946 14.98906047 41.25841803 10.39898248\n",
            " 35.22083116 21.63069807 39.90743515 10.28411726 31.71772011 32.80511585\n",
            " 18.33136018 30.56384317 14.75726988 24.31469643 21.89497595 20.65259847\n",
            " 16.27333943 28.37984266 20.0717283  31.52168199 34.64890829 34.44280902\n",
            "  9.01967817 12.32587778 20.13838833 46.02663127 34.72514766 38.98983465\n",
            " 38.22475253 22.44522265 12.42810389 35.95443466 31.00236432 19.95547784\n",
            " 17.71624377 43.15370657 42.51305533 35.43023051 12.82006831 24.37041387\n",
            " 14.36622352 39.50141647 30.60103344 16.10172656 12.24722426 22.79532419\n",
            " 35.74818861 33.7686577  39.59938834 27.42222059 40.35584534 32.72793687\n",
            " 16.25764896 21.18923885 18.63334367 34.62461306 28.20652387 41.63358205\n",
            " 49.54989253 12.15893107 31.17673527 20.0717283  13.45731858 13.76793244\n",
            " 16.93435493 39.20037304 34.26231052 29.5061782  43.69473928 49.98599016\n",
            " 19.96436532 14.77499425 20.50441593 31.64408122 42.14934817 40.8357232\n",
            " 17.90187151 17.82055426 41.54283922 11.20048707 17.79120333 43.58352636\n",
            " 21.43129498 27.21665306 17.30533128 14.90682389 37.43071828 19.29077708\n",
            " 19.92090538 35.34142526  9.77108058 41.5887959  19.88669086 29.70924734\n",
            " 12.76864647 24.07712827 11.12638792 36.9677595  25.48304039 14.49434738\n",
            " 14.10108827 18.53824224 31.78675723 17.59932937 20.8596703  13.69055958\n",
            " 12.6085283  31.70199948 24.85688465 47.01666727 32.89932129 25.30958268\n",
            " 26.99170813 31.89533339 11.97664665 12.69910634 49.99290936 35.33387136\n",
            " 16.15689099 13.09182065 31.59547471 26.16249533 24.19875281  9.02982273\n",
            " 43.33629772 24.40151581 28.31413821 33.18057139 20.78504273 37.20650674\n",
            " 23.06321147 12.03143361 28.68846363 44.4938896  20.42049652 43.6025972\n",
            " 26.98474092 23.44368831 38.25359346 24.7005978  18.29280268  9.49445314\n",
            " 22.21652684 28.720181   13.56189877 30.89848391 30.16434818 10.74075\n",
            " 29.69691833 18.57822174 34.05178523 18.34583897 37.09313691 19.6795673\n",
            " 43.0353043  24.81516189 38.70840259 12.86488877 14.68954914 37.87316242\n",
            " 21.30387056 47.37084292 15.57926709 29.25029561 14.81522584 47.5827176\n",
            " 39.60061388 22.21364734 31.2893069  15.05902331 13.47822807 27.10105017\n",
            " 49.83047296 23.36921353 26.8903211  31.74510889 46.12478292  9.35653603\n",
            " 36.49185983 42.77608993 29.99550736 43.56454826 25.27716732 19.7937626\n",
            " 42.2962613  10.55134237 48.3041479  12.60966829 33.5556212  21.11219128\n",
            " 21.92356769 16.01125479 30.87564602 17.1869632  29.88286831 40.75604313\n",
            " 33.48028344 37.06109374 12.28528443 45.77055048 46.71364095 13.75751286\n",
            " 15.78807598 25.66952512 27.74218657 10.74279066 15.81094356 17.73097699\n",
            " 18.62142838 12.76870621 10.30765795 43.54939369 24.4225005  17.14339487\n",
            " 31.90848842 27.04240199 14.3734819  22.72045928 20.62152744 25.32338645\n",
            " 27.07386553 31.26267195 28.67331509 36.16573629  9.14099877 26.99875014\n",
            " 14.30870105 36.39758833 12.53018959 20.37990081 45.10520174 30.69393993\n",
            " 14.96297649 18.8224809  49.97829909 48.31951039 34.83668029 34.26040725\n",
            " 47.98526479 34.97523605 24.98681377 26.80271938 11.36440212 10.26261745\n",
            " 24.35078479 34.85511177 28.26419137 34.68264856 29.611817   12.4327778\n",
            " 23.77463504 11.28664909 25.89024306 26.04055877 29.18275848 12.66453013\n",
            " 32.42368142 21.4292394  34.91574205 15.76061141 10.02690294 18.40977988\n",
            " 18.41430019 36.98393615 20.70502369 30.14994398 49.93113351 13.68098931\n",
            " 32.91996653 28.00970105 13.41945914 45.79244912 31.28317055 25.37013923\n",
            " 39.88502058 12.93130433 32.13772871 43.18063884 21.43297675 40.71710713\n",
            " 40.0013702  14.26457476 13.5419957  21.63439767 42.31983861 26.33217586\n",
            " 37.77026292 31.03232278 14.03861326 49.05211125 22.32317678 16.0529829\n",
            " 14.75789719 27.84337463 26.70632907 24.96653554 27.43159191 19.0776864\n",
            " 41.34122686 30.56384317 43.33487703 45.78914368 40.3295236  27.84246601\n",
            " 19.81033251 32.55859037 13.19873586 36.72465065 22.8003655  30.24536538\n",
            " 33.5636315  39.28707424 48.25201231 15.99411351 35.82920669 24.42988887\n",
            " 10.03529712 40.99136278 16.10089997 11.77767748 19.57655754 49.02849631\n",
            " 22.47908459 32.396991   46.7657774  44.39424795  9.81659502 32.35052456\n",
            " 27.8514072  29.23955948 46.05925034 13.54188251 48.6775374  29.06513174\n",
            " 46.65817958 21.98182282 26.12216422 30.50459881 33.43673136 21.59328843\n",
            " 44.56990518 21.36839634 47.95576254 37.96899205  9.49464321 30.37533458\n",
            " 39.89323046 21.53327026 49.31637709 23.32388167 27.06540343 21.92994124\n",
            " 33.74650761 17.11939923 21.59240879 23.03348408 28.03463525  9.00597666\n",
            " 41.90265848 28.40508351 27.73623659 42.91256314 37.47459672 34.22561489\n",
            "  5.18356953 30.29197309 24.46736156 49.34481262 44.08172849 24.9082139\n",
            " 17.56102056 28.20214866 24.3986423  22.59748839 27.90073867 39.08463803\n",
            " 30.98660662 39.34235511 28.01939079 19.05882338 30.60103344 20.28107828\n",
            " 19.51212213 28.40662009 10.82381352 44.66905338 25.76321435  9.8772152\n",
            " 19.28784505 27.3871936  16.62250904 24.73531847 31.30542597 35.02696298\n",
            " 15.19733249 22.11708248 12.79076105  8.41748243 47.10474212 32.15205347\n",
            " 30.7087764  17.48154015 15.62245412 33.08523781  9.15179402 25.89216592\n",
            " 26.89418178 18.07218379 14.74689117 16.63598801 32.5158293  13.78563413\n",
            " 31.46668389 44.64423199 24.9127719  34.84573255 32.9066325  29.02687632\n",
            " 26.61576582 37.44666093 32.14958901 41.93742713 38.66230838 43.82935676\n",
            " 28.06194774 28.52453182 20.61269591 15.16863067 31.13379355 40.23797933\n",
            " 37.9883242  39.27745934 25.33128788 49.71801608 17.93612923 39.18779358\n",
            " 33.43529094 29.14325655 27.01892701 33.69428046 24.5265703  15.85774569\n",
            " 36.13615516 35.16128754 30.86347774 48.77354958 21.3699837  24.61940818\n",
            " 14.32723258 21.46949328 16.56514097 39.10959783 49.67203985 22.29991873\n",
            " 27.92246092 47.39407153 18.56096455 25.17875411 20.11393736 26.44643367\n",
            " 15.32333202 32.72860874 25.63123821 26.90464106 20.9485103  16.53336886\n",
            " 25.82767779  8.05777161 40.93558254 16.97321663  8.28527162 50.\n",
            " 15.62580889 16.50923488 34.53697039 42.83355549  8.65308307 37.58646192\n",
            " 19.63721518 16.39469104 49.87269425 25.54716758]\n",
            "selection [834 153 224 248 423 937 940 867  32 946 160 404 827 492 593 694 874 144\n",
            " 647 173 147 611 814 560 796] (25,) [5.18356953 7.385005   7.7796336  7.78674541 7.90330863 8.05777161\n",
            " 8.28527162 8.41748243 8.61943795 8.65308307 8.90052383 8.98804303\n",
            " 9.00597666 9.01967817 9.02982273 9.14099877 9.15179402 9.27146018\n",
            " 9.35653603 9.38534297 9.44542602 9.49445314 9.49464321 9.77108058\n",
            " 9.81659502]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (375, 10) (375,)\n",
            "updated train set: (375, 10) (375,) unique(labels): [199 176] [0 1]\n",
            "val set: (927, 10) (927,)\n",
            "\n",
            "Train set: (375, 10)\n",
            "Validation set: (927, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.341014 \n",
            "Classification report for LogisticRegression(C=0.13333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 0]\n",
            "std (927,) [39.31938985 26.23007647 24.22336179 18.28783314 24.98737512 17.43301499\n",
            " 30.11065167 14.02527471 13.68216665 27.0623863  47.00691331 46.60152393\n",
            " 47.43707205 10.84702464 37.90505768 31.96003739 22.03520166 21.51567965\n",
            " 42.18964743 21.63773858 42.1062261  25.47510444 17.21123    42.36743589\n",
            " 29.22315549 43.44971273 27.45273623 22.04504044 34.37902864 31.93054345\n",
            " 36.40196566 23.67283028 30.15330092 24.30582255 17.97255765 15.47783366\n",
            " 19.81122241 25.29854422 16.11351271 41.33745235 30.86670212 11.28114747\n",
            " 24.68416184 31.93481213 13.59135442 37.07107951 11.917108   47.90539943\n",
            " 38.05462967 23.68632982 33.70436056 22.88144012 20.50099944 15.47349377\n",
            " 14.47417578 28.43936585 47.29664693 23.34925213 45.85856783 20.36569249\n",
            " 39.63815039 14.59820256 37.87822341 32.71728513 33.48613889 12.92295113\n",
            " 21.40687304 20.83906909 17.29648593 45.96537248 25.80730667 35.9550907\n",
            " 39.27567061 15.13686706 49.93714751 18.40477973 26.73163621 40.56325805\n",
            " 16.36869105 32.27006954 35.6857188  41.69797404 19.09871564 19.33970078\n",
            " 28.5973506  29.72124751 25.11781979 38.2327259  20.93850178 28.44462072\n",
            " 14.48617837 38.64641506 33.7700682  35.83820548 28.80756475 10.03473171\n",
            " 49.9426268  38.14477573 32.9346226  49.98696616 38.74738492 44.91757844\n",
            " 27.98437547 14.32747851 35.49100738 23.16539397 30.80921847 24.92773143\n",
            " 46.45896676 30.47720442  8.01849297 30.51353574 24.02805651 17.36747144\n",
            " 19.20263092 31.93544621 18.45601061 38.61101194 46.39386685 33.78217288\n",
            " 15.45830859 41.5119692  15.08216257 30.17296536 30.38108876 23.13933482\n",
            " 29.62937733 49.95198262 14.02100293 12.84581258 32.01611399 17.6594861\n",
            " 22.52089583 28.67485509 22.02660585 14.19413687 49.99822909 31.23595\n",
            " 27.23718357 37.75532838 26.2157072  26.41931802 35.24190678 34.47384763\n",
            " 24.76177957 22.59283802 35.39269861 15.58548577 25.5170693  31.82334764\n",
            " 28.87571078 36.23504155 25.33173245 21.30403697 20.7872642  28.4913614\n",
            " 15.3689076  18.28976165 27.61573708  9.61312533 27.85372664 29.00943991\n",
            " 27.46118303 17.17305626 49.29561122 23.7566917  23.23865652 23.86259537\n",
            " 22.25696305 38.40196914 15.28607863 28.55627101 49.97396906 29.88547001\n",
            " 36.524933   17.29223512 28.8319795  27.72166119 14.96923912 21.87234427\n",
            " 32.88214036 41.20311246 46.47675663 34.73780619 14.75910586 10.06955307\n",
            " 17.50750046 32.20186843 49.05361643 12.49037905 11.99803249 38.33475611\n",
            " 17.91450993 34.90151176 21.7160382  24.68032927 36.04482036 18.02716051\n",
            " 10.17402848 34.75523221 35.65463478 13.33717998 13.86198729 44.90451706\n",
            " 15.05444096 30.97955572 11.26850977 39.2338032  41.20873257 22.20576876\n",
            " 10.40257398 29.63001583 34.52361127 19.18408092 17.51911167 32.81530507\n",
            " 14.84310641 20.22743588 39.52880538 26.94258809 23.04484664 41.20434413\n",
            " 31.02022065 33.7247731  18.60967347 30.79882106 26.51807333 37.54499877\n",
            " 31.27631642 21.86393906 32.78057947 32.62003073 34.31576678 34.15972122\n",
            " 42.06857036 26.10039763 16.23951381 36.68646373 12.30576182 49.23503333\n",
            " 19.86422392 37.67888064 10.92240488 17.4812055  37.89913976 24.8092515\n",
            " 47.61470647 33.40332655 31.72281272 14.29166898 21.19170917 41.9954919\n",
            " 44.46140428 38.80567494 28.95415995 49.74503289 15.60447849 44.93132662\n",
            " 36.30174575 37.48218917 17.48123525 34.47093955 27.94101406 22.42374701\n",
            " 49.98344653 43.1625614  23.17757938 43.54703956 32.09315827 40.8006326\n",
            " 43.78320882 47.84517401 27.73508639 17.14595489 33.61031248 17.64555837\n",
            " 30.04816167 26.93446134 23.05331344 21.96435561 13.29883944 45.21941622\n",
            " 49.80898181 21.38516113 20.01463546 25.45837722 18.44348799 22.44737465\n",
            " 26.30998867 28.07368927 38.0185825  26.37017307 30.00660512 29.78776553\n",
            " 31.61588846 35.88713391 32.07496997 25.80730667 41.94818726 17.97256215\n",
            " 32.94667564 26.41730762 21.63559804 10.87157216 27.70469896 25.59919695\n",
            " 23.68250577 10.47346799 12.81067267 28.51696829 12.36214398 33.74139812\n",
            " 30.16316617 36.74658885 31.48910704 29.02079384 22.21606853 29.61519089\n",
            " 24.86123098 17.77226375 11.40826957 13.73489058 11.33747357 30.2046767\n",
            " 37.10295038 28.70835633 27.27247509 20.4332361  17.06968327 30.46141287\n",
            " 20.98797235 19.27497561 45.05417251 27.93688923 21.5459962  22.34744163\n",
            " 32.78954387 29.26565893 12.52355307 34.58876955 10.2038677  20.17458689\n",
            " 34.51959459 39.90230223 33.34526297 14.51793983 41.87322545 27.19922935\n",
            " 26.43931212 28.44179766 18.12028313 49.92486463 28.87320648 10.53416886\n",
            " 20.48023493 26.92988935 18.69316877 17.25267982 30.05103489 49.50331106\n",
            " 32.67640272 44.00608067 34.21722117 38.8483313  14.25570997 23.00012772\n",
            " 19.44542783 37.73769312 16.80361403 26.89832432 30.38512874 10.112216\n",
            " 37.80925255 36.43291374 25.65829946 40.11178685 26.06606254 17.61706178\n",
            " 15.63840869 12.51667695 30.45214893 30.12259103 40.06757707 18.46521469\n",
            " 22.51247793 19.72433686 48.78943483 38.97868453 11.26157486 26.59842044\n",
            " 34.02087106 28.96762117 28.94846378 33.77015067 24.63801685 23.30023952\n",
            " 41.72076475 29.6944181  22.07532038 26.52085946 17.9514009  43.31396061\n",
            " 34.06008606 20.30775216 27.47001693 14.604399   26.63040278 41.11864011\n",
            " 14.36445801 23.07021858 16.35699597 48.54706841 41.68435277 36.65433433\n",
            " 45.1935154  41.37544028  9.23372461 18.36192903 20.89329433 44.692441\n",
            " 20.9886546  38.58765217 20.32393189 36.90254453 32.97826392 28.40639684\n",
            " 25.77548538 33.87946538 39.01714801 27.7998838  36.18170449 48.92127696\n",
            " 14.80678612 23.95875388 14.09831116 37.46258142 30.35654672 38.98951023\n",
            " 25.88991352 44.95188706 49.99540343 29.8853409  26.22692    19.26390254\n",
            " 25.70434398 16.17913292 27.85850203 21.32296649 32.85869575 11.9682494\n",
            " 21.4609887  49.08067073 36.63052945 43.94515731 32.54724902 22.84185516\n",
            " 16.01973304 17.79149218 19.02633649 17.8998641  17.35362078 15.75661792\n",
            " 42.436665   11.20758351 37.11739573 23.56643795 40.40988888 10.61310124\n",
            " 33.26835008 33.8627402  18.70783752 31.02995017 15.03230295 25.28931979\n",
            " 22.78269043 21.08718331 16.92142394 29.33331268 20.60609045 32.38993493\n",
            " 36.25280345 35.18092851 13.56091482 21.90982504 46.47479982 35.96297988\n",
            " 40.31573713 39.51200862 23.60628976 12.63359853 36.5639074  32.62792399\n",
            " 19.76354675 18.87360626 43.98129234 43.33653886 36.13998307 12.47844059\n",
            " 25.39319449 15.62102636 40.62989193 31.20373709 17.56751221 12.03153562\n",
            " 23.55516372 37.10951219 34.63214687 40.84818922 28.23981213 41.72419931\n",
            " 33.68772748 17.06968327 23.22775909 18.99882358 35.40143731 29.20671623\n",
            " 42.38736485 49.68535141 11.50437755 31.60446939 20.60609045 14.29697755\n",
            " 13.53470582 17.44529134 40.0368923  35.24575765 30.61663413 44.26047123\n",
            " 49.99261647 21.05553836 15.91510928 21.26859346 32.35395074 42.82540016\n",
            " 41.15048021 17.93497327 18.62565649 42.03663362 12.72257065 18.81787708\n",
            " 44.67506727 21.68293928 27.83319187 19.24623184 14.12536379 39.24046322\n",
            " 21.0247023  19.24867769 36.81319157 42.23044665 22.37618914 31.89281327\n",
            " 11.55833495 24.96619895 11.13267669 38.11675527 26.65089325 14.84041237\n",
            " 13.62079003 18.54637621 32.66601873 17.11059917 21.4960382  15.545354\n",
            " 14.01751595 32.61092548 26.73738199 47.48121791 33.80159384 25.98681332\n",
            " 27.64438354 32.81019336 12.80102941 12.44916821 49.99630636 36.62928387\n",
            " 15.89579211 12.37971457 32.88359131 26.66472594 24.26474152 44.25768274\n",
            " 24.81448159 29.59734969 34.62225869 21.8286163  38.17112038 24.04715371\n",
            " 13.26390748 29.46110896 45.3864934  21.60909262 44.36253261 27.75009093\n",
            " 25.47587719 39.44654211 25.27246123 18.39328973 22.23413618 29.82561208\n",
            " 14.00157973 31.66932578 31.95850642 10.69430357 31.50075558 18.94872865\n",
            " 35.21770352 19.30987438 38.11832041 19.5267675  43.84370627 26.08957491\n",
            " 39.39101026 13.20112225 15.72996671 38.95785931 22.36044351 47.98005144\n",
            " 17.40209682 30.06203828 14.11415507 48.0703043  40.2995031  21.8846289\n",
            " 33.08106585 14.29575752 12.51410224 28.07368927 49.88013435 24.50634044\n",
            " 27.80180731 33.11942322 46.67739089 36.82434755 43.75724892 30.95873914\n",
            " 44.8370939  26.23060557 23.10559622 43.19264759 11.12367004 48.58259952\n",
            " 12.18152848 34.24845031 22.56407406 22.70615525 15.75049516 32.41656261\n",
            " 18.71715951 32.28062524 41.64524299 35.13948157 38.80527797 12.11059985\n",
            " 46.27067772 47.30340757 11.64684539 16.9383094  27.2322738  29.08540804\n",
            " 10.84702464 17.61121743 19.24845723 18.60623689 15.20028728 11.37179759\n",
            " 44.43508612 25.03893181 18.73199903 34.1284407  28.36368248 13.84504064\n",
            " 23.32266079 20.96502348 26.31310881 28.5338087  32.04392595 27.78143655\n",
            " 37.78960432 30.31179909 12.75615096 37.25860914 13.20217429 21.42981074\n",
            " 45.75808168 30.62167777 14.60362491 19.16661987 49.98717347 48.56257527\n",
            " 36.05500456 35.00229895 48.34630057 36.31617986 26.23203484 27.5222353\n",
            " 11.24486026 10.39977923 24.51682364 36.33154119 29.99753507 35.56362093\n",
            " 30.06167031 13.1693253  24.41375898 11.69064964 26.81992677 27.50809318\n",
            " 29.36733508 15.67226672 34.61330121 21.80528562 36.03766842 16.63217459\n",
            " 10.74523097 20.88306655 20.90651042 38.44318512 21.01225366 31.73407719\n",
            " 49.95374471 13.63690669 33.29473991 29.02402689 13.36067312 46.60723037\n",
            " 33.50871438 26.32316899 41.74165462 12.70894725 33.6778487  44.39866047\n",
            " 22.62724388 41.37457305 40.86205862 14.40876698 12.87210545 21.56194495\n",
            " 42.51760905 27.43796607 38.35002674 31.73055033 16.15819794 49.28515025\n",
            " 23.89736552 16.14117532 15.62673391 29.65760045 28.07131922 25.35909119\n",
            " 28.26389608 19.16708398 42.05368139 31.02995017 44.15892299 46.51722457\n",
            " 41.02073803 29.58214147 21.66179123 33.69506268 12.6715724  38.06009566\n",
            " 24.15825186 30.96362793 35.3108607  39.70747861 48.57751986 15.31279789\n",
            " 36.10270226 25.58135802 10.35923451 41.77254369 17.38588044 12.72800958\n",
            " 20.13352841 49.26890388 22.926185   33.14341443 47.29222714 45.23785842\n",
            " 33.44739035 28.39067432 29.99253787 46.62509848 13.68886508 49.02032035\n",
            " 29.04669283 47.09650056 22.28826868 28.19108185 31.52608544 34.52361127\n",
            " 21.68810613 45.34999143 21.42172645 48.29167252 39.318787   32.25202508\n",
            " 41.18778529 22.74144462 49.4628088  26.21065333 27.16110737 23.56607402\n",
            " 34.80919633 17.40821095 21.86948163 23.27960733 29.38044495 42.76455466\n",
            " 28.85109196 29.27378252 43.97594477 38.15004473 35.21004444 30.7281851\n",
            " 24.61094202 49.5051984  44.90147636 25.63598134 18.25735072 29.0387522\n",
            " 26.54701925 24.6305611  28.38948479 39.6944963  31.94983057 39.37712232\n",
            " 29.33550885 20.22743588 31.20373709 20.21710811 20.67075984 29.57932731\n",
            " 10.57794633 45.27884629 26.93167813  8.37835589 19.09871564 28.595083\n",
            " 16.7328547  25.14943358 32.20024952 35.54370642 15.55234167 22.73448753\n",
            " 13.91900456 47.51627747 33.53201274 31.88424888 17.61891669 16.72978981\n",
            " 33.93100556 26.54679047 28.23874197 17.78876958 14.92344364 17.59526664\n",
            " 34.34105116 14.00979293 32.40636632 45.53497695 25.02009257 36.3450242\n",
            " 33.90757758 30.92458734 27.80365338 38.44938894 33.75436613 42.61444096\n",
            " 39.35191517 44.46919342 28.62645942 30.4206149  22.15347859 16.55076511\n",
            " 31.82865288 41.40756268 39.55265039 40.90836022 25.83048077 49.77664018\n",
            " 17.69350804 40.25480007 35.8623967  29.78083966 27.10880728 34.16925344\n",
            " 25.40452605 15.4771637  37.29326059 35.59196504 32.0913307  49.02390037\n",
            " 21.40516234 26.6653342  16.81864396 19.70970592 18.35744637 39.69864584\n",
            " 49.74328095 21.33624115 28.09960154 47.81678784 20.5074113  26.17422759\n",
            " 18.74298113 27.77114193 14.54742537 33.17795976 25.7711291  28.3835758\n",
            " 21.56891385 17.95467025 26.05621204 41.66923251 16.69211878 50.\n",
            " 16.78891222 17.50230636 35.05092153 44.07430717 38.08263254 20.2623281\n",
            " 16.04134766 49.91016179 27.52330288]\n",
            "selection [110 837 416 159  95 185 371 198 340 770 697 210 307 353 834 467 603 714\n",
            "  13 660 303 242 640 554 463] (25,) [ 8.01849297  8.37835589  9.23372461  9.61312533 10.03473171 10.06955307\n",
            " 10.112216   10.17402848 10.2038677  10.35923451 10.39977923 10.40257398\n",
            " 10.47346799 10.53416886 10.57794633 10.61310124 10.69430357 10.74523097\n",
            " 10.84702464 10.84702464 10.87157216 10.92240488 11.12367004 11.13267669\n",
            " 11.20758351]\n",
            "trainset before adding uncertain samples (375, 10) (375,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [204 196] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "std (902,) [42.40581948 25.57242454 25.21292726 21.66115601 24.72329149 20.98212486\n",
            " 31.37735236 15.18779448 14.79435382 28.08138431 48.01441225 47.83281504\n",
            " 47.76534828 38.99243742 32.48500257 24.06848836 22.38647863 43.89725671\n",
            " 22.01795024 44.90876115 27.05626653 17.978269   43.16095782 29.94054247\n",
            " 43.93251898 28.43422426 21.97129829 34.94977048 33.7722461  38.34002008\n",
            " 24.66275551 32.08934685 24.12771064 18.79625939 17.55791442 20.93279316\n",
            " 26.69795823 16.30284386 41.31810623 31.02386182 11.66694324 25.7721622\n",
            " 32.97090417 14.37355856 38.03281748 13.66898879 48.72654158 39.11955084\n",
            " 24.4043795  34.8877375  23.76722715 20.18349991 17.42165908 16.75952678\n",
            " 28.47012842 47.71105719 24.41940934 47.14832922 21.16968339 40.47683907\n",
            " 15.60378678 38.71005221 35.09204299 33.88646115 12.69062879 22.63895728\n",
            " 23.49242987 18.22373427 46.5292434  27.84325724 37.11597072 38.51798578\n",
            " 15.63439919 49.97730303 20.03550336 27.31870132 42.47328487 19.43075343\n",
            " 33.64141833 36.48112731 42.47592732 20.48907176 20.52821464 30.76145877\n",
            " 30.32503985 28.32342365 39.1016223  21.64218611 28.6979079  17.07829737\n",
            " 41.09541128 37.56320707 37.63073378 32.64351613 49.98113317 40.27752163\n",
            " 34.09592523 49.99625446 41.52531649 45.57562307 28.62222889 12.36841234\n",
            " 36.09346885 24.88772224 31.76727759 26.94456655 46.91967059 32.68726145\n",
            " 32.43255027 25.04472699 20.62317855 19.17831406 33.89510243 20.35481926\n",
            " 39.42675502 47.57739807 34.93031897 16.7325124  43.48507042 16.08770185\n",
            " 32.12307937 31.28238743 23.56210289 33.23157106 49.9811189  15.08582377\n",
            " 13.83458139 34.04691189 18.54328999 23.66461115 29.14412938 21.7178745\n",
            " 15.18667044 49.99968924 31.86683664 31.67650862 38.73447243 27.30972917\n",
            " 28.69779081 35.58261058 36.59809985 25.15497644 23.55740228 36.37812485\n",
            " 17.38680605 26.51700697 34.1464963  29.08124477 37.72997679 26.29529653\n",
            " 21.95018048 23.33518434 29.81434877 16.28359799 19.3231596  29.71383978\n",
            " 30.03505002 30.03396345 28.40294074 18.54482096 49.39823715 24.3585807\n",
            " 25.14981377 24.98885912 22.31036209 41.0201417  15.87971671 31.14754269\n",
            " 49.98135814 29.97691513 36.89889794 19.57580195 29.08327918 29.78773885\n",
            " 16.23134933 24.79314321 35.34608977 43.07622037 47.57448264 35.7169362\n",
            " 15.79251286 18.64281898 34.023968   49.31771516 11.97722662 10.37663485\n",
            " 40.83285457 19.07480465 36.62940679 23.02935301 25.18096382 38.75017138\n",
            " 20.07687319 35.87018903 39.45396268 14.99817388 13.97562702 46.19140089\n",
            " 15.76607437 33.66183869 12.01977305 39.87891915 43.14867638 23.30906972\n",
            " 30.82430879 37.3860975  19.49168033 18.76585243 33.12750962 15.87273325\n",
            " 21.5915149  41.63583285 28.38024001 24.21122268 41.83092172 32.0458279\n",
            " 35.75220477 19.85058622 30.92533375 27.6223779  38.55494942 30.40560003\n",
            " 23.48268194 33.68921557 33.9638261  36.59235787 35.1051623  42.88120796\n",
            " 29.76837274 16.62474245 37.49517298 12.98326208 49.59717916 21.08309676\n",
            " 38.55994597 19.01972085 38.62864561 25.88177268 47.72025718 35.39089619\n",
            " 34.24558101 15.49728127 21.96394461 44.44356822 44.89948853 39.81217375\n",
            " 31.13296565 49.90257452 17.74425817 45.02228576 36.76723606 37.184286\n",
            " 18.62498182 35.44119981 29.19214367 24.89470021 49.99570646 45.11996216\n",
            " 27.24034017 45.53984376 33.47227571 41.44896482 45.70679759 48.72770164\n",
            " 28.11515536 19.25909558 34.49086382 19.49056036 31.85772783 27.59005171\n",
            " 24.13885796 24.13315161 14.54822619 46.23975752 49.91277334 23.34356166\n",
            " 20.83696718 27.62748922 19.4430743  22.73907052 27.49128086 28.8393496\n",
            " 40.40825563 26.68060542 29.59527245 30.22144505 35.75828612 36.68215953\n",
            " 32.27565028 27.84325724 42.82558516 19.34682933 35.1562839  27.61141451\n",
            " 24.73359945 29.75348669 28.86229755 25.88902245 13.58670665 29.81165161\n",
            " 12.51355143 37.06713519 32.00599431 37.93202347 33.31155696 30.55298992\n",
            " 22.64769913 31.29056127 25.72980928 19.56998107 12.24283212 14.81190356\n",
            " 11.3608406  32.57776466 38.1103069  29.7585757  28.06130799 21.78643878\n",
            " 18.09458147 32.38388429 22.17148981 21.39230668 45.47814477 28.38412508\n",
            " 23.74148576 23.28422589 33.79884928 30.35181977 12.72212457 35.42766028\n",
            " 21.4806123  35.60448787 40.27322443 33.81637401 15.13277659 42.64391854\n",
            " 27.50804701 27.13148034 30.85587676 19.61980355 49.97293777 29.95682011\n",
            " 22.01372499 29.12387955 19.38282121 17.1151951  33.09270951 49.60466544\n",
            " 33.28772599 45.83512733 35.45963569 39.84276733 15.77864264 24.16758889\n",
            " 21.77904753 38.10814847 16.27187575 28.55089635 31.38211335 38.0926564\n",
            " 37.42900305 27.6045931  39.7581666  27.1390848  19.21046196 16.95115962\n",
            " 13.41123884 31.52227748 30.92720451 41.07766428 20.50186876 22.32645055\n",
            " 21.00926007 49.29942781 40.01912321 12.15148918 29.11704701 35.1631182\n",
            " 30.88769631 29.88794055 34.74424983 26.73365036 24.41278041 43.53973921\n",
            " 31.70363894 23.75263677 27.5824865  19.19213017 43.79925358 36.41073399\n",
            " 23.14561942 28.60139959 17.24573238 27.92057907 43.01608007 14.40369644\n",
            " 26.31178479 17.63799342 49.13743442 42.56951997 40.02020798 45.73330963\n",
            " 43.81482494 18.21041572 22.38859284 45.31497835 22.23487609 38.67548007\n",
            " 19.52402327 37.73881862 34.03979709 29.68980995 27.28419763 34.91652356\n",
            " 39.37345281 30.47733353 38.84159649 49.36162551 14.98278697 25.09869204\n",
            " 15.34276745 38.03897013 31.32474454 39.68062048 26.8111949  46.54386433\n",
            " 49.99885315 30.48118105 26.84218639 19.82595531 26.91172524 16.73989719\n",
            " 30.39077415 22.59668535 34.83389492 13.6625241  22.36367359 49.55128504\n",
            " 38.58464372 45.73649567 35.00431476 24.00652742 17.39961975 19.78073215\n",
            " 21.95237837 20.29207624 19.58959635 16.99051741 44.17748184 39.27797698\n",
            " 26.4528975  40.65725966 33.43035441 35.00241731 19.68070096 31.69897024\n",
            " 15.97923419 26.70225125 23.65729407 22.11166448 19.51267549 30.73309062\n",
            " 21.54853419 34.11419286 39.03526651 36.13146716 13.84171702 23.09035905\n",
            " 46.90007818 38.32119617 41.26621123 40.52692563 22.56048581 12.20877598\n",
            " 37.12445215 33.0191558  20.27912669 21.00159664 44.48272338 44.33731876\n",
            " 36.79615089 12.45046827 27.93453171 17.78833198 43.13468894 32.03150504\n",
            " 20.25685112 13.17942499 26.26784592 37.45488849 36.93147462 41.91451337\n",
            " 29.29151893 43.78724873 33.73184466 18.09458147 24.05061396 20.2783288\n",
            " 35.91730297 31.18506194 43.1238564  49.74546996 12.51392193 32.74971539\n",
            " 21.54853419 15.20836885 15.08241292 17.32525132 40.59828619 35.83346555\n",
            " 31.38177685 44.81613224 49.99829389 23.46842545 15.98954805 21.26407957\n",
            " 34.23414756 44.89929667 41.62316556 18.42690068 19.69760001 42.36665561\n",
            " 13.04780741 19.78099067 45.27447911 22.55910931 29.09023651 20.73526315\n",
            " 15.37491668 41.68773285 22.26080352 20.97367275 39.90706833 42.8817541\n",
            " 23.76620835 33.77418985 12.32205816 27.29327853 39.30229213 28.1017413\n",
            " 16.07270213 14.80286299 19.37160383 33.93127728 18.29708188 22.63667367\n",
            " 17.03747141 13.96647576 33.88624423 26.69116362 47.8852935  35.42980749\n",
            " 27.9556342  28.51532629 35.17781671 13.39114458 13.41863584 49.99925946\n",
            " 39.43536149 16.93151811 12.35164719 35.9748079  27.40940187 25.5423005\n",
            " 44.8109007  25.78031094 30.45116011 34.92928757 22.34674087 40.61200311\n",
            " 25.29462927 13.9552956  29.62111446 47.34372542 22.58103914 45.7016107\n",
            " 29.14375388 24.79207631 40.03511603 26.73994094 22.2493131  23.27265832\n",
            " 31.80380752 15.26177682 32.91063737 33.81294953 33.4099246  18.99663631\n",
            " 36.48686625 20.18305086 39.105841   21.03647101 44.50780563 27.14164482\n",
            " 40.2475473  15.55398689 17.25096724 40.06418959 23.38494464 48.27121594\n",
            " 17.75813236 30.9787242  14.72245767 48.87021681 40.46630935 21.78314463\n",
            " 35.43086562 14.23350606 11.69485503 28.8393496  49.95126558 26.5889353\n",
            " 29.10296243 34.39239215 47.02629061 38.05901232 44.50715581 32.39360678\n",
            " 46.76574606 27.58714351 24.40174222 44.00343622 49.23781597  9.76540168\n",
            " 35.43064329 24.32471662 23.8739222  16.30343379 35.58278346 19.85468473\n",
            " 35.89554203 43.48228849 36.41551179 41.23187186 10.50386218 46.65409863\n",
            " 48.38357488 14.39028741 17.44265967 29.75213756 30.86825954 20.54300921\n",
            " 19.98206201 19.91607099 17.43505479 11.52959468 45.1685151  25.30480729\n",
            " 19.60048648 36.99413766 29.02748089 14.55578708 23.61571428 22.18259691\n",
            " 28.68414496 30.43732747 33.32443285 28.28842122 40.3173751  34.70413091\n",
            " 13.17634271 38.09359815 13.23275415 22.57848112 46.30418495 30.6624201\n",
            " 14.62717013 19.78233918 49.99646013 49.13839036 37.78850414 35.75307656\n",
            " 48.90073954 37.30413746 27.26442292 28.86984491  9.85713942 24.84169064\n",
            " 38.06013401 31.91460568 36.63438871 31.1177927  13.6485135  24.96052559\n",
            " 12.9405289  29.0470071  29.06948022 29.8699672  16.07367644 35.91351629\n",
            " 22.85506365 36.98369746 17.55245528 24.47236165 22.88171603 39.88964444\n",
            " 22.13728526 33.70841776 49.98346235 14.85949645 32.750656   30.02660533\n",
            " 13.66429383 46.93172796 36.28328339 28.41748777 43.27765907 13.80577629\n",
            " 34.69443251 46.15547843 24.31316575 41.9835959  43.44813277 15.06321461\n",
            " 13.7976648  22.29715734 42.8210673  29.19051112 40.07639209 32.4742504\n",
            " 20.40609    49.61808302 25.14514364 17.55791024 17.19423534 32.8043906\n",
            " 29.60078848 26.06169588 30.52738531 19.60965396 42.83878114 31.69897024\n",
            " 44.95014826 46.69725847 41.30912825 31.77049249 22.65294926 35.72429661\n",
            " 13.58675188 39.91553225 26.22419525 32.63180438 37.9343881  43.17239718\n",
            " 49.16229538 15.7054396  34.66707553 27.69081167 42.16122438 17.78074684\n",
            " 11.99169304 18.83764461 49.61603967 23.6957462  34.29555649 48.18673371\n",
            " 45.736443   34.01440496 29.05428704 30.9876759  46.92584432 14.28139117\n",
            " 49.49636121 30.89766719 48.21788494 23.40799344 32.29632429 33.38469287\n",
            " 37.3860975  22.9107987  46.89610061 20.58432723 48.42219975 40.85342287\n",
            " 32.82568104 43.21604817 24.99469561 49.72080574 28.70372127 27.88051615\n",
            " 26.1359667  35.66512639 17.88163885 22.55738049 24.45763064 31.533067\n",
            " 43.31132698 29.63038567 31.47521084 45.86859774 39.3161324  36.04955073\n",
            " 34.33909507 23.96346172 49.59342692 45.60592412 27.58908942 18.39071226\n",
            " 29.5949746  29.80073122 23.76007666 28.73721807 40.50350363 33.00316802\n",
            " 39.96772218 31.18623615 21.5915149  32.03150504 21.57907404 22.41324021\n",
            " 31.7746577  45.81417267 26.27854192 20.48907176 29.64858991 17.69042635\n",
            " 26.04525609 34.34503389 36.5951381  16.90627126 23.91371177 14.77770626\n",
            " 47.40376192 34.70984148 33.04683608 19.74906195 18.00636737 34.01144547\n",
            " 27.54390572 29.29767137 17.38519011 16.04230583 19.20757955 35.42074126\n",
            " 15.19610239 33.94926839 46.09113892 26.25018276 37.23362286 35.07364733\n",
            " 32.94746964 29.67108296 39.03382211 35.86568237 43.34805293 40.20941439\n",
            " 45.11939959 29.59017582 32.46062823 24.7259155  17.11989852 34.82476232\n",
            " 42.12427338 41.6016589  41.00056425 26.74084476 49.89822915 18.56714753\n",
            " 40.43512763 37.73212254 29.73742847 25.22275933 34.71668959 26.30060983\n",
            " 15.82509804 39.7357571  37.22347383 32.94482933 49.50634532 21.96638148\n",
            " 28.9065349  18.09106482 19.5454962  17.34685735 40.42626404 49.88136083\n",
            " 22.23481319 30.9834482  48.56935195 21.9212988  27.66771838 19.6898619\n",
            " 27.47090556 16.80378892 34.09765    26.93656836 30.35983413 22.50728054\n",
            " 19.57471103 26.40526439 41.50631773 16.62847487 50.         20.02814883\n",
            " 19.40922748 35.6797267  45.88939907 38.30040753 21.06297403 16.89285619\n",
            " 49.96432189 28.47193935]\n",
            "selection [623 676 185 634 312 645  40 608 184 750 200 375 473 310 536 560 101 481\n",
            " 300 502  64 328 684 231 522] (25,) [ 9.76540168  9.85713942 10.37663485 10.50386218 11.3608406  11.52959468\n",
            " 11.66694324 11.69485503 11.97722662 11.99169304 12.01977305 12.15148918\n",
            " 12.20877598 12.24283212 12.32205816 12.35164719 12.36841234 12.45046827\n",
            " 12.51355143 12.51392193 12.69062879 12.72212457 12.9405289  12.98326208\n",
            " 13.04780741]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (425, 10) (425,)\n",
            "updated train set: (425, 10) (425,) unique(labels): [213 212] [0 1]\n",
            "val set: (877, 10) (877,)\n",
            "\n",
            "Train set: (425, 10)\n",
            "Validation set: (877, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.11764705882352941, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "std (877,) [42.10545403 23.61066376 23.35851613 19.30173906 24.07635303 19.93669635\n",
            " 28.82992595 13.42492867 14.7294569  25.94732726 47.62135622 47.6318974\n",
            " 47.03756535 36.60112533 29.90720307 23.93220458 20.5591015  42.87109424\n",
            " 18.97630455 43.94034829 24.98775071 16.42229304 41.51755354 28.63231251\n",
            " 42.68928781 26.47240759 18.24285113 33.62475556 32.6254986  37.3613855\n",
            " 22.97056134 31.48329939 20.72481589 17.59412114 18.03139851 18.70108355\n",
            " 26.35266536 16.61640218 40.25627098 30.73390811 23.74712285 29.97030242\n",
            " 12.86947757 35.75370347 11.80756067 48.50796569 36.83245485 22.15148096\n",
            " 33.3278707  21.95012005 19.57118287 17.87402679 16.84815257 26.98560014\n",
            " 46.70111824 23.67530884 46.81231392 19.6855154  38.20493969 13.51421992\n",
            " 36.70758539 33.30029943 32.65296341 20.58777827 22.58125074 18.22458589\n",
            " 45.10546343 27.95770757 34.99204446 35.74738624 14.28605824 49.96831887\n",
            " 19.06280111 25.87256443 42.10925076 19.33434012 31.03298084 35.14847231\n",
            " 40.55967854 17.82581606 18.76388888 29.22589272 29.20410657 27.54888885\n",
            " 37.11688006 21.04858399 28.5283668  17.48709759 40.37253638 36.78802551\n",
            " 37.13020241 31.86161145 49.97450806 39.37412264 31.73934756 49.99467648\n",
            " 40.80044927 44.33001887 26.61445657 34.58858285 24.99248705 29.401126\n",
            " 26.98220508 45.52729966 31.94139264 31.57821712 22.91492918 19.063139\n",
            " 19.47030201 33.15397707 20.59256421 37.97551474 47.21028263 32.88439976\n",
            " 14.76322991 42.48776557 13.80731305 31.36714105 29.39429044 22.2850672\n",
            " 32.15841374 49.96831127 13.04091736 12.48054095 32.39070745 16.70420771\n",
            " 21.60171347 26.10831022 18.90065011 13.85601024 49.99962251 31.01635257\n",
            " 30.79331306 36.51956321 25.21269431 27.70329304 34.29901831 35.27005278\n",
            " 23.54556628 21.09899547 34.38762613 17.67300998 23.97527443 33.02984725\n",
            " 27.72664738 36.20808144 24.49718473 19.97493953 23.55311901 27.01541208\n",
            " 14.70375175 17.49834532 29.32541657 29.34976195 27.93284576 26.22247245\n",
            " 16.32648611 49.17025476 22.65766205 24.89883948 24.41982436 23.96800585\n",
            " 40.00343599 15.07191164 30.34748739 49.95865978 28.76585035 35.28156811\n",
            " 19.87602966 27.11112918 29.83027302 16.63752349 23.89141418 33.83204229\n",
            " 42.11710659 47.27247964 33.77950272 16.05989585 16.83447928 33.35407573\n",
            " 49.06373205 40.42097863 17.27053472 35.55591847 20.76666954 22.9382298\n",
            " 38.46831748 20.29477007 33.24867234 38.68063409 14.90262181 13.43158782\n",
            " 46.14426522 15.26958976 33.3603332  38.67702138 42.1942031  21.18189463\n",
            " 28.89588278 36.48406524 18.54656335 16.80697434 32.38855072 14.31159612\n",
            " 19.65250724 40.94719409 28.23919564 22.36955609 40.0935466  29.87725646\n",
            " 34.64593857 17.68261525 27.78751559 25.85371078 36.36301555 26.47988658\n",
            " 23.89956991 30.52477783 31.78479233 35.06708368 32.83084264 40.94796093\n",
            " 29.35844676 14.94809597 35.62461866 49.48578366 18.89938905 36.63645289\n",
            " 16.05067799 36.25620464 23.95953081 46.85221622 34.27496579 32.74410089\n",
            " 14.43576705 20.2357404  44.05454489 44.22411239 37.20412535 29.29657629\n",
            " 49.89939032 17.83858018 43.83706284 35.36038389 36.8130167  16.82702514\n",
            " 33.46493303 26.90762929 24.16272532 49.99469692 44.44868546 26.51993892\n",
            " 44.61545312 30.70215322 39.77139315 43.77927356 48.48189872 27.62703931\n",
            " 19.31570898 31.83775029 19.81605008 32.13505715 24.83816062 23.14242102\n",
            " 23.82032624 13.2272822  45.96218851 49.87791461 23.54776413 19.54472179\n",
            " 27.11308745 18.02552091 20.45707189 24.74970521 26.37723443 39.29687179\n",
            " 26.12213972 29.1778632  28.80304836 34.40012224 34.92599047 29.82780462\n",
            " 27.95770757 40.94324555 17.1528515  33.19032797 26.17183165 22.98326243\n",
            " 29.44992992 28.56307637 24.42191445 12.77093326 27.13592402 36.12723091\n",
            " 31.18209888 35.79185334 33.15960619 28.16907795 22.70373723 31.0018381\n",
            " 23.92861598 20.14893193 13.6997393  31.49197094 35.74853829 27.38823151\n",
            " 26.47785951 19.7462901  16.25551515 31.54956887 19.9868669  22.11191066\n",
            " 44.1407802  26.65782289 22.58083669 21.46370371 31.53345975 28.09980405\n",
            " 32.7584238  19.53720892 33.53115643 38.01933609 31.47184854 15.80260341\n",
            " 41.16535887 26.2784747  25.49657797 30.56879555 19.94946443 49.96417035\n",
            " 27.42608551 21.91920813 27.55872599 17.93488588 14.34733802 31.40999253\n",
            " 49.31830769 32.3311109  45.13670465 33.22765841 37.48260511 12.84771917\n",
            " 22.00389889 22.7716037  36.40306747 12.64492784 28.22590598 29.83108357\n",
            " 37.2052832  35.33393552 27.50913542 37.58977649 24.57875026 17.3096158\n",
            " 15.02351191 12.15185782 29.51508845 28.93879816 39.05171617 20.23425457\n",
            " 21.21643979 19.23092242 49.11890649 38.07739272 28.90975608 33.32743073\n",
            " 29.66649603 27.06088814 32.35234235 25.92671484 22.58941663 42.91982994\n",
            " 30.4797264  22.89690086 26.60058369 17.63141701 42.45977327 35.12160607\n",
            " 21.96966972 27.35451283 15.64543888 25.52548009 41.9998842  13.93271275\n",
            " 26.37967518 16.0198196  48.90414458 40.5049197  38.35780117 44.17164808\n",
            " 43.31141882 17.71397297 21.65323176 43.87390021 20.32960553 36.21278154\n",
            " 20.18933287 36.3557194  31.49246661 27.70081724 26.63763392 32.89929677\n",
            " 38.2921619  29.97551744 38.39861372 49.18137025 14.3196872  22.3911584\n",
            " 13.15717599 35.93952537 28.61112331 38.24117039 24.68389101 46.16581386\n",
            " 49.99832751 29.80847309 25.25824273 17.21675832 25.30993652 15.38005049\n",
            " 29.44945148 20.04917524 33.92467925 12.65483739 20.97450259 49.51353533\n",
            " 36.09611511 45.25204136 34.51862043 21.57874173 15.92299503 19.46059979\n",
            " 20.47437126 19.37282218 19.37978877 15.33581257 43.28933819 38.38556389\n",
            " 25.56434268 39.5808261  31.99261476 32.97742333 18.24422694 30.33275224\n",
            " 14.47137978 23.31433594 21.61251299 20.13847256 19.44186309 28.32354131\n",
            " 20.17661644 33.10403408 38.42642363 34.27059256 12.90573336 23.27428526\n",
            " 45.91906899 37.65421291 39.41174142 38.3009198  20.36188806 35.55566376\n",
            " 30.31183588 18.24458587 21.04180223 43.21902029 42.92920613 34.68643507\n",
            " 26.39681504 18.00526188 42.77006196 30.23740863 19.89441935 11.53867625\n",
            " 24.83006027 36.22249725 36.25895162 39.8816643  27.15247892 42.54767785\n",
            " 33.59980861 16.25551515 22.59281733 17.36803266 34.19070077 30.42051118\n",
            " 41.48330007 49.59006854 30.41731587 20.17661644 15.1902937  12.94171968\n",
            " 15.48802635 39.36963009 33.4568563  30.02513253 43.98584469 49.99802764\n",
            " 24.38855512 16.03275013 20.84311563 32.85886509 44.11627002 40.60902849\n",
            " 17.42575828 17.75098795 41.70892828 17.75248001 44.24806816 21.07147281\n",
            " 26.47343783 20.67456598 13.6555792  40.94637983 21.52173416 17.97422081\n",
            " 39.13246292 41.58372322 24.329912   32.748683   26.24852577 37.15057965\n",
            " 27.2313146  14.51282038 13.18046454 18.43659166 31.31096869 15.47683601\n",
            " 20.61596429 17.28260436 13.33519584 31.2224807  22.56278633 46.90425851\n",
            " 34.44224719 28.21382355 26.74843693 34.14449396 10.7216995  12.30639698\n",
            " 49.99901534 38.72698831 14.70824737 35.93503885 26.05169502 23.9941235\n",
            " 43.4446464  24.04044581 28.3365949  33.01426256 21.219932   39.40031834\n",
            " 23.238192   14.44275706 26.41610839 47.24940942 20.95575547 44.99765249\n",
            " 26.73577547 21.37793525 37.74261536 24.47495504 19.15584387 21.01460636\n",
            " 30.90082225 12.94888521 30.53834939 33.37860195 32.96820299 16.94189386\n",
            " 34.50170343 18.30408919 37.02053152 18.35399841 42.85445671 25.59573581\n",
            " 38.07145666 15.50720055 17.43839451 37.80510918 22.24206608 47.54504963\n",
            " 16.54155967 28.65217881 13.42905862 48.61196392 39.68079276 20.73499148\n",
            " 34.87847455 12.5346894  26.37723443 49.93253911 25.9514209  26.78990715\n",
            " 32.2180498  46.24611198 35.56603727 42.92835605 28.59349796 46.1855824\n",
            " 25.17960387 24.98070056 42.15507601 49.04141921 33.01499865 24.36302276\n",
            " 21.65443494 16.46604693 36.06808733 18.6369729  34.86216161 42.45065395\n",
            " 34.58900161 40.48833491 45.79592238 48.08053063 10.28252767 16.45765703\n",
            " 29.23750277 30.07337361 21.099583   19.58494968 17.68708258 17.92496111\n",
            " 44.61104639 24.06210889 19.27255889 36.13151716 26.72434432 12.73728103\n",
            " 22.56254705 20.0990658  27.79658451 30.18316339 30.60968569 27.70620616\n",
            " 39.70545915 33.54175323 12.73354887 36.47776909 12.60080684 21.20661933\n",
            " 45.05451706 30.04315857 11.9311166  19.20845655 49.99476238 48.85038512\n",
            " 37.05146017 34.13549421 48.64838672 35.60143669 25.58303451 26.13584117\n",
            " 23.81914049 37.62193718 31.81621364 34.80195468 29.40565461 13.6878733\n",
            " 24.38022198 27.99424952 28.04387007 29.10149244 16.02988595 36.92696374\n",
            " 20.64060316 35.23236203 12.4318234  22.10541038 22.16691135 38.62084566\n",
            " 20.45240584 33.07952463 49.97615559 13.07799687 30.29435199 28.55188301\n",
            " 12.37242889 46.23567535 35.13853112 27.24847188 42.84753286 12.1734638\n",
            " 32.79590397 45.76417674 23.85315653 39.90877143 42.70513358 14.13893009\n",
            " 12.35997095 20.41981938 41.80293736 28.64557569 39.01608515 30.2198176\n",
            " 20.2608801  49.55266766 22.85362656 15.58938102 16.76604883 31.73120807\n",
            " 28.93257467 24.40066124 29.13952118 20.36345782 41.19608553 30.33275224\n",
            " 43.33834274 45.64241347 39.22248792 31.16010121 21.62110955 34.93120417\n",
            " 11.90870336 39.34515734 24.39667124 31.62856132 36.06721374 42.43812195\n",
            " 48.915755   14.52247877 33.22393997 26.18716637 40.27859969 16.87759527\n",
            " 16.78269814 49.52593141 22.17120229 31.69541696 47.75674312 44.36908463\n",
            " 31.93926404 28.32653475 28.98067063 45.89051497 13.63883967 49.37011669\n",
            " 29.17334544 47.81840073 21.6273702  32.55340685 32.17833683 36.48406524\n",
            " 20.85881738 46.20625636 17.70125056 48.06727263 40.09828404 31.40135512\n",
            " 42.44638478 24.84503317 49.63016441 28.38627827 26.43112794 25.7209597\n",
            " 33.85354047 17.91217648 20.99111287 22.6443691  30.79198108 42.16775176\n",
            " 27.90968864 30.54014761 45.45917538 38.18788285 33.82480309 33.423017\n",
            " 24.13216857 49.32543694 44.01988178 27.15567301 16.73789136 28.89055845\n",
            " 28.05972348 22.46325937 27.21343916 38.79081372 31.07127458 38.29667534\n",
            " 30.80805449 19.65250724 30.23740863 19.45735761 22.46014153 30.77679768\n",
            " 44.75624346 24.02733441 17.82581606 28.23339102 14.15797745 24.33641076\n",
            " 34.54443667 34.6272762  14.82446481 22.24529941 15.09644342 47.23054404\n",
            " 32.64554678 30.28089477 19.66160401 16.18165438 32.53739825 25.74039591\n",
            " 27.04102045 16.73868503 14.67933536 19.38338201 33.3744965  13.48853736\n",
            " 32.88275809 44.69167541 24.10095073 37.50127855 32.79025287 31.72110836\n",
            " 29.09834009 37.69394464 34.22622723 41.71258    38.52904271 43.86320253\n",
            " 29.31327625 32.32039255 24.53987801 14.56320413 34.4872931  40.34270797\n",
            " 41.10077361 39.8258366  24.38380857 49.8479511  16.0453512  39.61840693\n",
            " 37.31123034 28.33757596 23.00483076 33.31393605 24.26158214 15.47738298\n",
            " 38.63555177 35.33283146 31.76860241 49.44292056 20.54967504 29.94220167\n",
            " 16.70828597 20.86743885 17.85896835 39.55941562 49.83113689 20.27056394\n",
            " 29.39736484 48.15208216 21.92613723 24.87511038 16.3518159  24.21044349\n",
            " 15.32557525 31.89845947 24.28541615 29.68901225 20.63521296 19.78743028\n",
            " 25.39472719 39.08014101 16.2991679  50.         19.98509433 19.04711458\n",
            " 33.64949787 45.71251271 36.94577449 19.22461007 14.55691543 49.94596543\n",
            " 27.29300945]\n",
            "selection [616 538 473  44 714 644 355 683 539 690 678 668 123 589 640 345 423 638\n",
            " 629 291 341  42 454 491 565] (25,) [10.28252767 10.7216995  11.53867625 11.80756067 11.90870336 11.9311166\n",
            " 12.15185782 12.1734638  12.30639698 12.35997095 12.37242889 12.4318234\n",
            " 12.48054095 12.5346894  12.60080684 12.64492784 12.65483739 12.73354887\n",
            " 12.73728103 12.77093326 12.84771917 12.86947757 12.90573336 12.94171968\n",
            " 12.94888521]\n",
            "trainset before adding uncertain samples (425, 10) (425,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [225 225] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0\n",
            " 0]\n",
            "std (852,) [42.2254691  22.47319575 22.6488907  20.99886049 23.16750501 20.23442879\n",
            " 27.7107137  12.22784115 15.63126348 18.33387235 47.37781846 47.46420258\n",
            " 46.4036547  34.2719522  26.63016586 23.79713946 20.55882005 42.0729151\n",
            " 18.93660184 45.03041009 24.84490966 16.3872394  40.23934828 27.45023751\n",
            " 40.91364227 24.71702168 18.03201872 31.11992534 31.58064989 37.80055073\n",
            " 23.04278922 30.82396043 16.82599484 17.78057229 18.6125898  18.31045653\n",
            " 26.33123661 15.87968347 38.50807725 28.40337994 22.32503791 27.12197606\n",
            " 33.76372353 48.34454663 34.13747745 19.20553201 32.33751779 20.34952622\n",
            " 19.65377812 18.7743813  17.08923669 24.41033822 45.54188116 25.19613339\n",
            " 46.48020143 18.12327739 36.32038213 12.66796263 34.69717014 33.59547755\n",
            " 29.16965355 19.42837503 23.39259717 17.32586141 42.87485167 28.27148405\n",
            " 33.42498655 32.5216122  13.63178241 49.95923359 17.77296509 23.2922462\n",
            " 41.30659082 20.00725655 29.43717396 32.89651937 38.37645911 17.15993429\n",
            " 18.94498503 28.85579912 27.7708398  28.40577212 34.04049287 22.63140173\n",
            " 27.55613752 20.24359093 39.80789512 38.07839373 36.44515863 31.72495453\n",
            " 49.97023277 39.25491084 30.57883937 49.99304063 40.29126441 43.09453509\n",
            " 25.40548118 33.61477377 24.02979454 29.19967106 26.80805997 43.22634706\n",
            " 31.87782811 30.78289792 21.13817824 17.6191413  18.86236499 32.61325942\n",
            " 19.67101381 35.99048926 46.9086506  30.38650671 14.60038179 42.43371016\n",
            " 12.80461631 31.01639686 28.19674133 20.91611221 31.42652742 49.95412301\n",
            " 12.05388217 32.03837712 16.83212758 21.44103295 23.15235195 17.11380487\n",
            " 13.90612974 49.99954014 28.88319936 31.65390799 35.09631274 24.70956129\n",
            " 26.61671583 32.61636225 34.91117641 21.76273574 18.9080732  33.09881185\n",
            " 16.83178279 23.30514004 32.60775338 26.40924828 35.80758679 23.84992492\n",
            " 20.04226933 22.62437665 24.26965936 13.71918829 16.62146267 29.42327422\n",
            " 29.79800374 27.87110484 24.38287179 16.60056905 48.7067264  22.36296868\n",
            " 24.58045429 24.35246765 26.71300963 40.04875704 14.17528168 28.96384644\n",
            " 49.88652785 26.89583096 33.15463842 20.41903827 26.71380691 29.85997541\n",
            " 15.51468327 23.05458843 33.96382243 41.65989921 47.02549195 32.16786784\n",
            " 16.20367521 16.65770128 32.80828779 49.11270182 40.56993623 17.8548803\n",
            " 34.8916622  19.21261107 22.22106482 37.80892609 20.14883759 31.66453993\n",
            " 37.77885382 14.45900599 12.44723112 45.94706718 14.47458899 32.88327238\n",
            " 37.44834546 41.52432657 20.19824034 28.59373532 35.65104064 17.39561242\n",
            " 15.41196178 31.966887   14.24883777 20.99195081 40.31668782 29.45426884\n",
            " 22.49202477 39.2270503  28.66483512 34.01393076 17.64004459 24.48583628\n",
            " 24.97663457 33.6385899  23.05878842 24.318558   27.4763371  30.54578602\n",
            " 34.80654874 31.95503103 39.04366927 29.39359787 14.22067671 33.82862509\n",
            " 49.40921271 17.71624707 35.18962474 16.65875498 33.56029898 22.28015342\n",
            " 45.23340193 34.5039513  33.13614973 13.85879462 19.89529198 44.26950478\n",
            " 42.80794174 34.87534671 28.62128836 49.8919672  17.74573261 40.15041635\n",
            " 32.88725029 34.28618562 15.53868711 30.91839134 27.25635843 22.80933373\n",
            " 49.99398431 43.81554576 27.66635388 44.2378376  29.50976491 38.05121751\n",
            " 43.77985432 48.37892934 26.37759486 18.13047193 29.34485071 20.43933636\n",
            " 31.01043756 24.23593452 22.19805498 23.28370907 14.68700486 45.47563541\n",
            " 49.8478121  23.81105162 18.33335109 26.85992568 18.50582814 18.8168982\n",
            " 22.84021895 25.01982208 37.9207752  25.32782052 28.16620332 28.15979853\n",
            " 34.23071433 33.07642133 27.72297481 28.27148405 38.81224294 16.43077197\n",
            " 32.12833442 27.02113677 21.1958681  29.53223118 27.97889998 23.50541862\n",
            " 25.57130066 35.57238637 30.14963525 34.77141408 32.13015623 26.36469264\n",
            " 21.58198306 32.44706189 22.8950969  21.16272875 13.57821982 31.2355552\n",
            " 33.63556756 25.00430882 23.96042263 19.88902689 15.27521433 31.29101346\n",
            " 19.49342746 22.31809803 42.26302    25.10786683 22.0059497  20.78384225\n",
            " 29.16514786 26.63675275 30.191944   19.60354911 32.28978484 35.97987966\n",
            " 29.21195083 17.07731227 40.48601123 24.67063014 23.61190737 29.37244931\n",
            " 20.31270432 49.95985111 26.2213978  21.60005282 25.97107396 18.51488134\n",
            " 10.77558884 30.28823649 48.90949214 31.72845759 44.37918547 31.23838109\n",
            " 35.35401363 20.66891243 23.02691142 34.36566075 28.44085289 29.14766186\n",
            " 35.71975976 33.04582158 26.66406171 35.61373692 22.14242937 17.80773297\n",
            " 14.52626924 28.73908368 27.12697228 37.22225895 17.26946245 20.89456954\n",
            " 18.20518617 49.00627202 37.12228215 29.88951111 31.76825368 28.90310252\n",
            " 24.82918014 30.7469642  24.72377478 21.34411566 42.71394053 29.27300686\n",
            " 21.57254495 25.20587648 17.91579083 40.80213226 35.10299922 21.76252677\n",
            " 25.89597129 14.51294919 24.86137443 41.15083724 12.88356317 26.71411348\n",
            " 14.75623632 48.76701695 39.0926216  38.69080953 42.76240933 43.48662466\n",
            " 16.59180849 22.86669703 42.28212968 20.59918724 33.23806075 20.25146467\n",
            " 34.66727716 28.97266293 27.74197672 25.67425312 32.3324244  36.19237938\n",
            " 30.20194602 37.58169338 49.02811999 11.46863744 20.2808591  12.65474578\n",
            " 33.6127797  27.93372454 37.45836073 22.73248874 45.9135313  49.99787108\n",
            " 28.3781233  23.97335801 15.76980225 25.34486008 15.92613374 29.53779253\n",
            " 19.47932006 33.34858664 20.60648893 49.47164531 34.92182966 45.29301486\n",
            " 34.13271682 20.66561197 16.7957204  18.94533943 21.59036101 18.8455659\n",
            " 19.99699019 15.03930148 42.96791156 38.27631062 24.50273103 37.58997777\n",
            " 29.36749065 32.14210252 16.90595692 29.30086723 13.5448234  21.72097736\n",
            " 20.62760498 18.11971418 19.32188303 26.38529548 19.53341572 31.83470933\n",
            " 38.23964986 32.71114114 24.04825555 45.18682214 36.76404705 37.37034209\n",
            " 36.73067201 15.95599461 35.01848481 27.31724621 16.35296877 20.48043664\n",
            " 42.05134232 42.3320157  32.73951224 25.10547858 17.22429702 42.66655046\n",
            " 29.32196756 19.89642396 24.19440834 35.46166071 35.14858406 37.78429121\n",
            " 26.05057689 41.69986382 31.33716605 15.27521433 23.38699596 16.48791753\n",
            " 31.48761096 28.90924264 39.63436163 49.18328851 28.32580302 19.53341572\n",
            " 13.15782001 15.05699017 37.77379426 31.19081406 29.01192762 42.85277406\n",
            " 49.99763453 24.98330895 13.84399024 18.82425987 31.87036365 43.37209022\n",
            " 38.95185057 15.62735875 17.16659616 40.83160971 17.01765906 43.27000803\n",
            " 21.62566128 24.14017947 18.9978726  13.04627844 40.66330212 20.10428373\n",
            " 16.78649647 38.52846244 40.44835845 25.90882286 32.26387966 25.77240831\n",
            " 36.22417887 25.75761106 15.29245397 12.85959976 18.60283168 29.70064583\n",
            " 12.58208879 20.0508363  16.25569018 12.92310285 28.04422474 17.36536323\n",
            " 45.41084402 33.71786548 34.07665909 24.97886031 34.26934842 49.9987992\n",
            " 38.35152729 14.09645849 35.32054765 24.52263559 21.94905186 42.09807841\n",
            " 23.39456757 27.02887642 28.26880118 20.5256429  39.4238253  23.17852856\n",
            " 13.97131257 22.35200984 47.13084287 20.28415756 44.45657082 25.35691458\n",
            " 15.79301691 34.72795274 24.2404134  21.09879718 20.33284017 30.50769978\n",
            " 27.97324852 34.1434786  32.44125698 16.30157906 33.13551606 17.35219876\n",
            " 36.02858114 18.16773725 40.96998524 24.2486977  38.25646888 15.18502203\n",
            " 16.92825731 35.6117866  21.62219858 46.60947429 12.76426564 27.24546403\n",
            " 11.78508326 48.47790033 38.81468439 20.30058354 34.92854787 25.01982208\n",
            " 49.91750751 25.80081829 24.63080808 31.03939522 45.49185844 33.835689\n",
            " 41.41815052 27.20138859 46.43257337 24.41121127 18.61935013 39.98181576\n",
            " 48.92374649 31.87537031 24.42945233 21.40564525 17.84152447 35.17118142\n",
            " 19.5647778  34.19523523 42.08071802 33.87567284 40.07499564 44.85417802\n",
            " 47.79505802 16.72148832 28.41460342 28.69226958 22.27769907 18.81061905\n",
            " 17.02295581 18.02860137 45.38724244 23.73833803 18.74753015 35.41041397\n",
            " 24.52002174 19.94667712 19.01983016 25.61346777 30.61973853 28.7229675\n",
            " 25.98555649 39.58685863 35.18576006 35.15595983 20.098533   43.32654471\n",
            " 28.51891008 18.55545867 49.99372666 48.57909164 36.88458595 32.81699188\n",
            " 48.57150834 35.03367404 24.83102144 25.24318256 22.94852024 38.02574233\n",
            " 32.13872095 33.7753545  29.84324035 14.23757502 25.11344504 26.72679335\n",
            " 27.58763204 27.41012741 16.45381286 36.23607329 18.49424314 34.55264588\n",
            " 20.62457403 21.59689378 37.74529094 20.51148691 32.98380926 49.97066857\n",
            " 13.59066822 30.71098287 27.55946729 44.65941426 34.04860975 25.18907438\n",
            " 43.07037711 30.97823537 45.55569369 22.69192872 37.51303734 42.98009054\n",
            " 14.02907061 19.03339718 40.37017669 29.02411617 38.44751441 29.3923968\n",
            " 22.04550158 49.48691821 22.21238015 13.23356247 16.33585312 31.0782441\n",
            " 28.32860877 23.35175286 27.59696246 19.72006427 39.57248547 29.30086723\n",
            " 41.58365899 44.06664696 35.96071768 30.63163457 21.05998207 33.63188177\n",
            " 38.84096592 23.7324801  30.35849794 34.89443755 42.54057586 48.69265961\n",
            " 14.22198374 30.67271845 24.75268455 37.35898516 16.72515555 14.35375834\n",
            " 49.47288141 20.61477873 31.33857984 47.47973201 42.17312525 31.60186493\n",
            " 28.53747614 27.60506556 44.56982856 13.37926201 49.34668871 27.12248973\n",
            " 47.47881598 21.34335331 33.19753963 30.6986523  35.65104064 20.12595611\n",
            " 45.77105112 15.72767649 47.21873171 39.40837566 31.25893114 42.12529751\n",
            " 25.89280916 49.56363302 26.23262707 25.79374157 26.00435733 32.8460521\n",
            " 17.31125762 20.28821702 22.56039011 30.23013095 41.14933565 26.51514422\n",
            " 30.23948319 45.20458897 38.63479573 31.11579759 33.9611506  25.25342518\n",
            " 48.83412899 42.71762594 26.8303676  16.20796247 27.92691951 27.82383873\n",
            " 22.12154285 26.51319919 36.95008422 29.55329647 36.4978561  31.28668377\n",
            " 20.99195081 29.32196756 19.03992948 22.22468587 28.84066067 43.78073312\n",
            " 22.53028877 17.15993429 26.32345903 12.77369865 22.36067408 34.14596931\n",
            " 32.99657966 14.89515306 20.40691855 15.15361761 45.14743844 31.41050519\n",
            " 27.6761179  19.03813301 17.64480148 31.03244842 25.0689636  25.12435844\n",
            " 15.2029427  14.36688524 18.90447698 32.25118274 12.74120887 31.86856835\n",
            " 43.22567217 23.60132062 36.87528532 30.93741171 30.46424013 27.72492759\n",
            " 35.5763597  34.15027013 40.24738134 36.27494427 42.43042197 27.89027847\n",
            " 31.07431309 22.16880087 14.51981731 34.41015756 38.20399764 39.59527031\n",
            " 36.25804588 23.78160872 49.79728047 15.05994962 37.17331914 36.19330926\n",
            " 26.43627709 20.97835926 31.3816387  23.59161597 13.87837492 38.21478754\n",
            " 33.81271776 29.52846202 49.4025084  20.9639245  29.13295253 15.55263669\n",
            " 19.88431371 15.41152189 38.60838654 49.78416062 18.84549165 29.80124426\n",
            " 47.91721746 20.78625103 23.10701483 12.25283249 20.80797662 15.8611773\n",
            " 29.86368899 21.61669049 29.05888818 18.78430946 19.49986752 23.64442056\n",
            " 37.27602136 15.66539158 49.99999998 20.35158057 19.81426926 32.20977571\n",
            " 45.28529068 34.91068948 19.2354958  11.96260239 49.92963736 27.12528307]\n",
            "selection [330 399 570 849 120   7 831 188 516 401  57 784 568 765 114 513 376 519\n",
            " 501 480 675 711 436 298 654] (25,) [10.77558884 11.46863744 11.78508326 11.96260239 12.05388217 12.22784115\n",
            " 12.25283249 12.44723112 12.58208879 12.65474578 12.66796263 12.74120887\n",
            " 12.76426564 12.77369865 12.80461631 12.85959976 12.88356317 12.92310285\n",
            " 13.04627844 13.15782001 13.23356247 13.37926201 13.5448234  13.57821982\n",
            " 13.59066822]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (475, 10) (475,)\n",
            "updated train set: (475, 10) (475,) unique(labels): [230 245] [0 1]\n",
            "val set: (827, 10) (827,)\n",
            "\n",
            "Train set: (475, 10)\n",
            "Validation set: (827, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.10526315789473684, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "std (827,) [44.0065909  24.34056946 24.59745065 22.63262482 25.40189362 22.97336831\n",
            " 30.73114892 16.85777704 21.43347636 48.39217374 48.39331754 47.62452319\n",
            " 38.1422338  30.78760921 26.48904674 22.347839   44.94602121 19.12099657\n",
            " 45.80088555 27.42344993 18.21475249 42.93738438 29.93736246 43.66084037\n",
            " 28.00683384 18.79499948 34.86863781 35.185344   39.92151908 24.53844492\n",
            " 33.95155011 20.40364548 18.75241034 20.13492453 20.47540304 28.70394119\n",
            " 19.19013108 41.21811284 32.61233136 25.10159382 30.21476809 37.24311453\n",
            " 49.1454288  38.2242709  22.81748839 35.53983529 23.6269756  20.12631972\n",
            " 20.48429882 19.19207218 28.24788437 47.32085396 27.06553787 47.7725091\n",
            " 21.42708416 39.31848537 37.98823916 35.33549801 33.60868758 22.5757743\n",
            " 25.67716995 19.74252685 45.67814225 30.22229733 36.75559842 36.56035793\n",
            " 16.13740043 49.98708775 20.48248359 27.10698369 43.70784123 21.78581186\n",
            " 32.98957297 36.26875425 41.52892367 19.10524454 20.58088335 31.98677979\n",
            " 30.77103791 30.29844795 38.00749326 23.37493773 30.54469728 19.64608388\n",
            " 42.12158832 39.2632488  39.31721672 34.1058143  49.99127405 41.31076015\n",
            " 33.5022589  49.99841399 42.61861688 45.42623823 27.61826916 36.05598156\n",
            " 29.29940742 30.93979546 29.59724662 45.87973279 34.77449033 33.89898888\n",
            " 24.39889743 21.08687454 21.19453395 35.61168987 22.70596079 39.56686841\n",
            " 48.11752309 34.24638185 16.43300419 44.4438383  33.8221769  31.07748172\n",
            " 23.55669402 35.35988555 49.99042733 34.91580714 17.92658255 23.47255926\n",
            " 26.52347309 19.2990275  14.49262687 49.99989163 32.92373761 33.3657804\n",
            " 38.24042899 27.30318887 29.53174253 35.56646532 37.5198842  23.97807128\n",
            " 22.21849283 36.28824901 19.36693495 25.19526172 35.43679767 28.20759488\n",
            " 39.00010172 26.34414394 21.11715711 26.43233535 28.23754595 15.77118551\n",
            " 18.76247605 32.0319362  31.403096   29.88159333 27.66987525 18.1207428\n",
            " 49.37058472 23.74380256 27.38620845 26.52634004 28.05338169 42.51327575\n",
            " 17.02533501 32.92260777 49.97343801 29.21457667 36.06072276 22.25268738\n",
            " 27.44979829 32.53678188 17.85957277 26.39315168 36.01004186 44.08503974\n",
            " 48.16242363 35.23067579 17.88747544 18.17480587 35.74451947 49.45246166\n",
            " 42.73659002 18.73504802 37.76966559 22.51202298 24.4084093  40.40036911\n",
            " 22.20805635 34.72939383 41.22335266 16.83644719 47.26497272 17.20627604\n",
            " 36.12338394 40.13384673 44.22502715 22.49081344 31.17957924 39.94150629\n",
            " 19.28479504 18.53982466 34.08418789 15.16182731 21.82857907 43.15071991\n",
            " 31.17190671 24.30580805 41.35566949 31.37375093 36.76793108 19.5461563\n",
            " 28.42016213 27.72957458 37.67147312 26.69536113 26.53943549 30.71047523\n",
            " 33.33405378 37.49830115 34.15201256 42.09165912 32.70865337 16.88426085\n",
            " 37.0887941  49.72779737 20.63273277 38.03893901 17.62231583 36.95778285\n",
            " 25.19857982 47.3260644  37.00799703 34.67883578 15.59342334 21.90412882\n",
            " 45.58056424 45.4281345  38.21412524 32.44451445 49.94753209 20.3041759\n",
            " 44.0016343  36.56722108 37.5341926  18.36910221 35.27359827 28.98308142\n",
            " 25.60409955 49.99791003 46.2236941  30.24374599 46.07132168 32.44807579\n",
            " 40.81636845 45.95921811 49.00964479 29.61831627 21.43975638 32.70583276\n",
            " 22.39389477 34.08169037 25.45712587 26.43802956 25.89507539 15.53983188\n",
            " 47.29986533 49.94776468 26.5102261  21.39450147 29.69728506 20.08451002\n",
            " 20.47597901 26.18957925 26.94835555 42.34938171 27.8647312  30.25474438\n",
            " 29.97587383 38.32032123 36.1012862  30.67270509 30.22229733 42.1950083\n",
            " 18.99758923 35.74894464 27.62712872 24.85405493 32.44462401 31.76819474\n",
            " 28.09589489 28.56905727 38.4906043  33.46050339 37.66299844 35.67452097\n",
            " 30.1383643  25.43940335 33.98816891 24.6100823  23.70785603 34.03759761\n",
            " 37.10571589 28.62652629 27.20804844 22.1003555  17.21009009 33.69195809\n",
            " 21.60409757 24.04575915 44.80391494 27.82592241 25.92963725 23.28184212\n",
            " 33.00627112 29.85326222 33.83549136 21.57453781 35.09466578 39.05149345\n",
            " 32.51868149 17.80524875 42.82780175 27.16405696 26.83883096 33.45386499\n",
            " 22.41467644 49.98500799 28.90781156 24.02398535 30.93252466 19.37935427\n",
            " 33.30527438 49.47203242 34.5160925  46.63087701 34.82361021 38.86742049\n",
            " 23.49191378 25.22115012 37.26016603 30.68219033 32.09504058 38.60933372\n",
            " 36.92843761 29.51342482 38.28090992 25.5968993  19.35037248 16.17162135\n",
            " 31.59134442 30.11529517 40.38843242 21.89942038 21.16746364 21.12500964\n",
            " 49.48809485 39.99083815 31.28545346 35.11858673 32.66874343 27.65892929\n",
            " 33.90871619 29.03930601 24.59304874 44.63180922 32.80926547 26.61749794\n",
            " 28.38409683 19.42797813 43.41611297 37.20226343 25.12358995 29.58363042\n",
            " 17.53453337 27.31902352 44.05276698 29.45195323 17.67679129 49.37124707\n",
            " 41.65862811 40.50165148 44.78477572 45.57682844 18.27416474 25.07260577\n",
            " 44.93591112 22.41063036 36.61408137 20.89206819 37.63864931 32.93751511\n",
            " 29.71084505 28.85918291 34.69503271 39.41281058 33.08172594 40.45744606\n",
            " 49.55900859 23.59905113 36.50729875 29.71939561 39.913972   25.87144662\n",
            " 47.19066352 49.99948639 32.32086529 26.40888226 17.69274686 27.49530812\n",
            " 16.90753454 31.7440303  21.67723261 35.94762606 23.13884584 49.73360822\n",
            " 38.97981073 46.60523371 36.92208548 23.05758576 18.22106196 21.14964883\n",
            " 22.69540743 21.05182367 22.20263413 17.12570088 44.83278368 40.67550224\n",
            " 27.58635171 40.60571001 32.3022037  34.95455899 19.82224723 31.66711293\n",
            " 24.3858905  22.86851313 20.73006519 22.28996512 30.05022721 21.05645886\n",
            " 35.39919118 40.69524681 35.90365727 25.88856392 46.78139526 40.27658975\n",
            " 40.68053308 39.68487303 17.69115609 37.11345157 31.06409402 18.74586961\n",
            " 23.13037219 44.27043174 45.90158732 35.91516108 29.09770377 19.78222754\n",
            " 44.64196838 32.04338497 22.57702859 26.87858085 37.45512511 38.79759783\n",
            " 41.35107745 28.76228985 44.33511344 34.65755087 17.21009009 24.63638225\n",
            " 18.62319259 35.4149336  32.7494106  42.54942927 49.65907847 32.10849668\n",
            " 21.05645886 16.68631406 40.64912673 33.96982936 31.73896213 45.20711955\n",
            " 49.99919536 27.49487021 17.19893237 21.54369578 35.55024152 45.89204401\n",
            " 41.99842553 18.79487083 18.85780916 42.95469527 18.99127439 45.44849701\n",
            " 22.85011358 27.87441483 22.72573297 42.62681302 24.08421746 20.04631776\n",
            " 41.1920069  43.02630759 27.64729936 35.42436424 28.53870406 39.024955\n",
            " 29.16668475 16.68939494 20.49578985 32.87590313 22.31596937 19.35832444\n",
            " 32.64720642 22.28744024 47.49321662 36.87956583 32.78879009 27.34410956\n",
            " 36.90650937 49.9996899  41.04297447 16.08347857 38.09656366 27.53496595\n",
            " 26.70580782 44.40811726 26.01574389 29.25917453 33.34511522 23.54617854\n",
            " 41.96206629 25.04850727 15.92375828 26.39197888 48.07148493 22.53685198\n",
            " 46.39076825 28.82564181 21.06393154 38.21497444 26.8578318  21.15225265\n",
            " 22.52940747 33.34661196 32.26238857 36.16251544 34.78206991 17.6700972\n",
            " 36.06101287 20.4558081  38.66374028 20.06867634 43.83791044 27.24612538\n",
            " 39.74926557 17.99331017 19.37357093 39.22505111 23.79864396 48.02872761\n",
            " 30.0427268  49.09298583 40.82006062 21.41006156 37.57868904 26.94835555\n",
            " 49.97030813 28.56812823 28.39011848 34.2067002  47.03412418 36.97241145\n",
            " 44.0292901  29.44376904 47.32294814 27.03179918 29.01477731 43.27395704\n",
            " 49.48189967 34.73990027 26.78273121 23.21017474 19.04037505 38.34682879\n",
            " 20.57739415 36.94556787 44.67392004 36.59643384 42.29089004 46.65274556\n",
            " 48.73507042 17.4455513  31.30155041 32.3312794  23.63207525 21.19646052\n",
            " 19.55760555 20.36593612 45.96381266 25.0824467  21.85436305 38.59663061\n",
            " 27.38063595 23.2904785  21.47181207 30.11409818 32.37429588 32.30474395\n",
            " 29.99941063 41.7989259  36.04890377 37.76503853 21.74060902 45.87400628\n",
            " 31.33370961 19.74470252 49.99817871 49.4089105  39.16037284 35.76970092\n",
            " 49.14790795 37.10797932 27.39059484 27.99538338 25.05117125 40.16869099\n",
            " 34.56654724 36.84272543 31.5435408  15.89558258 26.94325024 30.22615981\n",
            " 30.28135958 31.19692011 17.94933224 39.17735817 22.01398914 37.15914188\n",
            " 24.0835291  24.45268953 40.79095811 22.60813958 35.06717313 49.99160325\n",
            " 31.43912428 30.39624997 46.79062664 38.02911957 29.61499082 44.59361457\n",
            " 34.06686226 46.86527497 25.80035361 40.73679273 44.98329174 15.61040005\n",
            " 21.70167025 42.8617506  32.46271242 41.08905377 31.51981006 23.86555367\n",
            " 49.76446879 24.49321002 18.59508483 34.66985237 31.33533775 25.40986292\n",
            " 31.48648287 23.88635914 42.43148449 31.66711293 44.31891983 45.9646556\n",
            " 40.19121002 33.65285538 22.81438091 37.33849751 41.61618901 26.40872173\n",
            " 35.10792626 38.11700302 45.27915027 49.39011157 15.63737871 33.82124369\n",
            " 30.37084899 40.52951464 17.94353993 16.27446232 49.73459163 23.22977525\n",
            " 33.60016085 48.64690356 45.12658004 33.08922002 30.74142717 30.66690779\n",
            " 46.42859325 49.63401204 32.21079581 48.70660306 23.721313   35.07796819\n",
            " 34.71273863 39.94150629 22.93339474 47.48808323 18.32818736 48.37423693\n",
            " 42.19267175 34.08228276 44.20319455 27.78008632 49.81505852 30.12396212\n",
            " 28.42117598 28.52301075 35.45816988 20.61507811 22.29309627 24.36493304\n",
            " 33.22022216 43.36059768 29.43538504 32.48675564 46.74316071 41.05923083\n",
            " 34.77273701 36.90125571 25.10686558 49.47873552 45.00426096 29.73909592\n",
            " 19.16890766 30.41253021 30.95372785 24.03070988 28.29499592 40.20018917\n",
            " 32.70432476 39.47081564 33.28104743 21.82857907 32.04338497 21.62476562\n",
            " 25.01964371 32.89745232 45.78456061 24.70336253 19.10524454 29.71047992\n",
            " 25.20210387 37.13500106 36.47194929 16.45968163 22.92271804 16.89451712\n",
            " 47.60323717 34.38939194 31.56275686 21.89224685 18.28214708 33.13664893\n",
            " 27.61825118 27.89414631 17.47167603 16.16735948 21.55853117 34.78029725\n",
            " 34.9430223  45.39067635 26.3041877  40.63123529 34.32066141 34.47272579\n",
            " 31.3627087  38.78090768 36.73388782 43.05404617 39.94929941 44.90020527\n",
            " 32.42174466 34.32536059 26.39482023 14.3508892  37.44145428 41.20873929\n",
            " 42.91599463 40.32734261 25.64340815 49.94015608 16.81318792 40.90271188\n",
            " 39.38761754 28.70022201 23.02394922 34.865221   25.55703214 16.07383011\n",
            " 41.6368434  38.28347965 33.47390282 49.67831259 22.21109812 32.7444636\n",
            " 19.09098953 22.96172382 17.21785312 41.18996835 49.9287421  21.56868475\n",
            " 32.28939771 48.84180454 23.8419785  26.58931639 24.42328703 19.36450942\n",
            " 33.45959478 25.40506014 32.5182667  21.64372255 21.6884573  26.69589313\n",
            " 39.93465166 17.03235613 50.         24.20875021 21.52599862 34.74389963\n",
            " 46.90213946 37.76454294 20.5597085  49.97959001 29.99177808]\n",
            "selection [777 122 195 257 226 647 676 143 621 524 791 513  66 759 341 681 110 747\n",
            " 469 499 784 183   7 215 749] (25,) [14.3508892  14.49262687 15.16182731 15.53983188 15.59342334 15.61040005\n",
            " 15.63737871 15.77118551 15.89558258 15.92375828 16.07383011 16.08347857\n",
            " 16.13740043 16.16735948 16.17162135 16.27446232 16.43300419 16.45968163\n",
            " 16.68631406 16.68939494 16.81318792 16.83644719 16.85777704 16.88426085\n",
            " 16.89451712]\n",
            "trainset before adding uncertain samples (475, 10) (475,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [238 262] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "final active learning accuracies [72.11981566820278, 69.81566820276498, 69.35483870967742, 70.73732718894009, 72.58064516129032, 74.88479262672811, 71.42857142857143, 71.88940092165899, 71.88940092165899, 76.95852534562212, 77.88018433179722, 77.88018433179722, 78.3410138248848, 78.57142857142857, 78.3410138248848, 79.26267281105991, 80.18433179723502, 79.95391705069125, 79.95391705069125, 79.72350230414746]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-19.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 20, using model = LogModel, selection_function = MinStdSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial labeled samples size 10\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 10) (10,) unique(labels): [4 6] [0 1]\n",
            "Val set: (1292, 10) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 10)\n",
            "Validation set: (1292, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 69.124424 \n",
            "Classification report for LogisticRegression(C=5.0, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.75      0.78       321\n",
            "           1       0.42      0.52      0.47       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.62      0.64      0.63       434\n",
            "weighted avg       0.71      0.69      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[241  80]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1292,) [47.33770631 49.64586981 32.39839535 ... 49.98544331 45.92930728\n",
            " 21.65761338]\n",
            "selection [1016  329 1282 1185  539  576  631  160  223  824] (10,) [0.04300013 0.09379498 0.14544416 0.16022471 0.25196735 0.25196735\n",
            " 0.25199853 0.35594517 0.53332832 0.60025151]\n",
            "trainset before adding uncertain samples (10, 10) (10,)\n",
            "trainset after adding uncertain samples (20, 10) (20,)\n",
            "updated train set: (20, 10) (20,) unique(labels): [ 8 12] [0 1]\n",
            "val set: (1282, 10) (1282,)\n",
            "\n",
            "Train set: (20, 10)\n",
            "Validation set: (1282, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 69.585253 \n",
            "Classification report for LogisticRegression(C=2.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.73      0.78       321\n",
            "           1       0.44      0.59      0.50       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.64      0.66      0.64       434\n",
            "weighted avg       0.73      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[235  86]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1282,) [39.27511891 49.82745509 37.59175924 ... 49.86235263 43.21466359\n",
            " 19.91266021]\n",
            "selection [ 684  784  818  226  475   30 1111  711  763  936] (10,) [0.12176077 0.1267104  0.16707023 0.26667087 0.29592531 0.40390499\n",
            " 0.48478782 0.559806   0.56010209 1.01016363]\n",
            "trainset before adding uncertain samples (20, 10) (20,)\n",
            "trainset after adding uncertain samples (30, 10) (30,)\n",
            "updated train set: (30, 10) (30,) unique(labels): [13 17] [0 1]\n",
            "val set: (1272, 10) (1272,)\n",
            "\n",
            "Train set: (30, 10)\n",
            "Validation set: (1272, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 72.350230 \n",
            "Classification report for LogisticRegression(C=1.6666666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.80       321\n",
            "           1       0.48      0.60      0.53       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.68      0.67       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[246  75]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1272,) [40.51593554 49.3181691  32.97634819 ... 49.88983736 43.09348833\n",
            "  7.88038383]\n",
            "selection [  87  511 1093  562  370  992  221  573  413  523] (10,) [0.23088496 0.31803997 0.43059871 0.44582453 0.53104268 0.66039648\n",
            " 0.66537505 0.99865979 1.00021053 1.01261003]\n",
            "trainset before adding uncertain samples (30, 10) (30,)\n",
            "trainset after adding uncertain samples (40, 10) (40,)\n",
            "updated train set: (40, 10) (40,) unique(labels): [14 26] [0 1]\n",
            "val set: (1262, 10) (1262,)\n",
            "\n",
            "Train set: (40, 10)\n",
            "Validation set: (1262, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 70.276498 \n",
            "Classification report for LogisticRegression(C=1.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.75      0.79       321\n",
            "           1       0.44      0.57      0.50       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.64      0.66      0.64       434\n",
            "weighted avg       0.73      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[241  80]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1262,) [48.01914884 49.08910294 36.35537059 ... 49.99703557 41.6241993\n",
            " 12.31340756]\n",
            "selection [ 459  435  742  732  337  959 1031  520  917 1080] (10,) [0.19447277 0.43309303 0.48591693 0.61093278 0.75349803 0.80996748\n",
            " 0.96952368 1.00928706 1.09365407 1.10138318]\n",
            "trainset before adding uncertain samples (40, 10) (40,)\n",
            "trainset after adding uncertain samples (50, 10) (50,)\n",
            "updated train set: (50, 10) (50,) unique(labels): [16 34] [0 1]\n",
            "val set: (1252, 10) (1252,)\n",
            "\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 71.889401 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.76      0.80       321\n",
            "           1       0.47      0.60      0.53       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.68      0.66       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[244  77]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1252,) [41.35930989 49.32357971 39.12794076 ... 49.9210375  47.6918957\n",
            " 19.64557848]\n",
            "selection [212 133 382 595 870 211 913 750 162 174] (10,) [0.04740991 0.05630501 0.16609491 0.45138759 0.48448251 0.63779682\n",
            " 0.70293575 0.75342498 0.83151822 0.83780132]\n",
            "trainset before adding uncertain samples (50, 10) (50,)\n",
            "trainset after adding uncertain samples (60, 10) (60,)\n",
            "updated train set: (60, 10) (60,) unique(labels): [23 37] [0 1]\n",
            "val set: (1242, 10) (1242,)\n",
            "\n",
            "Train set: (60, 10)\n",
            "Validation set: (1242, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.022 s \n",
            "\n",
            "Accuracy rate is 73.271889 \n",
            "Classification report for LogisticRegression(C=0.8333333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.78      0.81       321\n",
            "           1       0.49      0.60      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.68       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[250  71]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1242,) [29.861678   47.78804562 35.26800634 ... 43.01326678 47.19678392\n",
            " 33.29879362]\n",
            "selection [ 846  692  727 1107  435  611  498  326 1234  211] (10,) [0.02895154 0.05925365 0.32385956 0.32945433 0.33579788 0.36390185\n",
            " 0.36846434 0.37809777 0.38648111 0.40067767]\n",
            "trainset before adding uncertain samples (60, 10) (60,)\n",
            "trainset after adding uncertain samples (70, 10) (70,)\n",
            "updated train set: (70, 10) (70,) unique(labels): [26 44] [0 1]\n",
            "val set: (1232, 10) (1232,)\n",
            "\n",
            "Train set: (70, 10)\n",
            "Validation set: (1232, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.811060 \n",
            "Classification report for LogisticRegression(C=0.7142857142857143, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.79      0.81       321\n",
            "           1       0.48      0.57      0.52       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.68      0.67       434\n",
            "weighted avg       0.74      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[252  69]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1232,) [40.75346908 46.73465909 35.29578118 ... 49.7165784  46.3180119\n",
            " 28.84207995]\n",
            "selection [ 391 1003  995 1045   40  488  441  122  621  845] (10,) [0.02070747 0.08292835 0.1490359  0.24628451 0.34440456 0.50167318\n",
            " 0.76929607 1.30555856 1.35268765 1.40452186]\n",
            "trainset before adding uncertain samples (70, 10) (70,)\n",
            "trainset after adding uncertain samples (80, 10) (80,)\n",
            "updated train set: (80, 10) (80,) unique(labels): [30 50] [0 1]\n",
            "val set: (1222, 10) (1222,)\n",
            "\n",
            "Train set: (80, 10)\n",
            "Validation set: (1222, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 72.350230 \n",
            "Classification report for LogisticRegression(C=0.625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81       321\n",
            "           1       0.47      0.53      0.50       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.65      0.66      0.65       434\n",
            "weighted avg       0.73      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[254  67]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1222,) [43.40479504 39.54544894 33.45486561 ... 49.94943073 43.74086967\n",
            " 23.69828531]\n",
            "selection [ 171  760  258   30  854 1033 1152  679  972  792] (10,) [0.04325625 0.13893873 0.14429449 0.14465984 0.20489685 0.24282347\n",
            " 0.26246163 0.48761924 0.80259555 0.83431156]\n",
            "trainset before adding uncertain samples (80, 10) (80,)\n",
            "trainset after adding uncertain samples (90, 10) (90,)\n",
            "updated train set: (90, 10) (90,) unique(labels): [36 54] [0 1]\n",
            "val set: (1212, 10) (1212,)\n",
            "\n",
            "Train set: (90, 10)\n",
            "Validation set: (1212, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.5555555555555556, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1212,) [44.2861851  30.61949347 31.20838899 ... 49.97452852 41.4373647\n",
            " 21.58233365]\n",
            "selection [1182  892  607  852   44  323  713  427  260  368] (10,) [0.0379865  0.03887365 0.08962597 0.10975377 0.12799996 0.13662994\n",
            " 0.26993801 0.274768   0.55393121 0.57453517]\n",
            "trainset before adding uncertain samples (90, 10) (90,)\n",
            "trainset after adding uncertain samples (100, 10) (100,)\n",
            "updated train set: (100, 10) (100,) unique(labels): [38 62] [0 1]\n",
            "val set: (1202, 10) (1202,)\n",
            "\n",
            "Train set: (100, 10)\n",
            "Validation set: (1202, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.5, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1202,) [45.49771963 33.3477936  30.18703528 ... 49.98799495 40.23520087\n",
            " 22.02667917]\n",
            "selection [ 839  705  264 1157  162 1153  675  892  562  323] (10,) [0.17938292 0.461169   0.46307365 0.53369591 0.55508104 0.89867507\n",
            " 0.92729914 1.21923087 1.29302723 1.35533173]\n",
            "trainset before adding uncertain samples (100, 10) (100,)\n",
            "trainset after adding uncertain samples (110, 10) (110,)\n",
            "updated train set: (110, 10) (110,) unique(labels): [44 66] [0 1]\n",
            "val set: (1192, 10) (1192,)\n",
            "\n",
            "Train set: (110, 10)\n",
            "Validation set: (1192, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.45454545454545453, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.60      0.46      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1192,) [45.18997049 32.72448737 27.21379311 ... 49.99642468 40.05526573\n",
            " 15.24546056]\n",
            "selection [ 189  987  130 1066 1067 1172   40  983  634  312] (10,) [0.02415421 0.08999817 0.11495545 0.28852154 0.29880884 0.42462373\n",
            " 0.4923371  0.52840106 0.55852506 0.82315489]\n",
            "trainset before adding uncertain samples (110, 10) (110,)\n",
            "trainset after adding uncertain samples (120, 10) (120,)\n",
            "updated train set: (120, 10) (120,) unique(labels): [47 73] [0 1]\n",
            "val set: (1182, 10) (1182,)\n",
            "\n",
            "Train set: (120, 10)\n",
            "Validation set: (1182, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.4166666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1182,) [45.48431382 32.62602271 28.63310931 ... 49.99112665 41.4187042\n",
            " 16.96067555]\n",
            "selection [1084  821   81 1144  743  157  775  307 1150  787] (10,) [0.23247712 0.24839782 0.25973773 0.31390126 0.38169523 0.4015592\n",
            " 0.44046207 0.54080142 0.62255764 0.62534683]\n",
            "trainset before adding uncertain samples (120, 10) (120,)\n",
            "trainset after adding uncertain samples (130, 10) (130,)\n",
            "updated train set: (130, 10) (130,) unique(labels): [52 78] [0 1]\n",
            "val set: (1172, 10) (1172,)\n",
            "\n",
            "Train set: (130, 10)\n",
            "Validation set: (1172, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.110599 \n",
            "Classification report for LogisticRegression(C=0.38461538461538464, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1172,) [46.33874383 29.75464171 26.3893357  ... 49.99253307 38.00161998\n",
            " 16.59884266]\n",
            "selection [ 161  252  434  241  203  968 1001  206  121  799] (10,) [0.01658955 0.07602208 0.08161004 0.30666421 0.35763937 0.60115847\n",
            " 0.76998617 0.77717961 0.85203292 0.88581343]\n",
            "trainset before adding uncertain samples (130, 10) (130,)\n",
            "trainset after adding uncertain samples (140, 10) (140,)\n",
            "updated train set: (140, 10) (140,) unique(labels): [57 83] [0 1]\n",
            "val set: (1162, 10) (1162,)\n",
            "\n",
            "Train set: (140, 10)\n",
            "Validation set: (1162, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.35714285714285715, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1162,) [45.70584036 27.4684274  24.82533032 ... 49.99863151 38.50507384\n",
            " 14.40145732]\n",
            "selection [ 561  261  537 1055  650   13  687  565 1048  724] (10,) [0.02095545 0.15712263 0.15714709 0.22487928 0.45580236 0.50710198\n",
            " 0.53107091 0.58719022 0.64314791 0.65829548]\n",
            "trainset before adding uncertain samples (140, 10) (140,)\n",
            "trainset after adding uncertain samples (150, 10) (150,)\n",
            "updated train set: (150, 10) (150,) unique(labels): [59 91] [0 1]\n",
            "val set: (1152, 10) (1152,)\n",
            "\n",
            "Train set: (150, 10)\n",
            "Validation set: (1152, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.3333333333333333, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1152,) [47.02017701 23.97073903 24.07400075 ... 49.99787126 35.75984927\n",
            " 17.1872121 ]\n",
            "selection [ 418  164 1000  466  715  504  356 1115  970  244] (10,) [0.23740779 0.3623306  0.39767496 0.50723937 0.59873994 0.63736673\n",
            " 0.65796461 0.69532199 0.85716846 1.08228596]\n",
            "trainset before adding uncertain samples (150, 10) (150,)\n",
            "trainset after adding uncertain samples (160, 10) (160,)\n",
            "updated train set: (160, 10) (160,) unique(labels): [67 93] [0 1]\n",
            "val set: (1142, 10) (1142,)\n",
            "\n",
            "Train set: (160, 10)\n",
            "Validation set: (1142, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.3125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1142,) [45.66907144 26.7762182  24.030225   ... 49.99351744 37.5390605\n",
            " 18.31689373]\n",
            "selection [ 135  565  759  449 1001  525  383  773  865  946] (10,) [0.09391487 0.35631255 0.56398044 0.61986538 1.3808505  1.45227448\n",
            " 1.5350081  1.82795592 1.87499721 1.98718301]\n",
            "trainset before adding uncertain samples (160, 10) (160,)\n",
            "trainset after adding uncertain samples (170, 10) (170,)\n",
            "updated train set: (170, 10) (170,) unique(labels): [72 98] [0 1]\n",
            "val set: (1132, 10) (1132,)\n",
            "\n",
            "Train set: (170, 10)\n",
            "Validation set: (1132, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 78.801843 \n",
            "Classification report for LogisticRegression(C=0.29411764705882354, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.63      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1132,) [42.89585293 24.12058658 20.28692341 ... 49.99032547 35.25795406\n",
            " 14.97424901]\n",
            "selection [1037  762  485  237 1036  681  772   18  796  671] (10,) [0.01157996 0.15327387 0.18922215 0.2806829  0.35738925 0.38930855\n",
            " 0.79735314 0.90760631 0.90760631 0.95889361]\n",
            "trainset before adding uncertain samples (170, 10) (170,)\n",
            "trainset after adding uncertain samples (180, 10) (180,)\n",
            "updated train set: (180, 10) (180,) unique(labels): [ 76 104] [0 1]\n",
            "val set: (1122, 10) (1122,)\n",
            "\n",
            "Train set: (180, 10)\n",
            "Validation set: (1122, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.2777777777777778, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.62      0.45      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1122,) [45.12440375 20.21359363 19.50060966 ... 49.99625949 33.53839788\n",
            " 14.19469442]\n",
            "selection [ 823  929 1102  329  822  793  412   97  700 1068] (10,) [0.04120014 0.54652014 0.93235423 0.95746063 1.15449123 1.33476161\n",
            " 1.66567703 1.72063502 1.9196911  2.07302683]\n",
            "trainset before adding uncertain samples (180, 10) (180,)\n",
            "trainset after adding uncertain samples (190, 10) (190,)\n",
            "updated train set: (190, 10) (190,) unique(labels): [ 79 111] [0 1]\n",
            "val set: (1112, 10) (1112,)\n",
            "\n",
            "Train set: (190, 10)\n",
            "Validation set: (1112, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.2631578947368421, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1112,) [45.06523947 16.8082084  18.01094875 ... 49.99379436 30.13284066\n",
            " 13.71282745]\n",
            "selection [266 585 462  47 373 825 470 850 537 369] (10,) [0.09730115 1.06355181 1.1532093  1.4251522  1.74533522 1.89444358\n",
            " 1.90533393 1.95191454 2.09274743 2.2695897 ]\n",
            "trainset before adding uncertain samples (190, 10) (190,)\n",
            "trainset after adding uncertain samples (200, 10) (200,)\n",
            "updated train set: (200, 10) (200,) unique(labels): [ 82 118] [0 1]\n",
            "val set: (1102, 10) (1102,)\n",
            "\n",
            "Train set: (200, 10)\n",
            "Validation set: (1102, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.25, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1102,) [44.84064895 17.86064491 17.1872886  ... 49.98993868 28.77409156\n",
            " 13.65369542]\n",
            "selection [ 332  907   53  584 1072   91  163  161  678    5] (10,) [0.19611337 0.71663169 1.24969627 1.54003891 1.79281706 2.07106542\n",
            " 2.10000528 2.41694956 2.43106365 2.77213251]\n",
            "trainset before adding uncertain samples (200, 10) (200,)\n",
            "trainset after adding uncertain samples (210, 10) (210,)\n",
            "updated train set: (210, 10) (210,) unique(labels): [ 86 124] [0 1]\n",
            "val set: (1092, 10) (1092,)\n",
            "\n",
            "Train set: (210, 10)\n",
            "Validation set: (1092, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.23809523809523808, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.44      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1092,) [43.86001351 19.34118057 16.76038983 ... 49.97135286 27.99550471\n",
            " 13.53236593]\n",
            "selection [ 251 1078  597  617  702  572  239  362    9  147] (10,) [0.93724318 1.49633699 1.69478287 2.02169654 2.26338377 2.40932911\n",
            " 2.74167374 2.8432369  3.00181323 3.03056034]\n",
            "trainset before adding uncertain samples (210, 10) (210,)\n",
            "trainset after adding uncertain samples (220, 10) (220,)\n",
            "updated train set: (220, 10) (220,) unique(labels): [ 87 133] [0 1]\n",
            "val set: (1082, 10) (1082,)\n",
            "\n",
            "Train set: (220, 10)\n",
            "Validation set: (1082, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.22727272727272727, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1082,) [45.60835349 19.34766017 18.56565429 ... 49.99247976 30.11330785\n",
            " 15.41249244]\n",
            "selection [1019  776  655  172  535  650  951  723  804  481] (10,) [1.96941878 2.92946909 3.44083475 3.54576075 3.82935769 4.20496863\n",
            " 4.25470061 4.2668998  4.45193435 4.53187067]\n",
            "trainset before adding uncertain samples (220, 10) (220,)\n",
            "trainset after adding uncertain samples (230, 10) (230,)\n",
            "updated train set: (230, 10) (230,) unique(labels): [ 91 139] [0 1]\n",
            "val set: (1072, 10) (1072,)\n",
            "\n",
            "Train set: (230, 10)\n",
            "Validation set: (1072, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.21739130434782608, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.44      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1072,) [40.11946309 18.78994539 14.83211676 ... 49.97901945 29.38540979\n",
            " 12.64958901]\n",
            "selection [847 993 439 731 816 779 256 852  67 388] (10,) [0.15033603 1.20884062 1.22869827 1.88635641 2.33990964 2.36616696\n",
            " 2.64624159 2.74982439 2.98763248 3.33562605]\n",
            "trainset before adding uncertain samples (230, 10) (230,)\n",
            "trainset after adding uncertain samples (240, 10) (240,)\n",
            "updated train set: (240, 10) (240,) unique(labels): [ 93 147] [0 1]\n",
            "val set: (1062, 10) (1062,)\n",
            "\n",
            "Train set: (240, 10)\n",
            "Validation set: (1062, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.20833333333333334, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1062,) [41.44871315 18.36880132 16.46298385 ... 49.96617005 28.63617999\n",
            " 12.2917403 ]\n",
            "selection [ 884 1006  405  871   21  937  637  381  642  203] (10,) [2.52690262 3.6609808  3.75990225 4.08883256 4.11639399 4.11639399\n",
            " 4.49163824 4.8028311  4.81969451 5.10667589]\n",
            "trainset before adding uncertain samples (240, 10) (240,)\n",
            "trainset after adding uncertain samples (250, 10) (250,)\n",
            "updated train set: (250, 10) (250,) unique(labels): [ 94 156] [0 1]\n",
            "val set: (1052, 10) (1052,)\n",
            "\n",
            "Train set: (250, 10)\n",
            "Validation set: (1052, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.2, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1052,) [40.29580527 12.5940142  13.35208745 ... 49.93955714 22.64316918\n",
            " 11.02654447]\n",
            "selection [ 609  785  362 1011  965  274  196   80  750  882] (10,) [1.00493212 2.1231695  2.47529203 3.00707312 3.11535197 3.27110443\n",
            " 3.62764184 3.80410766 4.08169786 4.08942423]\n",
            "trainset before adding uncertain samples (250, 10) (250,)\n",
            "trainset after adding uncertain samples (260, 10) (260,)\n",
            "updated train set: (260, 10) (260,) unique(labels): [ 99 161] [0 1]\n",
            "val set: (1042, 10) (1042,)\n",
            "\n",
            "Train set: (260, 10)\n",
            "Validation set: (1042, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.19230769230769232, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.63      0.43      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.67      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1042,) [42.26468763 11.75048698 14.23655401 ... 49.93028075 21.94288039\n",
            " 11.47351172]\n",
            "selection [302 723 999 750 126 428  51 125 731 807] (10,) [1.05083958 2.2609286  2.45904287 3.06349517 3.29138938 3.48928515\n",
            " 3.62485701 3.8970679  4.00392803 4.02316935]\n",
            "trainset before adding uncertain samples (260, 10) (260,)\n",
            "trainset after adding uncertain samples (270, 10) (270,)\n",
            "updated train set: (270, 10) (270,) unique(labels): [104 166] [0 1]\n",
            "val set: (1032, 10) (1032,)\n",
            "\n",
            "Train set: (270, 10)\n",
            "Validation set: (1032, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 78.571429 \n",
            "Classification report for LogisticRegression(C=0.18518518518518517, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.63      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.67      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1032,) [41.4469894  10.03150505 12.51636712 ... 49.91340012 20.35053483\n",
            " 10.38947113]\n",
            "selection [328 932 877 911 987 360 645 851 924 397] (10,) [2.57084792 3.2145961  3.26334223 3.30323537 3.44399113 3.52552656\n",
            " 3.53756728 3.67150843 3.81227533 3.8228694 ]\n",
            "trainset before adding uncertain samples (270, 10) (270,)\n",
            "trainset after adding uncertain samples (280, 10) (280,)\n",
            "updated train set: (280, 10) (280,) unique(labels): [107 173] [0 1]\n",
            "val set: (1022, 10) (1022,)\n",
            "\n",
            "Train set: (280, 10)\n",
            "Validation set: (1022, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.17857142857142858, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.45      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1022,) [41.44011591 16.20545037 15.55209394 ... 49.92338924 25.69916053\n",
            " 13.929308  ]\n",
            "selection [ 518  494 1008  519  384  468  844  987  685  200] (10,) [3.86516476 4.42569012 4.58191352 4.8664161  4.98771709 5.04819262\n",
            " 5.22455401 5.24487592 5.33323702 5.7708716 ]\n",
            "trainset before adding uncertain samples (280, 10) (280,)\n",
            "trainset after adding uncertain samples (290, 10) (290,)\n",
            "updated train set: (290, 10) (290,) unique(labels): [108 182] [0 1]\n",
            "val set: (1012, 10) (1012,)\n",
            "\n",
            "Train set: (290, 10)\n",
            "Validation set: (1012, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1724137931034483, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1012,) [41.21241598 15.56820979 15.40643754 ... 49.89027233 24.49331304\n",
            " 14.18193195]\n",
            "selection [677 501 349 215 928 287 885 328 829 786] (10,) [3.10903395 3.57035776 3.74941478 4.63749279 4.68848253 4.6931428\n",
            " 4.75419479 4.87070641 4.91398556 5.06152093]\n",
            "trainset before adding uncertain samples (290, 10) (290,)\n",
            "trainset after adding uncertain samples (300, 10) (300,)\n",
            "updated train set: (300, 10) (300,) unique(labels): [110 190] [0 1]\n",
            "val set: (1002, 10) (1002,)\n",
            "\n",
            "Train set: (300, 10)\n",
            "Validation set: (1002, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.16666666666666666, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.67      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "std (1002,) [42.09142448 13.58731753 15.25558707 ... 49.87214858 23.03714904\n",
            " 13.48522595]\n",
            "selection [186 189   5 444 609  93  28 731 521 236] (10,) [3.98943539 4.57216974 4.83773243 4.92991584 5.09675145 5.22472566\n",
            " 5.37042886 5.42233891 5.47314418 5.51676371]\n",
            "trainset before adding uncertain samples (300, 10) (300,)\n",
            "trainset after adding uncertain samples (310, 10) (310,)\n",
            "updated train set: (310, 10) (310,) unique(labels): [113 197] [0 1]\n",
            "val set: (992, 10) (992,)\n",
            "\n",
            "Train set: (310, 10)\n",
            "Validation set: (992, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.16129032258064516, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (992,) [33.30553558 14.20890325 12.54926288 21.47645028 49.12675983 17.96781754\n",
            " 12.1166443  21.21419058 11.54715884 14.57380187 42.89966769  9.63655167\n",
            "  5.95160186 41.46082842 41.88776309 29.06516687 18.48053143 17.21658785\n",
            " 11.05727408  7.85448277 13.67840716 42.11384597 11.96561193 10.94139896\n",
            " 39.42170475 18.27388338  7.50439568 33.4953706  15.23025547  7.85448277\n",
            " 33.73227409 17.63786925 18.4843812  31.05445224 29.62415917 15.68189087\n",
            " 38.37299397 16.17122652 12.90602356  8.48828108  7.87222386 13.62142742\n",
            " 12.03048993 14.28537485 20.8418012  16.04936945 27.00649927 10.86847579\n",
            " 10.41104642 17.22327247 23.85360996  8.8008563  27.72746495 33.45938695\n",
            " 13.19022764 46.65992504 29.9682408  11.71746498 23.07392862 13.07674479\n",
            "  5.82536257 14.36846255 18.08644362 15.00801897  8.84918079 11.541407\n",
            " 42.57723259 20.47715411 41.23572488  5.11194237  9.07899088 30.45107616\n",
            "  8.88899181  6.41441392 27.40739574 26.51868933 23.78578043 13.48518415\n",
            " 20.30802262 15.8742181  40.26732065 17.940438   26.6254806  30.02585804\n",
            " 17.09251922 12.65272876 21.35130363 49.61224009 16.66935868 12.85447968\n",
            " 32.32981112 13.62277535 24.20142525 19.27915286 15.28956843 10.64520302\n",
            " 31.34710803 10.32477375 12.5753094  28.19697066 14.81063797 17.62216054\n",
            "  7.85448277 28.51446881 13.14771129 12.72697065 10.35183725 31.68989516\n",
            " 26.29975821 12.04485912 27.12701202 25.01443453 49.78273191 32.46034354\n",
            " 12.457408   24.11630643 49.91779509 29.2469821  37.93331172 27.34073666\n",
            " 17.62227486 23.78962016 19.58385982  6.14209383 22.12379068 12.13545463\n",
            " 19.50310684 40.8952434  26.68765991 10.77104159 24.43694398 19.6972994\n",
            " 13.93121525 27.15343943 14.66915338  7.85448277 26.65692889 43.27101462\n",
            " 24.50664029  7.94963969 39.09493651 10.5243175  19.61067621 13.04425652\n",
            " 30.54145064 49.9485408   4.42377658 29.76644269 10.22929954 14.63734444\n",
            " 18.58009162 15.44311259  4.61626909 49.88991254 16.42888277 23.72192234\n",
            " 28.14953492 17.02872928 21.00228428  7.85448277 20.79307002 28.78624268\n",
            " 13.39751475 14.16711046 24.69951359 10.70441725  2.26998588 13.1569501\n",
            " 17.1542861  14.58342083 26.26311537 12.5424484  16.08192476 36.47473689\n",
            " 16.06654298 12.80129035 18.98788841 20.76810895 10.69290307 21.51451535\n",
            " 21.41413787  7.85448277 18.82353414 14.9664811  13.16231338 11.39427876\n",
            " 45.16353479 14.57730663 18.85031167 20.26181516 12.7053712  17.72344971\n",
            " 15.01758636 38.08528489 14.69775247 21.60607007 11.3337844  49.71877774\n",
            " 15.52266727 23.39409967 15.19218082 14.98342561 16.84007096 22.90097201\n",
            " 10.49880422 18.23529417 25.784191   39.29314863 42.28171824 24.94845021\n",
            " 13.17140387 10.07163413 28.03303263 46.80087205  7.85448277 11.76432452\n",
            " 31.11958802 14.2887391  13.90263458 26.71094117 15.60304499 26.19074645\n",
            " 34.96377011 13.70122279 12.14490925 12.41299263 34.59252566 13.25297728\n",
            " 22.17300141 39.30586545 14.93005126 20.96716334  4.72824323 37.88819661\n",
            "  5.73362776 10.47625479 16.1534879   6.29097817 13.71319907 32.2443086\n",
            " 22.1967384  15.41242719 30.65931656 15.82996769 21.72593813 27.94236983\n",
            " 12.09103407 20.74051458 16.84101164 26.36559347 22.33722151 18.36517623\n",
            " 24.98829371 21.92242822 29.46058167 25.10821022 33.79460311 26.79879454\n",
            " 19.74664799 24.58143522 48.22579843 11.87760253 11.73718537 28.29511883\n",
            "  8.99756956 12.35740136 28.72240946  6.92648322 16.84830191 41.19107137\n",
            " 30.53165847 23.56106899  9.50000091 13.13689005 34.58755179 30.21807677\n",
            " 31.48554012 46.56443514 14.94976765 33.59176002 24.40210799 16.6504226\n",
            " 11.11033134 23.96212978 19.26907038 14.78497046  7.85448277 49.50769489\n",
            "  7.85448277 42.69876083 18.09133791 40.52956391 24.70164596  7.85448277\n",
            " 30.5607111  46.91848527 21.25619523 43.88532883 12.94336376 13.73563368\n",
            " 23.16937467 13.80662959 20.92172864 17.6834497  20.08227809 18.68716964\n",
            " 10.01609129 36.48415854 49.53992808 17.14503149 17.81582681 23.47260139\n",
            " 11.5734979   6.89062899 18.24004581 17.88812675 41.51519929 10.68282896\n",
            "  8.00542054 17.62605191  7.85448277 25.48512399 20.78730125 17.940438\n",
            " 33.63692279 12.67947473 11.90703642 32.45266027  9.90255207 21.78560185\n",
            "  8.051454   21.40564911 19.3024459  29.62538458  8.26363042 19.858995\n",
            " 30.90467739 24.18209849 28.42133682 11.60634799 25.4706561  21.29019267\n",
            "  7.85448277 38.14661374 16.74044489 23.28966938 12.16065526 22.80126831\n",
            "  3.17640218 25.6032089  24.9014254  27.34873572 18.5885719  11.39305359\n",
            " 13.579323    9.70564132 24.68570568 14.23989641  9.8127101  11.56149539\n",
            " 37.31361392 15.49905739 25.75589596 11.77447775 22.1928767  19.86816299\n",
            " 24.37789588 13.17291696 24.20481725 17.97444074 31.25877459 24.29338198\n",
            " 14.1926331  31.98620014 14.56014185 17.14604941 21.9263876  15.98106618\n",
            " 49.44056128 19.45537262 16.66767041 31.11635353  9.61839341 27.47114448\n",
            " 47.70418694 17.86957495  7.01757739 42.17278604 24.60722477 30.71861203\n",
            "  7.85448277  8.81294704  7.85448277 15.29248259 15.3697924  24.25311265\n",
            " 21.48458978  5.48241661 20.01975213 21.96003914 26.35528947 20.48614411\n",
            " 28.06493504 18.07898919 15.52702597  5.94590426 10.7763141  20.10906015\n",
            " 17.89264734 31.52227322 16.28855151  8.71747501 47.05950126 29.69908234\n",
            " 19.82495219 23.95509635 30.72362048 19.59945104 23.42382738 27.3545542\n",
            " 13.79641042 37.03541001 10.48006138 27.66858437 21.97772038 23.18516141\n",
            "  9.96683766 33.00340595 28.76585269 24.54696657 17.42829423 18.93425066\n",
            "  7.85448277 18.53213317 39.84707334 14.05502294 23.21977726  9.90919157\n",
            "  6.71730518 47.36938524  8.35657302 34.51039964 11.1132317  33.90505174\n",
            " 16.97478517 39.16987552 41.15674708  5.22078961 12.27862873 21.74849891\n",
            " 16.59263444 11.71659381 36.7283222  14.16478598 27.95402462 23.17156202\n",
            "  7.94476539 22.90148485 31.38597851 18.14593717 21.41235861 23.46338489\n",
            " 23.86793387 11.94523915  9.57997668 21.41894514 29.10273068 48.16813621\n",
            " 12.66601249 16.28482175  9.55414859 25.08334253 22.14165585 26.10964881\n",
            "  8.54235337 18.05903644 37.11027719 49.91729698 23.96658025 12.29278245\n",
            "  9.62863837 16.58980511  8.70852674 19.66373899 14.94512067 24.72468435\n",
            " 14.75012861 19.580393   46.3901696  40.43638622 32.85754307  7.85448277\n",
            " 12.05525559 22.22580112 15.01662554 10.65496363 14.40553282 19.46429865\n",
            "  7.85448277 18.32539421 19.54305154  9.90728818 38.14324145 31.81109174\n",
            "  8.0316151  17.33989037 22.51005318 26.1659217  19.11869817 24.43874405\n",
            " 13.8221912   6.48067584  9.81558478 15.06323452 18.73583826 13.85991085\n",
            " 12.95664731  8.31485776 17.41338985 21.1282497   7.14678571 12.48152131\n",
            " 26.64189135 30.69542865 24.02276103 10.49801311 15.35175189 19.25059556\n",
            " 39.67347517 28.50687286 31.4405526  31.57707793  8.95342423  7.85448277\n",
            "  7.85448277 23.70067372 22.94881303 10.77393144 14.45299491 34.48502169\n",
            " 47.707156   25.36360607 16.71488182 25.35694098 13.85528021  8.14221758\n",
            " 30.28000897 19.7130863  18.44372976 19.88362814 23.69650083 26.71961146\n",
            " 33.60561801 18.05897709 40.86036463 15.08926966  9.70564132 19.44333839\n",
            " 13.24848179 21.23837431 12.3487412  23.09660546 32.82319845 48.02432242\n",
            " 16.6772844  21.30111781  7.14678571 12.76681778 10.5867707  27.16846791\n",
            " 22.30695226 17.98041499 33.93059225 49.31136689 15.39953594 11.76159167\n",
            "  7.10073648 30.56293109 42.103145   27.35649071  8.79131039 10.89490664\n",
            "  2.62924606 27.53779367 13.79344004 12.76151536 36.71554878 10.71904469\n",
            " 20.22547291 18.51465579 30.78219322 27.46584566 17.93811696  9.65617243\n",
            " 12.71247504 29.49575014 31.33825074 28.77898269 21.12114684 29.94525995\n",
            " 22.36198249  9.04691435  7.15644601 24.07925325 14.06555827 15.4322968\n",
            " 14.63132577 24.70068674 20.71908702 42.39753356 27.99447706 24.48555935\n",
            " 11.88458792 27.4246067   7.18015917 49.87354786 31.15498212 10.5505015\n",
            "  8.34486122 24.30511878 13.78171671 20.95374198 36.41966501 15.38440605\n",
            " 19.80789956 22.96090138 10.49806205 20.83251045 36.51796458 16.00219636\n",
            " 12.77004323 17.39973633 32.6935148  13.07187149 10.17086691 40.87875219\n",
            " 19.96244661 17.28482673 28.90368924 17.55632025 12.86498869 26.1851411\n",
            "  8.64664295 23.11734857 28.41970085 22.69881005 12.5732851   8.66560518\n",
            " 24.62743982 19.52849559 28.98074833 13.48484105 35.87537854 15.19165513\n",
            " 30.8465431  12.4685992  13.54099008 31.07109405  5.83946178 10.24973459\n",
            " 12.41646563 10.98117857 43.3521202  20.50716173 44.12362285 23.69939219\n",
            "  8.55858021 26.40461277 26.28756945 17.88812675 49.43455937 20.70483772\n",
            " 19.15360219 25.25375527 39.10430278  7.8889469  24.62243597 31.60871129\n",
            " 36.4939445  24.24973056 10.64620927 40.52920684 18.11477188 21.82784926\n",
            " 36.32320629  7.85448277 11.65605151 24.9958519  18.26026855 15.41486071\n",
            " 21.01266946 10.99814471 27.65583519  7.85448277  7.85448277  9.29367142\n",
            " 42.27577001 25.54476213 32.78020237 37.75506521 44.25411285 16.10839124\n",
            "  8.19775588 18.72942847 24.16839288 11.67935949 13.96652499 10.71675827\n",
            " 17.40748889 12.23694508 16.56042262 18.14991211 38.03665182 13.13609864\n",
            " 17.04175666 28.75326762 17.38058454  7.06734246 10.37566802 15.25909231\n",
            " 11.56905214 22.65718617 21.79749992 24.35355565 10.64802861 32.39218038\n",
            " 21.01420973 24.67151746  3.25349008  5.15302011 39.3665501  11.54911867\n",
            " 11.11160782 11.30026553  9.28066643 49.84277622 48.73068815 28.76397499\n",
            " 22.19455444 45.63590101 23.06725596 16.15250831 19.85805942 13.85728523\n",
            " 11.0869611  29.43475035 23.92227222 25.16954053  7.85448277 17.29694901\n",
            " 13.02986408 18.28651159  9.09964781 23.23454975  7.85448277 24.04255205\n",
            " 15.98583068 20.9943089  13.53125828 25.47086926 12.79219688  8.73669425\n",
            " 24.09422768 20.89270699 36.65799426 13.27770868 22.96616141 49.82676871\n",
            "  8.17960696 21.0620366  18.19436762 37.57891636 30.20663389 25.6749682\n",
            " 10.22906481 35.91907321  7.85448277 24.45049101 37.27745648 18.82429848\n",
            " 31.63586492 40.37059307  6.98235117 14.17146251 13.26160822 19.5290214\n",
            " 27.25518187 31.40688876 30.90590493 21.38956082 47.83620249 17.15146181\n",
            " 10.03708841 15.31414514 26.13737333 10.39551383 23.94673103 13.9425967\n",
            " 11.52090532  4.72824323 24.51748555 15.41500478 32.5072086   3.92199492\n",
            " 15.06323452 37.62539461 39.02064782 30.90993349 24.61272621  9.1360451\n",
            " 27.34935599 30.76598702 19.18256376 25.60634792 29.71712359 43.51402361\n",
            " 47.75315768 16.15515839  9.11055308  7.82512144 33.42229827 30.75472991\n",
            "  7.64481765 47.39037883 11.94477244  8.93515118 10.10992617 24.74857895\n",
            " 46.60601218 38.02330951 21.55335465  4.19281086 16.04931371 19.9509373\n",
            " 39.07013877  7.98350163 10.49773287 46.49320041 31.48768891 46.50443204\n",
            " 13.36671302 17.30396435 30.23823495 37.88819661 13.85644918 44.41838288\n",
            " 41.65153521 32.5062101  33.95553536 31.67068346 35.77961889 20.28861225\n",
            " 48.76640689 15.43287707 12.87574733 21.55699777 23.98087375 14.31717852\n",
            " 11.20378706 11.50544805 13.02745931 32.1059369  15.63114853 21.7612086\n",
            " 38.26364908 31.69022953 24.36021603 31.71927616  5.48697369 17.6058962\n",
            "  7.85448277 47.18626524 38.755279   19.84832857 23.01745168  7.85448277\n",
            " 11.98040807 26.20970414 13.96652499  5.84691328 27.91490381 21.48305597\n",
            " 26.7219869   8.01195067 23.40422235 13.71319907 19.7130863  13.55855816\n",
            " 17.10612135 23.82407092 36.9294165  13.14230611 10.32477375 17.48908979\n",
            "  8.50159673 10.72067012 12.61959698 20.80849248 24.32456966 10.5763495\n",
            " 16.03405147  8.6959625  13.52072631 40.76631293 24.98013175 23.67051463\n",
            " 13.27388103 17.39751161 18.69187395 15.80897393 19.2600888  14.79109507\n",
            " 23.50847348  6.06547914 26.26147804  7.85448277 38.49126761 16.94443274\n",
            " 27.57342497 23.70180687 31.50742831  7.85448277 24.23410073 26.51549701\n",
            " 31.32942794 34.28325148 28.52319993  8.0927062  35.3225764  21.7693471\n",
            " 21.33492318 17.0972568  12.28739063 22.84987387 32.52098971 34.95495497\n",
            " 27.81828728 15.79018644 49.72686251  9.6230844  26.68963113 26.56552371\n",
            "  4.81701968 14.05827296 14.09560819 20.52589846 15.31032135 39.63573403\n",
            " 37.75373984 18.05653414 45.28115266 11.05751325 15.56780917 19.81594713\n",
            " 29.25026778  7.53206225 25.66849179 49.55747887  9.36919809 27.71273563\n",
            " 46.41909267 17.69109343 19.41101991 10.79553118 11.30478576 17.26397698\n",
            " 16.78691425 22.59709209 17.54009868  6.9985594  27.60344845 13.97286183\n",
            " 15.26332795  7.85448277 11.57455209 33.62309013 11.8819653  11.23292352\n",
            "  7.85448277 49.99999958 25.21091735 33.33123208 16.5402439  22.7702987\n",
            " 36.96612449 22.44477916 11.46625234 10.07419941  9.95125015 49.79096851\n",
            " 24.72808502 10.73233652]\n",
            "selection [166 582 348 728 803 831 146 152 799 232] (10,) [2.26998588 2.62924606 3.17640218 3.25349008 3.92199492 4.19281086\n",
            " 4.42377658 4.61626909 4.72824323 4.72824323]\n",
            "trainset before adding uncertain samples (310, 10) (310,)\n",
            "trainset after adding uncertain samples (320, 10) (320,)\n",
            "updated train set: (320, 10) (320,) unique(labels): [117 203] [0 1]\n",
            "val set: (982, 10) (982,)\n",
            "\n",
            "Train set: (320, 10)\n",
            "Validation set: (982, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.262673 \n",
            "Classification report for LogisticRegression(C=0.15625, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.66      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.67      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (982,) [43.59467744  9.06564697 13.99328209 20.15892881 26.7021721  23.15910903\n",
            "  7.97466126 17.93936047  8.8788209  15.03774662 47.02774818  6.1759947\n",
            "  7.07154825 47.81772587 41.55635477 26.38617287 17.42725011 22.32699813\n",
            "  7.7176495  11.67355768 18.01626284 40.25771923  8.94736535 16.40058709\n",
            " 45.12236431 14.42840488 15.58679963 31.12205208 18.22435483 11.67355768\n",
            " 33.50841628 15.92519429 19.90484771 30.32017121 32.8846097  15.13439538\n",
            " 42.90821723 12.2529054  15.93455204  8.93640192  9.7028939  20.49315336\n",
            "  8.37141411 21.37460716 24.24618089 10.64146734 24.96054587  8.74333348\n",
            "  9.44721289 14.91143454 25.26797427  7.64932471 25.76769597 36.50329454\n",
            " 10.54183076 47.99953922 27.5077382  11.42054072 19.71510385 10.32884542\n",
            "  6.61790018 18.16163311 14.97698711 19.62959004  9.87106351  9.86858283\n",
            " 41.83342282 15.99984908 46.36738384  8.93202646  5.73731857 29.55768853\n",
            "  5.9741163   9.113859   26.74486082 32.26544086 24.91981953  9.88130224\n",
            " 23.76921967 18.7377329  39.54056527 27.2345678  23.74499532 14.16152332\n",
            " 11.63103948 18.84096263 20.87310365 49.96187893 21.46522378 11.30183316\n",
            " 41.28494371 21.96123973 20.54378961 23.55616157  6.47292983 12.40997019\n",
            " 32.06448902 10.13319316  9.90358539 27.32566471 15.01541824 26.14573728\n",
            " 11.67355768 27.41160863 11.25298136 11.12075461 21.07178073 40.46376988\n",
            " 38.11805499 15.42800978 33.33926153 37.30356403 49.97052958 38.74588975\n",
            " 10.70036298 21.88477748 49.99201511 41.30750856 36.2828863  32.44258286\n",
            " 17.06105633 22.97920317 13.93966227  5.35935522 20.72989936 15.76550293\n",
            " 24.52098519 41.07004485 28.67548259 11.98147178 28.97867268 25.39183262\n",
            " 15.06411145 30.50654136 20.28625516 11.67355768 24.38662134 46.93506009\n",
            " 22.70980594  5.87032606 40.81632979  9.23805528 17.64682492 13.19865081\n",
            " 33.30056728 49.90272676 29.62939032 10.44501312 11.81926473 17.89569447\n",
            "  8.50353224 49.99979562 14.13481571 35.7084573  25.32698373 13.79684804\n",
            " 28.30265663 11.67355768 20.48211476 32.61316066 16.50915452 12.28795346\n",
            " 21.69850162 11.88743294 19.70120849 15.6259713   8.14919763 30.57880643\n",
            " 22.10655388 18.55190385 32.03816923 13.7533535  11.65753653 22.70790642\n",
            " 19.97846985  8.18012524 26.39289689 29.35675232 11.67355768 16.13366516\n",
            " 12.69501449  8.98150694  7.77324379 45.36496773 14.10496042 23.40540623\n",
            " 22.6981948  12.39986593 15.22133359 19.09780753 39.66347343 12.09479873\n",
            " 28.28827713 15.50014419 49.70197465 16.63072225 24.8867505  16.21376716\n",
            " 21.16353648 18.11216638 27.97487871 19.19633828 23.54501554 31.95960759\n",
            " 40.59841208 46.57513904 24.42683317 16.15715416 10.03150985 31.07508174\n",
            " 47.24739706 11.67355768  9.29273367 33.67070383 10.97289695 10.8250398\n",
            " 38.80389579 22.62103378 24.13805383 41.83910406 16.76787118 14.88272046\n",
            " 14.34161412 44.14685157 12.5634531  29.94287611 40.35854847 13.33499306\n",
            " 17.89775969 34.97197339  6.82629918  6.99722314 15.24253838  7.7875263\n",
            " 10.31514268 36.81471116 24.35558538 12.76725336 29.34721304  9.86425742\n",
            " 19.92983352 32.5398361   8.72564772 15.8617917  15.42373159 26.12281627\n",
            " 10.57751273 21.81623241 25.47109527 22.55828647 32.00267483 24.18324033\n",
            " 32.42647166 32.34539083 11.43236084 23.69519532 49.34608975  8.37476342\n",
            " 10.56352143 26.85766849  4.53502635  8.81724145 29.04264717  7.90288109\n",
            " 15.47195346 38.71003262 30.90289458 32.08395394  6.74025857 10.92055875\n",
            " 44.50323053 28.71445203 26.53526284 49.93825583 18.81733573 36.15767582\n",
            " 20.34612006 20.16233507  8.11057189 21.00695055 17.18251886 26.35511664\n",
            " 11.67355768 49.99695796 11.67355768 43.84091806 26.1063787  44.79956094\n",
            " 22.00400487 11.67355768 29.76931606 42.92445251 29.55288122 48.46793627\n",
            " 11.13782809 19.47449326 23.45589713 18.69631423 31.73953087 17.83032093\n",
            " 13.73465688 25.01262851 13.48968397 40.03783943 49.78738655 20.79361525\n",
            " 16.44259838 26.55605393  8.53641042 10.59141769 15.84948848 19.20375845\n",
            " 36.15403301 10.34589859  9.57863518 17.54887276 11.67355768 24.74345133\n",
            " 17.81049415 27.2345678  31.73944907 13.11161665  8.19201041 30.74925579\n",
            " 15.89023544 29.1176127   5.88280491 24.68995316 25.96016689 21.60767598\n",
            "  9.87922898 19.75982289 38.52582931 28.42344394 25.69692785  8.98510326\n",
            " 30.00942925 18.18285669 11.67355768 37.65817914 14.52426495 26.4441245\n",
            " 16.62864756 19.22892479 29.27112298 21.13562902 25.44402852 18.43430293\n",
            " 16.08246516  9.52055837 10.15658723 30.44745653 11.62954481  9.92345365\n",
            " 23.7380921  37.92343338 14.53472766 21.22115505  8.73443688 20.22884184\n",
            " 17.23816222 22.18599889  9.92057407 24.45563447 15.61314299 26.17676135\n",
            " 18.90871918 16.23451318 28.84711714 14.97304823 15.92109197 26.60374269\n",
            " 19.13048481 49.96101948 17.2193247  21.05973132 24.50211742  8.84879535\n",
            " 36.44831311 47.41230383 15.05035002 13.13317036 44.70217548 24.2038064\n",
            " 29.11114671 11.67355768  9.16853913 11.67355768 12.46666573 24.62409512\n",
            " 24.72506016 25.65561311  9.02702997 17.14491114 21.7017931  24.29881194\n",
            " 27.74305769 20.62221121 16.73135415 13.54194028  6.01276693 17.35867193\n",
            " 17.17434756 17.4336679  30.25222377 22.07687703 10.37682449 48.78986062\n",
            " 26.36448101 28.60559247 21.7150257  27.35155841 19.78666272 21.33181951\n",
            " 24.56895022 10.51666481 41.66477451  9.71299134 29.27704711 16.32163233\n",
            " 24.86167957  9.04977484 32.77543821 35.46637849 22.68164533 14.2254606\n",
            " 21.34084228 11.67355768 15.68268523 40.15886799  9.74658301 29.32374874\n",
            "  6.86363276  9.62864051 48.50343145 10.40675959 33.33697788  4.54927814\n",
            " 41.23392037 15.78626043 39.38986673 42.19418441  6.60027243  5.59106299\n",
            " 17.92467038 12.82165512 14.71954454 34.83165249 10.89366004 24.9704161\n",
            " 25.01743957  8.76851901 20.22407169 40.36064961 18.16152289 24.14004208\n",
            " 22.24625322 23.85944892 15.69894     9.37127235 25.93217811 38.51565606\n",
            " 48.65607195 10.71395746 13.97544624  6.51993804 26.21302312 21.34485091\n",
            " 25.3264985   8.56634829 17.41898761 46.07122863 49.99801371 21.29905794\n",
            " 13.41864271  8.67354217 15.7659448   6.56420251 28.13274074 11.97646616\n",
            " 32.01058952 17.83367608 15.97080978 49.56619358 33.07138813 43.08245892\n",
            " 11.67355768 13.69281102 31.05758786 12.35645777  7.010936   22.08396877\n",
            " 24.43511698 11.67355768 23.93892498 20.47489423  6.90351269 42.38296922\n",
            " 36.31471706  9.51385196 25.93057986 23.67984135 26.37928876 21.96760025\n",
            " 21.54570376 17.00359653 20.50799745  7.17325165 17.64092233 16.66187833\n",
            " 12.32430205 13.36037476  8.22596363 20.52033913 18.09613965 12.01872394\n",
            "  8.33087375 29.51241462 38.21386571 21.74295033  6.16818289 10.47176382\n",
            " 20.73485058 38.12419972 32.79466287 31.49642975 29.89406922 15.17792832\n",
            " 11.67355768 11.67355768 23.37137401 19.39901676  6.84111422 21.5344882\n",
            " 33.62647289 32.21676569 24.45866169  9.9865104  25.15608164 20.97641001\n",
            " 13.45516982 41.3154159  17.554168   21.80107953 25.63006138 24.12647917\n",
            " 31.895021   31.45192324 15.92758223 43.16547782 17.0236641  10.15658723\n",
            " 18.06353866 10.53994363 19.49677252 31.53853422 27.65024885 32.62362259\n",
            " 48.08788521 14.93194578 18.37016667 12.01872394 10.34752168  3.81009022\n",
            " 26.12765041 23.9068139  17.20135733 31.53438486 49.99902988 22.14539323\n",
            " 13.05135105  7.55311122 29.70705769 43.65197153 25.54127736  7.20606692\n",
            " 11.15358173 26.3632202  10.96810445 11.18317122 35.1059546  10.16062767\n",
            " 17.84141955 20.52680412 40.89399377 30.16456648 16.41908804  7.88460932\n",
            "  7.68275803 40.53204199 29.13657235 29.94794009 25.98255719 26.9863873\n",
            " 25.61161199  5.85387038 22.55553775 21.53616962 11.31636796 17.65852371\n",
            " 11.88664646 22.50831272 14.32163566 41.37405655 30.29712246 23.43251594\n",
            " 16.88972328 31.24605462 13.2936014  49.99941312 38.14778016  7.66872161\n",
            "  8.18910253 37.24551231 14.30690805 17.43129676 35.63371537 12.62201166\n",
            " 19.02675799 24.77783149  5.58673212 15.60549545 38.32533369 13.14798991\n",
            " 15.4259722  15.74288631 48.01215469 11.1269997   8.19739753 43.08478309\n",
            " 16.54673295  9.22871614 29.82216023 13.4049818  11.25746914 29.09939747\n",
            "  6.0167085  20.67861238 30.1938309  32.11315872  7.88659584  6.14935362\n",
            " 24.45829442 13.27403611 26.77994067  9.807441   34.73623844 15.33798559\n",
            " 28.48332955 17.31867375 17.8944432  29.0107058  10.42666493 12.56126371\n",
            "  9.9555745  10.56131197 43.07347878 18.90972004 48.5643827  24.09905882\n",
            "  8.61740103 30.80518634 26.35588753 19.20375845 49.91325024 24.47837137\n",
            " 16.59467769 22.6563266  37.94675879 12.08430648 25.69042879 36.99680257\n",
            " 34.99571022 23.0066899  10.63648797 47.35310566 15.03795654 17.80880959\n",
            " 34.74010817 11.67355768 15.07717666 22.01148533 23.10648811 12.94918858\n",
            " 36.81482315 10.78426421 39.61265819 11.67355768 11.67355768  9.39060042\n",
            " 40.10667922 23.62724841 41.05442979 36.49789962 48.36573275 13.25791414\n",
            "  8.4499153  29.52855545 27.45075614  7.03139993 14.96911807 21.82294909\n",
            " 19.50537978  8.62911491 20.41413464  7.96706781 41.86332985 13.1747227\n",
            " 15.04488596 35.36722602 17.42103761  4.95065345 11.31959255  7.82277347\n",
            " 10.99458074 26.54236467 29.75674266 21.74254129  7.11671173 39.41733955\n",
            " 36.24783372 25.57843517 14.78891252 38.20091408 13.1233551  14.76409501\n",
            "  8.184336   12.04041489 49.99460788 48.03979841 34.02577608 20.53756347\n",
            " 47.95407258 25.65923357 15.96797896 16.80002471 23.79730996  9.92854346\n",
            " 32.59325959 28.13283795 22.1250293  11.67355768 16.96940191 10.40845895\n",
            " 17.1546573   7.02180626 27.29841955 11.67355768 26.60784004 13.72763769\n",
            " 32.40372116 11.24274607 23.01952112 14.58200355  7.76122636 29.33080612\n",
            " 23.42034456 35.26761603  9.86845972 31.78037673 49.96066089  4.34272993\n",
            " 16.14967868 15.82775055 38.29311325 32.51416101 26.61556549  6.08110606\n",
            " 40.40697802 11.67355768 22.96601292 45.86181441 23.48282342 31.33228484\n",
            " 42.25623173  4.59544958  8.04724312 11.57113258 22.68086173 28.11027933\n",
            " 24.27986355 35.03158055 20.30840677 49.39258126 15.09556225  6.51559698\n",
            " 18.62792801 30.95768788 10.57079853 25.94814504 12.46044073  7.35022137\n",
            " 28.01905656  9.92520657 31.23567137 17.64092233 35.78752979 41.02199926\n",
            " 26.69651751 29.03870494  8.05268421 31.38825808 34.67979506 22.19472752\n",
            " 19.17510072 36.82187723 43.37708718 48.42145444  6.9805717   8.4486374\n",
            "  7.64576815 22.02235639 31.99479712  6.9501574  49.44701552 13.89700351\n",
            " 10.49047394 14.66659194 22.01889327 46.54788094 37.77319116 21.41214054\n",
            " 13.12869346 17.81269146 39.1534027   7.2384913  10.74871313 49.36259497\n",
            " 25.97173285 47.46171479 10.06480439 35.89652658 29.96633304 34.97197339\n",
            " 10.26994495 45.73006112 42.44310975 33.54444545 35.85130992 24.01183692\n",
            " 41.29117991 24.28511933 49.43350016 13.00416543 17.66147023 25.51590662\n",
            " 22.32392253 12.08009648  9.57109998 21.23102003 13.18296673 30.93249136\n",
            " 14.68458107 30.05254236 45.30206766 26.87354681 23.6385586  33.71938778\n",
            "  6.7866915  21.60702772 11.67355768 47.13693866 37.21462367 24.14982787\n",
            " 12.06432329 11.67355768 14.94537162 29.87590203 14.96911807  8.2561719\n",
            " 27.07297001 19.25471431 24.99547312  7.3678657  28.72729236 10.31514268\n",
            " 17.554168    9.5698056  20.54984573 29.32647815 35.10870743  8.46139527\n",
            " 10.13319316 17.55337194  8.50897861  9.79257418 14.35562215 30.08659628\n",
            " 21.19639817  7.20550102 13.68473166 11.22687642 15.44253355 42.87527777\n",
            " 22.72661941 22.13758329 19.56747341 16.06890559 20.29599349 13.27766497\n",
            " 18.12334842 19.10663797 24.92604669  6.63086356 29.96211793 11.67355768\n",
            " 38.82537469 13.36520615 23.80435718 23.23163423 29.81333247 11.67355768\n",
            " 28.04090708 25.99037212 31.67351381 32.06941938 26.47838492 13.66093373\n",
            " 34.87610291 19.13642997 30.74461409 26.69562391 12.94598928 31.01483579\n",
            " 32.2056578  40.77853317 29.96452838 15.06882906 49.69135406  7.77827986\n",
            " 26.73824356 34.43683253  5.91656948 15.76944862  3.31394885 19.00042474\n",
            " 14.79672023 36.49343302 30.48527805 16.44121231 49.46052201  9.04976625\n",
            " 26.47598913 16.67399761 31.7269898  13.3259891  24.46509288 49.69062227\n",
            "  9.01059585 28.54258999 47.48560188 21.6605237  16.43292492 10.1407658\n",
            "  7.43478862 10.78319458  8.8786149  20.59182372 15.14886236  8.93240053\n",
            " 27.4316043  12.40283125 20.81337088 11.67355768 11.86681559 24.47331798\n",
            "  9.51250001  9.99881773 11.67355768 49.99999961 21.46610752 42.88083881\n",
            " 20.06521416 22.05939865 45.75699114 23.10923351  9.42464627  8.39987825\n",
            " 14.75690123 49.9041769  20.05685004 14.30140586]\n",
            "selection [934 563 761 260 437 775 711 123 620 443] (10,) [3.31394885 3.81009022 4.34272993 4.53502635 4.54927814 4.59544958\n",
            " 4.95065345 5.35935522 5.58673212 5.59106299]\n",
            "trainset before adding uncertain samples (320, 10) (320,)\n",
            "trainset after adding uncertain samples (330, 10) (330,)\n",
            "updated train set: (330, 10) (330,) unique(labels): [120 210] [0 1]\n",
            "val set: (972, 10) (972,)\n",
            "\n",
            "Train set: (330, 10)\n",
            "Validation set: (972, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.15151515151515152, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.66      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0]\n",
            "std (972,) [44.36454282 12.88343983 16.52058607 22.47996434 31.95022234 24.9051577\n",
            "  9.44135993 21.4794318  10.87188401 16.77121124 47.82297869  7.18589833\n",
            "  8.60273046 48.30147419 44.42950321 30.40696473 21.28005334 24.18171739\n",
            "  9.99838054 12.02189883 18.58487269 42.15261067 10.80224679 17.19758457\n",
            " 46.21695293 17.30518926 15.92503624 35.26336258 20.96024163 12.02189883\n",
            " 37.09561477 18.64364981 23.04866784 32.8094235  35.8413952  17.71337425\n",
            " 44.56171874 15.82551114 17.10211179 10.70135065 11.36056327 21.52884403\n",
            " 10.6281733  22.19860577 26.61581903 12.75376428 29.60627396 10.61486598\n",
            " 11.37534886 17.64878668 28.67613929  8.81592402 29.65860625 38.04323661\n",
            " 13.29825273 48.48735811 31.4195141  13.63962469 23.17121375 12.46914533\n",
            "  8.10366976 19.63702669 17.82627332 20.96700913 11.33121878 12.83686241\n",
            " 44.66659282 19.65001992 47.15960419  8.93214207  7.32880204 33.53812406\n",
            "  7.4218822  10.6230317  30.56652306 35.21853372 27.50115913 12.02990278\n",
            " 25.92081142 20.46548054 42.74682754 29.1225435  27.65039047 21.15820581\n",
            " 14.44236084 20.91370769 24.53303579 49.97733763 23.36037055 13.49774466\n",
            " 42.63631449 23.26470116 24.31281322 26.80726643  8.26445013 13.35532262\n",
            " 35.88161418 12.47722692 11.95360308 29.96187784 17.59907179 28.40415441\n",
            " 12.02189883 31.17035682 13.23722253 13.09238567 22.34585284 41.94577872\n",
            " 39.6979993  16.38467773 35.77487429 38.44115306 49.98152683 40.74351804\n",
            " 13.42943541 25.53486377 49.99552953 42.7285449  39.91743513 34.63358964\n",
            " 20.05210252 26.55675936 16.58281764 24.34035571 16.95799179 26.53062475\n",
            " 43.95218443 31.14246072 13.97605423 31.4582547  27.0465206  17.54650327\n",
            " 32.9271589  21.6829231  12.02189883 28.1229041  47.67039531 26.13921382\n",
            "  7.4486488  42.69474984 10.83914686 20.81995468 15.1769488  34.62243535\n",
            " 49.94741664 32.5408073  12.40225958 14.32412232 21.5070815  11.74827307\n",
            " 49.99988299 16.61001076 36.97433888 29.47430396 16.70413114 30.40004611\n",
            " 12.02189883 23.7948187  35.40361505 18.81721097 14.59825583 25.54885956\n",
            " 12.36800724 20.75482065 18.6547016  10.39175646 33.16742885 22.9947165\n",
            " 21.26954233 35.08828532 16.37480292 14.03923262 23.9306847  23.04601246\n",
            " 10.01216227 28.68110185 31.65362435 12.02189883 19.49337383 15.45336811\n",
            " 12.73079902  9.81761082 47.30363793 16.60415425 25.36579876 25.21243253\n",
            " 14.7174582  18.09325097 20.66094085 41.38012611 14.62181274 30.43391607\n",
            " 16.54278799 49.89306995 19.37517405 28.36442649 18.67353572 22.38773099\n",
            " 21.33520769 29.95951089 20.57731973 25.3657111  34.83115895 42.38369668\n",
            " 47.38199253 27.95218076 17.69353387 11.84306219 33.44299311 48.49791516\n",
            " 12.02189883 11.3837956  36.22586226 13.24217071 14.1176335  40.07107799\n",
            " 23.85533305 28.05475587 42.5457925  18.13698902 16.22462367 16.7110622\n",
            " 45.36511311 15.02319802 31.94187345 42.13206233 15.68843864 21.16668873\n",
            " 36.46371398  8.22311367  8.57746621 18.21032811  9.21748467 12.7967254\n",
            " 39.11473074 27.00640172 15.30625415 33.61057859 13.08075993 23.37498745\n",
            " 35.12844407 10.80732799 20.28475607 18.12859197 29.84348461 16.41760469\n",
            " 23.67141542 29.03707826 26.13223294 34.95079452 27.94987741 36.35920212\n",
            " 33.21626643 14.47090883 27.44227761 49.55266554 10.39664401 13.02962259\n",
            " 30.72293839 11.25726871 32.68938058  8.92451765 17.98074985 42.51383144\n",
            " 33.71454849 34.85580471  8.04469471 13.09574586 45.40039619 32.84526795\n",
            " 29.31517408 49.95417838 20.19697262 39.20069027 24.59872041 22.89176037\n",
            "  9.7319461  24.50856652 20.58349612 28.09773921 12.02189883 49.99818657\n",
            " 12.02189883 44.86764206 28.28273488 45.87169553 25.78654053 12.02189883\n",
            " 33.72854671 45.04514696 31.72941712 48.87631418 13.1215258  20.68269381\n",
            " 27.03244257 20.21543754 33.47147988 21.07410195 17.07517744 26.63676741\n",
            " 14.23015427 42.45172949 49.87033227 22.55734739 19.56486743 28.55650769\n",
            " 10.36970692 13.67420177 18.82986984 22.3690011  37.93811577 12.30345393\n",
            " 12.61585897 20.55487973 12.02189883 28.32094575 22.12858865 29.1225435\n",
            " 35.6726951  15.79950606 10.113687   33.67252256 18.82740587 31.00711344\n",
            "  7.19823577 27.13512886 27.73155879 23.78127472 10.99573405 22.96131969\n",
            " 39.89523851 30.91556082 29.6710464  10.98007798 32.09245197 21.30362548\n",
            " 12.02189883 41.22039557 17.25134273 29.26108025 19.20421919 20.39121448\n",
            " 31.81619405 24.9072928  29.40615408 21.38688161 18.72415542 11.78423973\n",
            " 11.82818324 32.83531097 14.01050419 11.43681844 24.92897275 41.18991631\n",
            " 17.28757693 23.14386384 10.93005714 23.6422204  20.44088949 26.56066502\n",
            " 12.10571454 28.08892645 16.02237895 31.09345542 23.55709031 17.85881762\n",
            " 33.21901705 17.34859827 18.43133386 28.5883519  20.78877175 49.97655902\n",
            " 20.56646166 22.95820768 26.856828   10.74109357 38.54519502 48.71900679\n",
            " 18.1256036  13.15379654 45.67348494 27.71216208 32.99543434 12.02189883\n",
            " 10.57452358 12.02189883 15.00070706 25.71178991 28.41644829 28.15054563\n",
            "  8.88684199 20.20670296 25.08081796 27.97269212 29.47655595 26.54818298\n",
            " 19.52087959 16.99151721  6.88923576 17.65292687 20.49108028 20.58112112\n",
            " 34.19127989 23.07317689 12.43264884 49.16969699 30.58876737 30.71461082\n",
            " 25.18329991 29.79876383 23.00727573 25.06646638 26.46785454 12.74203656\n",
            " 43.24496521 11.37225226 31.65576108 19.39483712 27.57990842 10.88678257\n",
            " 36.53049646 37.96595299 24.56040229 16.68438958 23.28630869 12.02189883\n",
            " 18.80765768 41.94865834 12.04387368 30.46538454  8.18130743 11.24962619\n",
            " 48.95122673 11.38208334 37.24478431 42.82665427 18.01635128 42.58178546\n",
            " 43.29431804  7.62583725 21.46108975 16.25051873 15.56716463 38.75516813\n",
            " 13.25122596 30.31505264 28.58347778 10.79714477 23.87938241 41.86858076\n",
            " 21.38468068 26.58430126 25.96019307 27.35305863 16.89537133 10.70317733\n",
            " 28.24677202 39.84153906 49.0789644  12.84073525 16.65838233  8.02776772\n",
            " 30.02960998 24.95861433 29.36205732  8.78135889 19.9877451  46.87968246\n",
            " 49.99899465 24.87786043 15.86528547 11.37725634 18.47412311  8.10147608\n",
            " 30.50897448 14.53987502 34.5028937  19.46351642 19.41290366 49.66503448\n",
            " 36.35421468 44.67126287 12.02189883 13.4786477  33.39668645 14.99337684\n",
            "  8.62501904 23.45827614 26.58889431 12.02189883 25.50590953 21.89014812\n",
            "  8.33291658 44.02557521 38.72314137  9.89295833 27.84002419 27.92133895\n",
            " 30.01733924 24.8353038  25.30363296 18.62212562 21.15298384  8.65222207\n",
            " 20.69358171 19.8744228  14.69481363 15.6087842   9.79289433 21.73165668\n",
            " 21.35067612 13.97103957  9.94544216 32.21069819 39.71951627 25.45016875\n",
            "  7.79556083 13.03316477 22.96122272 41.77583365 35.22072114 35.11951539\n",
            " 33.88915196 17.14051069 12.02189883 12.02189883 27.19878635 23.60579079\n",
            "  8.61344459 23.014179   37.50640812 36.34790543 28.20055286 12.44961754\n",
            " 27.43376239 21.88847331 14.15496736 42.78405968 20.84171863 23.20413629\n",
            " 27.81253511 27.72464224 34.2501021  35.27974893 18.98475714 44.65878889\n",
            " 19.35921864 11.82818324 21.89271581 12.89928263 23.53510693 33.59082632\n",
            " 29.96091977 36.48903025 49.05435021 17.05473985 21.81207011 13.97103957\n",
            " 12.21887246 29.95327793 27.5562229  20.18899758 35.53386197 49.9994296\n",
            " 23.40316504 15.09139312  8.63868382 32.38914147 44.70475659 29.43833246\n",
            "  8.27335848 13.13130837 30.38876471 13.08742025 13.13695952 38.76292492\n",
            " 12.44567992 20.92737638 22.06776072 42.42586831 33.10835173 19.31666824\n",
            "  9.5542346   9.98003558 41.88738245 33.26220155 32.77151707 28.16874628\n",
            " 31.11899869 28.05064586  7.35458159 22.94796772 25.30209381 13.64983838\n",
            " 19.01429129 14.36338919 25.9157722  18.47898089 44.31382441 33.1369161\n",
            " 26.69476446 19.7259966  33.95324244 13.28185885 49.99967457 39.70149237\n",
            "  9.40811186  9.75882529 38.33028539 16.66503192 20.87402752 39.30690236\n",
            " 15.15106057 22.32225936 27.72107943 18.88844806 40.19512231 15.93682754\n",
            " 16.6955049  19.61471059 48.33829893 13.20649802  7.15845912 44.69458928\n",
            " 19.63350647 13.08883242 33.51689998 16.36130138 13.71838146 31.51975345\n",
            "  7.39370713 23.91769582 32.73987624 34.29048703  9.9367173   8.74272823\n",
            " 28.24257247 16.66953102 30.87886139 12.25869783 38.52591622 17.76841761\n",
            " 33.01105078 18.32489381 19.30458209 32.91845258 10.41183268 14.50949447\n",
            " 12.18515482 12.6506249  45.57767223 22.24008842 48.96855415 27.91908623\n",
            " 10.48887741 33.31375484 28.42198756 22.3690011  49.9477712  26.80248898\n",
            " 19.51960901 26.21422424 41.52325953 12.57579032 29.78928216 38.46121297\n",
            " 38.77663733 26.93873792 12.55530574 47.96555818 18.09728335 19.39289366\n",
            " 38.46618111 12.02189883 16.05850609 26.02299098 25.00927418 15.61746975\n",
            " 37.619041   12.64664706 40.81763751 12.02189883 12.02189883 10.04260171\n",
            " 42.03358968 27.34218315 42.50827451 40.22818077 48.75732905 17.30547964\n",
            "  9.92757447 31.51896033 29.89465645  9.13132473 17.80333215 23.1145413\n",
            " 21.59771775 10.63281478 21.29117103 10.26882216 44.25033819 15.54607297\n",
            " 17.74663056 37.35360335 20.34463314 12.97047513 10.07387728 13.18749032\n",
            " 28.4483984  31.9322622  25.35874331  8.94201206 40.95911967 37.87842435\n",
            " 29.38197506 16.73346809 41.58934532 15.54641276 15.73635485 10.66170646\n",
            " 12.66068083 49.99715291 48.61246119 36.62998551 24.0720255  48.62336614\n",
            " 29.39450338 18.63460209 20.115195   24.88534173 12.05661427 35.37605215\n",
            " 30.51019449 25.91037545 12.02189883 20.38546452 12.64969041 20.74196014\n",
            "  8.93996358 29.45396615 12.02189883 29.34409918 16.02948351 34.13768939\n",
            " 13.33916655 26.8688027  17.06949523  9.58757597 31.37179082 25.57056265\n",
            " 38.052609   12.05970523 34.14858059 49.97759947 21.46704891 18.72339154\n",
            " 41.42364045 35.02405255 28.6450466   6.26501006 42.38114632 12.02189883\n",
            " 26.50861728 46.67907182 25.42589243 35.23891368 43.42935767  9.98717136\n",
            " 13.65671493 24.77444308 32.10071491 26.59170627 37.83606709 23.89504807\n",
            " 49.57248562 17.78591836  7.75715852 20.33151056 33.16213306 12.49868695\n",
            " 28.55009415 15.1140665   9.95414524 30.53993124 12.08268493 35.25802027\n",
            " 20.69358171 39.59344507 43.75594565 31.32084856 31.28142053  8.41025721\n",
            " 33.80539444 37.19327658 25.27406499 23.0811913  39.31621336 43.95411377\n",
            " 48.88785147 12.72247597 10.02339441  8.96938807 24.18646277 35.75571319\n",
            "  8.23712007 49.6232256  15.98468827 12.12940276 15.23663631 26.0111226\n",
            " 47.487868   41.02334754 25.153604   15.774314   20.97678709 42.47760594\n",
            "  7.28747667 12.87897915 49.55987718 28.50757841 48.03301515 12.33804615\n",
            " 36.82611351 32.41242207 36.46371398 12.53950848 46.70574999 44.9949142\n",
            " 37.26405217 38.46542035 28.25765696 42.92779788 26.1924351  49.6289506\n",
            " 15.62675603 18.86698906 27.3964023  26.01339422 14.45194787 11.61801622\n",
            " 22.43135755 15.7602114  34.92160097 17.553027   32.41774938 46.16405898\n",
            " 31.74518005 27.23155515 35.25078273  8.51183765 22.88745125 12.02189883\n",
            " 48.48929348 40.90751629 26.4526655  15.27641257 12.02189883 17.35297382\n",
            " 32.36506916 17.80333215  9.44254792 31.00472936 22.63472305 29.16171391\n",
            "  8.81186847 31.13569091 12.7967254  20.84171863 11.7772505  22.34731518\n",
            " 31.57870269 39.02750576 12.81674541 12.47722692 20.237301   10.31933723\n",
            " 12.09761185 16.77101542 32.14357191 25.01016837  8.96987873 16.28616269\n",
            " 11.65164912 16.92271563 44.64304118 26.33988855 25.56317012 20.97699914\n",
            " 19.58227277 23.57239419 16.01021052 21.24494583 20.556691   28.45582957\n",
            "  7.85981294 32.75853264 12.02189883 42.15554629 16.15946866 27.48008401\n",
            " 26.83466234 32.23823234 12.02189883 30.02715572 29.59755565 34.64213189\n",
            " 36.13566777 30.31147774 14.28432478 38.66579264 22.4463643  32.66951475\n",
            " 28.00563152 15.12535526 32.96541559 35.93757893 42.05980074 33.12805396\n",
            " 18.06903864 49.80861797  9.71026645 29.98471282 36.67734843  7.30762809\n",
            " 18.43264813 22.25189844 17.57698222 38.36089019 33.61459979 19.11114874\n",
            " 49.6002584  11.17729129 27.75624294 19.58207485 34.25183526 13.81731927\n",
            " 28.36550911 49.80395532 11.12808001 31.15895871 48.26553228 23.40078287\n",
            " 19.45403524 12.14730854  9.1173846  15.40913031 11.8380414  24.07313977\n",
            " 17.99888253 11.08130575 29.62400556 14.52363174 22.26376726 12.02189883\n",
            " 13.86336299 30.62019932 11.59599961 12.59229471 12.02189883 49.99999999\n",
            " 22.03777548 44.26160314 21.97383692 25.8359749  46.45758098 26.75241088\n",
            " 11.62943431  9.8052079  15.23340707 49.94453262 23.90375264 15.37902859]\n",
            "selection [759 398 622  11 324 816 923  70 590 630] (10,) [6.26501006 6.88923576 7.15845912 7.18589833 7.19823577 7.28747667\n",
            " 7.30762809 7.32880204 7.35458159 7.39370713]\n",
            "trainset before adding uncertain samples (330, 10) (330,)\n",
            "trainset after adding uncertain samples (340, 10) (340,)\n",
            "updated train set: (340, 10) (340,) unique(labels): [123 217] [0 1]\n",
            "val set: (962, 10) (962,)\n",
            "\n",
            "Train set: (340, 10)\n",
            "Validation set: (962, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.14705882352941177, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.67      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (962,) [42.98708982 13.42943062 15.97249373 20.96189642 38.47828556 23.27603377\n",
            " 10.34656046 21.0608242  10.77344506 15.91942343 47.05514336  7.20600539\n",
            " 47.69918377 44.17505187 29.57806713 20.21267862 22.55711042  9.71879191\n",
            " 11.07483119 17.55840781 41.60630783 10.99561819 15.34836492 45.20343941\n",
            " 18.55358949 13.30274911 34.81754104 20.42567218 11.07483119 36.64900825\n",
            " 18.85136429 21.93352991 32.05940189 34.27700062 18.12488998 43.49139628\n",
            " 15.54298385 16.12187189  9.10936068 11.5182634  19.68783774 10.38794456\n",
            " 21.78802778 25.21685205 14.21967454 29.51942926 11.181804   10.84897467\n",
            " 17.56629457 27.34333752  9.59184237 28.88623165 36.85230328 13.06062179\n",
            " 48.19530518 30.58914193 12.30269477 24.24797685 12.49519287  7.56061901\n",
            " 19.30174658 18.1058705  20.34390845 10.22116758 12.74669027 44.14118818\n",
            " 20.32789509 46.33883653  7.51058696 32.45730067  7.45822574  9.64670713\n",
            " 29.61605629 32.68907745 27.77435312 12.47744459 24.89343041 19.3691058\n",
            " 41.80010251 27.02663613 27.51083754 22.48731909 14.15474159 18.37984418\n",
            " 23.84535192 49.95587589 20.61753583 13.26765653 40.99374478 21.77174895\n",
            " 23.90835841 25.73540765  9.21877266 12.27661245 34.64678989 11.35132398\n",
            " 12.9085645  28.44188511 17.72393565 26.69212483 11.07483119 30.23966094\n",
            " 15.10096642 14.36704706 20.28089085 40.23037471 37.91216591 16.07978909\n",
            " 34.30101536 36.04842914 49.96847953 39.30762373 12.80635513 25.38716351\n",
            " 49.99162006 40.82893588 39.85280904 34.37584484 19.30470066 26.3136718\n",
            " 19.74602899 23.82184168 15.36976215 25.75178376 42.88188209 30.20293484\n",
            " 11.75382926 29.43794458 24.93358192 17.13452149 31.6459778  20.48443123\n",
            " 11.07483119 28.25097233 47.12188714 26.18637787  7.59184234 41.64905272\n",
            " 10.97537033 20.76074196 15.05634195 33.03517841 49.94009246 30.79505828\n",
            " 12.0745236  14.6804412  20.39206297 11.95628229 49.99954426 17.52669203\n",
            " 34.63698716 28.66569683 16.77396354 27.95977943 11.07483119 23.1711468\n",
            " 33.65799218 17.75059656 14.05186247 25.22728793 14.03309881 18.83035783\n",
            " 18.00879087 10.80993115 31.59186374 21.50506032 20.47748975 34.44878796\n",
            " 16.75915361 13.61846246 23.561059   22.53950958 10.27244925 26.94523319\n",
            " 29.47294786 11.07483119 19.34749152 14.96069545 12.16574597 10.27367596\n",
            " 46.94606476 16.50924996 23.85188674 23.51427484 14.03437346 19.80789299\n",
            " 18.63585783 40.78743164 14.3504367  28.43388135 15.96377406 49.85782991\n",
            " 18.32716045 27.02921731 18.158882   20.79728806 19.80994256 29.12882433\n",
            " 18.8658599  23.49606552 32.59766267 41.31606011 46.6213073  27.74361799\n",
            " 16.27876368 12.09629682 32.35280363 48.19582013 11.07483119 12.01893214\n",
            " 34.91499192 13.53292736 14.50482215 37.9789122  21.96224018 27.18431306\n",
            " 40.86329218 16.93445442 14.57103406 15.50250417 44.56152942 14.69496087\n",
            " 30.46038142 41.18983984 16.0482568  21.8210494  36.63828189  7.5685328\n",
            "  9.22327745 18.58262569  9.18165382 13.74056872 38.31220704 26.40398968\n",
            " 16.04722067 32.51493291 13.24609514 23.07605153 33.449737   11.19888601\n",
            " 19.79673262 18.58636296 28.90566503 16.32851904 23.32074664 27.25744874\n",
            " 25.6224877  33.28894857 27.41600762 35.57158081 31.71687857 14.40479342\n",
            " 26.69686007 49.36228958 10.55963198 12.17629326 30.29735485 11.07306464\n",
            " 31.41896709  8.30907925 18.4065677  42.0462842  32.75958813 32.26879091\n",
            "  9.3989946  13.21287097 44.01544429 31.38591417 28.57759346 49.89702161\n",
            " 19.34033106 37.40320875 25.54932574 21.93234789 10.36152168 24.17152065\n",
            " 20.46330942 24.65818384 11.07483119 49.99417408 11.07483119 44.09437315\n",
            " 27.14902661 44.42689412 25.40800561 11.07483119 32.81673369 45.0531722\n",
            " 28.85370857 48.32853816 14.20768258 18.94853232 25.52279006 18.6100226\n",
            " 31.88040812 19.60046371 17.44210066 24.78235579 12.71503952 42.17198717\n",
            " 49.81876235 21.79830962 18.83708314 27.42232334 11.3750572  12.48307403\n",
            " 18.20563269 20.90680879 38.48405621 12.79267363 12.97685563 20.25030302\n",
            " 11.07483119 27.99500383 21.85864571 27.02663613 35.10635891 14.39003116\n",
            " 10.64849266 32.49544377 17.74101355 28.70045509 26.2753122  26.6183359\n",
            " 23.92370889 10.64984873 22.31240809 38.30897489 29.13975531 29.7359419\n",
            " 12.04192999 31.13465995 21.98685801 11.07483119 40.24323483 17.50788776\n",
            " 27.94896569 17.94893846 21.21820573 30.2582441  24.95760469 28.51526858\n",
            " 20.67450356 16.43671682 12.56016203 11.81057369 30.80235401 14.26996597\n",
            " 13.4655212  22.4391354  40.1436475  16.40099771 22.51584578 11.02746972\n",
            " 23.14338117 20.1790886  26.04325152 13.00568072 27.56546589 15.59166811\n",
            " 30.79578843 23.65743713 17.26487457 32.95235867 17.21273072 18.42871312\n",
            " 27.87658072 20.00306259 49.95339735 19.85824699 21.41539792 26.86378056\n",
            " 10.81633206 36.97497111 48.32023288 18.76927753 10.56497585 44.77824591\n",
            " 27.49182416 32.27889496 11.07483119  9.98845446 11.07483119 14.94090785\n",
            " 24.84333066 27.00546929 26.85929713  7.83900236 21.02313131 25.03281289\n",
            " 27.70204586 27.81852351 26.38437981 19.02610909 16.24160286 15.50705962\n",
            " 20.64102183 19.6731627  33.61752133 20.74371548 11.29872721 48.8803095\n",
            " 30.44165905 29.42698702 25.76482509 29.34687371 21.72372656 24.23053306\n",
            " 26.29848199 13.29789085 42.15997947 12.75473937 30.17330955 21.1039527\n",
            " 26.13146074 11.52128267 35.85207473 36.43210354 24.3087379  18.13887954\n",
            " 22.09787246 11.07483119 18.82249389 40.99134603 11.64811667 29.37793116\n",
            "  9.4749489  10.52867573 48.63340219  9.46340775 36.54447176 41.3776331\n",
            " 19.30498341 41.73893492 42.74533918  7.37227412 23.13085872 16.83351771\n",
            " 15.27296768 37.98478339 14.01017189 29.93485978 27.91282654 10.05171216\n",
            " 22.72620953 40.02597072 21.30884018 25.20598935 25.7856003  26.9419299\n",
            " 15.1900368  11.17871983 26.94919803 37.95463045 48.87003483 11.81066769\n",
            " 15.95594672  8.46191561 28.16230646 23.99107464 28.66696322  8.79897421\n",
            " 20.15366348 45.61598549 49.99736786 24.89411121 15.20410711 10.93173708\n",
            " 19.18212776  8.35999927 27.89224697 14.41423113 32.20297049 17.59333484\n",
            " 19.20059366 49.46704296 36.3544412  43.37157971 11.07483119 12.32955847\n",
            " 31.92133415 14.70957703 10.37783707 20.31493531 24.19458585 11.07483119\n",
            " 23.20862058 20.98863885  9.58652125 42.71937216 37.82003389  9.93249361\n",
            " 25.53622501 26.0538818  29.13528199 23.90834792 25.42394383 16.40427544\n",
            " 18.63120638  9.49747227 19.66893814 18.90235636 14.59115764 14.85436749\n",
            "  8.58231888 20.68615953 21.43602732 13.01525631 10.9760122  30.60304917\n",
            " 38.0907237  25.24570687  9.50319233 12.96402799 22.57664873 41.17120045\n",
            " 34.59176096 34.76117682 33.33303403 13.26329983 11.07483119 11.07483119\n",
            " 26.41868359 23.46663735  8.27894244 21.4144283  36.90408952 40.22438918\n",
            " 26.99503082 11.37801214 25.99125941 19.96924911 12.27222279 41.51056551\n",
            " 20.70776886 22.14030786 25.1885287  27.34759274 32.97338019 35.17390812\n",
            " 18.91001021 43.7876419  19.83642793 11.81057369 20.88683702 12.38808895\n",
            " 23.49647071 29.09976383 28.36021488 35.50308348 48.81883021 17.21598823\n",
            " 21.35060919 13.01525631 13.20140509 29.70236491 25.68027723 20.37151136\n",
            " 35.85235041 49.99761822 22.63975006 14.47758407  9.10472081 31.07438783\n",
            " 43.74523233 29.17518222  9.02048021 13.00387216 30.28271818 13.23774465\n",
            " 13.46975225 39.08102412 12.33033536 20.50624289 21.55123133 40.57353637\n",
            " 31.3063687  19.84961351  9.53306992  9.91679572 40.14164429 33.07196707\n",
            " 32.42316171 25.74805646 31.10722181 26.30525764 21.39058871 24.72579459\n",
            " 13.99263893 18.73866614 13.83682774 25.47827352 18.62474613 43.6839466\n",
            " 31.74116206 26.27341912 17.78904439 32.45948557 11.71487785 49.99889909\n",
            " 38.03705989  9.33033779  9.21533384 36.14181789 16.65021811 20.50559852\n",
            " 38.66856586 15.54840262 21.63564734 26.10701076 18.93796413 38.84692741\n",
            " 16.33166769 15.70166329 18.15944875 47.46026115 13.94569973 43.75703645\n",
            " 19.93202716 14.2949962  31.85220383 16.93248242 13.18415834 29.8884105\n",
            " 23.97666505 32.08500162 32.27735724  9.78101481  8.80049919 27.83012714\n",
            " 16.14625703 30.45075952 12.30334848 37.67918995 18.24939557 31.90581688\n",
            " 16.75356195 18.03378894 32.48723029  9.68790404 14.89575578 11.76182011\n",
            " 13.22971646 45.19444988 21.54973691 48.39679416 27.37510459  9.56880748\n",
            " 32.626884   26.97423098 20.90680879 49.90647818 25.4216845  19.69755232\n",
            " 26.63958717 41.09702077 11.11458266 27.97307843 36.89934288 38.34387544\n",
            " 25.45267854 11.5218479  47.08685294 18.2312318  22.48103573 37.78635601\n",
            " 11.07483119 15.75603129 25.37607279 23.76677889 15.68505635 35.61295597\n",
            " 13.84335457 38.75397509 11.07483119 11.07483119 10.52653517 41.73599433\n",
            " 28.04131486 40.92492413 39.81624939 48.20604157 15.83704715 10.3462745\n",
            " 29.17120036 28.56341304  9.26356393 16.71678801 21.26290073 20.38056043\n",
            " 11.03760939 20.75429278 10.9999289  43.06584125 15.31047673 18.42358516\n",
            " 36.34778217 19.34077063 12.73499284 10.24153883 12.9466158  26.80430988\n",
            " 29.92344206 24.86182117  9.83529074 39.47210905 35.11948609 28.50579167\n",
            " 15.24566981 40.99091677 14.71578104 15.6433423  10.08101081 11.81718542\n",
            " 49.99300166 48.488596   35.02764523 23.97513993 48.13433026 28.69398949\n",
            " 18.83359143 19.81142945 23.96922964 11.87719543 34.77393904 29.8036545\n",
            " 26.22600126 11.07483119 20.01848237 13.20909115 19.98401447  8.24978171\n",
            " 27.42167054 11.07483119 27.88587047 16.91411856 33.0074718  13.33879838\n",
            " 27.03722012 13.48066087  9.07516327 29.64324971 24.99699542 37.35149688\n",
            " 12.84337427 31.9389368  49.96382599 20.35745673 19.45317033 41.15553267\n",
            " 34.93907427 27.17507162 41.70094289 11.07483119 26.35523351 45.49620376\n",
            " 23.69191879 33.63253371 42.40873516 11.68887542 13.50161126 24.76979779\n",
            " 30.77535982 27.467184   36.07315105 22.87168874 49.40816869 18.21518708\n",
            "  8.79295638 18.63679241 32.17214909 11.82918016 27.43641165 14.58844592\n",
            "  8.91338484 28.65851523 13.84682211 34.55317347 19.66893814 38.95647576\n",
            " 42.55160358 30.7530827  30.02088512 11.1685915  32.42936142 36.46624764\n",
            " 22.96537105 24.77976612 37.42143839 43.38333476 48.60832597 13.95435325\n",
            "  9.08653109  8.78905653 25.7664244  33.65818881  8.94226367 49.40896757\n",
            " 15.54196023 12.08813597 14.46991233 25.34414032 47.19798268 40.31632622\n",
            " 24.04329104 16.96406911 20.9398551  41.33438652 12.10003237 49.28612535\n",
            " 27.47637463 47.58053814 12.99227737 34.28263866 31.28476126 36.63828189\n",
            " 13.00111941 46.148992   44.48161807 35.42806169 37.76661253 29.03396753\n",
            " 41.5862594  24.84996936 49.48545995 15.90262686 17.11995601 26.53204132\n",
            " 25.82383065 14.79017054 11.67823733 19.25441681 15.47420954 34.67265306\n",
            " 17.09672311 29.88909383 45.09675637 31.91323603 26.24536566 34.0905474\n",
            "  8.25814065 22.02272871 11.07483119 48.14450066 40.23421314 24.92125008\n",
            " 15.75869433 11.07483119 17.31795713 31.20911743 16.71678801  9.40629654\n",
            " 30.29171893 22.67411857 27.70215978  9.28207024 29.48555526 13.74056872\n",
            " 20.70776886 12.38196328 21.41511037 29.77066464 38.71925769 13.56241219\n",
            " 11.35132398 20.60452701  9.79198842 10.14254925 15.98123872 30.88666472\n",
            " 24.83593392  9.63440985 16.16798899 10.78488426 16.02760605 44.36869023\n",
            " 26.54310698 24.89726886 18.82591543 18.63909329 22.14474758 16.36553646\n",
            " 20.76597565 19.21638824 28.04634474  7.90652073 30.83940338 11.07483119\n",
            " 41.21046604 16.46408081 29.69190657 26.20098273 31.83756136 11.07483119\n",
            " 28.457187   29.21108639 33.26017103 35.44767959 30.07240551 12.02448178\n",
            " 38.04233953 23.21006418 31.0057085  26.16586853 14.14685137 31.40779945\n",
            " 35.17282148 40.86354341 31.92604725 17.10122834 49.76708332  8.88222577\n",
            " 29.76192382 35.36773842 17.05203747 21.89461475 17.11565975 38.12335915\n",
            " 33.23045376 20.2605434  49.31249828 11.21678392 26.94373504 19.98599118\n",
            " 33.16662784 14.2309305  28.67524855 49.74017645  9.9206089  30.32028125\n",
            " 47.80870265 22.49951681 19.44273386  9.84211738  9.64417325 15.85835009\n",
            " 13.22207166 23.46038405 17.18292921  9.20383956 29.06077197 14.6151114\n",
            " 20.70188647 11.07483119 13.8191944  30.42327484 12.05066579 10.92872697\n",
            " 11.07483119 49.99999996 22.50545103 42.70801467 20.50228755 24.56172676\n",
            " 45.49991828 25.29581036 11.47889413  9.43757622 14.76550114 49.91940865\n",
            " 24.43686994 15.06657477]\n",
            "selection [ 11 435  70  68  59 227 136 387 885 731] (10,) [7.20600539 7.37227412 7.45822574 7.51058696 7.56061901 7.5685328\n",
            " 7.59184234 7.83900236 7.90652073 8.24978171]\n",
            "trainset before adding uncertain samples (340, 10) (340,)\n",
            "trainset after adding uncertain samples (350, 10) (350,)\n",
            "updated train set: (350, 10) (350,) unique(labels): [127 223] [0 1]\n",
            "val set: (952, 10) (952,)\n",
            "\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.14285714285714285, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (952,) [43.69706676 13.26970977 16.3650096  21.10150046 29.74629115 24.32780654\n",
            "  9.81488243 21.05514973 11.22834711 15.99351668 47.44735123 48.06261766\n",
            " 44.38911304 30.08349799 21.47727005 23.62942595  9.45851045 11.51196104\n",
            " 17.52724413 41.16903677 10.40815249 16.38528088 45.58749034 17.8460634\n",
            " 14.71780359 35.17035664 21.01980224 11.51196104 37.12001016 18.77297558\n",
            " 23.01906528 31.95178971 35.18867389 17.58002143 44.04303606 15.9501389\n",
            " 16.70877846 10.24123908 11.44676113 20.44790336 10.03379593 22.4347537\n",
            " 25.8866326  13.78646286 30.14205814 11.69492955 11.39900715 17.24743193\n",
            " 27.93550776  8.80929587 29.26123447 36.92864192 14.05124984 48.18406974\n",
            " 31.04851613 13.08076908 23.9776537  12.36093206 19.72480715 17.75918854\n",
            " 20.89565945 11.11881482 13.71251387 44.5229324  20.31581954 46.74172364\n",
            " 33.0081052  10.51785588 30.17736696 34.0986435  27.76662142 12.12199908\n",
            " 25.47879105 19.93826846 42.46846229 28.39941859 27.64777198 22.01380122\n",
            " 14.19858164 19.59834575 24.32018542 49.96545511 21.89699312 13.67590332\n",
            " 42.02118687 23.01679926 23.96288027 27.12799323  8.0660918  12.82755295\n",
            " 35.62157575 11.82678008 12.12034026 28.54538619 18.04359119 28.37192144\n",
            " 11.51196104 30.88098771 13.6263636  13.99074011 21.52131951 41.25513643\n",
            " 39.17048416 16.26847124 35.53961212 37.26831112 49.97120755 40.10882685\n",
            " 13.21926651 25.349356   49.99273448 42.30079359 40.1174711  35.02973593\n",
            " 19.31196089 26.31031794 19.80055243 23.58581638 15.82011535 26.61129468\n",
            " 43.59124383 30.63235105 12.32638135 30.59180216 26.08563973 17.97207755\n",
            " 32.30847566 21.57998785 11.51196104 28.6184422  47.36388234 26.37836836\n",
            " 41.79948173 10.49973983 20.70818969 14.88395641 33.09398974 49.9216872\n",
            " 31.17059989 11.81068359 14.15271783 21.13227597 11.25614486 49.99977409\n",
            " 17.61057796 35.88696081 28.9830151  16.43615548 29.44757452 11.51196104\n",
            " 23.54841441 34.70042949 18.18452525 14.15087977 25.37194633 13.80832625\n",
            " 19.925191   17.87421131 10.42316874 32.64193512 22.71000183 20.67580174\n",
            " 33.73462618 16.33543932 13.19152075 24.03709036 22.86273625  9.87688173\n",
            " 27.96106455 30.72320445 11.51196104 19.08105723 15.37955914 12.9591275\n",
            "  9.6013575  47.32623254 16.07367225 24.77248248 24.16321376 15.04306491\n",
            " 18.98229396 19.58587417 40.64338092 14.9559279  30.12014323 16.07798767\n",
            " 49.88874027 18.68309157 27.70592073 18.61559263 21.55112765 19.94898165\n",
            " 29.68651983 20.54392437 24.85718653 34.07864331 41.36727916 46.95468216\n",
            " 27.91240982 16.88357367 11.84704427 32.81189231 48.36711112 11.51196104\n",
            " 11.19799075 35.4230565  13.27822888 14.56151387 39.36245074 22.83426285\n",
            " 27.43365153 41.29772208 17.61799528 15.20725392 16.57260846 45.40957644\n",
            " 15.59317421 31.94654583 41.25638746 15.65178021 21.28397321 35.43215525\n",
            "  8.84232533 18.55038665  9.23269103 12.76258493 39.2876508  26.65187422\n",
            " 15.34481969 32.93162153 13.92014165 23.00698582 34.47320833 10.60230932\n",
            " 20.37791918 18.36345891 29.8283895  16.42361258 23.6481134  27.87862976\n",
            " 26.16303937 34.22357459 27.41291244 36.09960831 31.75976876 13.90543872\n",
            " 27.26141178 49.40943293 10.32320744 12.54082326 30.453341   10.59276631\n",
            " 32.02710457  8.62226792 18.11280028 42.61700001 32.90339455 34.02240745\n",
            "  8.46557881 12.64661884 44.78206937 32.0908581  27.854025   49.9373441\n",
            " 19.99378619 38.70163119 25.76828287 23.0200663   9.8546483  24.47077889\n",
            " 20.1392876  27.04604689 11.51196104 49.99687836 11.51196104 43.9270143\n",
            " 28.75415664 44.90762723 25.38142348 11.51196104 33.30926256 44.222567\n",
            " 30.4317178  48.62088468 13.93253732 20.23630644 26.26625482 19.51538078\n",
            " 33.31491187 19.69398576 17.85647437 25.74433285 13.52020781 43.08143837\n",
            " 49.82033476 22.46719676 19.76157584 27.77469333 10.42932964 14.1915247\n",
            " 18.29233846 21.34551638 36.96520481 12.77441381 14.06725037 20.09606702\n",
            " 11.51196104 28.18885866 22.29800913 28.39941859 35.6427083  15.35402865\n",
            " 10.12573563 32.64126032 18.87697654 30.10484812 27.15942389 28.06838228\n",
            " 22.33855192 11.59408884 22.63339363 39.04650814 30.2735033  29.67855541\n",
            " 12.2286504  31.84359481 21.81537441 11.51196104 40.85215418 18.17666677\n",
            " 28.48358207 18.80420405 19.50258098 31.15389131 25.36921265 29.03063278\n",
            " 21.15650539 18.11850894 11.83239834 11.73537989 31.83721831 13.74438972\n",
            " 13.36469103 24.23697391 40.77105269 16.59379905 21.62742468 10.82696788\n",
            " 23.52243197 20.15314637 26.79766228 12.29218194 27.93241917 14.56702592\n",
            " 30.94438545 23.72935879 17.1957918  33.11294426 17.20650886 18.21362432\n",
            " 28.99562042 20.41505723 49.96485348 19.94643773 22.33381    25.72584738\n",
            " 10.33756269 38.23400298 48.54426075 18.59792384 11.55753573 44.95616097\n",
            " 27.89320926 32.67810553 11.51196104 10.61069975 11.51196104 14.68647273\n",
            " 25.71420563 27.72931513 27.66116375 20.58919714 25.38500587 28.06026162\n",
            " 28.83782479 26.71657861 19.13499794 17.40853118 16.33727339 20.4612702\n",
            " 20.14832195 34.07229612 22.04928302 11.36648527 48.98678106 30.53334844\n",
            " 30.52019307 25.72148743 28.6338532  22.20937073 24.54832111 25.59092081\n",
            " 13.01447343 42.59983209 12.83870838 30.6293896  21.1815056  26.85339209\n",
            " 11.12137993 36.4216814  37.50614731 23.80941305 17.65722451 22.78917844\n",
            " 11.51196104 18.54089181 40.9450241  11.92918176 29.56614305  8.89208073\n",
            " 11.06460225 48.67726136 10.26068594 36.85816158 42.16521432 18.69687007\n",
            " 42.155508   42.28676408 22.09484057 17.42994816 15.40428567 38.57906435\n",
            " 13.22631331 30.82632059 28.73658691 10.91264066 23.26811021 41.10698698\n",
            " 21.34804344 26.07583789 25.81438817 27.59677542 16.03679939 12.02106213\n",
            " 28.05054016 39.03807264 48.8534069  12.96179307 16.04763963  7.8539158\n",
            " 29.12645936 23.96503267 29.12124098  8.67021778 19.93400055 46.3969815\n",
            " 49.99823078 25.50347957 15.71105369 11.48788183 18.7758306   7.56926653\n",
            " 29.73137919 14.01721404 33.70609565 18.46525096 19.65447318 49.56920044\n",
            " 35.18254516 44.60595228 11.51196104 11.44514394 33.63736252 14.4986931\n",
            "  9.21391624 21.75582367 25.05113543 11.51196104 24.09697025 20.71494988\n",
            "  8.79403046 43.24805097 38.52897405  9.95918602 27.4198863  26.65279107\n",
            " 29.79250602 24.5265649  25.26390891 17.3997155  20.63606164  9.11340062\n",
            " 20.53817789 19.04274897 14.36803933 15.09487326  9.46410871 21.06670054\n",
            " 21.41622553 13.95113659  9.82219812 31.62975708 38.88276803 25.4546233\n",
            "  9.1858937  12.88716724 22.64617806 41.51070738 35.6927898  35.26265717\n",
            " 33.6138104  14.67948828 11.51196104 11.51196104 26.6547077  23.84019396\n",
            "  8.55096474 22.74615704 37.32269588 35.68967792 27.42709651 11.41294207\n",
            " 26.38303444 21.00064925 13.3150095  42.77894951 20.64841434 22.51571625\n",
            " 26.67114546 27.44148834 34.28978508 35.47309173 18.8620242  44.02323342\n",
            " 20.30219433 11.73537989 21.62449593 12.16363194 24.41897087 31.7514373\n",
            " 29.61660386 36.23342912 49.0038011  17.95879649 21.64351074 13.95113659\n",
            " 13.92267412 30.12714625 26.68008882 20.37638673 36.12040708 49.99901512\n",
            " 23.47335064 15.69948323  9.09045415 31.24836896 43.70161735 29.71092534\n",
            "  8.60103648 12.91093119 30.58488239 13.51198035 12.90732002 39.1886154\n",
            " 12.15241763 20.58104941 21.92828766 41.78447205 32.20237436 20.68902959\n",
            "  9.72079681  9.80444415 41.4273427  33.40111405 32.79706819 26.89993634\n",
            " 31.10674856 27.29212695 23.68572949 24.89858188 13.49540979 19.13302496\n",
            " 14.01717168 25.82409042 18.93690855 44.24669784 32.58136176 25.01375549\n",
            " 19.27803787 33.39316657 11.80593468 49.99939921 38.88242771  8.82132823\n",
            " 10.38040235 37.54712793 16.93108437 21.56316859 39.03963416 15.11992606\n",
            " 21.68193638 27.03882739 18.95983818 38.92042328 15.81024193 16.11753421\n",
            " 19.39590873 48.21019864 13.42530325 44.07975717 19.69131557 14.47380865\n",
            " 32.79238759 16.40542885 13.20073965 30.48844391 24.10952066 32.08116741\n",
            " 33.75725441 10.02901857  8.87896934 28.326072   16.45655663 30.57542954\n",
            " 11.89500393 38.28158101 18.29911038 31.89601681 17.78025747 18.92179962\n",
            " 32.84722443 10.20300655 15.1477493  12.18657263 13.17657438 45.52283591\n",
            " 21.61329605 48.71020595 27.84526196  9.51459286 33.4714228  27.09044514\n",
            " 21.34551638 49.9228629  26.31935097 19.70612892 26.38620643 41.42809459\n",
            " 11.90050271 29.28935922 37.34791366 38.71715797 25.8598085  12.22820084\n",
            " 47.51845222 17.95957135 22.82875312 38.28704872 11.51196104 15.84642034\n",
            " 25.51356218 24.52892563 15.21496796 37.29996287 13.11998827 39.992792\n",
            " 11.51196104 11.51196104 11.01271251 41.07930923 27.91570652 41.80986118\n",
            " 40.26529958 48.4692864  16.99526243  9.78124276 31.11873428 29.51323236\n",
            "  9.40352052 17.09089455 22.992333   20.93076847 10.54015282 20.7615349\n",
            "  9.95878199 43.48655968 15.03417201 18.80940075 37.34850465 19.72114867\n",
            " 13.04612545  9.2096532  13.12619572 27.91250387 31.01279755 24.96069089\n",
            "  9.96720394 40.1653129  36.98883896 29.17265476 17.07747613 41.49204139\n",
            " 15.47182984 16.01436908 10.32028513 12.35544127 49.99516401 48.23325165\n",
            " 36.01577134 24.18514788 48.32617135 29.40851626 18.73629215 19.64960175\n",
            " 25.1588945  11.77972343 35.32156189 30.40194317 26.16936389 11.51196104\n",
            " 20.17055453 13.33430368 20.75115966 28.44855107 11.51196104 28.60435759\n",
            " 16.66411825 34.40703871 13.28059322 26.92492052 14.6983437   9.38944807\n",
            " 30.65096088 25.43561438 37.25999907 12.10689797 33.43313214 49.96602617\n",
            " 20.29695444 19.1501635  41.80343984 35.62939275 27.59542127 42.07225217\n",
            " 11.51196104 26.3817698  46.19756444 24.72981272 34.47242462 42.17228707\n",
            " 10.57531514 13.23469616 24.81192452 31.82514608 25.50032236 37.26284923\n",
            " 22.95603344 49.46696081 17.71552954  8.47337002 19.41928306 33.14890069\n",
            " 12.74402277 28.03514267 14.59089069  9.42245945 29.86472655 13.74317999\n",
            " 35.12461024 20.53817789 39.42076293 43.17700006 31.4603533  30.83332327\n",
            " 10.76292173 33.49818864 37.30145224 24.63877238 25.2853139  38.94781281\n",
            " 42.43817372 48.61473871 14.80917992  9.38524508  9.38721808 23.20696463\n",
            " 34.76664287  8.3357341  49.50824629 15.89342565 12.66236852 15.12577024\n",
            " 25.23981279 47.04847426 40.9238325  24.18972264 16.19846432 20.87802966\n",
            " 41.98932579 13.0669039  49.42706203 26.81710484 47.52737976 12.47080218\n",
            " 36.24116289 31.42917413 35.43215525 12.54486417 46.17544521 44.92609638\n",
            " 36.36991313 38.27550519 28.17916786 42.26398896 25.23132473 49.50901052\n",
            " 15.61378091 18.34248673 26.99995988 25.83834773 15.32311856 11.45320821\n",
            " 21.15308502 15.57315427 34.94786376 17.43843872 31.48770186 45.67716921\n",
            " 32.34908834 26.85956728 33.85622417  7.99994285 22.22830827 11.51196104\n",
            " 48.41095286 40.64347627 26.03800415 14.49417372 11.51196104 17.94416367\n",
            " 32.01684407 17.09089455  9.89559608 31.00471817 22.70718995 28.45033518\n",
            "  9.49374831 30.29367301 12.76258493 20.64841434 11.78388018 22.12736174\n",
            " 31.18608025 39.0804385  14.04108314 11.82678008 20.7620031  10.79857377\n",
            " 10.46298915 16.68436156 32.49880257 25.11390563  8.89894965 15.84606828\n",
            " 11.07901505 16.34732648 44.59441803 26.38790449 25.20692514 20.30209401\n",
            " 19.29312358 22.70256957 16.12258207 20.86385817 20.05096946 28.47184285\n",
            " 31.95338909 11.51196104 41.84967464 16.00608729 29.87409358 26.70158026\n",
            " 31.51980003 11.51196104 29.15565363 29.5985515  33.61780873 35.81097575\n",
            " 30.56717079 13.36447373 38.7262882  23.9823348  32.56578643 27.87792555\n",
            " 13.6835355  32.89823507 35.68760721 41.45740428 32.79995741 17.22872003\n",
            " 49.73041764  8.89958671 30.14867703 36.72042682 17.64496902 22.20471636\n",
            " 17.11069268 37.02349051 32.16203558 20.43116715 49.48099836 10.68645836\n",
            " 28.55793197 20.19435651 33.88168866 13.84814964 29.02067242 49.72438656\n",
            " 10.55139093 30.62688103 47.89286385 23.21096788 19.32803189 10.8593387\n",
            " 10.3074752  16.43093847 13.37247129 23.76342318 17.43707488 10.2631968\n",
            " 28.88382697 14.36420267 21.52730545 11.51196104 14.07181376 30.27551647\n",
            " 12.54520428 11.72752195 11.51196104 49.99999999 20.6519263  43.81512441\n",
            " 21.1246203  25.04128154 46.13375984 26.15957132 11.04854979  9.38479561\n",
            " 15.26828887 49.91853416 24.47853035 15.90816666]\n",
            "selection [461 449 831  88 787 258 759 516 558 253] (10,) [7.56926653 7.8539158  7.99994285 8.0660918  8.3357341  8.46557881\n",
            " 8.47337002 8.55096474 8.60103648 8.62226792]\n",
            "trainset before adding uncertain samples (350, 10) (350,)\n",
            "trainset after adding uncertain samples (360, 10) (360,)\n",
            "updated train set: (360, 10) (360,) unique(labels): [129 231] [0 1]\n",
            "val set: (942, 10) (942,)\n",
            "\n",
            "Train set: (360, 10)\n",
            "Validation set: (942, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.1388888888888889, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.72      0.47      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (942,) [34.86272536 13.49443447 12.81024181 20.75623559 49.32469596 20.09589128\n",
            " 13.01620318 22.03966807 12.78524525 13.49186454 43.64399762 42.99140572\n",
            " 42.54195921 31.14609404 20.79203838 17.92408273 10.12175917  8.30826511\n",
            " 13.4460684  42.84773166 11.02535598 11.93184088 39.49753417 20.12624484\n",
            "  8.4695607  34.95037278 16.11197155  8.30826511 35.43888403 19.27980627\n",
            " 20.16458392 31.74841663 29.81433522 15.73535099 38.89277993 17.83730461\n",
            " 15.01028247  9.43017038  7.88040599 12.95072812 11.9302526  16.90945941\n",
            " 20.4931151  17.84672239 27.83115613 12.75438103 10.11938033 18.0728868\n",
            " 24.6779357   9.62711673 29.15699856 34.32287409 15.41907118 47.266223\n",
            " 32.29256728 12.64012884 25.44776195 14.32716013 15.353617   19.72115382\n",
            " 16.63535978 10.78179181 13.48687491 43.83374562 20.82728838 42.29626872\n",
            " 31.25310369  5.28547408 28.69183914 25.39870526 26.88406076 15.26217991\n",
            " 21.43198509 16.08327914 42.07742923 17.75392902 28.38440166 30.55196648\n",
            " 17.13413658 10.51637049 20.96773485 49.72566774 16.39212322 14.29546244\n",
            " 34.04576947 14.9383818  25.82994026 21.69740122 11.49571788 33.11952742\n",
            " 10.63033682 13.19943643 27.894621   15.90688991 18.90670695  8.30826511\n",
            " 30.5595633  12.61360216 13.71420275  8.50196487 32.91737413 27.47460321\n",
            " 12.31355969 28.95332625 26.14690563 49.85256024 33.14571248 10.83887505\n",
            " 25.21682132 49.94957002 32.19270649 39.91269242 30.12927261 16.88894455\n",
            " 23.70983443 26.04812929 21.14771139 11.35579546 21.16378524 42.34785066\n",
            " 27.54324727 10.83396198 25.00581029 22.08935742 14.48248989 27.97440542\n",
            " 16.47999987  8.30826511 29.19371047 44.41553248 27.10588084 39.25994282\n",
            " 11.28873996 20.34759852 13.77433586 31.67276173 49.96786936 29.1145201\n",
            "  9.78291195 14.95509777 19.63037639 14.64963599 49.9352675  19.10751637\n",
            " 24.68856351 29.08468819 17.50140377 21.60837547  8.30826511 20.79031662\n",
            " 29.31011965 13.37883676 15.40830359 25.81869087 13.63672136 13.71232036\n",
            " 16.4942347  14.43296886 27.28572845 13.93628516 14.79545993 36.10347585\n",
            " 16.69213144 11.1999372  21.90667526 23.24544181 11.27966905 21.80064554\n",
            " 20.55796854  8.30826511 18.25074298 15.89692285 11.71031307 11.64938998\n",
            " 46.10814809 13.49738892 19.5011606  19.16895184 14.88436953 17.15004271\n",
            " 15.53739898 39.03125754 16.03359008 24.55740987 10.11979455 49.85569485\n",
            " 14.11102265 23.21838496 17.16584552 14.90061814 13.21496245 24.01685179\n",
            " 12.28226052 20.0602271  25.70423123 39.59475034 43.02859095 26.47431338\n",
            " 12.42029734 10.87888652 28.67319053 46.83638813  8.30826511 11.34833049\n",
            " 31.59775433 16.22221302 14.62186083 28.52177065 15.43613728 26.93204656\n",
            " 36.74206029 14.57237129 11.60273085 12.50212376 37.58084391 14.94786432\n",
            " 25.05326965 40.17701033 15.9219379  22.00043348 39.41080328 12.60229384\n",
            " 15.22194286  6.64261792 13.25581679 35.51955041 21.53205008 15.85704385\n",
            " 29.97876736 17.64940326 22.16302131 28.64522201 12.58992668 22.3322728\n",
            " 18.28899385 29.05877361 22.36525734 19.00549362 25.27818792 23.58485812\n",
            " 29.94834932 24.86977342 35.3570601  27.68370259 19.19877763 25.78367142\n",
            " 48.53021793 13.20744149  9.49495406 29.22047402 11.84428778 29.68185455\n",
            " 18.4870416  42.62192703 30.42530944 22.96140876 13.03245774 35.26809841\n",
            " 31.14104263 31.42664401 47.50003195 16.45401887 35.53305128 27.27277042\n",
            " 16.51099655 12.96152221 26.43218053 18.90782691 15.73049279  8.30826511\n",
            " 49.64383657  8.30826511 43.52193188 21.97064717 40.6793312  25.78355606\n",
            "  8.30826511 31.15514657 47.22167798 20.70631076 44.51382079 14.33227152\n",
            " 15.47093094 23.85817942 13.60292139 23.19122677 15.29208835 22.50031886\n",
            " 19.09163128 10.52106152 40.28959834 49.65691444 18.29048496 19.1253784\n",
            " 23.92842958 11.9689274   7.84056707 19.40052662 16.95516528 42.84812993\n",
            " 10.90042137  8.11885099 16.31427518  8.30826511 26.58676663 21.61123247\n",
            " 17.75392902 35.94164963 11.80953065 13.3911683  33.00668112  9.02511176\n",
            " 24.33098256 22.92159485 23.26692382 30.08661572 11.26018331 21.31379699\n",
            " 32.65736106 25.21964865 29.93569553 14.88060212 27.28646885 24.2725607\n",
            "  8.30826511 38.80818217 19.00062098 21.85077502 12.22861442 22.43616231\n",
            " 26.30118016 26.41192105 28.72580045 20.47780942 12.51656722 14.53566144\n",
            " 10.85086094 24.35684292 14.71323196 11.75065431 11.80924225 38.44292017\n",
            " 14.90759558 25.85078367 12.03399432 24.01289167 20.97284794 26.66772552\n",
            " 14.05153455 25.4583786  18.18537473 31.89592236 25.59570303 12.594249\n",
            " 32.74695758 14.42139322 18.37711684 25.55307164 16.18514148 49.5666576\n",
            " 19.59869348 16.84869793 32.21555963  8.51574159 30.60361936 47.98814758\n",
            " 18.34595342  6.29721677 43.36709798 27.19934554 32.51659787  8.30826511\n",
            " 12.23711752  8.30826511 16.28291159 17.22851825 23.78655111 21.53451867\n",
            " 21.51228554 22.43306539 28.73944288 21.35418901 27.30817952 19.56799136\n",
            " 16.18212981 10.97794642 20.92990893 18.23291735 33.446996   18.40067647\n",
            "  4.94282699 47.54419601 31.0313998  20.00350035 26.25754056 31.00193226\n",
            " 19.85327217 24.07550492 28.55991044 15.53168361 37.71353533 12.1253416\n",
            " 28.25496109 26.81473205 23.44681386 10.83107843 34.17142242 29.82628315\n",
            " 25.66898602 20.46391826 21.22448901  8.30826511 19.12482474 40.61928835\n",
            " 14.31300014 24.45249708 12.57415375  4.34944023 47.70628871  7.40452689\n",
            " 35.43137308 35.14858119 16.66749053 39.67467237 41.68107218 22.01261668\n",
            " 17.97571962 13.00560337 38.39879566 14.65227439 29.83138454 24.87762993\n",
            "  6.91066524 24.35823886 32.58179799 18.74643845 22.20975533 23.87877837\n",
            " 25.05675779 11.86952811 12.80215927 22.94214584 30.78651305 48.51006966\n",
            " 14.5783477  17.4474818  24.69806656 21.04714027 26.36928391 10.14561306\n",
            " 19.64872982 38.2331801  49.94482991 25.95564695 12.29660714 10.50992216\n",
            " 17.8244193  20.00259208 15.35535105 25.11215429 15.44216337 20.00282828\n",
            " 47.20456591 40.95163956 35.40628385  8.30826511 11.98262858 25.09011983\n",
            " 15.34388997 11.80179625 13.48458047 18.17954505  8.30826511 18.27388788\n",
            " 19.06572101 11.5579783  38.37585522 33.51010767  8.58610848 19.25412344\n",
            " 19.82816796 26.65858314 19.18277643 25.46925041 13.40504894  7.27168505\n",
            " 11.60582666 14.99227982 19.2169108  14.02867698 14.0317283   8.05483015\n",
            " 18.73064601 23.34903609  7.68005491 11.0284401  27.9393397  31.70083491\n",
            " 25.31497148 12.50886878 14.82009502 18.80764838 40.23213981 32.46145419\n",
            " 34.05508746 32.90718569  4.80042999  8.30826511  8.30826511 22.69862255\n",
            " 24.7318159  15.95842616 35.1945499  48.02311727 25.58761482 16.55973141\n",
            " 26.39323781 14.57083383  8.06925544 34.07006057 19.97869485 19.51846044\n",
            " 20.19371941 22.56911835 29.9698897  36.401377   18.70705114 42.17647761\n",
            " 16.53757498 10.85086094 18.01037821 13.51768006 24.1707448   6.71930021\n",
            " 25.18719594 34.43674016 48.5289436  20.2596443  23.00559286  7.68005491\n",
            " 17.166835   28.31005243 21.70896449 18.70491623 36.11323772 49.53417324\n",
            " 17.03719417 13.7440638   7.636185   30.99951017 42.91689192 28.89576554\n",
            " 11.52790008 27.45653725 15.17018335 13.47307943 38.72821139  9.84493455\n",
            " 22.1247016  20.43199232 31.8200574  27.34110224 21.61252257 10.76174074\n",
            " 14.49972172 32.57353402 32.65185101 30.64201101 20.94321912 31.40065459\n",
            " 23.0410379  11.34529389 25.20112538 14.75094749 17.3958046  13.72889462\n",
            " 27.71687714 23.77373973 44.17228344 28.90091613 18.33755537 12.04289734\n",
            " 28.43774854  4.92178367 49.92153575 32.55236924 10.96343495 10.21391782\n",
            " 26.32852073 15.21821546 24.24192419 37.38565337 16.12430018 19.36696886\n",
            " 25.05561844 21.25602178 36.51670742 16.17354953 12.5722491  18.807054\n",
            " 37.45789608 13.99071138 41.67346261 22.00590194 21.00350859 29.64358952\n",
            " 18.69295113 12.8965433  26.16747835 26.1421     27.9582973  23.35592737\n",
            " 13.17407616  8.41832031 26.45665596 19.79702584 29.66590283 13.98365792\n",
            " 37.48641891 17.32388415 28.98399189 14.00362079 14.35848656 33.38739606\n",
            "  6.93175656 11.98633984 12.27235547  9.94113215 44.48333705 20.7014545\n",
            " 44.70612602 22.23133984  5.79013082 28.43860374 26.88100265 16.95516528\n",
            " 49.56142908 21.63442951 21.56046525 27.24777395 39.78592055  8.18704823\n",
            " 25.99763246 32.38937583 38.19407199 24.05239735 10.10455944 40.93319219\n",
            " 19.00671823 32.0783509  38.50196871  8.30826511 11.28343732 25.51643509\n",
            " 18.58300396 15.39860713 24.89204837 11.71043935 29.88179482  8.30826511\n",
            "  8.30826511 13.67120379 42.75835824 27.5551985  33.94312752 38.87998917\n",
            " 45.30075887 15.39299095  7.4452104  20.75577589 25.79179636 11.95052597\n",
            " 12.14311913 11.64980301 17.15490453 13.49749089 17.33638642 18.29377075\n",
            " 35.30404429 11.51040919 19.51533227 32.02718136 17.36859333 11.60725438\n",
            " 13.70185729 12.73626431 25.15097382 21.06139976 26.03019981 12.63332535\n",
            " 33.16405723 20.66200911 25.37338354  6.88130405 41.27177865 10.85429494\n",
            " 12.39488714  9.64200043 10.37224869 49.88633642 48.99325024 29.03606692\n",
            " 23.02372754 45.75349879 23.66395799 17.34431689 20.56860671 16.02504227\n",
            " 10.10203178 30.32758985 24.83915138 26.79112328  8.30826511 16.70812032\n",
            " 13.81784043 17.49518494 24.09016622  8.30826511 24.18807028 18.06217092\n",
            " 23.63309955 15.42206763 26.23052762 11.92149254  7.09934382 26.82085297\n",
            " 22.57566934 37.07532375 13.92644939 22.92512508 49.87804738 16.70400602\n",
            " 19.53169015 39.95005784 34.48109991 27.023074   36.40051876  8.30826511\n",
            " 25.72370222 38.3147323  19.60605516 32.33033483 40.31798053 13.81516217\n",
            " 13.94455623 19.39500399 27.31411423 31.23080682 31.91937403 20.27589819\n",
            " 48.32464911 18.11676688 15.41940834 29.43877582 11.76946799 24.51021081\n",
            " 13.12231634  9.95942911 26.38047733 18.48106058 34.10387544 14.99227982\n",
            " 39.39478423 38.99409904 33.2076901  25.81232636 13.54348104 29.61910845\n",
            " 33.15929561 19.50453629 30.42646262 32.54634533 44.0400199  48.18966861\n",
            " 16.39580535  7.45813857  9.52617553 34.66828698 30.71293662 47.76289115\n",
            " 13.22283284  9.26937092 11.50055872 24.39648953 47.09010727 40.16629859\n",
            " 19.2142301  16.18267483 21.03983082 39.33435222 11.28866593 46.86300477\n",
            " 32.05239558 47.07762314 14.16417427 19.5742541  31.21714252 39.41080328\n",
            " 15.15115151 45.32362321 42.45668795 32.56228668 35.56974523 31.53688499\n",
            " 36.69229293 19.69770463 49.00061889 15.74148496 14.39400723 22.65310249\n",
            " 24.38018069 16.38610538 10.98754418 11.41501358 13.38912095 32.81573095\n",
            " 16.06748561 21.61087688 39.69021564 32.85002885 25.69531996 32.30067432\n",
            " 19.10890442  8.30826511 47.81420689 39.98308979 20.80642401 22.12664259\n",
            "  8.30826511 12.82950813 28.60896042 12.14311913  5.47787568 29.73906099\n",
            " 22.84196591 26.68942966  8.77494761 22.78117352 13.25581679 19.97869485\n",
            " 14.8984461  18.13794351 26.35526901 38.19408147 13.84960382 10.63033682\n",
            " 20.01818219  9.51926908  9.15900137 13.81666645 23.96591823 26.05981913\n",
            " 11.29859308 16.86228763  8.78150068 13.09809332 43.18427111 26.51102065\n",
            " 25.60194944 14.29329639 16.78525393 16.48821481 16.41253838 19.93398055\n",
            " 15.47448973 24.72832866 26.66346955  8.30826511 39.58840795 17.91409226\n",
            " 32.14443805 25.54958448 32.7439302   8.30826511 25.22789448 27.86627582\n",
            " 31.03787414 35.57839584 31.06935891  8.51587564 37.31857655 25.74173\n",
            " 23.77497787 20.89903935 10.09506198 25.75332456 33.93611722 37.23636263\n",
            " 29.41522776 14.88540512 49.80910179  9.05180252 28.70941015 29.38324463\n",
            " 12.15115543 21.49719588 14.99141628 40.10345632 38.1483301  21.29077838\n",
            " 46.04295863  9.61416532 19.77287569 22.62348075 31.01623656  8.00857188\n",
            " 27.30172657 49.66982268  9.69052293 28.83068663 46.8092468  19.03196589\n",
            " 21.44316454 12.18415368 15.57858488 19.42422639 20.52156006 23.94958127\n",
            " 19.10764048  3.99948098 28.38232022 15.32826125 15.33359352  8.30826511\n",
            " 12.36020407 32.85817727 14.87057267  8.96278263  8.30826511 49.99999996\n",
            " 25.97527549 34.98953562 16.22740234 21.94928105 39.37559818 21.6154106\n",
            " 10.35321787 11.92469753 11.40323288 49.85351266 25.6728889  13.8707246 ]\n",
            "selection [919 417 506 583 390  67 832 632 367 223] (10,) [3.99948098 4.34944023 4.80042999 4.92178367 4.94282699 5.28547408\n",
            " 5.47787568 5.79013082 6.29721677 6.64261792]\n",
            "trainset before adding uncertain samples (360, 10) (360,)\n",
            "trainset after adding uncertain samples (370, 10) (370,)\n",
            "updated train set: (370, 10) (370,) unique(labels): [132 238] [0 1]\n",
            "val set: (932, 10) (932,)\n",
            "\n",
            "Train set: (370, 10)\n",
            "Validation set: (932, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.006 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(C=0.13513513513513514, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.72      0.47      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 0]\n",
            "std (932,) [33.26279846 14.93685601 12.81145637 17.71628154 49.04063478 19.9440296\n",
            " 13.03776891 21.89768321 13.14347776 13.48430538 42.6715329  42.44246758\n",
            " 42.46299403 30.38527778 19.83187088 17.50073406  9.97362532  7.96268648\n",
            " 12.90266672 41.53149122 11.97298893 11.01996206 39.00241845 20.58150716\n",
            "  6.60592766 34.51358877 16.63737244  7.96268648 34.46695929 18.46763497\n",
            " 19.49620413 30.40849386 30.25540537 16.88202823 38.33082666 17.04751192\n",
            " 14.78465975  9.76188934  8.54941967 12.2402467  12.11064568 17.36026141\n",
            " 20.57982228 18.17555232 28.25826566 10.07883282 10.3942368  18.91015942\n",
            " 23.48526126  9.67221682 28.50626157 32.13407126 15.16194438 46.50411491\n",
            " 32.11940485 10.62740927 25.35196006 13.7885393  15.65169047 19.9082285\n",
            " 16.43260109 11.62118434 12.84321269 43.35705305 22.55383885 41.1980799\n",
            " 29.98974406 28.24848864 24.51033027 26.72996139 15.03699616 21.48856466\n",
            " 15.5292515  41.24730197 18.05297372 28.15344234 29.8927958  16.86844348\n",
            "  8.44804121 23.24499218 49.56817198 15.38765123 12.24553568 33.06464575\n",
            " 13.94161884 25.95532928 21.45070302 10.49669024 31.81192679 10.78083384\n",
            " 13.5452707  26.51392449 14.91828702 17.95862262  7.96268648 29.83466317\n",
            " 14.74857969 13.37145854  8.35935069 31.25861786 26.48514175 12.14760752\n",
            " 29.00260331 24.99885075 49.75333432 32.56368412 11.04893596 24.94640977\n",
            " 49.91123989 31.75026034 39.55438261 30.15469482 16.57853036 23.68809513\n",
            " 26.75145203 21.55109735 11.21170455 20.97057771 41.07640965 26.89775419\n",
            " 12.62553756 24.466926   21.13534125 15.6859443  27.36351259 15.61410114\n",
            "  7.96268648 28.17601155 43.63066655 26.52678116 38.06769168 10.77429063\n",
            " 19.93986246 14.28748749 29.02151707 49.93993085 27.67315436 10.9142884\n",
            " 15.15108768 18.92639    14.61051819 49.88722428 17.8769704  23.39635064\n",
            " 28.77547638 17.70239035 20.14517579  7.96268648 19.65309193 28.51375234\n",
            " 13.53287296 14.26272854 25.54956865 13.36582613 12.5491257  16.21440343\n",
            " 14.01859238 26.30011198 13.62739379 14.41520783 35.16661986 16.7863404\n",
            " 11.34689439 20.5028051  22.50946954 11.08404066 21.81097036 20.28757944\n",
            "  7.96268648 18.35056034 13.61927432 11.99735889 11.96728144 45.89800843\n",
            " 13.67132648 18.99467467 19.30346475 15.2031605  22.70556563 14.72433507\n",
            " 37.97760243 16.20609419 23.8445015  11.4317481  49.81930351 13.10843144\n",
            " 22.3802843  18.05449453 14.11776224 12.63474816 24.1279516  12.65023559\n",
            " 18.09231589 24.91402984 38.2943479  42.25978971 26.52049414 12.41216942\n",
            " 11.35846495 28.06835391 47.28620873  7.96268648 12.11122183 31.17627162\n",
            " 15.83719104 15.42428696 27.08843353 14.48118311 26.54039331 34.62239573\n",
            " 14.07842597 11.45387848 13.05328364 37.96786941 15.78818006 24.34353585\n",
            " 38.74313584 15.56960687 22.6709599  37.5251079  11.96366099 14.11364224\n",
            " 14.24890131 34.95746434 22.43561976 16.45429655 29.48791413 17.25491146\n",
            " 22.21310327 27.861128   12.7409401  22.15289245 18.52240944 27.6760881\n",
            " 22.56618912 19.15187316 24.00667568 24.05765351 29.02286083 24.78613325\n",
            " 34.37713403 25.11464532 18.43957331 24.89051585 48.05125627 12.58800623\n",
            " 10.70464866 29.16882848 11.68641572 28.34707131 18.28952028 41.68132611\n",
            " 29.86306522 22.3042099  13.63733772 33.73841836 30.02211218 29.65096686\n",
            " 46.88216469 16.11068881 32.34979341 28.12347421 13.52069564 12.98682673\n",
            " 25.98256876 19.93641705 14.61673686  7.96268648 49.49141572  7.96268648\n",
            " 42.02595064 23.15481476 38.64242633 25.75551766  7.96268648 30.55398897\n",
            " 46.56166944 20.23301167 43.38093534 13.66177686 14.12173774 22.48155348\n",
            " 13.63034695 23.70681863 14.50086524 23.00800023 17.80142141  9.31906848\n",
            " 40.67907785 49.49182425 18.06005625 19.36640742 23.03903595 12.98576691\n",
            "  8.66515063 18.71709373 16.67174898 41.06685248 10.2172899   8.51583894\n",
            " 16.20827733  7.96268648 26.26013808 22.23110418 18.05297372 34.96000519\n",
            " 13.06996835 13.36679693 31.14129991 10.6240848  23.69065222 22.93754092\n",
            " 22.54246876 27.97769079 10.94886158 21.19351391 31.17768033 24.5885498\n",
            " 30.04808323 16.41757392 26.91512396 23.73889802  7.96268648 37.95263963\n",
            " 20.03386416 23.21893823 12.83914962 21.66153283 25.40357414 26.96550274\n",
            " 27.60077931 19.70710314 13.5829453  15.06154105 11.05726793 23.84863564\n",
            " 14.98765924 13.87092518 11.96595472 37.85604788 14.11912265 23.94435457\n",
            " 11.2525657  22.71470845 20.75111464 26.88606456 14.24352622 25.86940308\n",
            " 16.09832716 31.5593752  27.28199594 13.45093059 32.89276701 13.42609415\n",
            " 18.62362785 24.39870813 16.28497636 49.37476541 19.07785553 16.57026317\n",
            " 30.07092692  9.13837168 31.45713962 48.00781395 18.01071479 41.56421423\n",
            " 26.89980276 31.70034628  7.96268648 10.35570229  7.96268648 16.50707639\n",
            " 17.62625571 21.97227568 22.07326406 21.79915688 20.90177582 27.64112081\n",
            " 20.52627371 26.5852551  18.71669269 15.89668748  9.16717478 20.72423688\n",
            " 17.46185506 33.15085843 15.98323175 46.91833577 31.00475681 19.82784019\n",
            " 25.6478533  29.59072026 18.42269054 23.20162142 26.9402643  14.98589485\n",
            " 36.7558696  13.33735213 26.7517857  27.89668416 22.95653258 11.72210387\n",
            " 32.81665849 30.22880267 23.862799   20.62072013 20.65714665  7.96268648\n",
            " 19.21194128 39.11986179 14.37030457 23.83219386 11.98460672 47.03997734\n",
            "  5.96523561 35.0540398  33.69582881 17.7427983  39.00034226 40.22056645\n",
            " 23.54940954 19.2615259  13.3694275  37.56562826 15.44600858 31.16636478\n",
            " 25.27884323  7.44746714 23.47457257 31.4391368  19.94023813 21.72177518\n",
            " 23.99401684 22.99034639 11.08117201 14.13518356 22.53801854 29.06978172\n",
            " 48.04915317 13.87901342 16.78556165 23.20418587 20.98163737 26.75611441\n",
            "  8.46062792 19.11916618 36.94821985 49.90918055 26.63626095 11.18274186\n",
            " 10.9561842  19.18160271 19.32246609 15.56154228 24.76131903 14.77035805\n",
            " 19.8843473  46.43473845 39.45108695 35.0408814   7.96268648 11.21697304\n",
            " 24.66263408 15.15434828 12.5654847  12.36737725 17.50006286  7.96268648\n",
            " 16.76666447 17.87833981 11.4453273  37.20361088 33.56762759  8.12302023\n",
            " 17.41732997 22.01339354 25.33595516 17.02676569 25.73717441 12.25259625\n",
            "  7.5331102  10.82714291 15.04147039 18.25166434 13.43676721 14.87238606\n",
            "  7.55331505 17.68661989 23.07293647  8.50914151 11.89122168 27.23003857\n",
            " 30.0915766  24.71545846 13.61747918 14.48254666 19.45331956 40.26250872\n",
            " 31.74678486 33.66162881 32.42249994  7.96268648  7.96268648 22.81094895\n",
            " 24.40489127 15.11225781 34.52448371 47.57174386 24.930481   15.5703945\n",
            " 24.19463784 12.91976099  7.48880485 34.00644909 19.67222511 18.32150461\n",
            " 18.2571547  22.01209229 29.20173046 35.77271778 17.96700403 41.43696282\n",
            " 15.0048126  11.05726793 18.82390924 13.03959765 24.07496307  5.86029505\n",
            " 24.10413978 34.12990549 48.43183465 18.54002805 22.12053092  8.50914151\n",
            " 17.05322477 27.28836945 19.23147921 18.69138049 35.97596101 49.3450367\n",
            " 17.19577203 13.70137908  5.66989377 29.70742505 41.03571111 27.89854521\n",
            " 11.95583951 27.33013954 15.31964041 13.48761686 38.57933718 10.74153107\n",
            " 21.82291094 19.11962805 30.39478159 27.18355398 22.0074491  12.31313178\n",
            " 13.54513858 31.86868653 32.07237893 30.57549028 19.76001131 31.42248393\n",
            " 22.19013877 12.04553734 24.80060491 14.98910541 16.70414119 13.75755542\n",
            " 26.8473519  22.54777273 43.2665703  28.49831855 23.43360719 12.20878155\n",
            " 28.68977524 49.86388944 30.91891691 11.20647713 10.65033328 25.05874617\n",
            " 14.81508063 24.32146948 37.11259465 16.3061138  19.27632321 23.65823969\n",
            " 21.25031047 34.6706141  16.59239336 12.20037934 17.13651358 36.72997978\n",
            " 13.75334908 40.75274014 21.96083663 19.59072292 27.16182304 19.1417093\n",
            " 12.66759146 25.45440545 25.36484631 28.21990831 23.14122673 12.98177657\n",
            "  9.33909111 27.05051279 19.1095915  29.54451785 13.88759666 36.43249206\n",
            " 17.22872229 30.03543212 12.95345332 13.81696473 32.74951639  6.4736147\n",
            " 12.52163548 11.79867946 12.09234812 43.9799836  20.40628    43.56667872\n",
            " 20.72042623 28.07551406 25.28215571 16.67174898 49.35657899 21.6194104\n",
            " 20.81101797 27.4951803  39.74846615  7.57887953 25.7659286  30.46410973\n",
            " 37.7056048  22.60358953 10.20675454 40.30213505 18.91860132 33.04065681\n",
            " 37.84439763  7.96268648 11.73980098 25.51810155 18.40581664 15.80760258\n",
            " 23.71482112 13.07859258 28.79537641  7.96268648  7.96268648 15.06315522\n",
            " 41.60153694 27.71990579 32.89213966 38.23689589 44.38308813 14.64640999\n",
            "  8.0599883  20.65629733 24.94768242 11.06540787 11.46909706 11.77675323\n",
            " 16.52434684 13.4621391  15.99324475 16.84762047 37.68302488 11.27952078\n",
            " 20.63041201 31.17629519 15.60395663 10.41292843 14.67257002 12.5492781\n",
            " 23.10234662 21.66185188 25.81376915 10.85564973 31.94540404 18.49939723\n",
            " 25.7938925   7.76527231 40.72906781  9.72824575 12.62181411 10.94498746\n",
            "  9.53172558 49.81639863 48.55474586 29.27282853 21.89098826 45.39813979\n",
            " 24.24903556 18.08078411 20.49767642 15.65207459  9.56862863 30.98278067\n",
            " 25.24561821 26.57530992  7.96268648 17.89420212 14.93409527 19.38226937\n",
            " 22.53222665  7.96268648 24.19059467 17.9301049  25.59014559 14.38587851\n",
            " 26.23757709  8.00721265  7.95007884 25.62871601 22.34587588 36.21209934\n",
            " 14.62381831 22.93620242 49.80530117 19.41910996 19.48610404 38.59059508\n",
            " 33.96115083 24.93512415 36.48746497  7.96268648 26.00952866 36.90005444\n",
            " 18.59241436 30.73852356 38.61285688 14.82191774 13.83335    19.9452733\n",
            " 26.06289173 30.493728   31.77543374 20.13850706 47.89529731 18.20642503\n",
            " 15.0442694  28.87459475 12.72759544 24.28250048 12.82823498  9.58195114\n",
            " 25.56757649 19.58040426 33.66750458 15.04147039 39.14331742 38.07790444\n",
            " 31.83314822 24.99774267 15.47551992 28.83788447 32.88384501 18.4056176\n",
            " 30.33708365 32.54015252 42.21969609 47.5367565  17.05669523  8.02587169\n",
            " 10.24310185 32.33071571 28.61708793 47.19078568 13.61464976 10.79712428\n",
            " 10.56870924 24.79297136 46.39931875 38.94605746 18.51813331 17.37523317\n",
            " 20.72114715 38.35883186 11.76945432 46.02314615 30.07505926 46.07955984\n",
            " 14.13898881 19.35547276 29.92851612 37.5251079  15.17903694 44.12614298\n",
            " 42.5085883  30.40370003 35.3262459  31.8969897  35.44468373 19.74864906\n",
            " 48.63859032 15.59800136 12.91314957 21.79328405 24.2126188  17.43131927\n",
            " 10.2068854   9.75435975 14.58810885 32.55985317 15.11904509 20.97804152\n",
            " 38.16811085 34.62745001 24.16030598 30.63631011 18.822738    7.96268648\n",
            " 47.34730205 39.56677057 20.58904557 21.51962929  7.96268648 12.77501201\n",
            " 28.53111886 11.46909706 28.81728314 22.41026502 25.89482484  9.51235787\n",
            " 23.26691026 14.24890131 19.67222511 15.27854494 17.69617001 25.09509014\n",
            " 37.89221526 15.78053841 10.78083384 20.38025156  9.90270328  7.37802214\n",
            " 13.49815702 24.45407243 25.22814846 11.62186825 18.64851871  7.97765909\n",
            " 12.95953824 43.86550033 26.76499415 24.54506369 13.36085227 18.4768836\n",
            " 14.62856362 15.82857918 19.54635284 14.77489418 24.72250976 26.26824822\n",
            "  7.96268648 38.98240849 18.3650552  33.9387952  25.33426682 31.38886691\n",
            "  7.96268648 24.04239768 27.32203432 29.97230505 35.56155433 29.97493069\n",
            "  7.5328093  37.00708554 26.82291907 23.21488611 19.63666554 10.42610841\n",
            " 25.14421134 33.06662624 35.78777055 27.47778806 14.7846052  49.67934038\n",
            "  7.64006567 28.05999641 29.41621933  9.75932369 20.59075119 14.9458281\n",
            " 38.46358683 36.66653741 19.07630773 45.18491616  9.8620121  19.8000755\n",
            " 23.69645472 30.27348657  8.08095583 27.84627644 49.46839629  9.44892066\n",
            " 27.62420036 46.23977005 18.25418352 21.03393603 10.10108942 16.12277124\n",
            " 19.79975967 21.30868405 23.08030172 18.44802028 27.28179722 15.2549046\n",
            " 14.74796016  7.96268648 11.12136882 32.49667315 16.19059362  8.95639323\n",
            "  7.96268648 49.99999988 23.61148023 34.27573875 16.51557304 20.86873621\n",
            " 38.35938185 19.43971782 10.4243387  10.77338939 10.7466899  49.75586855\n",
            " 26.05180459 14.35926245]\n",
            "selection [542 527 414 617  24 839 427 512 870 480] (10,) [5.66989377 5.86029505 5.96523561 6.4736147  6.60592766 7.37802214\n",
            " 7.44746714 7.48880485 7.5328093  7.5331102 ]\n",
            "trainset before adding uncertain samples (370, 10) (370,)\n",
            "trainset after adding uncertain samples (380, 10) (380,)\n",
            "updated train set: (380, 10) (380,) unique(labels): [136 244] [0 1]\n",
            "val set: (922, 10) (922,)\n",
            "\n",
            "Train set: (380, 10)\n",
            "Validation set: (922, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.13157894736842105, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (922,) [40.78577733 14.14845009 15.74586841 19.90398541 34.67366484 23.76588826\n",
            "  9.17263036 21.82189631 11.94695119 14.64166221 46.25013857 46.79751226\n",
            " 43.29860918 30.12842558 21.3259158  21.73158774 10.5738107  10.02581639\n",
            " 14.62967781 39.51986087 10.40470168 14.43635572 44.44252965 18.80060151\n",
            " 34.85737051 19.76372738 10.02581639 35.07725784 17.84760116 21.77538323\n",
            " 29.52873647 34.69922097 17.26571006 42.5932548  15.55358548 16.03453821\n",
            " 10.90724688 10.90670164 17.2360563  11.72190175 20.23691674 23.84442437\n",
            " 13.65755195 28.9041358   9.80269281 10.73880909 17.73370352 25.57080958\n",
            "  8.30247302 28.78093174 34.02995159 15.76831757 47.06872639 31.22510348\n",
            " 12.08526175 23.92298145 12.76388186 17.94425574 18.51389955 18.73119722\n",
            " 11.46821303 13.78846746 43.80858524 22.00079922 45.02349924 31.58336927\n",
            " 29.31479952 32.43460772 24.86020847 13.27507269 24.47985627 17.12956106\n",
            " 41.54137495 25.67697216 27.69384311 23.37215309 14.65079793 18.03956299\n",
            " 25.00412768 49.89792705 19.70732381 12.21588857 39.3623302  20.29218055\n",
            " 25.11033127 25.53595539 10.52481005 34.16794453 13.26600474 12.50792931\n",
            " 26.98353858 16.33188917 26.42762594 10.02581639 29.63910868 13.63634582\n",
            " 12.51986432 18.27628925 38.33106857 36.62893188 13.56267287 33.81021139\n",
            " 33.83241549 49.91125135 37.86792904 13.00909855 25.02741353 49.97344416\n",
            " 40.49279718 39.3900764  32.7234948  17.97562485 24.78712669 21.7844252\n",
            " 23.05462199 14.00809849 24.3047263  42.08961528 28.87423765 13.80274924\n",
            " 28.79954765 24.34810859 17.03050579 30.16244145 18.88520911 10.02581639\n",
            " 27.56015809 45.98035841 25.40403311 40.05086639 10.06480518 20.08583763\n",
            " 14.19423266 30.03778762 49.85113886 29.56044072 12.36001772 14.71685927\n",
            " 20.3333046  11.75309155 49.99798961 16.36410122 32.74857122 29.25912602\n",
            " 17.31249063 26.46490287 10.02581639 21.25691325 33.11246824 16.21894632\n",
            " 13.87363556 25.58238899 11.53412867 16.7097871  17.43231202 10.62463674\n",
            " 30.72801448 19.37630338 17.45708829 32.59613756 16.29662247 12.72429499\n",
            " 21.16718538 22.26754295 10.24853162 26.42099983 27.97160649 10.02581639\n",
            " 19.30534863 14.48555753 15.82253383 11.10686119 46.61421    14.76327423\n",
            " 22.64209708 22.37599117 15.47472285 21.43487463 18.08119373 38.60565128\n",
            " 15.37142063 28.74305781 14.92805715 49.83372422 15.56415028 25.11545025\n",
            " 18.42826016 18.63419193 17.14480832 27.21208861 18.04761719 22.64473934\n",
            " 32.49966926 39.34021313 45.51764752 26.91574717 15.02387913 12.06485184\n",
            " 30.31547666 48.34815819 10.02581639 11.71079467 33.44351826 14.14917279\n",
            " 15.75215635 35.99515978 19.28136274 27.2079054  38.278913   15.90059837\n",
            " 13.67043651 15.29426442 43.84428706 16.14664937 30.05365713 39.30054221\n",
            " 14.99589248 21.80413578 32.80897129  9.84730697 16.18233339 13.89461098\n",
            " 38.10733098 25.23886108 15.70599657 31.9438366  16.35739109 22.52131673\n",
            " 32.36551307 11.84289424 21.06104737 18.25453945 28.81499097 18.81642385\n",
            " 21.1991933  25.82974047 26.04334717 33.01458225 26.20852969 35.08368724\n",
            " 27.69871247 14.31454862 26.25321483 48.91310239 11.39238869 12.40439014\n",
            " 29.57857519 12.06176294 29.79836272 17.16685171 41.36977966 31.37555875\n",
            " 32.07102874 13.1448714  42.33895522 31.3233997  26.55442382 49.77660981\n",
            " 18.19870625 34.49533333 25.76147632 17.469651   10.73991614 24.72007206\n",
            " 21.01245574 24.23388677 10.02581639 49.98374179 10.02581639 41.81631706\n",
            " 29.23299932 43.00431282 25.6884644  10.02581639 31.84714291 43.9789748\n",
            " 27.73233689 47.59063003 12.6658368  17.76733551 24.70090398 17.93409445\n",
            " 30.13096438 17.96820378 20.18467675 22.38529721 10.9348514  42.76143854\n",
            " 49.63217371 20.74521566 19.84768196 25.09250466 11.50787473 14.45943749\n",
            " 18.465652   19.84334834 34.63584093 11.31739023 12.76849864 18.07554844\n",
            " 10.02581639 26.55845622 22.44438299 25.67697216 34.89137395 15.63850742\n",
            " 11.57969969 30.97178166 18.68947546 28.19643227 26.03599165 26.79072357\n",
            " 21.14752736 10.59499435 22.40383111 36.10661565 28.35979942 29.71295515\n",
            " 14.55839071 29.34968908 21.88210639 10.02581639 39.50647032 18.82643972\n",
            " 27.72388144 17.66251629 16.92235707 29.3308073  25.62430172 28.27754439\n",
            " 20.4087567  18.39558049 13.55956717 11.61121748 29.35585827 14.30565411\n",
            " 12.88791298 21.0165542  39.23524353 15.45944497 20.16144105 11.26148969\n",
            " 22.67724525 20.36165589 27.18716429 13.096543   27.47729429 11.78119802\n",
            " 30.4887532  25.22506869 15.10086384 33.39822152 14.50348956 17.55067579\n",
            " 26.95022363 18.407892   49.89332125 19.85700696 20.13030645 24.15089402\n",
            " 10.43752503 36.74250594 48.39711871 18.13912924 42.886539   27.30491693\n",
            " 31.92575098 10.02581639 10.12888097 10.02581639 15.42272986 22.38371705\n",
            " 24.89991453 25.94514345 20.67772706 22.4675762  27.22163262 25.26171654\n",
            " 25.94947277 18.32051709 18.29036726 12.82752173 20.63520114 19.1507875\n",
            " 33.55866327 18.55292208 48.29559921 30.9738241  27.41638962 24.80467215\n",
            " 26.597157   20.41525579 24.10029887 23.26282906 13.5678358  40.37485869\n",
            " 12.2408436  28.11222536 23.46465481 24.65334939 12.00982692 34.24608248\n",
            " 35.9095513  21.79320592 17.80439618 21.61098028 10.02581639 18.98954024\n",
            " 38.94983401 12.36751067 26.41053256  9.29464143 47.88098832 35.97983877\n",
            " 40.00950397 17.15472034 40.60634429 40.00761174 22.88228723 19.70404502\n",
            " 14.65482153 38.03551299 14.32731141 31.19878952 27.71218411 23.46711357\n",
            " 38.64760688 21.86538871 24.14375965 25.33350125 24.41881216 13.57783818\n",
            " 12.2742809  26.97167733 35.73081639 48.19480473 12.89484296 16.30928794\n",
            " 26.7347058  22.989807   28.91860485  7.01394409 18.44658854 44.59399452\n",
            " 49.99176751 25.54272776 14.12435065 12.49228842 19.28485653 27.8608788\n",
            " 15.03080321 31.66399716 16.84282491 19.65517464 48.99176833 34.2808278\n",
            " 43.44631696 10.02581639  9.91820427 31.70664356 15.02681356 10.44971694\n",
            " 18.94194472 23.25353917 10.02581639 20.97172768 18.24400661  9.41274733\n",
            " 41.25876433 37.0586008   7.61746644 24.65756694 27.71906153 27.04451842\n",
            " 19.87928188 25.47167103 14.66872022  9.13639169 19.43614486 18.9824085\n",
            " 13.72102085 15.25371946  8.25054684 19.01532654 21.73860422 13.53810748\n",
            "  8.69983219 30.001385   35.89303099 24.82987701 10.95607066 12.69298284\n",
            " 20.8269024  41.14164783 34.46145417 34.15327397 32.91610196 10.02581639\n",
            " 10.02581639 25.57613071 23.38224189 20.03175158 35.8860999  36.74582138\n",
            " 26.28000283 11.75095242 24.46597061 17.28409346 41.31937862 20.2015998\n",
            " 20.12476681 24.32231088 24.6360241  32.88246129 34.95359625 18.30565614\n",
            " 42.58683748 16.09014188 11.61121748 21.72297119 12.98812152 24.23488645\n",
            " 27.50403564 35.46470784 48.60710566 18.03686362 21.74769283 13.53810748\n",
            " 14.30250272 28.16967907 23.62092233 19.44612821 35.38983156 49.99300412\n",
            " 21.26630964 14.41749882 29.68052738 41.40839593 28.30107831 12.94885326\n",
            " 28.75199858 13.31199231 12.66106578 38.25049555 12.80087134 20.74748101\n",
            " 18.85965604 39.15880481 30.62946958 21.02901334 11.3126476  12.15351853\n",
            " 39.30778723 32.60677115 31.39936816 24.79214462 31.28628534 24.94853197\n",
            " 20.4302007  24.92337356 14.09251607 16.71550498 13.13153531 25.48296953\n",
            " 18.81741073 43.5138664  31.20224666 27.00618871 18.2771411  32.68451868\n",
            " 49.9958313  36.24370443 10.14020429 10.71093896 33.88658107 15.8816619\n",
            " 23.0326177  37.92553066 15.52716143 20.43916547 24.19975052 19.01912228\n",
            " 36.74065601 16.29576288 13.52475837 18.49320246 46.94573924 12.94508301\n",
            " 42.55412603 20.49568448 14.23709776 29.75715484 17.94249964 13.73998215\n",
            " 28.35423991 23.79779306 30.12793667 30.74404728 10.63370492 10.10651453\n",
            " 28.33590821 17.29305235 30.25188533 13.31886051 37.17630444 17.49289491\n",
            " 32.41838587 16.16670573 16.65412448 32.32760949 14.54731723 11.63267532\n",
            " 12.97838117 44.53617437 21.09111925 47.79444526 24.40483842 31.73590487\n",
            " 24.76767095 19.84334834 49.80619271 24.94634994 19.48748476 26.38172242\n",
            " 40.60023113 10.11490493 29.67363889 34.18658613 37.9321128  24.94518521\n",
            " 10.98456746 46.28858617 18.48243288 24.70607025 37.69262825 10.02581639\n",
            " 13.8318517  25.97415354 22.08983592 15.64537219 33.39029561 13.32076557\n",
            " 37.0409218  10.02581639 10.02581639 10.91481666 39.37352147 27.55408428\n",
            " 39.23878131 39.14406013 47.4187414  18.80868173  8.94684399 28.83659716\n",
            " 27.35442668  9.53811751 15.3140428  20.66772635 18.29090028 11.94102948\n",
            " 17.09923252  9.96320925 42.80481308 13.07644146 19.247467   35.24804147\n",
            " 17.40535248 10.90575784 10.57658392 13.48275158 25.2675966  28.48657441\n",
            " 25.24908772  9.71638405 37.4009719  33.9026648  28.35718189 15.9646073\n",
            " 40.73056745 13.28059365 14.33016918 11.85197249 10.1118568  49.9807579\n",
            " 47.4807707  34.29510268 22.75812416 47.5571198  28.49694222 18.67808339\n",
            " 20.37863544 21.69609818 10.63979126 34.0912421  28.4867167  26.13913114\n",
            " 10.02581639 20.9273057  13.80103295 21.6598975  25.98212525 10.02581639\n",
            " 26.95440068 16.12215354 31.90371082 13.19159071 26.54536948 11.85864403\n",
            "  9.02340429 28.66090726 23.41259324 35.48904533 13.2978514  30.97675232\n",
            " 49.90895631 21.9910898  18.63361697 39.35329407 34.65011147 24.88829238\n",
            " 40.11254546 10.02581639 25.56111444 44.17068906 22.00188606 32.7373799\n",
            " 39.92405654 10.57626745 13.12991676 22.37700246 29.77159865 24.08323166\n",
            " 36.31393707 22.10912206 48.95059454 17.53752091 17.68387461 31.84799414\n",
            " 13.11839461 26.19731253 13.86447003 10.55002614 28.54022026 15.22563628\n",
            " 34.49953521 19.43614486 39.08408367 40.8543064  30.55290735 28.4122631\n",
            "  9.80006742 31.75820713 35.79526816 23.80865383 27.01074692 38.13656842\n",
            " 39.98320927 47.82041756 15.64605094  8.2271676   9.73287066 21.75187851\n",
            " 31.78463234 49.02721379 15.19538601 11.76403713 12.34502706 25.80964922\n",
            " 45.96341946 39.30077365 22.22966612 16.51120159 20.47223199 40.40433302\n",
            " 13.13714823 48.87557262 25.56206618 46.27675398 13.22838241 32.88002534\n",
            " 29.34975845 32.80897129 13.75376227 44.68331724 43.41253155 33.60703453\n",
            " 36.81483488 27.97048968 40.12834535 23.30759669 49.11435008 15.50529466\n",
            " 15.87619176 24.4704059  24.92686088 16.22130449 10.71574014 17.97763727\n",
            " 16.45724697 33.53917927 16.6355168  28.99898525 43.60103929 34.71917193\n",
            " 25.03447392 31.55980171 20.93721276 10.02581639 47.85858413 40.16936209\n",
            " 24.53254778 15.07409227 10.02581639 16.24491365 31.23037471 15.3140428\n",
            " 30.0986826  22.12943693 27.64576654  9.4063727  28.30666977 13.89461098\n",
            " 20.2015998  13.49486481 20.1068376  28.92180374 38.38883437 15.8542869\n",
            " 13.26600474 20.04607679 10.93193455 15.74138162 30.68456285 25.04979521\n",
            " 10.22100393 16.65745432  8.61770398 14.28719603 42.4862442  26.10014124\n",
            " 24.36674623 18.38030076 20.71893193 18.89501602 15.84427167 19.76306126\n",
            " 17.66312513 27.17023235 30.18980421 10.02581639 40.75144418 17.27115743\n",
            " 31.15654559 26.36472896 29.25793525 10.02581639 26.25295529 27.60379505\n",
            " 32.10889329 35.74617073 29.52959826 38.09244924 25.09035953 29.47080168\n",
            " 24.58113625 12.303351   31.38130621 33.86211085 38.49581589 28.74741769\n",
            " 17.01273706 49.51338138  8.76362628 27.73964179 34.6665048  13.64600492\n",
            " 20.95090451 16.53905438 34.81312844 31.35644912 18.01361536 48.90453111\n",
            " 10.7583123  25.67421577 21.20855927 32.16971999  8.36650704 28.53956784\n",
            " 49.46909332 11.54913348 29.02854805 47.08454303 20.15944538 19.82634423\n",
            " 10.98134799 12.74861833 17.83231918 17.62789308 23.203151   17.68428918\n",
            " 26.44326358 13.9674762  18.52787467 10.02581639 12.18678047 30.32038443\n",
            " 14.75484626 11.54978922 10.02581639 49.99999994 17.80189913 41.70965509\n",
            " 19.87474078 23.40926469 44.04324742 22.8579583  11.18083255  9.32507593\n",
            " 12.56456422 49.81646296 24.53100324 15.53827945]\n",
            "selection [441 470 759 482  48 886 836 486 872 652] (10,) [7.01394409 7.61746644 8.2271676  8.25054684 8.30247302 8.36650704\n",
            " 8.61770398 8.69983219 8.76362628 8.94684399]\n",
            "trainset before adding uncertain samples (380, 10) (380,)\n",
            "trainset after adding uncertain samples (390, 10) (390,)\n",
            "updated train set: (390, 10) (390,) unique(labels): [139 251] [0 1]\n",
            "val set: (912, 10) (912,)\n",
            "\n",
            "Train set: (390, 10)\n",
            "Validation set: (912, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.1282051282051282, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.45      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0]\n",
            "std (912,) [40.17577715 13.9514734  15.27049257 19.03595436 36.29630421 23.75985403\n",
            "  8.62340829 21.712328   11.9002117  14.62456614 45.89550644 46.47653971\n",
            " 42.48082845 29.38075519 20.03746813 21.55813439 11.18201291 10.0319212\n",
            " 14.55701468 39.31942008  9.83254155 14.13000859 44.25307794 19.0128666\n",
            " 34.19749735 19.31547066 10.0319212  33.66732943 17.12279696 21.25102648\n",
            " 28.83512431 34.88806795 17.26801884 42.28377155 13.63611873 15.59524259\n",
            " 10.1837188  10.81755382 16.88223129 12.36717947 20.52615189 23.5650752\n",
            " 13.85710649 28.15976409  9.58884817 10.40249078 18.03072063 23.45551941\n",
            " 27.93793832 33.50337847 15.31962139 46.90165477 30.74164328 11.10009435\n",
            " 23.80721336 12.86843756 17.88001636 18.50078243 18.26047312 10.74719451\n",
            " 13.29785955 42.86757603 22.76359597 44.58576097 30.03835645 28.46006126\n",
            " 31.54840195 23.56633973 13.57324474 24.27900847 16.31838681 40.30570785\n",
            " 25.36225317 27.27509373 21.89208365 14.36853697 16.65061609 25.07389221\n",
            " 49.87518501 18.93440397 11.47705688 38.7328103  19.50771757 25.11724087\n",
            " 24.62575676  9.55309322 32.57523652 13.37255387 12.66194668 26.6572648\n",
            " 15.68842684 25.66431538 10.0319212  28.60500409 14.50291763 12.80681225\n",
            " 17.65343276 37.35098708 35.90157377 13.31895545 33.50021513 32.86680734\n",
            " 49.8974982  37.14722218 12.31839744 24.37899452 49.96837496 39.93504305\n",
            " 38.53756823 32.2934111  17.29450385 24.18165683 22.50335146 22.56980753\n",
            " 13.65894452 23.90942651 40.37539455 28.65080253 12.63054575 28.43739951\n",
            " 23.5019285  16.86770845 29.70781239 18.24810402 10.0319212  26.80958511\n",
            " 45.58514959 24.53191666 39.70944119  9.42193866 19.65694198 14.19692406\n",
            " 29.58796057 49.84633127 29.04497225 12.64495756 14.84780528 18.72683304\n",
            " 11.15068894 49.99707789 16.11620484 31.94189388 28.86433178 17.59171393\n",
            " 25.29059665 10.0319212  20.34808875 32.47570125 15.35839516 13.21919127\n",
            " 25.38526902 11.44490649 16.01382755 16.85499354 10.46833126 30.03089915\n",
            " 18.93790603 16.19041317 32.52448456 16.27292424 12.53695457 20.64549854\n",
            " 21.16641858 10.36938979 26.49862235 27.20218378 10.0319212  19.28744043\n",
            " 13.50738162 15.89694876 11.58761014 46.12024587 14.29805738 22.31696906\n",
            " 22.11587729 14.77444776 24.12761731 17.54149179 38.22964717 15.1476772\n",
            " 28.37841023 16.02016317 49.75170588 14.52491824 23.78685249 16.70636981\n",
            " 18.31338419 15.888119   27.00451788 17.2322624  21.80778785 31.71072995\n",
            " 38.96869766 45.25816797 26.16251288 14.94080181 12.00931619 29.74793467\n",
            " 48.36505737 10.0319212  12.04133085 32.88314541 14.1835457  15.60578305\n",
            " 35.14909848 18.60137027 26.45024604 37.81499853 15.55536929 13.52548388\n",
            " 14.38544879 43.58751192 16.10241518 29.77023056 38.90150854 14.39068933\n",
            " 21.97725462 32.70081527 10.00421193 15.67355107 14.44696403 37.79955156\n",
            " 25.40385323 15.89473076 31.11666643 16.50217877 22.08051689 31.59256193\n",
            " 12.19742888 19.88079781 18.12400465 27.46850013 17.67937976 21.00342101\n",
            " 23.87644685 25.56229063 32.47964867 25.26643443 33.79521104 27.0656911\n",
            " 14.01034495 25.37679921 48.80871757 11.55339568 12.50630895 28.86550635\n",
            " 12.11895408 27.90278435 16.56111344 39.84493049 31.14145037 31.07307612\n",
            " 13.53469547 41.77678101 30.02566387 26.30457164 49.7180316  17.9884986\n",
            " 31.3917756  25.53290697 15.44076511 11.05651856 24.52304509 21.25853895\n",
            " 23.16777376 10.0319212  49.97793556 10.0319212  41.60350301 29.66050716\n",
            " 42.47102344 25.18696021 10.0319212  30.80641317 43.74677229 27.0336247\n",
            " 47.25774454 12.75196554 17.21537074 23.13289755 18.1795118  29.4285463\n",
            " 16.66762858 20.96852996 21.47277302 10.32371667 42.83663236 49.59394648\n",
            " 20.76476989 19.31615627 24.57221539 12.28556753 13.45267625 17.93061525\n",
            " 18.7665768  34.48531931 11.2593321  12.30442961 17.4383932  10.0319212\n",
            " 25.62644426 21.59975947 25.36225317 33.73593918 15.5322531  11.95286133\n",
            " 30.10719396 18.63030758 27.13236539 26.10558015 26.68991736 21.29147831\n",
            "  9.37902436 21.63029915 35.24438352 27.8443204  29.27458542 14.78853748\n",
            " 28.995776   21.30312374 10.0319212  38.33078685 19.33069034 28.3461196\n",
            " 16.99314283 17.44367797 28.79516126 25.58172675 27.16832201 19.48838629\n",
            " 18.63335267 14.23031305 11.34990379 28.69807958 14.38423348 13.3624722\n",
            " 20.58973273 37.93740678 15.00515589 20.22933924 11.36208981 21.68549736\n",
            " 20.15278786 26.28059662 13.32444825 26.90809231 11.8250879  29.32029507\n",
            " 25.36683649 15.15044063 33.31907294 13.47253983 17.38750793 26.41165778\n",
            " 18.34776015 49.8701717  19.36558125 19.75611952 23.88492608 10.72848535\n",
            " 35.82718067 48.15541145 18.41631258 42.34500688 26.40686658 30.73028195\n",
            " 10.0319212   8.73134587 10.0319212  15.64013065 22.00257329 23.23755023\n",
            " 25.82340766 20.79817794 21.3778853  26.18759345 24.33262708 24.20376341\n",
            " 17.37418838 17.66266814 12.11249325 20.50368638 18.47020285 32.69293188\n",
            " 17.48743619 48.10370671 30.7432568  26.53061675 24.02133795 26.28487914\n",
            " 18.76652266 23.3278808  22.93552969 13.66223581 39.86858711 12.33535659\n",
            " 27.27221511 24.11645343 23.77348328 12.45254481 32.66357885 35.20378532\n",
            " 21.27306132 18.06388482 20.74738795 10.0319212  18.87718801 38.56643851\n",
            " 12.49655953 26.22116025  9.26664629 47.7104527  34.84908143 38.89572568\n",
            " 17.19235422 39.08532497 40.127834   23.57431156 20.48631761 14.90295187\n",
            " 37.1176636  14.84944586 30.07959818 27.22934391 22.95845471 37.9735967\n",
            " 21.96558562 23.60618583 24.82847711 22.88108434 13.04441825 13.13686895\n",
            " 27.07223345 34.91489937 48.08469677 12.4275668  15.87708475 25.04686537\n",
            " 22.06329822 28.8429382  17.38292388 44.08748398 49.98931008 25.54699324\n",
            " 13.27582591 11.98502134 19.62020453 27.25632871 15.14941378 31.04221764\n",
            " 16.04443559 19.11764206 48.85862501 33.64954169 43.08417554 10.0319212\n",
            "  9.57717031 31.07002993 14.94789146 11.09302889 18.4168304  22.69638936\n",
            " 10.0319212  19.91037815 18.11884698  9.55719589 40.63919457 36.63951492\n",
            " 23.42178529 28.18808994 25.85113013 17.46853656 25.34868196 13.58856808\n",
            "  9.11385259 19.04771408 17.98617727 13.21149728 15.33219898 18.920528\n",
            " 21.41804573 13.34475428 29.48819552 35.22992579 24.27531606 11.61917729\n",
            " 12.25898415 20.87447387 40.68690819 34.09290619 32.97561631 31.86797351\n",
            " 10.0319212  10.0319212  25.13729408 22.00034453 19.29457679 34.82861994\n",
            " 37.82930275 25.37083155 11.3117236  23.74890813 16.36241184 41.05881253\n",
            " 19.94782691 19.68448452 23.26836557 23.50788709 32.55850168 33.90202735\n",
            " 17.72165248 42.05041914 14.96472785 11.34990379 21.36495298 12.73327608\n",
            " 23.50255973 26.77610797 34.60219383 48.25991624 17.59193564 21.28619845\n",
            " 13.34475428 14.11962702 27.19038161 21.29716604 19.20368888 35.10796131\n",
            " 49.98948207 21.5635472  13.65470843 29.32360624 41.01775089 27.75642974\n",
            " 12.78788899 28.54452132 12.98034435 12.35209143 37.52936477 13.34612521\n",
            " 20.30672237 17.90411394 38.24979027 30.16263797 20.69394418 11.5234042\n",
            " 12.39979767 38.63816544 31.99725384 30.86107538 24.2468506  30.82742173\n",
            " 24.10762091 20.42850225 24.32392234 14.2684144  16.09968722 12.56780824\n",
            " 24.51598174 16.38923463 42.41045736 30.84133616 29.83512757 17.66008482\n",
            " 32.74129322 49.99399147 35.68792188 10.54406517 10.95272483 33.14849168\n",
            " 15.44051005 22.98517753 36.98164794 15.76632726 19.75876701 22.4623063\n",
            " 18.78276091 36.31036312 16.4508118  13.03941501 16.40847048 46.55770184\n",
            " 12.70517401 42.18859402 20.42442699 11.79084513 27.19270077 18.40590521\n",
            " 13.70074705 27.94849432 23.00691611 30.16295146 29.79460086 10.72153932\n",
            " 10.08459534 28.03003026 16.95131961 29.64383187 13.48833864 35.86219123\n",
            " 17.01026094 32.40660011 15.94150901 16.19033254 31.31115738 14.34785933\n",
            " 10.93794738 13.44964841 43.54811911 20.53236351 47.4862463  23.21066606\n",
            " 31.36268    24.28092276 18.7665768  49.77195478 24.75073894 18.88278883\n",
            " 26.00399894 40.07270007 10.0096916  29.02661284 33.60385237 36.93528524\n",
            " 23.16969346 10.31226645 45.95345153 18.35007451 25.48998908 36.62055329\n",
            " 10.0319212  14.17720065 25.75859351 21.75745993 15.7123055  32.63567717\n",
            " 13.63664361 35.93574645 10.0319212  10.0319212  11.23453128 39.21858806\n",
            " 27.06884078 38.4566864  38.31135154 47.11514275 17.54480224 28.11553294\n",
            " 26.62499681  8.87861161 14.64320958 20.4679204  17.33377261 12.28609977\n",
            " 16.44934152  9.45739813 42.98975864 12.57096598 19.32039169 34.45384229\n",
            " 15.79831871 10.10759041 11.03261153 13.32520332 24.28768232 28.16665888\n",
            " 24.73388591 10.10013612 36.8030137  32.68377547 27.89687545 15.14045965\n",
            " 39.77684128 12.92491643 14.33249934 12.43561129  9.46545445 49.9750359\n",
            " 47.43739409 33.98719379 21.90654025 47.40245704 27.80366736 18.65846461\n",
            " 20.17804207 21.07674264 10.64332155 34.15186625 28.43625205 25.85051824\n",
            " 10.0319212  21.36645277 14.07909569 22.22275313 25.17348267 10.0319212\n",
            " 26.44005372 16.43662525 32.16801573 12.7082675  26.20963434  8.42812794\n",
            "  9.01480416 27.22656594 22.6432287  34.91808452 13.97085292 30.29116647\n",
            " 49.89638494 22.2689579  18.52427831 37.65239601 34.08825015 23.96623046\n",
            " 39.7570109  10.0319212  25.10058705 43.57340017 21.16894462 31.04292794\n",
            " 39.93205931 10.99277248 13.04256311 22.16798769 28.80291814 24.78712984\n",
            " 36.06568691 21.50474877 48.84967944 17.12736759 17.29657969 31.40288862\n",
            " 13.01965083 25.80337842 13.74379191 10.35908932 27.94948091 16.43126467\n",
            " 33.7144467  19.04771408 38.33982408 39.09981631 28.86886622 27.81157341\n",
            " 10.71687896 31.29592469 35.4804282  22.91888785 27.2518507  37.37677436\n",
            " 40.33981269 47.64226004 14.75265514  9.40707329 21.729238   29.56503533\n",
            " 48.89650534 14.8908427  11.74014974 11.83624764 25.63517512 45.80310515\n",
            " 37.67417618 21.095781   17.48822401 20.07646027 39.09654604 12.67485399\n",
            " 48.69750039 25.44446238 46.10097828 13.54113553 32.29446832 28.80199673\n",
            " 32.70081527 14.12824551 44.26931811 42.7487218  31.20201194 36.47896298\n",
            " 27.80916957 39.5355128  23.45916483 49.03501424 15.66210065 15.05626513\n",
            " 23.92295765 24.36980442 16.68821642 10.42798257 16.97345385 16.88609384\n",
            " 32.81514812 16.15373638 28.14426779 43.06598961 35.62629407 23.53787398\n",
            " 31.64699247 20.71142762 10.0319212  47.21416177 39.25452582 24.37184469\n",
            " 15.09232879 10.0319212  15.87580292 30.75635918 14.64320958 29.17095218\n",
            " 21.64317705 27.12811007  9.05307368 28.23049033 14.44696403 19.94782691\n",
            " 14.10245685 19.84261213 27.95459988 37.76821446 15.45364362 13.37255387\n",
            " 19.80067269 10.73122732 15.18079574 30.62777951 24.60477204 10.63914162\n",
            " 17.55578476 14.02438539 41.9328427  25.70396353 23.12695509 18.07592347\n",
            " 20.74949798 17.14602381 15.60256491 18.98230486 17.26679959 26.07800564\n",
            " 29.60616586 10.0319212  39.52111345 17.7071405  32.25538888 25.65341454\n",
            " 28.59292445 10.0319212  25.54550392 26.69164393 31.53569269 35.31772476\n",
            " 28.56483857 37.33825674 25.6217204  28.53376362 23.36746223 11.41581223\n",
            " 31.35698325 32.38266005 37.68159233 26.54275917 16.64847692 49.48640053\n",
            " 26.93210639 34.06382513 11.83748122 20.46312577 16.13666351 34.81779811\n",
            " 31.28042897 16.68559801 48.7663745  11.13378918 25.38289692 21.08721036\n",
            " 31.76586431 28.73624073 49.42663383 11.85649368 28.35127942 46.88864018\n",
            " 19.23715332 19.37539799  9.98023172 13.33576966 16.54991916 18.74344087\n",
            " 22.48598191 17.28760339 26.06569778 13.73034508 17.95103652 10.0319212\n",
            " 11.54739723 28.92953381 15.09645881 10.9664175  10.0319212  49.99999969\n",
            " 18.1622491  41.09174207 19.89523968 22.34970582 43.54224332 21.2469431\n",
            " 11.32011107  8.7599721  12.20966245 49.79263701 24.27720491 15.59083494]\n",
            "selection [701   6 367 907 649 702 812 474 410 312] (10,) [8.42812794 8.62340829 8.73134587 8.7599721  8.87861161 9.01480416\n",
            " 9.05307368 9.11385259 9.26664629 9.37902436]\n",
            "trainset before adding uncertain samples (390, 10) (390,)\n",
            "trainset after adding uncertain samples (400, 10) (400,)\n",
            "updated train set: (400, 10) (400,) unique(labels): [145 255] [0 1]\n",
            "val set: (902, 10) (902,)\n",
            "\n",
            "Train set: (400, 10)\n",
            "Validation set: (902, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.125, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.43      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (902,) [40.60962241 13.72691407 14.94857554 16.39012208 13.38245786 23.22496676\n",
            " 19.6102166  10.77078214 15.11707792 45.56016508 46.57391659 41.20550777\n",
            " 25.46312607 16.44110139 22.18133503 11.4680969  10.43351007 15.20284128\n",
            " 36.10668705  8.27353599 13.9543168  44.55162062 17.68275977 31.5969557\n",
            " 19.40826894 10.43351007 30.65482308 14.78284392 20.38951082 26.37233338\n",
            " 35.45917813 17.23891514 42.37206548  8.3437818  13.87507275  7.7649486\n",
            " 11.53901309 18.24025507 11.56014257 21.99898502 24.02090292 13.05081599\n",
            " 27.93655712  9.67663176 10.43591116 17.3106966  19.53263999 24.58210646\n",
            " 31.28630684 13.19787601 46.12446425 27.28861028  8.46028371 22.51962448\n",
            " 11.18401738 19.05052843 16.37415381 18.27810515  7.89651248 12.06460374\n",
            " 39.81536078 23.63742717 44.30575032 26.28393335 25.91898228 31.71199128\n",
            " 20.17727256 11.57633853 23.68134418 15.8871216  36.34438991 27.33510725\n",
            " 24.99981803 16.08373562 12.40304322 17.46330952 25.24440608 49.83949625\n",
            " 17.82260908  9.91251319 38.71976298 20.32168331 22.5272557  24.05745055\n",
            "  7.62852301 28.85509421 12.1878802  12.04315517 24.77966921 15.3197649\n",
            " 26.97321439 10.43351007 25.88637895 16.31277062 14.07707209 20.79939601\n",
            " 37.21002996 37.21333646 14.38295437 33.85679749 32.66879933 49.85513686\n",
            " 36.98659926 12.69492849 21.89295744 49.95389113 40.23260402 35.76400828\n",
            " 32.64213446 16.3866633  23.58256158 20.9299661  21.53395203 13.50873014\n",
            " 24.60183504 35.73846293 28.37708419  7.9082327  28.22169573 20.99025698\n",
            " 17.68794667 29.28895077 18.3611798  10.43351007 24.49628549 44.98148722\n",
            " 21.88338183 38.74229043  6.928476   18.23034859 13.24289323 27.041044\n",
            " 49.58599184 27.50732026 12.45901431 13.85831348 14.73740203  8.42516178\n",
            " 49.99798019 15.18222364 32.55376364 26.23985976 16.37119653 24.66795197\n",
            " 10.43351007 19.52916603 31.98559095 14.51755314 10.08207124 23.46454709\n",
            " 12.19797709 16.18147431 15.37187173  9.11358522 29.61699163 20.45414794\n",
            " 16.32973298 29.57722897 15.25216888 12.26282998 19.79634034 17.0198421\n",
            "  9.66366653 27.49682863 28.41712405 10.43351007 18.76150823 11.10074902\n",
            " 17.06033137 10.80907733 45.38436494 14.16347388 22.530644   22.41024725\n",
            " 12.59023247 28.44539114 16.20181571 36.73200984 13.71802591 27.71555013\n",
            " 19.60593744 49.28072733 15.11514258 22.39353401 12.61198517 19.47811911\n",
            " 16.4930991  27.79443219 17.64282092 20.95432592 32.05561664 37.36660987\n",
            " 45.09408711 24.21632796 15.86213794 11.22022889 29.19098526 48.38599144\n",
            " 10.43351007 12.43964536 31.79092769 11.82947286 14.56999682 35.69016171\n",
            " 19.30630022 23.55490669 36.00689246 15.13275455 13.65332976 13.93451627\n",
            " 44.46870528 15.6442496  30.31703113 36.84935468 12.49823627 20.83810086\n",
            " 28.24650761  7.77218749 17.14077306 14.83598935 37.29561711 26.79947639\n",
            " 15.404835   29.71628091 15.26785178 20.74130338 31.13079891 10.9296428\n",
            " 15.58012083 16.94964563 23.37933274 12.46025343 22.1612962  19.62592781\n",
            " 24.42328951 31.795406   23.68952216 30.00536347 25.22304844 11.34181547\n",
            " 23.12432387 48.50523454  9.62652433 14.06527875 26.98248844 10.63098112\n",
            " 23.73386269 14.81320972 35.40070587 30.56201802 31.91234884 12.88489075\n",
            " 42.57820529 26.31277516 22.85747086 49.8085134  18.07349307 24.93473345\n",
            " 25.06439797 15.49671438  9.40661347 21.52548745 20.83599495 23.38327669\n",
            " 10.43351007 49.9844614  10.43351007 39.34391187 30.78898562 41.30732297\n",
            " 22.47582228 10.43351007 28.9212869  39.51243504 27.40141062 47.23599027\n",
            " 13.00470205 16.92984632 19.50786104 19.80011448 30.32079522 15.10246131\n",
            " 19.69788563 21.26429295  9.74667236 42.8853714  49.37760252 21.49985758\n",
            " 17.82492324 24.11978106 12.26913965 13.27349616 14.77305622 17.4560114\n",
            " 28.72194288 12.22621776 15.15179633 17.87804062 10.43351007 24.07574524\n",
            " 19.63436291 27.33510725 29.6619725  15.8951774  10.14633205 26.90122171\n",
            " 21.35458742 24.529679   26.75733082 26.81572265 16.84258235 18.79125058\n",
            " 34.17747253 27.33183745 27.00654537 13.45947397 28.84405062 18.04945007\n",
            " 10.43351007 35.86385301 19.25332721 30.52394026 17.13680178 17.0867\n",
            " 28.47270384 24.47025677 23.74628433 16.37903726 19.27207291 13.12143189\n",
            " 10.05493822 28.8534637  13.15363785 15.67764983 22.94133306 34.74026764\n",
            " 14.15809785 17.12837984 10.52377111 18.38789604 18.07016972 23.37739459\n",
            " 12.33584467 25.30721651  9.57152513 26.02799507 24.51234825 16.75367527\n",
            " 32.07668755 12.99766936 15.89025329 25.5705263  19.22326269 49.86624848\n",
            " 17.4399729  20.26112374 19.01769117 11.3488058  34.53513743 47.35102627\n",
            " 18.98109428 40.10656593 23.37423928 26.42702582 10.43351007 10.43351007\n",
            " 14.40899765 23.62070333 21.23588949 26.88325024 19.6827674  20.87937455\n",
            " 22.40466777 24.45325237 21.29856772 14.14294668 17.13261538 11.38416414\n",
            " 19.00486236 17.27993413 29.80844115 15.16227045 47.75087622 28.58523192\n",
            " 28.40390852 21.80336574 23.37063681 15.51189063 20.61268865 19.7921019\n",
            " 11.92238461 39.51227228 13.58644057 25.23525193 21.88527589 22.40247607\n",
            " 12.50659563 29.71363895 35.12200333 18.70756112 16.91371814 17.99446672\n",
            " 10.43351007 17.43700711 36.05730898 11.42885623 26.24336305 47.04870328\n",
            " 31.83069295 37.69777738 18.79527272 35.91564689 38.97130259 24.57109514\n",
            " 21.71741343 15.29292249 33.72326105 14.33426886 27.74157174 27.05259737\n",
            " 19.79196284 37.97439779 21.71767886 23.02508355 23.61512297 21.35786574\n",
            " 12.8704172  13.73445679 27.95840102 34.70074069 47.30365677 10.13460819\n",
            " 12.88403763 22.97530399 20.36694476 28.47237409 14.37807794 44.36849262\n",
            " 49.9884944  24.64234153 12.67488014 10.47749113 19.33967216 28.07558628\n",
            " 13.41186517 31.37729005 14.21243698 18.10240782 48.92535752 27.50991502\n",
            " 43.94154216 10.43351007  6.27985254 31.87186674 13.39216028 10.79544219\n",
            " 18.71958797 22.81458634 10.43351007 18.71047973 17.77374056  8.11541375\n",
            " 39.97526381 36.45760749 23.14706445 29.28211139 24.56243979 15.53199425\n",
            " 23.96282797 12.13746992 19.77715441 14.46791085 11.75874979 14.79390133\n",
            " 18.45318881 18.89583115 14.28374476 28.37663214 35.18768171 22.58884926\n",
            " 11.26746296 11.03626653 22.05898032 39.23653706 33.31256041 29.58932031\n",
            " 28.63362401 10.43351007 10.43351007 24.90624381 18.25824068 19.59744336\n",
            " 32.7573306  28.75434926 22.82108217  7.53588372 21.00814727 16.21123995\n",
            " 41.83257098 18.94812724 19.02262941 22.39256291 23.5178405  32.0037485\n",
            " 29.59192256 16.09394209 39.88975055 16.07373536 10.05493822 21.95829295\n",
            " 10.15234911 21.76926038 25.63787377 32.22208987 47.58813585 15.5161838\n",
            " 18.58949713 14.28374476 12.34050201 26.03119475 18.2813078  19.04225895\n",
            " 34.24988126 49.99440449 23.69797697 12.76436762 26.93386483 38.57160832\n",
            " 26.67351547 11.89414905 29.98707267 11.5800274  10.49105815 35.43384697\n",
            " 14.44445837 17.23336347 16.16995869 38.64037433 29.58743048 19.21065141\n",
            " 10.38130814  9.27856192 38.40322468 30.28377039 29.51634733 23.91025013\n",
            " 28.43034742 22.89390095 23.45409578 21.75003741 13.18251519 15.2630933\n",
            " 11.88531285 19.8606241   8.2985245  38.3552332  29.96958869 35.13511251\n",
            " 18.35509221 32.92452071 49.99504273 35.34082808  8.84953999 10.8582174\n",
            " 33.8999544  14.59300393 20.36303059 34.65702966 14.84683631 19.18791974\n",
            " 18.0471949  17.23397329 34.39408218 16.11748539 13.15941422 11.57449112\n",
            " 47.19270813 11.72813412 41.04572317 17.75107764  4.25183882 22.29270999\n",
            " 17.11857043 12.46852645 27.48822282 18.88890741 31.01538456 30.63853502\n",
            "  9.54847555  9.76619958 27.14347936 14.73419854 27.5777112  11.87801973\n",
            " 31.97576052 15.72371197 31.89306429 15.39458037 16.35371175 27.4298607\n",
            " 14.48077177  9.68483858 15.86416429 40.92450292 18.60891965 47.41306028\n",
            " 24.30937221 31.81912649 21.42007796 17.4560114  49.71491222 24.79839197\n",
            " 15.98631744 23.53364424 39.02226664 10.44362998 26.92065845 32.59381575\n",
            " 33.89852836 18.92780529  9.75245013 46.0652661  16.73893227 19.81007805\n",
            " 32.32886876 10.43351007 16.3731095  24.0993541  22.57283771 14.96366537\n",
            " 33.64621681 14.10733868 35.03584856 10.43351007 10.43351007  9.12277944\n",
            " 36.45849202 25.60413297 38.26652795 36.5180277  46.70363572 15.74955355\n",
            " 28.71522968 25.49539904 15.07756847 23.4019551  16.29380193 10.41356494\n",
            " 16.28641609  5.59065383 44.28869716 13.2231805  18.74073863 33.59390511\n",
            " 13.25371865  9.03952954  9.70670275 12.12768315 22.04302217 29.74914216\n",
            " 21.33167734 10.05830127 36.88681635 34.40475261 27.66147753 15.94195482\n",
            " 36.40514105 14.28378026 15.46890962 13.50821819  8.64611302 49.97274179\n",
            " 45.50493267 34.60654029 20.42867428 47.27671467 27.59484286 17.88824327\n",
            " 17.83370793 22.60472196 11.75868354 35.11141061 29.61039359 24.00799569\n",
            " 10.43351007 22.28558855 14.57208466 23.81125555 23.82389586 10.43351007\n",
            " 25.81509154 15.77041772 34.66784393  9.69859874 24.98439031 23.63568912\n",
            " 21.388019   32.5966656  13.5644036  31.38457054 49.8480141  23.00542436\n",
            " 18.10575834 35.0622585  32.07081327 20.80436143 40.33920277 10.43351007\n",
            " 24.05336772 43.87460654 20.50029264 26.87670404 38.93625579 11.6284066\n",
            " 11.31772108 23.46474252 28.16159132 22.84092987 35.84314072 20.36308372\n",
            " 48.70799238 15.20206545 16.87431898 30.30757576 12.36384941 25.40187992\n",
            " 14.40289145  9.85401685 26.20965879 16.62360664 31.36458799 19.77715441\n",
            " 35.66935389 36.05838757 23.17346672 27.58494126 11.59538723 30.33423712\n",
            " 35.37657749 22.12352455 25.32030301 35.88458682 37.41134959 46.61188516\n",
            " 14.67600989  8.05750865 14.81477669 26.14591961 48.82979039 14.04800809\n",
            " 13.1136775  11.98991272 23.91272986 44.30770862 32.74582082 20.58172478\n",
            " 19.07529662 18.33022378 36.6167143  11.95441778 48.69552721 20.6561459\n",
            " 44.55226209 12.70444783 34.49371607 26.1312782  28.24650761 12.49942204\n",
            " 42.40184968 42.4912285  26.77008006 35.73007276 26.00099145 39.10830131\n",
            " 24.60665987 48.69335469 15.19374455 13.94039344 23.78217022 23.26385131\n",
            " 16.33092131 10.0562469  17.42708661 17.20496246 31.98706951 15.29006576\n",
            " 28.72898657 42.96764976 36.10137138 20.3372674  30.49310151 19.27458136\n",
            " 10.43351007 45.2108512  36.24691058 24.81586044 11.85037378 10.43351007\n",
            " 17.12702024 29.2181839  15.07756847 26.62177704 20.13796603 26.09629262\n",
            " 29.5759294  14.83598935 18.94812724 12.61988036 20.10062851 26.35704006\n",
            " 36.17220848 14.86105416 12.1878802  19.09357526 10.7942577  14.6226286\n",
            " 32.13462148 22.35977786  9.65089517 18.93148613 14.40094656 39.62702189\n",
            " 23.95192267 18.77152897 18.3691768  20.76827071 16.83237149 14.63600634\n",
            " 17.70224459 17.42120892 24.37909687 28.96789978 10.43351007 36.67861395\n",
            " 16.26944756 32.80821285 23.14232141 25.47078032 10.43351007 24.31248715\n",
            " 25.52599128 30.27496151 32.89776088 25.52263039 35.23693104 24.97996384\n",
            " 28.89207994 22.10386073 10.2743503  32.20499755 29.37920089 35.97475717\n",
            " 22.84868421 15.77681456 48.94195441 24.77293436 34.16622435 11.36286659\n",
            " 19.19886274 15.45844231 31.46039164 26.20697461 14.51576824 48.91294576\n",
            " 11.81057477 26.89272249 18.59093903 30.72401895 29.74234746 49.00995239\n",
            " 11.27306653 26.85734237 45.98458917 18.60971325 16.04856872  5.19805314\n",
            " 11.03853572 12.6809701  17.22288384 19.57560541 14.40771949 24.54650381\n",
            " 12.21197248 18.50569329 10.43351007 10.92220489 24.80750156 13.72665747\n",
            " 11.04154888 10.43351007 49.99998442 15.42661017 41.42090361 20.44786667\n",
            " 21.03900283 43.65758802 20.3057711  11.44782173 12.82624511 49.63343765\n",
            " 23.40567344 15.24860382]\n",
            "selection [580 875 649 452 128 495  84  35 217  58] (10,) [4.25183882 5.19805314 5.59065383 6.27985254 6.928476   7.53588372\n",
            " 7.62852301 7.7649486  7.77218749 7.89651248]\n",
            "trainset before adding uncertain samples (400, 10) (400,)\n",
            "trainset after adding uncertain samples (410, 10) (410,)\n",
            "updated train set: (410, 10) (410,) unique(labels): [148 262] [0 1]\n",
            "val set: (892, 10) (892,)\n",
            "\n",
            "Train set: (410, 10)\n",
            "Validation set: (892, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.12195121951219512, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0]\n",
            "std (892,) [39.02805072 11.56282541 13.39402    16.67767667 17.70280149 22.28998796\n",
            " 18.48205907  9.62990401 14.26178313 44.46303193 45.3024319  37.85506327\n",
            " 23.10453906 13.71952278 20.96210504 12.1957769   9.98565394 14.40031628\n",
            " 35.16330835  6.25285262 13.29729006 43.80751459 16.35144511 28.94054158\n",
            " 16.99707636  9.98565394 26.39004672 12.78496405 18.52843996 24.3263945\n",
            " 34.36908161 15.35256692 41.05969824  4.75283017 12.51137773 10.2368555\n",
            " 17.20067035 12.04420512 20.41751233 22.16071856 11.42153842 24.51224865\n",
            "  9.62141978  8.99678511 16.25271233 15.45514998 21.97442927 29.97758466\n",
            " 12.04022461 45.33562335 24.90320604  7.70988482 20.565393   10.76306362\n",
            " 17.46892027 15.08307183 16.32826149 10.52500639 36.09588898 22.41487935\n",
            " 42.89017658 22.81307872 23.15560371 30.13933867 15.26030907 11.21845513\n",
            " 21.98298038 13.68489436 32.48660639 25.48607979 22.91457337 12.40776877\n",
            " 11.35415173 17.33907023 22.7771239  49.74562742 16.45489077  9.13147921\n",
            " 36.58209164 18.69719174 21.1915454  21.12715283 25.01668311 12.02496081\n",
            " 11.1526863  24.00884664 13.5439963  25.42053672  9.98565394 23.06614563\n",
            " 14.87555971 13.07344627 19.40173694 35.0486828  35.57201166 12.81932396\n",
            " 31.72677312 30.34416347 49.78414075 34.68606633 11.35671935 19.52885188\n",
            " 49.92364072 38.34956203 32.19906413 29.81796473 14.60004284 21.04752667\n",
            " 18.83301882 19.43002305 12.36965607 22.47589784 30.83832849 26.89952063\n",
            "  5.08712786 26.65910211 19.05155442 15.24632936 27.33920638 16.53806318\n",
            "  9.98565394 21.97153878 43.44047745 19.13656525 37.64946883 16.50985068\n",
            " 11.58908735 26.60650821 49.50300499 26.53836044 11.55680618 13.05520091\n",
            " 11.44742642  6.91631865 49.99506912 13.81120424 30.94228034 24.35586346\n",
            " 15.64829253 22.51917276  9.98565394 17.30817896 30.21803689 11.97239364\n",
            "  8.97884538 21.86722042 10.39067563 14.77971431 13.94249741  8.43503922\n",
            " 27.77975439 18.57125183 13.45910626 28.46952348 13.9821666  11.34656287\n",
            " 18.05802584 14.23419901  9.43851961 26.38226979 26.36020363  9.98565394\n",
            " 17.74365221 10.30289401 17.44646882 10.89220815 43.19238032 12.44900434\n",
            " 21.00481831 20.77987241 10.43142906 27.27402527 15.04101624 35.13511888\n",
            " 12.17193701 26.10642464 19.43780121 48.19870857 13.28237508 19.28493191\n",
            "  8.37604124 18.43135013 14.49253275 25.66012939 14.8249111  19.83642429\n",
            " 30.42473593 36.11674028 43.96260715 21.19388054 14.85856511 10.02535828\n",
            " 26.91362066 47.80694241  9.98565394 11.92374302 29.45683098 11.1934565\n",
            " 12.87069449 33.55465247 17.73620743 21.26503289 34.88963168 13.88959941\n",
            " 12.82443111 11.27138978 42.67380089 13.92227465 28.71818328 35.54662127\n",
            " 10.72621532 19.16227334 27.4045063  15.75284203 14.34893608 35.415179\n",
            " 25.03103003 14.25634241 27.32627686 15.20223448 18.79324772 28.96845786\n",
            " 10.7282155  12.5185249  15.20729639 20.22428465  9.60717706 20.16377206\n",
            " 16.06555982 22.02890226 30.38103874 20.88536547 26.47308106 24.07811427\n",
            " 10.67961069 20.71568568 48.07805678  9.63000136 13.01965903 24.17790778\n",
            " 10.68915076 19.5945904  12.68021921 30.14044985 29.10717425 30.01631591\n",
            " 12.20334767 41.30755439 23.589713   22.46015842 49.65523292 16.70730881\n",
            " 18.44540648 22.27782297 12.3258716   9.00950892 19.80559191 19.59867431\n",
            " 21.53044587  9.98565394 49.96759652  9.98565394 38.55455537 29.65121651\n",
            " 40.50649539 20.39489805  9.98565394 25.96848233 38.77191671 25.46647312\n",
            " 46.39007416 11.95796586 15.77926155 16.58376003 19.20215033 26.97018446\n",
            " 13.05690909 19.15611166 19.35160693  8.74897799 41.23074724 49.16000336\n",
            " 20.21045325 15.65799248 22.35709258 11.86854636 10.73573648 13.39742893\n",
            " 15.17704347 28.14448915 11.35915516 13.0722471  15.86039947  9.98565394\n",
            " 21.12331942 16.54811711 25.48607979 26.26183653 14.36371172 10.01695684\n",
            " 25.44019434 19.80855153 21.73414376 25.342465   25.54641107 17.24847113\n",
            " 16.3236261  31.98194716 25.44564576 24.48715579 11.84702755 26.53121474\n",
            " 15.84456934  9.98565394 32.48055175 17.62393274 29.5555138  14.90118979\n",
            " 16.66789835 26.91290537 22.25048405 21.19287842 14.07005342 18.32035207\n",
            " 12.91245858  8.71635169 26.8366102  12.28200342 13.2239778  21.12184593\n",
            " 30.45768763 13.09295343 17.31109465 10.55401927 16.21643639 16.6755609\n",
            " 20.24873753 11.65906091 22.54569713  9.84238575 22.56578752 22.37825219\n",
            " 15.16086295 30.25013545 10.84067328 14.03756897 23.48564767 17.75406018\n",
            " 49.78252151 16.13320535 18.67260336 18.60836391 10.83470633 30.70904665\n",
            " 46.00703369 18.12774257 38.85656348 20.23521977 22.97845885  9.98565394\n",
            "  9.98565394 13.78326981 21.06057041 18.30783536 25.02542322 17.98852972\n",
            " 18.23103691 19.54250865 21.88419521 17.41784222 12.06821957 15.94885386\n",
            " 10.55325962 17.62230746 15.7447826  26.71080187 13.72206539 47.05515832\n",
            " 26.5718493  25.88858226 19.11107638 22.16101932 12.86290323 18.6958582\n",
            " 18.56927424 11.34779807 37.72370848 11.4189902  23.3360772  19.92941079\n",
            " 19.79448174 11.80595323 25.6799331  32.40716753 17.77134929 15.41847187\n",
            " 15.77780748  9.98565394 16.23306184 34.85323107 10.69883802 24.61886121\n",
            " 46.39788418 28.2861537  35.22800719 16.4896162  31.34005058 38.63863639\n",
            " 23.10955236 21.1092516  14.48774153 30.73185738 13.70305799 23.11699581\n",
            " 24.46748807 18.45088203 36.1357562  19.92920993 21.03459762 21.29997975\n",
            " 18.31740342 11.65890171 12.28646867 27.10603798 32.68762196 46.69202584\n",
            "  8.8920851  11.83083718 20.05892339 18.00029679 26.62008123 11.45567429\n",
            " 43.05824632 49.97736861 22.23528302 11.19640135  8.95349677 17.53882718\n",
            " 26.63939934 12.76510581 29.38902179 12.59373346 16.31273238 48.48766531\n",
            " 26.18675602 42.66461588  9.98565394 29.62814021 12.72953264 10.33279015\n",
            " 17.90851652 21.87143462  9.98565394 17.0543461  17.22572852  7.51192153\n",
            " 38.3197987  34.15261746 21.27933649 28.17180182 21.58125949 11.45757461\n",
            " 22.11664251 10.39488367 18.11790365 12.87342165 10.53104011 13.67729577\n",
            " 17.70996289 17.25580877 12.82687167 26.42086766 33.55022346 20.62442127\n",
            " 10.28383697  9.87708157 20.26391952 36.83666652 31.23018042 25.47550868\n",
            " 25.37576157  9.98565394  9.98565394 22.78489393 14.63424323 17.72514118\n",
            " 29.41210778 29.44033921 20.31820546 20.17024292 14.71908361 40.23743378\n",
            " 17.48656305 17.87260822 21.17646953 20.46618291 30.22619329 25.85892337\n",
            " 14.59613513 37.74931869 13.39357635  8.71635169 20.20989051  9.64329952\n",
            " 18.94620934 23.50876739 29.16645984 45.88056719 14.8683484  17.23015876\n",
            " 12.82687167 10.4692461  23.27862314 15.04079372 17.25548246 31.81192298\n",
            " 49.98581672 22.52680938 10.25060643 25.68617717 37.81573702 24.52409048\n",
            " 10.61794249 28.08349559  9.79930437  9.04721821 31.82850324 14.11412236\n",
            " 15.50970285 13.7867942  36.66271131 27.54698765 16.63834475  9.03713253\n",
            "  9.88940686 36.19308853 27.7582627  26.67424341 22.84859254 25.95452351\n",
            " 20.63057841 21.17836447 19.85761239 12.43113683 13.19711547 10.2215844\n",
            " 17.1269638   3.67880205 34.47058831 28.02605492 35.74136328 16.73624227\n",
            " 31.61073217 49.98878588 33.91162519  8.81161994  9.8757473  31.83163591\n",
            " 12.83195183 18.69497993 31.18184029 13.9723553  17.29130099 14.1265953\n",
            " 15.55964324 33.43067122 15.25369911 11.53259544  8.41899125 46.06300297\n",
            " 10.45375489 39.56513076 16.28791419 17.81474992 16.49460365 11.96698127\n",
            " 25.92347345 16.30007851 29.36263794 27.82258278  9.06125469  8.71197018\n",
            " 24.88282043 13.96453659 25.10420075 11.75936199 28.27206303 13.50832953\n",
            " 30.29273707 14.70201161 14.83280659 24.19791354 12.58075575  8.20531791\n",
            " 14.10001998 36.94416861 16.8014317  46.65693712 21.82714178 29.83029228\n",
            " 20.09722289 15.17704347 49.576859   23.18462879 14.18264101 20.9133806\n",
            " 36.39659028 10.03872357 24.95491612 31.16625681 30.40451941 16.2294063\n",
            "  7.89897397 45.02296368 15.61648884 15.83698778 28.63189428  9.98565394\n",
            " 15.44153697 22.66501509 20.79108416 14.05357785 31.15766658 12.55949062\n",
            " 32.12981823  9.98565394  9.98565394  6.59130435 35.4626303  22.86668519\n",
            " 36.08654384 33.50280615 45.68868659 14.84137374 26.33348793 23.19117601\n",
            " 13.74877002 22.16142712 13.85657341 10.23396614 14.70196956 42.97468929\n",
            " 11.74898546 16.43165454 30.96022783 10.92653737  7.45994724  9.3067624\n",
            " 11.29677548 20.22267565 27.70296829 19.19476898 10.51954808 35.15581055\n",
            " 32.96574009 25.31270019 13.30330141 32.8025794  13.38452721 14.10010599\n",
            " 13.30786368  7.31137391 49.95022432 45.11276089 32.65590058 18.27326658\n",
            " 46.49049792 24.68944346 16.00099026 16.56528434 20.14214594 11.44900569\n",
            " 33.2483266  27.64989179 21.97436804  9.98565394 21.28587953 13.16513028\n",
            " 22.51212039 22.23635373  9.98565394 23.57878773 14.48337038 32.02943746\n",
            "  8.63860696 22.80437847 20.54380983 18.64915793 30.39493059 13.19570782\n",
            " 29.11789247 49.77531068 21.2238838  16.56989206 30.17566348 29.30934518\n",
            " 19.15885289 38.2353588   9.98565394 21.67872376 42.42646918 18.48373196\n",
            " 23.30113217 38.77102087 10.44074996 10.29349201 21.02311096 25.71556504\n",
            " 22.50930647 34.25411686 18.52622388 48.21615432 13.30620924 15.56698916\n",
            " 28.26117116 10.68019221 23.36366726 13.91760948  9.93364727 24.36911738\n",
            " 15.75507737 28.56566614 18.11790365 32.79802429 30.74446319 18.95995022\n",
            " 25.56131047  8.92409451 28.3322052  33.25084838 20.79101785 23.60883271\n",
            " 33.16095195 38.16755015 45.94146119 11.44523996  6.31778108 14.79060682\n",
            " 22.24723109 48.38475503 12.16929224 10.94068166 10.65581938 22.27603804\n",
            " 43.40137424 27.70438524 18.33796117 18.34913764 16.57616239 33.01194525\n",
            " 10.20677729 48.23222717 20.48220738 43.95020223 12.27195107 32.51167102\n",
            " 24.34536835 27.4045063  12.11299533 41.18657921 39.61623492 22.20633055\n",
            " 33.48521702 23.54108097 37.44198127 23.61365883 48.32433711 14.34161662\n",
            " 12.55298814 21.9754533  21.02022003 14.82584624  9.46453153 16.3789457\n",
            " 16.30359763 29.12792015 14.15580126 26.72880028 41.56655033 35.07392679\n",
            " 17.49234965 30.26808004 17.77057401  9.98565394 42.13893461 33.03175114\n",
            " 23.40413404 11.29564414  9.98565394 15.22752375 27.04029494 13.74877002\n",
            " 23.91009227 18.27894374 24.6586798  27.92551635 14.34893608 17.48656305\n",
            " 12.42475391 18.5598093  23.92849573 33.48321739 11.94398875 12.02496081\n",
            " 16.88553702  9.56599959 13.08098815 30.06006482 20.72202055  9.5563898\n",
            " 18.57143369 13.03250319 33.79589265 21.51160279 15.9120153  17.53156026\n",
            " 19.18611651 14.25756803 13.58030086 15.79403431 16.02589966 20.92105748\n",
            " 26.86365055  9.98565394 32.83162577 15.55835383 30.59072448 20.59295506\n",
            " 23.37073928  9.98565394 22.25763476 22.60439499 28.83255821 30.4102769\n",
            " 22.71293432 32.22596036 22.89830694 25.9441138  19.14119805  8.39693599\n",
            " 31.06616949 25.4470766  33.31117615 17.83147462 14.6619752  48.75674605\n",
            " 21.18919538 31.32358335  9.31866962 17.47925528 14.03559245 31.03570624\n",
            " 25.68062742 11.96892159 48.51555076 11.642529   24.27767171 15.98734241\n",
            " 28.77808273 28.02083764 48.79889461 11.55396468 25.23876866 45.07715992\n",
            " 16.02834047 14.40854353 10.34060837  9.21608685 17.34629676 17.55279589\n",
            " 13.37105744 22.88075155 10.90631265 16.79652987  9.98565394  9.52034801\n",
            " 21.0431767  12.37852861 10.21011041  9.98565394 49.9996115  16.01557371\n",
            " 39.43558009 19.32082473 19.02718814 41.88714556 17.84582027 11.20519688\n",
            " 11.49808006 49.50220229 21.13686378 13.74722546]\n",
            "selection [547  33 114  19 736 627 133 661 646 455] (10,) [3.67880205 4.75283017 5.08712786 6.25285262 6.31778108 6.59130435\n",
            " 6.91631865 7.31137391 7.45994724 7.51192153]\n",
            "trainset before adding uncertain samples (410, 10) (410,)\n",
            "trainset after adding uncertain samples (420, 10) (420,)\n",
            "updated train set: (420, 10) (420,) unique(labels): [153 267] [0 1]\n",
            "val set: (882, 10) (882,)\n",
            "\n",
            "Train set: (420, 10)\n",
            "Validation set: (882, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.11904761904761904, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (882,) [36.73094898  7.7851196  10.20097752 16.56938137 29.61625109 20.00014657\n",
            " 15.29479255  7.64831981 13.86431495 42.08029678 42.7749996  30.97054332\n",
            " 17.16270758  6.85700502 19.02681303 12.06059844  9.92500763 14.82398246\n",
            " 34.54038591 12.15002676 42.36182637 14.13278089 23.08845838 12.79757525\n",
            "  9.92500763 18.63559844  9.38414824 14.0938818  22.26146044 31.86098271\n",
            " 12.88405771 38.57760569 10.3129934   8.27114271 16.50390122 11.35092685\n",
            " 19.14181446 19.82251503 10.38072345 18.41208255  8.96020404  7.18497093\n",
            " 14.02585016  8.11349442 16.30252011 28.84751792  8.70396366 44.60705192\n",
            " 19.01627522  4.68799967 17.35858942  9.10195056 16.15490357 12.2822608\n",
            " 14.30020384  6.43210696 26.88664945 20.67973023 40.42152856 15.65679244\n",
            " 17.29863715 26.31518104  8.6855029   9.61158305 19.44485793 11.4043548\n",
            " 22.60813396 22.75023509 18.44914801  5.33108178  9.65411159 16.05871269\n",
            " 18.80130047 49.5143906  13.68607084  7.01367192 32.68685219 16.40946279\n",
            " 17.38551609 14.91405642 16.36869755  9.54967704  9.92642197 22.88577033\n",
            " 10.65599801 22.21528334  9.92500763 17.03717444 14.48068486 12.3932939\n",
            " 17.94962487 31.18346356 32.49900925 12.229349   27.75349756 26.47806764\n",
            " 49.68041346 30.77731589  9.44573166 15.09078248 49.86749242 33.65941656\n",
            " 24.89512824 26.17809863 11.56257919 17.20645025 15.92471813 15.89652986\n",
            " 11.24414239 19.82833763 18.70482013 24.98714144 23.61116588 15.21946423\n",
            " 12.04239118 24.54286477 14.16473266  9.92500763 17.13828348 40.83840293\n",
            " 14.13311446 36.14144735 13.38072125  9.04565858 26.50060131 49.50184526\n",
            " 24.7621357   9.65986837 11.39996638  4.68673731 49.98236909 11.57777889\n",
            " 28.13508973 19.61449593 13.67877799 18.58207287  9.92500763 13.6656236\n",
            " 26.61638092  7.85144535  6.05328474 18.12003453 10.13173866 13.07610725\n",
            " 11.07904671  8.03089953 24.39483352 16.50196967  9.75769661 27.68020271\n",
            " 11.86113196  9.81800202 16.50217845  8.6325278   8.47794064 24.42899775\n",
            " 23.11068121  9.92500763 15.56520009  7.64725373 16.20753067 10.20844371\n",
            " 37.1408167  10.31146768 18.83656747 18.59931686  6.2770015  27.12103136\n",
            " 12.83463217 33.65513718  9.45099983 22.1822778  20.3907352  41.58700233\n",
            " 10.66416984 13.8361404   1.976583   17.58739594 11.62057165 23.36609778\n",
            " 10.52284428 17.35751034 26.53678608 34.61735831 42.00577364 15.95991455\n",
            " 13.91992458  7.83706855 24.09402241 46.23388098  9.92500763 11.221338\n",
            " 25.96162444  9.10418411  9.19279172 29.7424843  16.20248159 16.19525391\n",
            " 33.44030305 12.31470686 11.81343285  7.26830431 39.03889424 10.80081405\n",
            " 25.54660029 33.87811835  8.00359568 16.42019868 28.53571098 14.26487334\n",
            " 13.66247276 31.56887968 23.32182188 12.58247223 22.30632883 13.45554062\n",
            " 15.23779079 25.1118424   9.64541739  5.61999137 12.28097918 13.04392072\n",
            "  2.71763029 18.58963221  9.04750625 16.95806331 27.23394775 16.19995741\n",
            " 18.8477696  23.83675266  9.89441468 15.63778625 47.3883889   8.31529721\n",
            " 11.82121512 19.04575997  9.47891104 11.79782401  9.5077271  18.20287456\n",
            " 27.30618123 25.57861268 10.8713143  39.09288127 16.88050004 22.51183202\n",
            " 49.17960275 15.14692166  5.85519471 17.32917762  8.07986054  7.7457812\n",
            " 15.54444791 17.02599055 17.02618249  9.92500763 49.89798349  9.92500763\n",
            " 38.18202673 26.50591518 38.68430607 15.93407717  9.92500763 20.15139518\n",
            " 38.24930421 21.74141918 44.54131094 10.8401717  13.83100685 10.36682782\n",
            " 18.41017113 21.93769209  9.4192564  17.24200247 16.93957681  7.70807691\n",
            " 37.36594678 48.85417282 18.78113698 11.63169468 20.59327988 11.34624885\n",
            "  4.45271233  9.84617837 10.81325878 29.85778184 10.26624471  9.45818561\n",
            " 13.29363299  9.92500763 16.22425952 10.44947541 22.75023509 18.72909844\n",
            " 11.45406848  8.80842902 23.05868949 16.16947387 16.20210958 23.09017147\n",
            " 22.77697748 19.27481003 11.23396301 28.63930455 22.02031129 19.68218437\n",
            "  9.08529703 23.56501139 11.7175683   9.92500763 24.83662301 14.99852468\n",
            " 28.30550169 10.47636041 19.26144036 24.1636474  18.67410645 15.43193477\n",
            "  9.19996865 14.18924277 11.88666172  6.16923    23.58816726 10.48598901\n",
            " 11.67435921 18.35740494 21.19565293 10.73572498 18.54293215  9.61783362\n",
            " 11.54796643 13.4682137  13.11606001 10.38302039 17.19868009 12.20933129\n",
            " 15.81504492 17.67776985 14.56477115 26.15234612  8.13656558 11.0750662\n",
            " 20.13411167 16.54293012 49.57472058 12.90757883 16.5608859  19.25593979\n",
            "  9.97153246 23.04643091 41.09008488 16.71289586 37.10193902 14.13065962\n",
            " 15.79339919  9.92500763  9.92500763 11.86050915 18.54946198 12.94325576\n",
            " 22.47464726 15.34473986 14.46932292 13.79303851 18.65355659 10.16838722\n",
            "  7.9852392  12.69691096  9.91025064 14.79775622 12.25926266 20.07819824\n",
            " 11.28413575 45.77891314 22.07864045 22.65953716 14.68254376 21.77009237\n",
            "  7.68630239 14.2847394  18.38932513  9.65770408 35.11790264  9.74382402\n",
            " 20.7062024  16.59386528 16.03401788 10.35879745 18.14680092 27.29099786\n",
            " 17.56386518 13.19924546 12.1128031   9.92500763 13.61396393 33.44400772\n",
            "  9.43664918 23.59515649 45.45714849 21.05245865 30.4744615  15.61171394\n",
            " 21.71902391 39.43123913 21.6077     19.40917776 14.04296894 23.43427419\n",
            " 12.51632696 13.26088841 19.28776269 14.15567418 32.52943083 16.45676242\n",
            " 18.02434903 17.16474502 13.39913796 10.23534512 10.34272605 25.25843107\n",
            " 29.3911028  45.94193121  5.95003063  8.7115308  13.95196961 13.78823215\n",
            " 22.57316828  7.1832218  40.04941479 49.94145754 18.35547727  8.25634108\n",
            "  4.85768923 14.76547059 23.10577004 10.77559399 25.28551033  9.87764548\n",
            " 13.40389435 47.69639054 25.1872127  39.14176247  9.92500763 25.13721817\n",
            " 10.66882436  9.95966992 16.16097142 20.06773323  9.92500763 14.71661103\n",
            " 17.7607716  35.38856192 30.24619011 17.33062805 24.74493677 16.47068006\n",
            "  5.8201154  18.58579207  7.85771027 14.5088767   8.83078396  8.29226591\n",
            " 10.87219875 17.15665716 13.59314972  9.40126667 22.73686398 31.14484982\n",
            " 16.66784357  9.45798032  8.59934721 18.97813516 31.25908354 27.17275184\n",
            " 17.44045365 18.79864027  9.92500763  9.92500763 18.9981189   7.75556379\n",
            " 14.9512889  22.93202067 34.96133651 15.186308   18.67328941 13.07761127\n",
            " 36.53509005 14.70063375 16.84890304 18.2078039  16.40177157 26.43937902\n",
            " 18.19375079 11.66004977 34.01060833 10.45893958  6.16923    17.10376895\n",
            "  7.49099923 12.81884541 19.63629896 22.16677456 40.01765625 13.03797566\n",
            " 13.50863691  9.40126667  7.47995911 18.53547146  8.67146277 14.41622072\n",
            " 27.25393786 49.93427853 21.52107434  6.39601142 23.80295459 37.24364262\n",
            " 20.32148872  8.11651857 25.18808484  7.50479918  6.85260401 25.34595361\n",
            " 13.00403488 11.53966247 11.46735055 32.64500586 23.90552858 12.32416002\n",
            "  6.86497903  8.43396475 31.4271742  22.76199879 22.65512152 20.58749553\n",
            " 20.96251466 17.04312689 17.79873552 15.49958412 10.79079818 11.25783038\n",
            "  8.35035455 11.22601389 24.11316651 24.54751225 37.52449925 12.28545227\n",
            " 28.83065987 49.96336052 31.50024681  7.71993641  7.74827107 28.45264315\n",
            "  9.70823432 14.42503616 24.14532082 12.2298668  14.00474827  6.87330195\n",
            " 13.2886553  32.28037736 13.60020765 10.28663671  1.08106384 42.80291962\n",
            "  8.68567944 37.00356566 13.02054774  9.09252616 14.65895931  9.93464215\n",
            " 23.68413038 11.16026733 28.03592983 23.14347175  8.10459327  5.98165682\n",
            " 19.99740795 11.95467427 20.17214507 10.30663998 19.99492041  9.95645374\n",
            " 26.62597146 13.36705507 12.9688211  17.39233477  9.46760333  6.1541687\n",
            " 12.95664138 27.49686224 13.14598296 44.7767987  18.41375349 26.79437703\n",
            " 18.59812746 10.81325878 49.26767588 20.55291073 10.48508018 16.40651621\n",
            " 30.91114618  9.68150552 18.66722617 29.54908686 22.99315807 10.2073634\n",
            "  5.44624526 42.77660222 13.0513579   9.64247421 20.19203531  9.92500763\n",
            " 15.6492474  18.96487896 18.75906513 12.20252169 27.08136624 10.83959873\n",
            " 26.69626559  9.92500763  9.92500763 35.27863841 18.37245255 32.24314195\n",
            " 27.19791016 43.5653798  11.01236539 21.50106829 19.52033874 11.27769854\n",
            " 20.08205864 11.07451506  8.92689455 14.31518633 40.66750167 10.09771376\n",
            " 13.22621766 26.38080452  6.76821564  9.49712386  8.82017707 17.00963184\n",
            " 24.76076533 14.40577762 10.352622   32.64105009 29.55603453 20.46989238\n",
            "  7.51042229 24.35164443 11.49686109 13.21854569 12.66783433 49.8805987\n",
            " 45.24382397 29.21823059 14.48834465 44.91581627 19.08876762 12.80121739\n",
            " 13.39056073 17.42949353 10.83353804 30.6228489  25.33514996 18.10833317\n",
            "  9.92500763 18.81540642 11.63526352 20.0865617  19.40055097  9.92500763\n",
            " 20.16572687 12.60782744 28.26621592  6.02210201 19.03218516 14.95418768\n",
            " 15.28401731 27.64058556 12.30533231 25.01171558 49.66036757 17.30645902\n",
            " 14.25420235 20.75518417 24.4879351  16.91729663 35.40314908  9.92500763\n",
            " 17.59041435 39.51140216 15.55626248 15.34739091 39.30031436 10.85138989\n",
            "  8.19669683 19.28831272 20.47712741 24.6962126  30.4392629  14.97776155\n",
            " 47.42957102 10.34463598 13.66464026 24.45657214  7.69087505 20.58283268\n",
            " 12.59584122  9.04152748 20.38832405 15.00034197 22.30521055 14.5088767\n",
            " 25.89371958 19.6449275  10.02221503 22.69205382  7.79259912 24.59771783\n",
            " 29.62804615 16.95047353 20.09824516 26.21568563 40.67632198 45.0381598\n",
            "  5.14107982 17.55546162 13.98501424 47.44316305  8.68375481  8.68173756\n",
            "  9.68080061 18.57896735 42.57497149 16.9291465  14.65151313 17.77068441\n",
            " 13.27550789 24.75720777  7.09071875 47.16181506 20.52605625 43.45112477\n",
            " 11.10325677 28.79673758 22.04306962 28.53571098 10.61814084 39.57151142\n",
            " 33.55899947 12.77460214 29.91109793 21.19827178 34.48457511 22.81163706\n",
            " 47.74924875 12.72350153 10.27179692 20.07885616 17.17401609 12.45816055\n",
            "  8.28119729 13.95381849 13.93191217 24.11482541 11.47001061 22.78211228\n",
            " 39.07069057 32.46460549 11.80139423 30.71643474 16.07536841  9.92500763\n",
            " 32.16926427 25.43787285 20.90457953 11.39733152  9.92500763 12.16957473\n",
            " 22.92202593 11.27769854 17.94178938 14.78534827 20.62975313 25.66039913\n",
            " 13.66247276 14.70063375 11.19215628 16.75229339 19.24967658 27.80040366\n",
            "  6.09102894  9.54967704 13.20281035  7.27264027  9.58639602 26.33061288\n",
            " 16.79172818  8.81557052 17.41405618 12.0343874  23.28013245 17.30250461\n",
            " 10.13257978 15.60721301 15.92711033 10.41265497 11.56934361 12.28363944\n",
            " 14.38858271 14.8597325  22.91894099  9.92500763 23.75265627 13.61389421\n",
            " 27.56934786 15.12120596 21.43063093  9.92500763 19.56112051 17.69063986\n",
            " 26.45531593 24.66636591 16.96601116 25.21696641 19.17283623 21.12710137\n",
            " 14.20695881  5.80222906 28.43453672 17.92577345 29.40215626  9.60269368\n",
            " 11.916853   48.67756165 15.61682411 26.12787614  6.26343607 14.16549472\n",
            " 11.32565249 32.23501922 25.58535568  8.32602883 47.56859764 11.23151136\n",
            " 20.58359849 11.8997144  25.57829833 24.85920828 48.61082956  9.7703899\n",
            " 22.91032675 43.45677252 13.13516036 10.67555061  8.06731291  1.88731108\n",
            " 16.00421523 13.13760399 10.04894959 21.68167777  8.56223807 14.99388702\n",
            "  9.92500763  7.25502088 13.75222238  9.59763928  8.36369462  9.92500763\n",
            " 49.88432443 19.63913532 35.1360938  17.8138908  14.90682524 38.69910552\n",
            " 13.35402073 10.39864655 10.79844816 49.32177284 18.0895173  11.46251433]\n",
            "selection [562 857 176 222 282 129  49 432 726  69] (10,) [1.08106384 1.88731108 1.976583   2.71763029 4.45271233 4.68673731\n",
            " 4.68799967 4.85768923 5.14107982 5.33108178]\n",
            "trainset before adding uncertain samples (420, 10) (420,)\n",
            "trainset after adding uncertain samples (430, 10) (430,)\n",
            "updated train set: (430, 10) (430,) unique(labels): [154 276] [0 1]\n",
            "val set: (872, 10) (872,)\n",
            "\n",
            "Train set: (430, 10)\n",
            "Validation set: (872, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(C=0.11627906976744186, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (872,) [34.42145516 14.52314673 13.12141112 15.94952276 28.65641424 20.40066708\n",
            " 19.14073596 11.17301551 14.59356919 42.36188715 42.6031034  38.66166667\n",
            " 23.51052215 14.25588898 19.63184284 12.76035655  8.69817547 12.56899072\n",
            " 34.12270853 11.22448186 41.79108885 17.11289679 29.80554745 16.61278459\n",
            "  8.69817547 26.66327278 12.80282106 18.01548957 23.89055859 34.01209259\n",
            " 15.8016312  39.55435139 11.24814267  9.93286536 15.09667031 12.54659059\n",
            " 18.28415654 21.82300031 13.63836706 26.40385109  9.27801517 10.46283543\n",
            " 16.57309642 14.87745198 22.43008117 27.19464097 13.48884148 43.32902339\n",
            " 25.22657089 21.28306192 10.8245523  16.75589671 15.48004584 14.98457834\n",
            " 11.32695949 36.67554677 25.01418239 40.10644305 23.17847185 23.40167948\n",
            " 29.01809227 14.67226672 11.44533889 21.18922268 13.22838799 32.51174582\n",
            " 23.59010509 23.5513722  13.61451788 15.39513996 24.82956862 49.28860253\n",
            " 15.02890753  8.64397189 33.24093112 16.16804557 21.86977151 20.21623947\n",
            " 24.83969862 11.8008398  11.60056642 23.61661625 13.36226749 23.39580572\n",
            "  8.69817547 22.94261809 15.95163163 13.34970451 17.18509925 31.49141589\n",
            " 32.12113471 12.13025589 30.40080966 25.58767825 49.37835184 32.75042691\n",
            " 13.2687155  20.15045957 49.74692637 34.85537565 32.94843739 28.56807277\n",
            " 14.82772907 21.7019349  20.70179208 20.30086349 11.56565671 21.33058579\n",
            " 30.53212539 26.14360916 25.46225594 16.49728397 16.60381521 26.31466515\n",
            " 14.80143375  8.69817547 22.28360653 41.05050232 19.1406863  36.19473744\n",
            " 16.93341581 11.46076194 23.11735241 49.23351839 26.20575447 11.64272621\n",
            " 13.60659811 49.95152752 13.75864853 26.52211626 25.18260705 16.34377395\n",
            " 20.21315697  8.69817547 17.50092252 29.24116372 11.30766912  8.70842678\n",
            " 22.62872644  9.99283959 12.57848278 14.34299642 10.82737662 26.49704937\n",
            " 15.85788506 13.21933163 29.42730256 14.45939619 11.78359846 15.97739641\n",
            " 13.88750632  9.48154554 25.29599443 24.73592176  8.69817547 18.63166415\n",
            " 10.14962514 19.82980514 11.40098445 43.47353871 12.78884209 19.81202112\n",
            " 20.83521968 11.35503511 30.01496195 13.66577661 33.3455698  13.79297908\n",
            " 23.9467987  18.92592307 48.30706559 13.04185687 19.15782997 16.4510889\n",
            " 14.80275356 24.42031927 13.19632224 17.53772059 29.15106758 34.47807659\n",
            " 41.80163293 21.47158757 14.47618975  9.87371623 25.98474146 48.01331753\n",
            "  8.69817547 12.45311433 28.75967699 11.27960544 14.44138362 29.24539298\n",
            " 15.48348331 21.83473781 29.48851826 12.95792665 12.28825675 12.22643224\n",
            " 40.6062026  15.24611953 26.40150637 33.74933838 10.7871829  20.0890456\n",
            " 25.59313599 16.2993055  15.31969137 33.87541337 25.45071257 14.89854593\n",
            " 28.28966292 17.26971638 19.31434788 27.73443429 11.18299512 14.65565004\n",
            " 15.47454397 19.89239861 19.63550755 15.68789056 22.07102361 29.64194599\n",
            " 21.54956066 26.91977255 19.98984717 13.5108775  20.9550538  46.75511021\n",
            "  9.73137069 15.03611846 24.80320323 11.24479508 19.42720176 12.62600147\n",
            " 32.11462541 28.97884379 28.39344602 12.67673205 37.61726158 23.85904911\n",
            " 22.82962582 48.69715561 15.46906083 16.72885112 23.83667255 11.41747722\n",
            "  8.97611123 20.11002425 20.52230833 18.28539886  8.69817547 49.81392629\n",
            "  8.69817547 35.68729154 27.94901984 37.31728913 21.06732812  8.69817547\n",
            " 26.43595597 39.3155952  23.48303609 44.0366793  12.10358032 13.61940048\n",
            " 16.33604458 18.10213991 24.50052009 13.37663779 21.39475676 17.20386066\n",
            "  7.4816329  40.72669843 48.51578727 19.33204861 17.14958516 21.12465597\n",
            " 12.49587214 13.52036753 15.04921569 27.13867125 11.37055245 14.1542102\n",
            " 16.29253293  8.69817547 21.37432961 18.46317227 23.59010509 26.61986627\n",
            " 15.87614079 10.25547769 25.12907638 19.44029138 19.03656741 24.76649897\n",
            " 23.16871729 17.81568031 16.27811466 28.14956153 24.33121976 25.3751478\n",
            " 13.34915312 25.15831637 16.0647124   8.69817547 33.02692746 19.32150486\n",
            " 29.71570131 14.16079591 16.71775493 25.76111125 24.3981849  21.50777179\n",
            " 13.69678849 17.23557036 13.61238216  8.24577352 25.48593788 12.684164\n",
            " 14.88480717 18.29696992 30.6256646  13.20728252 17.17126153 10.84850769\n",
            " 16.25371782 17.10167269 21.38034068 12.17244342 22.82862306  8.70618781\n",
            " 24.90652091 24.55445346 15.58987322 31.46709955 10.63162657 14.10321561\n",
            " 21.67106222 17.23259906 49.37451345 16.55689349 17.77513124 18.8527104\n",
            " 11.17377052 28.47997397 46.38073349 18.91394643 35.39954793 20.22737055\n",
            " 23.37541966  8.69817547  8.69817547 14.02479709 18.66340783 18.32946789\n",
            " 24.61367227 18.73326465 18.50308572 19.75077001 19.71441794 21.53503543\n",
            " 11.8438599  17.55244046  7.75531972 18.33776025 15.69652816 27.12015596\n",
            " 10.63962766 45.55993816 27.63684635 24.13950791 19.4910547  22.24549903\n",
            " 12.53672579 19.15392802 18.07546548 11.5473701  35.61616807 12.99264713\n",
            " 22.23983253 21.90843162 19.84829178 11.92981224 26.0019146  31.14421652\n",
            " 17.00377206 15.71574429 14.70098325  8.69817547 16.81881317 33.12242828\n",
            " 12.65610512 21.81156023 44.85705497 28.99766942 31.9918673  18.19309515\n",
            " 31.82203073 36.17916212 24.6495666  23.25557348 13.47730313 31.36320164\n",
            " 14.40312788 25.8071923  24.32036463 18.6307816  32.86986905 20.39776042\n",
            " 20.54617057 22.00907659 18.20305107 10.6700512  13.31771328 25.84192928\n",
            " 28.64895317 45.46534305  9.88220094 11.77683589 19.8475262  18.59935717\n",
            " 27.39268681 11.31687554 39.77824239 49.89909097 24.12419538 10.81720809\n",
            " 18.03687451 24.68701624 13.20322288 27.79503634 11.61787111 18.39713367\n",
            " 46.54980674 27.66458898 40.60220499  8.69817547 27.61679939 13.04172391\n",
            " 10.94797872 15.4917228  20.42234022  8.69817547 14.82278825 16.20948306\n",
            " 36.3090887  33.08956126 18.44461567 29.12554035 21.70884404 10.62356787\n",
            " 22.98933676  9.65236549 17.88597601 12.98082163 10.62782885 13.20436389\n",
            " 15.9638685  17.51633833 11.87957637 25.63296379 30.24033395 21.1055018\n",
            " 12.43827755 12.12623072 20.65802675 37.83500678 29.70936048 25.52955707\n",
            " 26.01535875  8.69817547  8.69817547 23.58754414 16.21822335 15.70132013\n",
            " 30.16929672 35.10777996 20.69340435 19.0078644  11.99174554 37.46782385\n",
            " 18.09624266 16.13541025 19.04614146 21.08864956 28.47135408 26.29200329\n",
            " 14.92152535 35.77220794 12.71399204  8.24577352 22.33155867  9.80668494\n",
            " 19.83848143 22.00713923 29.42459238 45.98089304 14.63288375 17.48440892\n",
            " 11.87957637 11.49048233 23.67563013 14.59532277 17.56982567 32.60518902\n",
            " 49.88425294 20.70380448 10.93890404 25.32134025 34.54761383 25.01315512\n",
            " 10.38777126 28.9134639  11.38573101  9.11226389 32.72107266 14.51414632\n",
            " 15.57350034 12.77041382 33.27453287 27.33940622 17.92961255 10.5798047\n",
            " 10.11868635 32.35149742 28.62250871 26.68784326 21.2125599  26.94784336\n",
            " 19.85223395 17.42375536 20.33913117 12.81133244 12.41167612 12.15344058\n",
            " 16.98402276 34.75478785 27.64841694 37.19707076 15.73546882 30.82185478\n",
            " 49.91765463 30.66151551  9.00071989 10.8110825  27.14464205 12.50802952\n",
            " 20.24027298 31.87515844 14.48099148 17.5576609  12.9901217  17.96841342\n",
            " 31.47227327 15.89688463 11.13400965 42.18582192 10.61460481 38.05876425\n",
            " 16.76769329 17.28607697 17.31504375 12.13042715 24.96829943 16.32402228\n",
            " 29.18977304 25.76444092 10.92879356 10.13472256 25.10431172 16.29854606\n",
            " 26.01051524 12.1954429  28.6688245  13.27594843 31.79890287 12.79458831\n",
            " 13.71374201 24.56820726 12.05762019  9.89037702 16.21235769 37.47791871\n",
            " 17.19174975 44.43266839 22.40645005 28.75042873 19.12312911 15.04921569\n",
            " 49.01338355 22.40492145 14.17774656 21.60541711 37.31459902  8.66481085\n",
            " 24.86282389 27.94031492 31.03730322 16.46737131  9.14045838 42.42392255\n",
            " 16.14968798 16.43589148 29.02577625  8.69817547 14.92144405 23.46217046\n",
            " 19.75697029 14.57045987 25.73919394 12.85938796 27.38001905  8.69817547\n",
            "  8.69817547 34.72493386 23.56308681 32.88273135 34.3143899  42.90137688\n",
            " 16.69067082 23.73361498 22.20721498 13.8404701  19.60349854 13.88429733\n",
            " 10.52220558 12.81377308 43.30327478 11.99229022 18.11809927 28.4863143\n",
            " 10.59213739 12.26171603 11.03317667 17.98584969 26.27410115 19.63813466\n",
            " 10.55201207 32.23202537 28.6800752  25.52311962 11.39435607 33.21697735\n",
            " 13.08990155 13.31785921 15.57821052 49.80771425 43.95952422 31.76778487\n",
            " 18.7028767  45.45654982 24.84654945 16.18170131 17.18265554 17.52531228\n",
            " 11.58956631 33.05766656 26.99235396 22.79634796  8.69817547 22.00493042\n",
            " 15.07984561 24.59125167 20.47422187  8.69817547 23.43079564 14.73198468\n",
            " 30.24407179  8.37803648 23.75517952 18.07827855 18.09647101 30.57428631\n",
            " 13.84092326 27.42359306 49.46948107 24.9042692  16.99663148 29.9244659\n",
            " 28.1564975  17.65754189 37.23157616  8.69817547 22.02581284 38.98710169\n",
            " 17.10875883 23.39722649 36.16339312 13.20889409 10.31052173 21.1079268\n",
            " 25.96687188 23.6741034  33.45444204 19.15589504 46.85560209 13.68342848\n",
            " 14.75792496 26.38887733 11.73695925 23.15186785 14.0962765  12.0406748\n",
            " 23.10543382 17.93065729 29.06496369 17.88597601 33.34705288 30.74981934\n",
            " 20.61596603 24.15357225 10.17207724 26.9145915  32.25449121 20.41559823\n",
            " 25.60200954 31.3931669  34.37076045 44.29044686 16.13981186 21.79961264\n",
            " 47.03179673 11.57676061 12.15675304  9.04920383 23.3590822  42.17049836\n",
            " 27.67498309 19.04809573 19.35734903 16.95752003 33.47554299 11.32995765\n",
            " 46.65931678 20.60332692 41.47137226 12.7730804  27.46037428 23.66229931\n",
            " 25.59313599 12.52757172 38.78830584 39.89829044 21.95619309 33.01213829\n",
            " 26.94374015 35.02696748 22.60940852 47.28672012 14.88077362 10.76750789\n",
            " 20.5218359  21.69424726 16.53152239  9.53953578 13.28067928 16.46048722\n",
            " 29.94039339 14.19856962 24.82901958 38.08781814 37.23312037 17.29684768\n",
            " 27.77080881 16.13315501  8.69817547 42.43344834 33.79243213 22.48246795\n",
            " 14.87656019  8.69817547 14.78676262 25.8237185  13.8404701  24.14308046\n",
            " 18.64771867 25.15924007 27.09280917 15.31969137 18.09624266 12.93528374\n",
            " 17.71286596 21.92703245 34.35873599 15.32699746 11.8008398  16.62816805\n",
            " 10.61259154 12.35431468 28.16126495 21.26805075  9.88480954 18.7038086\n",
            " 12.93656187 33.42273611 22.14133663 15.86747881 15.56834313 21.20121601\n",
            " 14.20644192 14.00745725 15.77671177 14.79613336 20.97761554 26.21823619\n",
            "  8.69817547 33.12545195 16.25332294 32.54361774 20.67634124 22.9879622\n",
            "  8.69817547 20.63853517 22.74651349 28.58503348 31.26877666 22.9207176\n",
            " 32.59807599 24.53133322 23.44556524 15.66304263  8.40272708 28.62367697\n",
            " 25.59282279 29.7097437  16.96151393 14.95810096 48.05625582 21.28354049\n",
            " 29.54013238  8.85099249 17.71701147 14.22936006 29.95523345 26.84792263\n",
            " 11.61710402 46.76671987 12.24406082 21.53386657 17.7510805  27.80212104\n",
            " 28.62971866 47.97674883 11.21683062 24.45105745 43.8500652  14.99374365\n",
            " 14.62005545 11.56815163 19.63738339 17.80992553 13.21776139 22.26936296\n",
            " 10.72087497 15.31688517  8.69817547  9.10095274 25.65764801 13.8155649\n",
            " 11.91460764  8.69817547 49.99965523 14.41872882 36.54873131 18.73262987\n",
            " 19.3990144  38.07215853 17.78051332 11.62644746  9.86143825 49.02532551\n",
            " 23.60129966 12.84486333]\n",
            "selection [270 362 315 489 667 820  73 593 435 611] (10,) [7.4816329  7.75531972 8.24577352 8.24577352 8.37803648 8.40272708\n",
            " 8.64397189 8.66481085 8.69817547 8.69817547]\n",
            "trainset before adding uncertain samples (430, 10) (430,)\n",
            "trainset after adding uncertain samples (440, 10) (440,)\n",
            "updated train set: (440, 10) (440,) unique(labels): [158 282] [0 1]\n",
            "val set: (862, 10) (862,)\n",
            "\n",
            "Train set: (440, 10)\n",
            "Validation set: (862, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.11363636363636363, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.43      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (862,) [36.93434027 15.1471737  13.92659245 17.36996908 31.82323286 21.93362965\n",
            " 20.31623352 11.92322537 16.01363671 44.12135345 44.35441418 40.2282743\n",
            " 25.14865471 15.27666378 21.41590026 13.50152759  9.74854478 14.25940442\n",
            " 36.79749677 12.68366147 43.26674112 17.80949396 31.45005133 17.79603442\n",
            "  9.74854478 28.68642195 13.79768037 19.59162037 26.27804759 35.7585296\n",
            " 16.71090178 41.32642131 12.26350856 10.40927963 16.9111402  13.26620828\n",
            " 19.88585988 23.75470293 14.66361923 27.99803885 10.28807763 11.39798181\n",
            " 17.71918419 16.37892909 24.0639815  29.93602363 13.86433603 45.15502503\n",
            " 26.9979668  22.37821224 11.66911271 18.0767685  16.39196995 16.3574041\n",
            " 12.01212416 38.45346554 26.04561324 42.24777938 24.94022774 25.12716505\n",
            " 30.87704103 16.49418884 12.12190924 22.74358538 14.88732895 34.60311619\n",
            " 25.56518003 24.95326648 14.69476387 16.08496599 26.11740904 49.62544648\n",
            " 16.98907177 35.78124519 17.54581899 23.18548694 21.67534195 26.67816069\n",
            " 12.40819192 12.10769275 25.8602258  14.39615312 24.7505667   9.74854478\n",
            " 24.80818888 16.65625967 14.36687044 18.56355049 33.93703319 33.94130241\n",
            " 13.49734946 32.40193321 28.16862625 49.6848294  34.8918298  13.9180849\n",
            " 21.42554503 49.8854726  36.86713437 34.59494422 30.42747    16.10775867\n",
            " 23.25840876 21.31977258 21.59792224 12.97440288 23.03602647 32.89356307\n",
            " 28.12046909 27.60156787 18.34106048 17.88543332 28.45172146 16.40616352\n",
            "  9.74854478 23.82614656 43.03222331 20.53013873 38.38295547 18.12984514\n",
            " 12.56901168 25.94128197 49.60192018 28.42228584 12.34211563 14.34930206\n",
            " 49.98234453 14.86383528 28.74441717 26.76319322 17.32288249 22.32018049\n",
            "  9.74854478 19.13061727 31.20683152 12.53691546  9.56712004 24.05035759\n",
            " 10.82364963 14.35182817 15.36910665 11.68949874 28.42799823 17.57367562\n",
            " 14.60383391 31.86089352 15.37936607 12.64999324 17.74237306 15.03655165\n",
            " 10.05128084 27.23891639 26.7035393   9.74854478 19.69795663 10.98666128\n",
            " 20.0269048  11.84276829 44.78879871 13.76884658 21.68039354 22.77741179\n",
            " 12.0651756  31.24007414 15.23565577 35.65232499 14.77621963 25.88375039\n",
            " 20.36595544 48.8731701  14.54333669 21.0401806  18.22664286 16.2082132\n",
            " 26.39330795 14.6066395  19.22887023 30.91223098 36.99807658 43.69617013\n",
            " 22.93767598 16.05779299 10.34412709 28.19924307 48.50006478  9.74854478\n",
            " 12.99189061 31.02106302 11.98368811 14.94512818 31.79405965 17.43313534\n",
            " 23.33026889 32.72924706 14.40530924 13.78913763 13.33817741 42.19515439\n",
            " 16.18095417 28.25781374 36.29133114 11.50955584 21.15568258 28.5747286\n",
            " 17.46910368 15.84827917 35.64169525 27.15726509 15.65533251 30.03091862\n",
            " 17.81840775 20.6141558  29.81212765 11.71710189 15.58082598 16.39869356\n",
            " 21.46164204 21.29760897 17.41748196 23.26553021 31.52433086 22.9900035\n",
            " 28.72443047 22.77251434 14.72376556 22.50635949 47.8644312  10.3013411\n",
            " 16.10797384 26.42774526 11.66460911 21.34926322 13.55762552 34.19843455\n",
            " 31.00351567 30.14056435 13.55600153 39.7785435  25.62800726 25.14711205\n",
            " 49.24025868 16.94330591 19.90340841 25.05854876 13.30066773  9.52535153\n",
            " 21.63817943 21.55360644 20.27989289  9.74854478 49.9158747   9.74854478\n",
            " 38.57173334 29.11669019 39.83995032 22.32922114  9.74854478 28.29793416\n",
            " 41.20278446 25.80226132 45.60767125 13.06242387 15.2829007  17.89258706\n",
            " 19.7540696  26.67505804 14.58800169 22.56145154 19.22293586 42.00995036\n",
            " 49.12720981 20.95130178 18.32277075 23.20720784 13.09065015 14.588187\n",
            " 16.37051419 30.25791734 12.24553039 14.72657899 17.59546234  9.74854478\n",
            " 22.98554159 19.50105414 25.56518003 28.37941146 16.99188677 10.7416175\n",
            " 27.35490858 20.15898674 21.04331935 26.43859834 24.77251863 20.1318511\n",
            " 17.39071655 30.69773484 26.41888973 26.74807779 13.52611913 27.33755245\n",
            " 16.98552427  9.74854478 35.02483346 20.59740696 31.47378898 15.15925328\n",
            " 18.75351337 27.70439033 25.89600404 23.08809281 14.87827005 18.60427976\n",
            " 14.20124406 27.66001838 13.41226363 15.49198932 20.1442558  32.84398282\n",
            " 14.485037   19.38324127 11.5036184  17.58402312 18.29474933 22.53541001\n",
            " 12.6806909  24.22038362 10.71438179 26.45581163 25.76999818 17.1227216\n",
            " 32.98128121 11.76456235 15.35876834 23.44303661 18.85119111 49.6643685\n",
            " 17.72475063 19.54328273 21.23578051 11.80655564 30.34823193 47.27421618\n",
            " 20.08599346 38.24620973 21.53840069 25.07133431  9.74854478  9.74854478\n",
            " 14.96109973 20.41804942 20.1977941  26.44929535 19.83174599 20.1209226\n",
            " 21.24492721 21.87511313 22.81602926 12.9317744  18.20607907 19.4383778\n",
            " 16.97425012 28.75582938 12.78585511 46.85601651 29.07202651 25.77833616\n",
            " 20.68153963 24.64425294 13.7876162  20.57142957 20.35952742 12.2516875\n",
            " 37.88752067 13.65716123 24.57408973 22.86704289 21.87139965 12.37375141\n",
            " 27.98584102 32.96121448 18.82938916 16.63026504 16.12612501  9.74854478\n",
            " 17.78654697 35.81680537 13.8158688  24.09018946 46.36554571 30.71055454\n",
            " 34.08410094 19.30197022 33.81851976 38.73725354 25.54019888 23.99891\n",
            " 14.51670333 33.19789549 15.07641929 27.00385701 25.85017348 20.13090853\n",
            " 35.21874452 21.38273922 22.45291947 23.27756459 19.95985613 12.25114324\n",
            " 14.50305884 27.52123347 31.31012685 46.8645836  11.02302516 12.78024464\n",
            " 21.72774302 19.94331704 28.96605973 12.34151422 41.85094842 49.95785095\n",
            " 25.68600399 11.72009598 18.91719525 26.51362556 13.99215007 29.8818149\n",
            " 13.10542522 19.49504013 47.73883045 30.12680795 41.95221482 29.28231387\n",
            " 13.89083847 11.17737978 17.58229468 22.19361932  9.74854478 16.82317391\n",
            " 18.10960858 38.56038108 34.90178182 20.14520156 30.53698121 23.75551663\n",
            " 12.20955615 24.27498601 11.36323519 19.11409186 13.93772425 11.43326068\n",
            " 14.24570975 17.72901832 18.62622441 12.52902784 27.75365384 32.69094564\n",
            " 22.5109509  12.76293799 13.08927876 22.33258007 39.41459511 31.44534308\n",
            " 27.19881376 27.6299686   9.74854478  9.74854478 25.16455998 17.30443315\n",
            " 17.31334097 32.02072579 37.51172186 22.46357311 21.09788304 13.8127788\n",
            " 39.11662019 19.33120781 17.8732527  20.94428006 22.73151015 30.40124828\n",
            " 27.90367945 15.92030518 38.12065852 14.12124379 23.51790613 10.44680032\n",
            " 20.98755956 24.03713858 31.24848533 47.02508623 15.44554018 18.73836285\n",
            " 12.52902784 12.16485638 25.38683869 16.20740251 18.74393856 34.22632126\n",
            " 49.95035945 22.38098455 12.02586316 27.68480805 37.54793056 26.80609529\n",
            " 11.0056229  30.66451378 12.32615961  9.75986461 34.32192104 15.24755981\n",
            " 16.78189908 14.50160796 35.49712305 29.46827421 18.8654207  11.07796546\n",
            " 10.53832921 34.54366609 30.26948423 28.48214117 23.28643868 28.32035485\n",
            " 21.93338942 19.10517083 21.65998601 13.55934787 13.84089353 13.29199632\n",
            " 18.40154429 36.69992373 29.7003995  38.49605931 16.87159532 32.63314491\n",
            " 49.96728804 33.08501838  9.59098519 11.76767463 29.74419534 13.47033635\n",
            " 21.46408428 33.7496509  15.38910873 18.89496346 15.10682528 19.28991626\n",
            " 34.03349391 16.70537646 12.65027656 43.81384589 11.25300765 40.23163351\n",
            " 17.76889784 19.32498965 18.1235124  12.91447347 27.1752339  17.57779356\n",
            " 31.18162472 27.8655619  11.87174778 10.43766848 26.42900012 17.4370398\n",
            " 27.51591311 12.74978428 30.59786206 14.11366793 33.31472075 14.24949379\n",
            " 15.23916192 26.18614417 12.65197439 10.72523243 17.09211063 39.27621996\n",
            " 18.51591963 45.93817493 24.18009054 30.45119164 21.59514357 16.37051419\n",
            " 49.44839626 24.17689597 15.17248533 22.85803675 38.96043587 26.29366213\n",
            " 30.65856668 32.76785247 17.68336497 10.22345388 44.09326775 17.04470033\n",
            " 17.69174517 30.90587699  9.74854478 16.2955727  24.84427009 21.57205417\n",
            " 15.41361173 28.26171477 13.31007961 29.91014389  9.74854478 37.24391555\n",
            " 24.69368359 35.26716269 36.08454983 44.78094337 16.87142247 25.63135884\n",
            " 24.24073228 15.03693465 20.99313639 15.58180408 11.06020853 14.5307727\n",
            " 44.54364835 13.04885065 19.12248049 30.41619908 11.77661334 13.2221538\n",
            " 11.66300511 20.11584392 28.34911445 20.996748   11.34003353 34.61852924\n",
            " 30.44909084 27.08404805 11.92755222 35.1367273  14.38785499 14.42383603\n",
            " 16.39504961 49.91181845 45.79586878 33.76122192 20.04785323 46.68962192\n",
            " 26.1491568  17.16020787 18.22111465 19.11372951 12.56720454 34.7669464\n",
            " 28.81918024 24.08398112  9.74854478 23.05396898 15.92346167 25.89489536\n",
            " 22.66384592  9.74854478 25.37655898 15.92448556 32.32000088 25.10145605\n",
            " 19.87859039 19.70668151 32.92765288 14.52773434 29.44388601 49.72767618\n",
            " 26.11716387 18.02109361 32.04375155 29.77056376 20.02530276 39.06380762\n",
            "  9.74854478 23.51346699 41.13600571 19.04880034 25.49784558 38.72620431\n",
            " 14.00965923 11.23232555 22.6959266  27.93848275 26.00280151 35.36979739\n",
            " 20.60460202 47.92454202 14.49663361 16.40259619 28.15575967 12.62396165\n",
            " 25.11602406 15.16099563 12.71324907 25.12409654 18.9689112  30.7855209\n",
            " 19.11409186 35.062514   33.35088962 22.34823208 26.17984152 11.13736121\n",
            " 29.02556971 34.09359075 21.87867111 26.56711439 33.18679536 37.63195322\n",
            " 45.95770077 18.53832442 24.20526784 48.03374489 12.51584726 13.09014967\n",
            " 10.33146476 24.7023028  44.08193892 29.90029127 20.4944145  20.41261182\n",
            " 18.14751491 35.61946919 12.09482864 47.71492622 23.27174614 43.73903331\n",
            " 13.42626445 29.56347236 26.11735346 28.5747286  13.22182312 41.12137936\n",
            " 41.69929054 24.24985117 35.00718345 28.62004601 37.26549925 24.52225436\n",
            " 48.25054113 15.89678289 12.30171856 22.32685876 23.11305657 17.58030301\n",
            " 10.23831214 15.0338879  17.30163718 31.66461237 15.26354458 26.85735806\n",
            " 40.34379339 38.34081797 18.90090903 30.28038047 17.58282099  9.74854478\n",
            " 44.01141421 35.48476356 24.28325363 16.32383059  9.74854478 15.76744747\n",
            " 27.49600951 15.03693465 25.78481773 19.88314963 27.01009211 29.09653111\n",
            " 15.84827917 19.33120781 13.56425817 19.36813483 24.02033435 36.00014139\n",
            " 15.58307453 12.40819192 17.7387767  11.30636938 13.32762935 29.94703341\n",
            " 22.61295576 10.25301875 19.93608363 14.41692656 36.60882302 23.46266011\n",
            " 17.23107247 17.23808924 22.16575062 15.85532352 14.85132584 16.99285707\n",
            " 16.4662261  22.26502767 28.35865368  9.74854478 35.00252678 17.17256954\n",
            " 33.76526266 22.04603202 25.33971727  9.74854478 22.98624459 24.55097727\n",
            " 30.63664966 33.0226295  24.56635942 34.29493996 25.82246929 25.51445542\n",
            " 17.59881474 30.43609065 27.51591398 32.63685576 19.45839267 16.01769045\n",
            " 48.8552012  23.41867701 31.51762825 10.20041021 19.22674499 15.21383957\n",
            " 32.79980867 29.49926659 12.50234231 47.84125017 13.02943544 23.27221639\n",
            " 18.79298497 29.91961367 30.07796228 48.79258536 12.03272047 26.24166178\n",
            " 45.43636009 16.72807816 15.55984624 12.17633775 20.09461578 19.23893542\n",
            " 14.36793814 24.51487806 11.61916591 17.11037157  9.74854478 10.04079407\n",
            " 27.16750404 14.28522921 12.83888843  9.74854478 49.99990445 16.72641829\n",
            " 38.7038692  20.35733498 20.99965155 40.3633074  19.67017798 12.35386162\n",
            " 11.07759783 49.4650831  24.87557758 13.79184557]\n",
            "selection [245 136 536 844 301 114 153  83 672 655] (10,) [9.52535153 9.56712004 9.59098519 9.74854478 9.74854478 9.74854478\n",
            " 9.74854478 9.74854478 9.74854478 9.74854478]\n",
            "trainset before adding uncertain samples (440, 10) (440,)\n",
            "trainset after adding uncertain samples (450, 10) (450,)\n",
            "updated train set: (450, 10) (450,) unique(labels): [165 285] [0 1]\n",
            "val set: (852, 10) (852,)\n",
            "\n",
            "Train set: (450, 10)\n",
            "Validation set: (852, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(C=0.1111111111111111, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.43      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0]\n",
            "std (852,) [40.15519313 16.13512053 15.72467076 19.71566958 36.58508691 24.36416166\n",
            " 22.79009904 12.38554272 17.6055178  46.07758718 46.29449017 42.028796\n",
            " 27.78764247 16.72030916 23.54800225 15.49388352 10.99081606 16.34334962\n",
            " 40.02456009 14.35943159 45.40227516 19.87515462 33.94378243 19.81045076\n",
            " 10.99081606 31.02862465 15.61549378 21.69028446 29.00360539 38.15892945\n",
            " 18.82145468 43.51463905 13.68274849 11.91174747 19.0361332  15.41758418\n",
            " 21.94333281 25.96055963 15.20579089 29.59995146 11.33196763 11.75249576\n",
            " 19.95159856 18.42152784 26.67185871 33.66371084 14.28207909 47.08005513\n",
            " 29.67174551 24.45643858 13.55235285 19.61069522 18.57701005 17.93541272\n",
            " 12.96195127 40.60539993 27.47496543 44.6179353  27.45798925 27.6558637\n",
            " 33.31496178 18.5860658  14.05782255 25.07127279 16.24818546 36.92505285\n",
            " 27.94298396 27.35943499 15.56190494 17.61645252 27.79326438 49.86498484\n",
            " 19.12683314 38.69353022 19.30076832 25.81512889 23.62254896 29.09762157\n",
            " 14.46134407 13.84170465 28.71250917 15.97403295 26.63679191 27.05875487\n",
            " 18.72264074 15.90010817 20.45643281 36.85984569 36.63032049 14.73630274\n",
            " 34.75080474 31.6834634  49.89753462 37.53519922 14.2846117  23.83325598\n",
            " 49.96814646 39.51542154 36.8425664  32.54569065 18.21786958 25.58714091\n",
            " 21.97125974 24.11996884 14.66425249 24.91827009 35.25018832 30.60273455\n",
            " 30.15058885 20.77979513 18.72255139 30.9801467  17.92141592 25.94629458\n",
            " 45.24224381 22.54395479 41.14373079 20.27677884 14.65533874 29.78046611\n",
            " 49.86217775 31.20080892 14.44573625 16.39691625 49.99653038 16.39226205\n",
            " 31.97886641 29.48098269 19.66794631 24.65223586 10.99081606 21.21053348\n",
            " 33.65057631 14.45440953 26.57590666 11.21525664 16.10069366 17.52864614\n",
            " 12.13854675 30.78872657 19.4796611  16.35965249 34.92548156 17.44236314\n",
            " 14.67664961 19.63203586 17.15860333 11.61228542 29.71071237 29.08521545\n",
            " 22.03191032 12.48506349 20.7015278  13.72065069 45.96490625 15.67826533\n",
            " 23.85390646 24.95977297 12.46344239 33.11232072 17.24338428 38.71430498\n",
            " 15.49912939 28.30563871 22.43194009 49.30163688 16.18485602 23.31376357\n",
            " 20.33920226 18.21494748 28.74810778 15.76656883 21.24168976 33.21374753\n",
            " 40.05586738 45.78541492 25.26570659 17.67656921 12.02866714 30.74686274\n",
            " 49.05621842 10.99081606 14.75871808 33.69682659 13.85566268 16.37740282\n",
            " 34.86697116 19.55832995 25.93180628 36.99952048 15.92819881 15.35946594\n",
            " 13.84920201 44.03156867 16.8979496  30.62966776 39.40428753 13.19713027\n",
            " 23.57168721 32.5174547  19.09564226 17.8802175  37.90480984 29.34084642\n",
            " 17.68094935 32.58095368 18.46126587 22.91496182 32.24472514 13.6688951\n",
            " 16.8887697  18.43461597 23.75522726 23.08811676 19.76008422 25.51576859\n",
            " 33.97125966 25.38426257 31.23438017 26.65291963 15.69853618 24.85341392\n",
            " 48.90462792 12.05372649 16.97501956 28.88473943 13.62584364 23.77032793\n",
            " 15.26785811 36.05632657 33.6109324  32.3967367  15.74031601 42.4960495\n",
            " 28.17981697 28.12162556 49.67690071 18.64119701 22.15227468 26.39632711\n",
            " 14.53010245 24.15969173 24.08174901 22.51322911 10.99081606 49.97662507\n",
            " 10.99081606 42.00317028 31.31336128 42.90238116 24.86178124 10.99081606\n",
            " 30.69141739 43.80565069 28.47687336 47.28603222 14.5903884  16.9811116\n",
            " 20.204689   21.80584018 28.98021159 16.78919208 23.90727289 21.44638367\n",
            " 43.56183111 49.62429769 22.9129451  19.2816969  25.6644384  15.08743866\n",
            " 16.81651947 18.54979731 34.12097025 13.71012624 15.17053089 19.58971604\n",
            " 10.99081606 25.16008118 21.00919599 27.94298396 30.8446945  18.02860214\n",
            " 12.57005352 30.0684448  22.07104644 23.65682736 28.55051347 26.94951507\n",
            " 23.09726345 19.71052709 34.02207782 28.80150964 29.25134019 13.89935414\n",
            " 29.79005926 18.96391114 37.32755433 21.71168255 33.90083908 17.06053192\n",
            " 21.27302684 30.13530842 27.35983334 25.51174642 16.95814859 20.85393282\n",
            " 16.34033796 30.2437742  15.41892263 15.70073609 22.27835563 35.4005343\n",
            " 16.53368544 22.25005039 13.23930688 19.70987895 20.62946276 24.33202511\n",
            " 14.50827299 26.68050573 12.94503041 28.41796182 27.55013009 18.73646399\n",
            " 35.46340849 13.242495   17.50772019 25.4563696  20.65488233 49.87709909\n",
            " 20.0042995  21.42545156 24.11877633 13.67411371 32.85291447 48.14386296\n",
            " 22.14020198 41.60787176 23.82609778 27.71758112 10.99081606 10.99081606\n",
            " 17.03450881 22.40802645 22.38793997 28.62111923 22.10031309 21.99899974\n",
            " 23.56911096 24.13533289 23.87290017 14.86802268 18.75108508 21.73970035\n",
            " 18.99835348 31.21720175 14.81199202 48.16607877 31.66028112 27.76055559\n",
            " 22.66724668 27.54543819 15.74036159 23.02157071 23.03845565 14.05153874\n",
            " 40.60568766 13.77758029 27.23318607 24.07856492 23.82323519 14.14521086\n",
            " 30.30063196 35.31502928 21.16191854 18.47096392 17.87059932 10.99081606\n",
            " 20.00223198 39.06777389 14.63604059 27.15301639 47.90490237 33.23365604\n",
            " 37.06978538 20.389067   36.21390569 42.03003518 27.83966063 25.02458527\n",
            " 16.11875642 35.67334601 17.17787708 28.25424332 28.01526808 22.62004269\n",
            " 38.16724922 23.76684442 24.46605851 25.66824042 21.72778843 13.63077887\n",
            " 15.12899248 29.81962021 34.52550995 48.24115907 11.47510225 14.88107226\n",
            " 23.96628516 22.40167704 31.51294881 14.19376928 44.25634743 49.98961411\n",
            " 27.12130705 13.28375269 21.18090841 28.86970141 16.18501262 32.36933029\n",
            " 14.66618883 20.38348916 48.84592147 33.22001348 43.76965143 31.29383163\n",
            " 15.94872273 12.82450087 20.05445629 24.71809895 10.99081606 19.13801759\n",
            " 20.50623968 41.24302198 37.23461618 22.01502647 33.37252645 25.987258\n",
            " 13.57555588 26.71770424 12.67339967 21.1398855  16.08044286 13.18803274\n",
            " 16.21116832 19.95790698 20.77912415 14.16391847 30.16839833 35.80625358\n",
            " 24.73674792 13.20358207 13.66626705 24.21292251 41.50004402 33.63617346\n",
            " 29.59683256 30.16551001 10.99081606 10.99081606 27.69922549 18.6959149\n",
            " 18.98139766 34.40561079 40.5236338  25.08727757 23.61195065 15.66309673\n",
            " 41.35585036 21.58669272 20.04002186 23.23126505 24.88576236 32.76950469\n",
            " 30.38030398 17.87344801 41.02931928 15.11778554 24.77516859 12.43932033\n",
            " 22.4136086  26.25901144 33.70060886 47.80656463 16.34302973 20.97767552\n",
            " 14.16391847 12.20270836 27.47356128 18.1468021  20.75668157 36.2821721\n",
            " 49.98692309 24.47639591 12.25399915 30.59077534 41.1254087  29.02653101\n",
            " 12.77532693 32.71946185 12.7619289  11.52718188 36.52310379 17.30577686\n",
            " 19.05171681 15.93383881 38.23265277 31.93362355 19.52548538 11.68136312\n",
            " 12.45316895 37.46044604 32.62886334 30.65418338 25.85227008 30.84647759\n",
            " 24.05504197 20.79588105 24.06403165 15.53935791 15.02244027 13.91489743\n",
            " 20.72813098 38.9422023  32.1658568  40.84761579 18.71742947 35.13868597\n",
            " 49.99261422 36.20938694 12.16924041 32.9459823  15.19023938 22.71982183\n",
            " 36.16111093 17.50293592 20.97427916 17.35760822 20.42590572 37.48994696\n",
            " 18.7376993  13.88541338 45.80802741 12.83474078 42.74667596 20.0831442\n",
            " 21.52885138 20.42156651 14.9533172  29.83676172 19.80217464 33.69635391\n",
            " 30.16135611 12.39969839 11.50699255 28.70426846 18.43252131 30.06510305\n",
            " 14.72050146 33.09777677 15.826396   36.03640861 16.08195265 16.71934246\n",
            " 28.67009954 14.1914697  11.05067689 17.87575106 41.26021421 20.8816503\n",
            " 47.53047677 26.09508491 32.58222094 24.51277259 18.54979731 49.78141785\n",
            " 26.38518922 17.06866274 25.29359764 40.96926417 28.85527919 34.14703723\n",
            " 35.1354963  20.01640055 10.60884831 46.18118026 19.17456807 18.7580819\n",
            " 33.51128354 10.99081606 17.87620035 27.37621058 23.61645933 17.50607109\n",
            " 31.33748245 15.08794481 33.31465347 10.99081606 40.35730605 26.8648651\n",
            " 38.19576054 38.23393339 46.79124336 17.36169359 27.90762394 26.40057262\n",
            " 16.9762273  22.82364375 17.06462617 12.97754009 16.37101346 46.06728365\n",
            " 14.79825014 19.9892049  32.87753083 13.45170852 14.33753832 13.38591614\n",
            " 22.36504713 30.88433585 23.58358809 12.67125744 37.60170804 33.28822212\n",
            " 29.3967795  13.31671303 37.47922588 16.04580559 15.61247709 17.38252375\n",
            " 49.97507604 47.6467544  36.16066565 22.09480866 47.97452625 28.43433422\n",
            " 19.33333873 20.64617891 20.89896781 14.16406024 36.95189876 31.03816777\n",
            " 26.45279045 10.99081606 25.49030237 16.55434767 27.40979001 25.17055138\n",
            " 27.6372466  17.91503915 34.61136522 27.50689703 22.27230883 21.45778762\n",
            " 35.62296741 16.60458776 31.8415284  49.90720016 28.34353628 19.88941183\n",
            " 33.82011802 31.89000334 22.6079114  41.26902567 25.7118707  43.63447672\n",
            " 21.014167   28.13831992 42.06005779 14.5091844  13.20783445 24.59943172\n",
            " 30.26655627 29.00750111 37.75330113 23.09837014 48.9136216  16.50429671\n",
            " 18.22514541 30.58674161 13.18530726 27.30072598 16.92479147 13.29247835\n",
            " 27.55858598 19.87850279 33.1987959  21.1398855  37.3261192  35.93963154\n",
            " 24.23195448 28.54231015 11.21887496 31.49241751 36.30909481 23.56033558\n",
            " 27.5710025  35.64316943 41.93318181 47.64951743 21.54636969 26.56295599\n",
            " 48.9734893  14.37117118 13.47576262 11.35584303 27.46215004 46.17958571\n",
            " 32.42355899 22.80580959 22.60646506 20.32561962 37.93816573 12.51044587\n",
            " 48.76691877 26.50641428 46.21062315 15.32989257 32.58794355 28.95648759\n",
            " 32.5174547  15.26725992 43.8686513  43.25288486 26.73392053 37.33200555\n",
            " 30.38376001 40.01549726 27.094107   49.12946874 17.98589026 13.69321749\n",
            " 24.55479989 25.42411986 18.53741952 11.73996532 16.97436828 19.49736327\n",
            " 33.88979121 17.132338   29.27461424 43.02458821 39.96449214 20.97005921\n",
            " 33.89405387 20.05255997 10.99081606 45.45809551 37.84415842 26.42993913\n",
            " 17.70201934 10.99081606 17.34296352 29.96322409 16.9762273  28.15829722\n",
            " 21.96140034 29.45931319 31.5925855  17.8802175  21.58669272 15.66674001\n",
            " 21.18021758 26.28124257 38.15052015 16.05201434 14.46134407 19.57089391\n",
            " 11.48325343 14.95938506 32.10326075 24.90297281 11.97426785 21.94305975\n",
            " 15.79918888 39.31117016 25.79993032 19.52465303 19.16347431 23.53877118\n",
            " 17.67352889 16.70345765 18.86196759 18.19774167 24.48252513 30.77530483\n",
            " 10.99081606 37.31800433 19.54127621 35.17555426 24.43648503 28.06104174\n",
            " 10.99081606 25.55475127 26.66102053 33.26238023 35.6117938  26.78439725\n",
            " 36.54399734 27.11930489 27.65080834 19.49697758 32.96456443 29.81645171\n",
            " 35.93361144 21.6208894  18.24351832 49.5132692  25.88644518 33.76477505\n",
            " 11.54756481 21.37940909 17.23623629 36.47361692 32.73872564 13.51053416\n",
            " 48.87122188 14.9996112  25.16800071 19.93233754 32.42134584 32.13471502\n",
            " 49.47165814 13.99940911 28.62034278 47.14717303 18.23217753 17.73073284\n",
            " 12.68406761 21.0667761  21.60721037 16.47834748 27.132853   13.34226746\n",
            " 18.94971287 11.43542835 28.8607431  14.90570372 13.43184848 10.99081606\n",
            " 49.99997531 19.97608253 41.18666155 22.49839849 23.28423829 43.00096016\n",
            " 21.63783743 14.21903778 12.16198366 49.79616812 26.11477884 14.90789707]\n",
            "selection [584 251 244 246 276 786  16 589 643 597] (10,) [10.60884831 10.99081606 10.99081606 10.99081606 10.99081606 10.99081606\n",
            " 10.99081606 10.99081606 10.99081606 10.99081606]\n",
            "trainset before adding uncertain samples (450, 10) (450,)\n",
            "trainset after adding uncertain samples (460, 10) (460,)\n",
            "updated train set: (460, 10) (460,) unique(labels): [173 287] [0 1]\n",
            "val set: (842, 10) (842,)\n",
            "\n",
            "Train set: (460, 10)\n",
            "Validation set: (842, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.10869565217391304, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (842,) [41.42610584 16.05904502 16.33551748 20.86660514 41.21398748 25.15756127\n",
            " 23.8029053  12.10902154 18.11460865 46.67539546 46.88199099 42.46885329\n",
            " 28.67660908 16.69502289 24.17166842 16.55380326 17.52835955 41.70236667\n",
            " 14.9392021  46.09333566 21.11199265 34.7350668  20.41214263 11.5104716\n",
            " 31.75291285 16.61434801 22.38159496 30.34065015 38.75020353 19.80016417\n",
            " 44.1964963  14.11060422 12.59260134 19.85085431 16.52300914 22.94142746\n",
            " 26.50724758 15.51306253 29.83299634 12.45652084 11.31708643 21.0762937\n",
            " 18.85296076 27.53217409 35.51808985 13.76695213 47.87397303 30.60172382\n",
            " 25.645715   14.65324813 20.19361526 19.70346821 18.54203781 13.0753086\n",
            " 41.05311887 27.66381855 45.44627813 28.08971578 28.40946395 33.6208768\n",
            " 19.61900632 15.24264578 25.7587424  16.55762456 37.43005905 28.42300489\n",
            " 28.34911324 15.55095054 17.82192298 27.61197589 49.91944682 19.46093855\n",
            " 39.58547302 19.83441085 26.86693007 24.00500333 29.53096448 15.10885181\n",
            " 14.97547438 29.94321255 16.81680001 26.95764064 27.89855144 20.01983723\n",
            " 17.14736629 20.76806874 37.80355583 37.44012271 15.31463211 35.29149568\n",
            " 32.78799921 49.94704313 38.27618797 13.735967   24.78115862 49.9847105\n",
            " 40.07489385 37.56547874 33.35234424 19.0532085  26.49081719 22.45282574\n",
            " 24.97799515 15.06442624 25.53770364 35.57351323 31.61344038 30.80133648\n",
            " 21.49024434 18.35092231 31.85164435 18.41260872 26.96725513 46.04034962\n",
            " 23.55329439 42.28632428 21.27538394 15.6313871  31.98642143 49.93571554\n",
            " 32.2186956  15.26926894 17.48479887 49.99833636 17.60202492 33.10863259\n",
            " 30.3624324  20.77874354 25.10906141 11.5104716  22.06682438 34.24341122\n",
            " 15.00435966 27.57585471 11.91332399 16.67392685 18.37599521 12.23149354\n",
            " 31.45214569 20.11419696 16.96546888 36.3578906  18.5340017  15.56241505\n",
            " 20.83347457 17.96934578 12.67050045 30.43527786 29.50353607 23.02181257\n",
            " 13.31420576 20.22405867 14.84495964 46.20485456 16.57459663 24.51247003\n",
            " 25.39998575 11.84407473 33.57024445 17.7095428  40.21180018 15.26029091\n",
            " 28.8906778  23.2922382  49.3520148  16.91244163 23.92262019 21.20045701\n",
            " 18.75893306 29.63755116 15.46924544 21.95241221 33.53565336 41.39046779\n",
            " 46.48822363 26.12668689 18.07236052 12.86132019 31.68043587 49.15428158\n",
            " 11.5104716  15.8505033  34.52448703 14.98747544 16.67780058 35.82301428\n",
            " 20.33037524 26.76915882 39.00087707 16.39830127 15.70587815 13.11046444\n",
            " 44.48471026 16.53394568 31.37761847 40.83821048 14.17178678 24.72042123\n",
            " 35.28732842 20.05080326 19.03554517 38.66619689 29.95746628 18.81750936\n",
            " 33.24192487 18.36539261 23.86758016 32.85164605 14.78630584 16.76438489\n",
            " 19.45808701 24.40628226 23.80207912 20.26500823 26.18663722 34.63817565\n",
            " 26.17730818 31.92068175 28.88282666 15.9768404  25.63866811 49.23814841\n",
            " 13.15023332 16.62498009 29.74842594 14.58202615 24.35120454 16.30065671\n",
            " 36.18380642 34.61778395 32.50644474 16.84737654 43.41812348 28.829875\n",
            " 29.8137634  49.76820672 19.31123249 22.35705996 26.98073355 15.05539185\n",
            " 25.25671439 25.03014686 22.57345834 49.98598308 43.6844486  31.81214049\n",
            " 44.01567317 25.80357757 31.41195267 45.13859324 28.88054756 47.78355753\n",
            " 15.80037371 17.56764819 20.75861309 22.43482785 29.36674557 17.35594553\n",
            " 24.20464126 22.1929094  43.96256052 49.77193515 23.65457205 18.93850207\n",
            " 26.72641464 16.33467541 17.75888018 19.09418753 37.05565934 14.68092739\n",
            " 14.91960433 20.45121083 26.04666047 21.05368633 28.42300489 31.62922601\n",
            " 17.48797893 13.74010984 31.27438764 22.25562031 24.12376307 29.21100118\n",
            " 27.78538447 25.38417262 20.4341679  35.36148441 29.39743026 30.21337979\n",
            " 13.6495546  30.70937746 20.09117293 37.8229583  21.74739328 34.53615081\n",
            " 17.39424105 23.34814242 30.90098386 27.5329922  26.31173419 17.71593475\n",
            " 21.23865201 17.5751459  30.85037696 16.49330314 15.59350622 22.58054899\n",
            " 35.87252752 17.4090206  24.13292326 14.28236749 20.62008296 21.66508535\n",
            " 24.54701815 15.66404529 27.40373216 14.8940742  28.84101297 28.0082075\n",
            " 19.18756125 36.29805739 14.07418276 18.59471793 26.27229437 21.29235853\n",
            " 49.92402465 20.90443941 21.88187229 26.24733084 14.59230208 33.02732305\n",
            " 48.26971482 23.29857357 43.14745714 24.62215061 28.50530436 11.5104716\n",
            " 11.5104716  18.14473638 23.13428843 22.99372543 29.12983179 23.29029973\n",
            " 22.892096   24.50850459 24.82351617 23.54074202 15.75394875 18.20734095\n",
            " 22.80014932 19.77365706 31.96747862 15.66914059 48.60342605 32.59228008\n",
            " 28.18507165 23.71625182 29.22676357 16.3091087  23.88089333 24.8496842\n",
            " 15.20955071 41.6191183  13.59701569 28.29840504 24.49691726 24.15756253\n",
            " 15.11321246 30.94886316 35.65773502 22.76158509 19.8138967  18.32652564\n",
            " 11.5104716  21.03956374 40.61746384 14.5974033  28.74115209 48.457588\n",
            " 33.88509159 37.9473169  20.81250553 36.61039676 43.75658662 29.04137703\n",
            " 25.02234341 16.88015373 36.34525018 18.36938652 27.75294539 28.65029238\n",
            " 23.54696748 39.0457165  24.5696456  24.95701041 26.51616604 22.53420923\n",
            " 13.93158822 15.0546198  30.61183385 35.71777021 48.75444429 11.08293533\n",
            " 15.8280154  24.42216609 23.14006907 32.27273005 15.12001469 44.91042577\n",
            " 49.99470189 27.24697898 13.89334243 22.20111869 29.17888832 17.2363998\n",
            " 32.7605134  14.89519447 20.24328451 49.17936189 35.08836918 44.11209127\n",
            " 31.62763466 16.95013926 14.11130433 20.63676118 25.38760432 11.5104716\n",
            " 19.88447103 21.89311948 42.13507513 37.90599796 22.33409453 33.86165043\n",
            " 26.7712927  14.02933345 27.75061891 12.70977544 21.67946657 16.85501174\n",
            " 14.10831258 17.05539388 21.14701381 21.86695954 14.4860644  30.79903385\n",
            " 37.0561404  25.70738984 13.23205485 13.57920433 24.84403748 42.0192123\n",
            " 34.41449515 30.28754939 30.93537176 11.5104716  11.5104716  28.45762966\n",
            " 18.85877315 19.37287765 35.06771139 43.12862771 25.87211801 24.86503014\n",
            " 16.41053797 41.94899229 22.58768169 21.14774191 23.76821061 25.65077472\n",
            " 33.51667608 31.26123306 18.82677216 42.11812802 15.99474434 24.4498352\n",
            " 13.3583705  22.69542462 26.85100594 34.30141975 47.94871104 16.86958853\n",
            " 21.9783802  14.4860644  12.03924025 28.3440071  18.44162581 21.71873811\n",
            " 37.17129533 49.99162915 25.3979678  11.65099775 31.81027157 42.88681161\n",
            " 29.97959882 13.56329718 33.6022393  12.58606176 12.52957363 37.32480203\n",
            " 18.18774743 20.07158691 16.66980066 38.95176228 32.42302379 19.3377608\n",
            " 11.31707372 13.5566257  38.24270117 33.48279683 31.33680992 26.57504627\n",
            " 31.76579079 24.51378841 21.089354   24.98285092 16.64963147 15.5884913\n",
            " 13.66555112 21.70768603 39.42574936 32.80620646 41.62328022 18.93606336\n",
            " 35.82280662 49.99624538 37.46250964 11.74753881 34.05141987 16.01618945\n",
            " 22.67247623 36.79647522 18.64616881 21.82190635 17.99739172 20.56574546\n",
            " 39.01278752 19.82609474 14.22199614 46.24760757 13.90001954 43.67296677\n",
            " 21.22464427 21.8970069  21.61657597 15.82550971 30.75134384 20.85941538\n",
            " 34.70125858 30.46100381 12.33231002 11.62374862 29.45073406 18.4485553\n",
            " 30.91136232 15.77645263 33.72975486 16.71374588 36.71508319 16.74761623\n",
            " 17.1129195  29.53244974 14.8790924  10.64277144 17.67635411 41.66949878\n",
            " 21.78147686 47.97136866 26.82585326 33.3197037  25.88056881 19.09418753\n",
            " 49.86300741 26.96916928 18.12283538 26.36942689 41.5225824  29.26001036\n",
            " 35.75019847 35.83028054 20.54388811 46.74979266 20.22883227 19.51815948\n",
            " 34.23275656 18.59522098 28.3036296  24.24134739 18.54843581 32.39146316\n",
            " 16.08276231 34.22581758 42.0323495  27.84005989 39.10685297 38.85669681\n",
            " 47.43136019 16.48707242 28.12150003 27.01773566 17.6118545  23.18097265\n",
            " 17.25688145 14.12775982 17.58517439 46.28538327 15.65656369 19.93879174\n",
            " 33.72211946 14.08952984 14.59510895 14.1925594  23.29088954 31.40712247\n",
            " 24.55950498 13.92199631 38.73981346 33.8846115  30.04272475 13.097696\n",
            " 38.10279022 16.75564647 16.12514236 17.15403683 49.98629408 48.45637493\n",
            " 36.70210812 23.01811579 48.33746299 28.89318327 20.25634069 21.64953581\n",
            " 21.48834219 15.13840406 37.61322411 31.78614025 27.50527856 26.27855767\n",
            " 16.46166703 27.28519317 26.00169984 28.10198803 19.19928314 35.09454199\n",
            " 28.49052987 22.8743996  21.99915836 36.74665529 17.8331561  32.15222196\n",
            " 49.95026996 28.30942123 21.02698713 34.38334954 32.73953621 23.86479089\n",
            " 41.99421054 26.69519032 44.39380816 21.52054916 28.67525034 43.65886949\n",
            " 14.83764099 14.23732051 25.25147155 30.84223205 31.13932953 38.24701359\n",
            " 23.90349821 49.23257776 17.53619314 18.6517746  31.45075073 12.68020269\n",
            " 27.93738113 17.86229777 12.88864609 28.1554114  20.31649891 33.90922356\n",
            " 21.67946657 37.99145696 36.15845168 24.52463933 29.37121498 11.52578736\n",
            " 32.26802374 37.03029207 23.48383128 27.97248911 35.81367109 44.28158653\n",
            " 48.3061781  24.47034768 26.91788374 49.23966224 15.07231857 13.00527482\n",
            " 11.86606865 28.35782487 47.09980098 33.00964815 23.45350801 23.89122181\n",
            " 21.34693979 38.33903512 11.87377536 49.05671879 28.38959579 47.25809002\n",
            " 16.48903116 33.33659646 30.23637862 35.28732842 16.44802815 45.10095374\n",
            " 43.64349155 27.02151516 38.11566714 31.01483506 40.97305379 28.07828121\n",
            " 49.41106439 19.09371855 14.04792065 25.54036355 26.35143182 18.48361256\n",
            " 12.682806   17.26679564 20.31852071 34.66637758 17.97505082 29.59607996\n",
            " 44.00161496 40.23859092 21.7137425  35.87478674 21.08492934 11.5104716\n",
            " 45.6456434  38.41706412 26.97922305 18.39155171 11.5104716  17.97620066\n",
            " 30.66258674 17.6118545  28.89674067 22.97811935 30.24533833 32.24762489\n",
            " 19.03554517 22.58768169 16.89677356 21.80744738 26.82622569 38.82708883\n",
            " 15.33364343 15.10885181 20.53487027 10.90072936 15.57940664 32.58839821\n",
            " 25.91131453 13.11421426 23.14819594 16.19341936 40.08951915 26.82764048\n",
            " 20.37079811 19.64080964 23.19288705 18.19954206 17.74529229 19.75213944\n",
            " 18.76028408 25.07168447 31.23056298 37.69040374 20.71692335 35.68760049\n",
            " 25.19874825 29.56270874 11.5104716  26.53976062 27.55516569 34.17375279\n",
            " 36.38041665 27.72440244 37.122194   27.30718707 28.05670867 19.94878431\n",
            " 33.81689551 30.49046296 37.32840262 22.2301231  19.00535225 49.73058127\n",
            " 26.88822259 34.1844064  12.03834341 22.3719544  18.05284628 38.74236077\n",
            " 34.59809846 14.54879977 49.14422871 16.0290407  25.72367059 19.9364532\n",
            " 33.32143052 33.04380752 49.69064197 14.74101022 29.66227625 47.73890531\n",
            " 18.69450154 18.77450116 12.61789911 21.36669783 22.53985608 17.45274545\n",
            " 28.53035883 14.3914192  19.511017   12.26912008 28.90117347 14.65782548\n",
            " 12.79087469 11.5104716  49.99997711 22.91122245 41.76151297 23.06530577\n",
            " 23.99757967 43.91206052 22.25279095 15.19475809 12.77607326 49.88714949\n",
            " 26.30683655 15.16519122]\n",
            "selection [561 759 401 504  40 737 742 425 335 782] (10,) [10.64277144 10.90072936 11.08293533 11.31707372 11.31708643 11.5104716\n",
            " 11.5104716  11.5104716  11.5104716  11.5104716 ]\n",
            "trainset before adding uncertain samples (460, 10) (460,)\n",
            "trainset after adding uncertain samples (470, 10) (470,)\n",
            "updated train set: (470, 10) (470,) unique(labels): [182 288] [0 1]\n",
            "val set: (832, 10) (832,)\n",
            "\n",
            "Train set: (470, 10)\n",
            "Validation set: (832, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.493088 \n",
            "Classification report for LogisticRegression(C=0.10638297872340426, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.66      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (832,) [42.08074667 17.07898157 16.98107024 20.70446882 40.72351288 25.55969582\n",
            " 24.6375527  12.64718504 18.89406695 47.18991291 47.31794148 43.15041929\n",
            " 29.93991946 18.24193508 25.48006516 17.54715667 18.09556253 42.59596828\n",
            " 15.85115624 46.18552743 21.04782791 35.84072965 21.64859977 11.88579988\n",
            " 33.30100875 17.33829568 24.65152738 31.43037956 39.7502346  20.01120395\n",
            " 44.79854953 13.72788508 12.80493153 20.89231081 17.25860796 23.75316459\n",
            " 27.84359608 16.35776929 31.82426886 15.21263771 21.85807824 19.33667104\n",
            " 28.75561924 36.56725812 13.45733958 48.20386672 31.94732506 26.32524531\n",
            " 15.74988564 20.62288477 20.06806283 18.79038329 14.97565604 41.88777972\n",
            " 28.21970471 46.10387673 29.18310264 29.82390673 34.41249684 21.10341606\n",
            " 15.87520341 26.16815349 17.68442136 38.66256228 29.77676378 29.25453632\n",
            " 16.23928607 16.01732291 28.14086127 49.94458765 21.00029382 40.91673269\n",
            " 20.04742549 27.76896455 25.77819808 30.92662903 15.67484241 14.8732914\n",
            " 31.08299206 18.19344946 27.26603748 29.41001509 20.02691541 18.83111611\n",
            " 21.00126763 38.75885026 37.65360176 16.05026262 36.65503463 33.59872035\n",
            " 49.96234445 39.07028045 13.43590058 25.40724331 49.99011676 40.94124432\n",
            " 38.3816251  34.19354912 20.08517712 27.72891938 22.81394111 25.53348273\n",
            " 15.64337736 26.50668342 36.92950659 32.5829946  32.33043714 22.03283339\n",
            " 19.77225018 33.04769817 19.51788674 28.45231927 46.52045776 24.39858583\n",
            " 42.93244593 22.26396643 16.90190048 33.03258662 49.95378804 33.34666635\n",
            " 15.66789741 17.8194105  49.99908395 19.32833329 33.49492843 31.60706863\n",
            " 21.5856476  26.39422163 11.88579988 24.02934859 35.22876691 16.22326341\n",
            " 28.816467   11.99188716 17.97780518 18.88038789 12.66970706 32.39618989\n",
            " 21.16965567 18.16500757 37.47233507 19.16911321 16.10366587 21.67025629\n",
            " 18.53369555 13.05835244 31.78826763 30.49200331 23.74313715 14.42531589\n",
            " 19.33389532 14.822498   47.02204982 17.08468517 25.81136363 26.84739529\n",
            " 11.91913346 34.65451399 18.51877215 40.62027898 16.10594465 30.34218449\n",
            " 24.20015424 49.5390306  18.73626004 25.82650605 22.15696839 19.81489931\n",
            " 30.66883055 16.61418499 22.8273589  34.30351265 42.26906205 47.06734453\n",
            " 26.9998368  19.33633549 12.88143296 32.83445023 49.3002902  11.88579988\n",
            " 15.69558235 35.67082546 15.5086537  17.16785366 37.14090218 21.46350812\n",
            " 27.66780213 40.00438817 17.25459358 16.8378338  13.95721416 45.2256503\n",
            " 17.36160331 32.58093136 41.73701775 14.25182707 25.2237629  36.10678192\n",
            " 21.60157417 18.78459029 39.51635913 30.91737198 18.97856576 34.5383834\n",
            " 18.50417987 24.64649588 33.93633709 14.98312076 17.89950049 20.02856979\n",
            " 25.81009879 24.66695455 21.20799867 26.83105773 35.49028506 26.78070094\n",
            " 33.01383758 29.82619416 16.6772919  27.11675441 49.39753082 13.73452818\n",
            " 17.51895144 30.75246431 14.33425599 25.62822064 16.63954887 38.06359309\n",
            " 35.47547273 33.24352612 17.59338288 44.01271435 29.98509671 30.76419274\n",
            " 49.83301141 20.017049   26.15838573 28.19789642 18.42244677 26.86841932\n",
            " 25.47524791 24.19893571 49.99132832 44.48505416 32.02464582 44.76463686\n",
            " 26.28161705 32.81807161 45.05962423 30.66588935 48.12622403 17.38318197\n",
            " 18.89880234 22.00794147 23.73072149 30.89133215 17.98083703 25.50070238\n",
            " 23.33249484 44.79096646 49.8291954  24.73859428 19.82464166 27.70569872\n",
            " 16.59774232 18.55860772 20.00689274 37.87748263 16.20371772 16.73828961\n",
            " 21.52248085 27.15751449 22.08403056 29.77676378 32.7735831  18.45742282\n",
            " 13.95161782 32.10908843 22.85926048 24.83569275 30.33958922 28.6895322\n",
            " 26.3746633  20.97597736 36.06578707 30.89061034 30.81598982 12.50365701\n",
            " 32.13115056 20.31590613 39.21936973 23.37752474 35.65850618 18.25895109\n",
            " 24.03250618 31.84781506 28.79279785 27.47502648 18.74142441 23.41524327\n",
            " 17.83122827 32.14287013 16.72476421 15.79926406 24.02872908 37.41234977\n",
            " 19.02073685 25.24817693 15.11819623 21.80671549 22.64036046 25.42120805\n",
            " 15.64741987 28.24890973 16.06868254 29.97391718 29.03817195 20.08691839\n",
            " 37.41309123 15.2528096  19.75149442 27.42569015 22.296664   49.94595793\n",
            " 21.75503957 23.18450328 27.2763506  15.051743   33.29863985 48.64233215\n",
            " 24.8446785  43.97086372 25.3355581  29.49750363 11.88579988 18.85668464\n",
            " 23.90467544 24.98394433 30.29840971 24.12648058 24.92918923 25.68999636\n",
            " 26.13801712 25.17799298 16.44000277 18.19807676 23.64508018 21.11303763\n",
            " 32.89599786 17.56861085 48.84064779 33.52548471 28.55792638 24.31861671\n",
            " 30.2772911  17.05121095 25.0540826  25.87783696 15.94736774 42.43102472\n",
            " 13.92537623 29.53893696 24.98693581 25.5950586  15.18586843 32.51303095\n",
            " 36.21890851 22.96759312 20.57626173 18.34055971 11.88579988 21.46954527\n",
            " 41.60711549 15.97399868 29.46989833 48.72701167 34.57517376 37.95639558\n",
            " 20.9681375  37.38955789 44.34100645 29.15846087 25.48419783 16.66183803\n",
            " 37.6711871  18.53184655 28.81403451 30.08534468 25.06827979 40.0449092\n",
            " 25.04829668 26.34525885 27.21498183 24.69301289 15.32377116 17.31259842\n",
            " 31.60740443 37.01748237 48.99346375 16.66782747 26.1676926  23.65477328\n",
            " 33.77103492 15.50714393 45.66111914 49.99682602 28.79738159 15.06704312\n",
            " 22.56148537 30.37111127 17.58623231 34.15825427 15.58225401 20.78943086\n",
            " 49.35712647 35.56694264 44.64805636 32.60956697 17.49950009 13.6399403\n",
            " 22.35321441 25.81337232 20.76747576 22.64949066 42.92114651 38.53002508\n",
            " 23.21978853 35.02328077 29.16434808 15.62790412 28.50829545 14.23036779\n",
            " 23.23730063 17.10071753 14.63929024 17.94639095 22.0132905  22.50119753\n",
            " 15.27995796 32.31862266 37.99434468 26.87584345 13.03295809 14.06802966\n",
            " 25.80153829 42.97373542 35.34511359 31.18296608 31.67627433 11.88579988\n",
            " 11.88579988 29.88318309 19.70958378 20.40275096 36.39178677 43.42018464\n",
            " 27.47788321 25.89289576 17.51515039 42.60839308 23.72224253 21.72120321\n",
            " 24.77891685 27.03018114 34.80671498 31.98990137 19.51028247 42.68172548\n",
            " 18.2258475  25.09039019 13.58260924 24.24868066 28.34832793 35.65609419\n",
            " 48.41989108 16.87217488 23.15480162 15.27995796 12.71981337 29.98215188\n",
            " 19.90953053 22.83639051 38.49784972 49.99520729 26.48227769 13.03018372\n",
            " 33.17435364 43.74785137 32.06782357 13.79167001 35.4362491  13.25586\n",
            " 12.62965929 38.18003175 18.90201636 20.89976169 17.66700819 39.81892391\n",
            " 33.87810355 19.9157705  13.90433931 39.04791303 34.84641473 31.99763065\n",
            " 27.90250826 32.34343312 26.06217648 23.00235832 25.73777111 17.03627859\n",
            " 16.30922676 14.46769917 22.67594936 40.59261863 34.19172267 42.08066605\n",
            " 20.41400546 36.76641239 49.99774584 38.4509373  13.49982379 35.40116829\n",
            " 17.17113148 23.91184406 37.93478791 19.42814168 22.66242975 20.50378081\n",
            " 21.53189469 39.87210854 20.07466268 15.37435601 46.705386   14.14847363\n",
            " 44.49086477 21.78907837 23.53147663 21.98330195 16.56578172 32.01530603\n",
            " 21.71579277 35.54993245 31.76189473 13.40455504 12.1636403  30.24909461\n",
            " 19.24467008 31.7586875  15.84137034 35.00033722 17.2889723  37.53331805\n",
            " 17.7137358  18.19729741 30.39462549 15.34393868 18.2184821  42.54029364\n",
            " 22.7820973  48.32225417 28.91482977 34.02744352 27.49562407 20.00689274\n",
            " 49.90220493 28.02885267 18.73147555 26.83813561 42.60725721 30.53750081\n",
            " 36.86587312 36.71428583 20.58933177 47.03944599 20.60153575 21.26791013\n",
            " 35.33498528 19.37031707 29.2230691  25.39119239 18.85090155 33.90365761\n",
            " 15.83501298 34.94753519 42.68091171 28.14831561 39.97373411 40.16682172\n",
            " 47.84338682 14.78695239 29.38430459 28.38169949 19.08486803 23.85216173\n",
            " 18.45354387 14.42977424 18.06105054 46.8318142  16.66145384 20.5524321\n",
            " 34.28021566 15.08332036 14.70128299 14.70083492 24.7281838  32.65385768\n",
            " 25.36649577 16.02276622 39.60857525 33.96517828 31.35114878 13.78515114\n",
            " 39.28412834 19.23365579 16.44573923 17.70497109 49.99104471 48.72540446\n",
            " 37.85727613 24.29771189 48.65495318 29.74168602 20.92815294 22.16660415\n",
            " 22.28777114 16.56660787 38.47499905 32.77922838 28.40112342 27.18125623\n",
            " 16.96673953 28.56653948 27.39465648 29.25492599 20.94020866 36.94498656\n",
            " 29.36326014 22.71415387 22.40844338 37.76268217 18.25810364 33.41291379\n",
            " 49.96500023 29.72852278 21.77147696 36.04123803 33.03540607 25.30341635\n",
            " 42.61479656 27.58136764 45.07684976 22.9036588  30.42112319 44.27233043\n",
            " 14.87311633 15.14877655 25.89299704 33.23266458 32.06101626 39.45808011\n",
            " 25.00030598 49.39973215 17.55522045 19.72922635 31.99346844 13.51421049\n",
            " 29.2078734  18.96338923 13.23630768 29.44914068 21.74590821 35.15305584\n",
            " 23.23730063 38.89271337 37.98890453 26.47985676 30.44471627 13.00492994\n",
            " 33.66973148 38.00913695 24.41035849 28.69845215 36.51456886 44.81498222\n",
            " 48.6003385  25.18425215 29.15050005 49.40404693 16.06226743 13.91732994\n",
            " 12.64576608 29.07574897 47.49450142 34.52798018 24.46942017 24.99147373\n",
            " 22.26449833 39.88025385 12.3833044  49.22528858 30.0684656  47.6905648\n",
            " 16.9756294  34.20461786 31.61885437 36.10678192 17.00031743 45.5681005\n",
            " 44.89539009 28.76732927 39.14053069 31.75056225 41.80426918 29.05712211\n",
            " 49.54150033 20.25723568 15.28624497 26.12173067 27.3484246  19.72710252\n",
            " 13.4215742  18.51806223 20.98155493 35.94035788 19.28231948 30.84323443\n",
            " 44.67758261 41.02289606 23.06783164 36.59482778 20.92897331 46.46063065\n",
            " 39.1931847  28.30876147 19.26471352 19.39864635 30.86657387 19.08486803\n",
            " 30.40356687 23.87932679 32.2222437  33.45036458 18.78459029 23.72224253\n",
            " 17.26788984 22.93321423 28.24644621 39.91326486 16.1088116  15.67484241\n",
            " 21.42784374 16.70082933 33.9860227  27.13136827 12.96139018 23.98973316\n",
            " 17.22978821 42.6528666  27.42676024 21.17874556 21.07433944 23.3295213\n",
            " 20.09673756 18.42137226 20.42457831 20.00648859 25.61939077 32.7659462\n",
            " 38.70791365 21.37372632 36.90620962 26.15958316 30.4769843  28.09135248\n",
            " 29.17904438 34.9634536  37.61557086 29.16419666 38.33545364 28.77352069\n",
            " 29.48833708 21.20798422 34.90523474 31.66968364 38.68683653 25.12931282\n",
            " 19.8408794  49.79474816 29.4617349  35.4591031  13.91495247 24.2481939\n",
            " 18.74459904 39.54038039 35.87068644 15.52241486 49.34270819 16.8637192\n",
            " 27.18706815 20.11606315 34.56963383 34.46395764 49.76627208 16.18306279\n",
            " 29.90054963 48.09237357 19.76489323 19.12173329 13.33302368 21.32578788\n",
            " 23.87105036 18.5248548  29.60407024 15.02641739 20.68671211 13.80064958\n",
            " 30.25950123 14.44144194 13.42615516 11.88579988 49.99999181 23.43664198\n",
            " 42.83734284 23.94517491 25.48915184 44.68550991 24.64644013 15.74142491\n",
            " 13.29805937 49.91688379 26.90256555 15.44223222]\n",
            "selection [819  23 450 449 128 179 370 334 156 133] (10,) [11.88579988 11.88579988 11.88579988 11.88579988 11.88579988 11.88579988\n",
            " 11.88579988 11.88579988 11.91913346 11.99188716]\n",
            "trainset before adding uncertain samples (470, 10) (470,)\n",
            "trainset after adding uncertain samples (480, 10) (480,)\n",
            "updated train set: (480, 10) (480,) unique(labels): [190 290] [0 1]\n",
            "val set: (822, 10) (822,)\n",
            "\n",
            "Train set: (480, 10)\n",
            "Validation set: (822, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(C=0.10416666666666667, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.45      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0]\n",
            "std (822,) [43.14496047 16.0375456  17.32090894 24.9993247  46.21421932 26.39243739\n",
            " 25.53202065 12.34169953 19.60107049 47.56116315 47.48839244 43.48645434\n",
            " 30.37316211 17.34713702 25.77656192 18.85123248 19.50565296 44.28145625\n",
            " 16.30550248 47.03005552 22.80106951 36.4169322  20.97082318 33.53011793\n",
            " 18.46435317 24.06897444 33.1622306  39.96184288 20.51780259 45.39009996\n",
            " 14.57638204 13.12156083 21.66880009 18.21527792 24.14966425 27.76171013\n",
            " 16.89254029 31.09032629 16.83925409 22.30168738 19.73259477 29.1040751\n",
            " 38.56217057 14.31823716 48.77817825 31.90798526 27.90130008 16.51008564\n",
            " 21.48078205 21.08422436 19.70159475 14.24910014 42.05306594 28.57071332\n",
            " 46.59368476 29.86336941 29.80961536 35.21314591 19.94234258 16.93954705\n",
            " 26.83756202 17.67946078 38.83459502 29.36622848 30.20971572 16.85628225\n",
            " 20.68380334 26.93787465 49.9645665  20.51831537 40.88161113 21.52779976\n",
            " 28.35724598 24.92254033 31.00612519 15.81444938 16.48048776 32.78891796\n",
            " 19.10263636 28.86939034 29.83945756 20.89646586 19.90509832 21.99020056\n",
            " 39.6650678  39.13050486 16.72275728 36.3851326  33.96587576 49.980959\n",
            " 39.62293428 14.53890524 26.34697049 49.99512121 40.77211078 38.99479044\n",
            " 34.84274809 20.63455527 28.22957617 23.72527269 26.18427306 15.72342198\n",
            " 27.1063489  37.09576718 33.88467388 32.36399345 22.38825482 18.08246753\n",
            " 33.74085356 19.96480202 29.52949132 47.0520134  25.5162928  44.27793337\n",
            " 23.21532428 16.12516612 35.8156218  49.98337912 34.92370382 15.40109619\n",
            " 19.04013647 49.9993611  20.66377695 34.93067398 31.9865916  22.402836\n",
            " 26.73812372 24.49586004 36.0130725  15.13026051 29.49126008 18.25518153\n",
            " 19.86424614 14.01510486 33.4552104  21.22255055 18.26642236 38.9940489\n",
            " 20.13314512 16.84789351 23.50649399 18.86971483 14.50710388 32.02649695\n",
            " 30.76654623 24.91416018 16.40349662 22.06314619 16.43756236 46.97427942\n",
            " 17.89897617 26.12959927 26.54408765 32.98020582 18.70836405 42.48475962\n",
            " 15.71714067 30.2174398  24.7210709  49.4623524  19.29914215 25.48887517\n",
            " 23.24651841 20.57904304 31.16777613 14.66662685 24.54518004 35.14904638\n",
            " 43.55618814 47.40000773 27.32241874 19.25684645 13.4064093  33.58329472\n",
            " 49.30239734 17.35570068 36.04092529 16.6715317  17.05477515 37.41708278\n",
            " 22.10452586 28.2372339  41.42474074 17.60995385 16.61660788 12.34919535\n",
            " 45.01266731 16.42497072 33.16086155 43.19362459 15.58306202 26.10859983\n",
            " 39.40368325 23.34004422 20.60449865 40.26608699 31.18887194 20.27945745\n",
            " 35.03559691 20.41318343 25.47161025 34.54277873 16.25468585 16.83708972\n",
            " 20.7689526  26.07265651 25.41637168 21.31341661 27.06959885 36.6648159\n",
            " 27.61789964 33.62509643 32.39738579 18.2050217  27.55685097 49.58639412\n",
            " 15.10051941 16.90724596 31.25624828 16.21183887 25.73720454 17.78690705\n",
            " 37.63872104 36.82197016 33.75801485 17.9336304  44.86429268 30.65124527\n",
            " 33.55382423 49.84166159 20.78447004 24.57955791 28.44450499 18.44337765\n",
            " 27.01341947 26.09994464 23.22349191 49.99258017 45.86290709 32.42338606\n",
            " 45.77424505 27.17150375 33.25067211 47.06754067 29.79148178 48.46243908\n",
            " 18.46654978 19.40930582 22.16301357 23.96336764 29.41130708 18.68978703\n",
            " 25.66030003 24.09111682 44.70438181 49.90278036 25.42133625 19.23414813\n",
            " 28.99184425 17.53588987 19.22413203 20.00859663 41.55277348 17.14865703\n",
            " 16.04493538 22.32334196 27.84611545 21.24196075 29.36622848 33.45743125\n",
            " 16.94391312 15.27657999 34.47261625 22.42755751 24.2150161  30.88608112\n",
            " 29.72575321 30.09112316 21.05035787 37.35769721 30.95376595 31.69656059\n",
            " 12.92443962 32.36663976 21.90865363 39.41987502 22.24631227 35.70672799\n",
            " 17.36209401 27.03759645 33.00100244 28.60546105 28.33304341 18.81938686\n",
            " 21.23239095 19.05081702 32.28673541 17.82247221 15.37458913 23.04887477\n",
            " 36.7492374  19.41737363 28.06468377 16.67258188 22.7748824  23.32371988\n",
            " 25.01265148 17.30672603 28.18057303 18.74268826 30.38807157 28.57894712\n",
            " 19.99908204 37.93855623 16.14720151 19.68312126 28.41152016 22.6959433\n",
            " 49.96404371 22.72873756 23.19914237 30.74202555 15.70381901 31.96091206\n",
            " 48.53955753 26.05528356 45.36019549 25.67566315 29.86774558 19.69806156\n",
            " 23.91914723 25.40375402 30.247594   24.98688389 25.86587229 26.4763226\n",
            " 26.31277526 25.25676656 17.14199887 19.51451369 24.72815854 21.7244409\n",
            " 33.31824974 17.83627688 49.14982347 34.25620134 29.62862904 25.89868233\n",
            " 32.56057726 17.79198902 25.78628415 28.36594253 17.26036401 43.27834626\n",
            " 13.81911254 30.81539803 25.0614168  25.31422673 15.96958091 33.05616065\n",
            " 36.14137252 26.41215034 21.79316215 19.0283007  22.69947369 43.11998928\n",
            " 15.58412725 30.89645115 49.11270349 35.14752229 39.51178818 22.04607724\n",
            " 37.67001398 46.00991462 30.71808989 26.29527088 17.93796884 38.02820728\n",
            " 19.72818837 26.82409182 29.80490187 25.48201294 40.50157017 25.17949577\n",
            " 26.43085474 27.97343906 25.72556387 15.06334801 14.76891552 32.69130902\n",
            " 37.73885599 49.32452751 17.24725846 26.41592394 24.32369311 33.68670607\n",
            " 16.09283974 45.87186499 49.99804959 28.05823335 15.71910471 22.79536268\n",
            " 30.53564444 18.48766445 33.85855    15.47155334 21.67185636 49.50033111\n",
            " 38.86022917 44.95483991 32.93194268 18.58336701 15.56042989 22.06063253\n",
            " 27.12667078 21.64871799 24.86254865 43.74618919 39.0834413  24.27050479\n",
            " 34.14516834 29.20772304 16.09157032 29.42484395 13.50596793 22.98626111\n",
            " 18.32100767 15.79269002 17.59834087 23.56988078 23.67547764 14.3662712\n",
            " 32.43551525 39.30963636 27.95400964 13.54258849 15.00793229 26.19140603\n",
            " 43.0853857  36.25923531 31.26234097 32.3929876  29.99410531 19.51456862\n",
            " 20.70486755 36.79956691 46.51114204 27.37618467 28.27141711 18.38045989\n",
            " 42.81905948 24.60218326 23.58192148 26.04697575 27.74623719 35.28360461\n",
            " 32.74412265 20.96755899 43.50606239 19.17648808 25.21511068 14.63197124\n",
            " 23.8688743  28.55448607 35.5252061  48.39829764 19.8514163  24.16416131\n",
            " 14.3662712  12.7187907  30.94943262 20.65855168 23.56583105 39.18351864\n",
            " 49.99487191 27.1702377  11.47107025 34.41125238 45.35996647 32.6619466\n",
            " 14.07453746 36.13952643 13.04882891 13.42461523 38.78167809 19.37779246\n",
            " 21.41610765 18.63341235 40.49165889 33.64291064 19.75689781 15.68107221\n",
            " 39.14523534 35.58009109 32.66775438 28.58940155 33.29842102 25.94234041\n",
            " 20.96877915 26.73097662 18.12525575 17.08024284 14.43212495 23.1358633\n",
            " 40.9000818  34.31152453 42.41865272 19.52834718 37.11572251 49.99853095\n",
            " 39.71308262 11.69868809 35.65271887 17.45038074 23.403879   38.04885775\n",
            " 20.29011435 23.59156438 18.77088242 22.0284742  41.61399024 21.59423881\n",
            " 15.27306315 46.57959633 15.63112968 45.17215873 22.71179606 23.98171738\n",
            " 23.28737258 17.24220187 32.75191905 22.40833988 36.56363886 31.27169473\n",
            " 13.52944792 11.65056203 30.58817837 20.41009801 32.49558906 17.48336009\n",
            " 35.42155378 17.88712297 37.71963736 18.32000031 18.36194322 31.10173818\n",
            " 15.41700949 17.46249569 42.70933352 23.29304048 48.5856381  30.05149347\n",
            " 35.23179233 28.53763057 20.00859663 49.93597431 28.2654705  20.04075912\n",
            " 27.63842076 42.77170201 30.16057587 38.4434637  37.30623752 22.11869788\n",
            " 47.36209012 22.03145407 18.13119884 35.50391283 20.07168841 30.08420678\n",
            " 25.72569654 19.99328277 33.68017094 16.58302277 34.95183791 44.56266084\n",
            " 29.49127462 40.53215125 40.59628948 48.14414971 17.7322799  28.54460354\n",
            " 28.71705415 19.48744201 24.35008884 18.19571716 15.60002172 20.20572215\n",
            " 46.40892064 17.46761724 20.09259477 35.35290948 16.11713657 15.24494092\n",
            " 15.41306883 25.86796151 32.21405364 25.80454887 17.74790144 40.68271505\n",
            " 35.90387322 31.28687269 11.6957348  39.38958669 19.29967345 17.25717372\n",
            " 17.59702792 49.99454633 49.2866739  37.82686366 25.59971878 48.8089685\n",
            " 29.59974266 20.97513348 23.10992648 22.671346   17.690153   38.89764826\n",
            " 33.2855343  29.49156781 27.40288972 16.93535813 27.63882551 28.23279302\n",
            " 29.15798228 21.23362278 35.04422944 30.33614746 23.68647234 23.0591614\n",
            " 38.998954   19.3205856  33.01132051 49.98211185 27.9618646  23.23745093\n",
            " 36.77224478 34.39879252 26.82067784 43.19872974 28.33598703 45.56218596\n",
            " 23.06604934 30.44593794 45.85398552 16.18106478 15.37811287 26.50839656\n",
            " 33.11450193 34.7799124  39.42491685 25.41605642 49.55570413 18.73828509\n",
            " 19.70969358 33.06315899 11.87819834 29.4986077  20.26853515 14.24334154\n",
            " 29.58389035 21.55901267 35.5137774  22.98626111 39.37739548 36.81077701\n",
            " 26.13061166 31.37398554 10.94163219 34.01183826 38.61407954 25.0588722\n",
            " 30.12399401 35.83774842 46.82732404 49.07845424 30.16771067 29.08401909\n",
            " 49.53497557 15.34201209 12.23895838 13.35204219 29.61224461 48.26960683\n",
            " 34.47194724 25.40391453 25.80087653 23.15307758 39.83508255 11.40213754\n",
            " 49.41104996 32.18182227 48.45238257 18.39866343 34.04708174 32.79564034\n",
            " 39.40368325 18.05760005 46.81594857 44.63190847 28.90250608 39.6477248\n",
            " 32.86903563 42.714535   29.90448891 49.70083088 21.10044368 15.49957256\n",
            " 27.7838424  28.23474223 18.65604372 14.91536517 18.76108751 20.96337937\n",
            " 36.59212036 20.22997317 30.79928533 45.46725389 41.04013867 23.93342131\n",
            " 39.04048954 22.16056224 46.43245837 39.66577599 28.49336589 20.96826792\n",
            " 19.46542985 31.6931415  19.48744201 30.80387954 25.12388118 32.60180355\n",
            " 33.48839582 20.60449865 24.60218326 18.35068136 23.48300955 28.38900967\n",
            " 40.46794488 14.15189835 15.81444938 21.79899645 16.78522495 33.47826013\n",
            " 28.32606227 14.53513177 24.63714834 17.37456804 39.11483109 28.32472719\n",
            " 21.77409141 21.1663814  22.90554984 20.77401945 20.0214433  21.65643137\n",
            " 20.31341169 25.88381265 32.48687462 38.73653789 22.1390588  36.4895255\n",
            " 26.33518267 32.5503351  28.58948995 29.69951235 36.54541231 37.6435179\n",
            " 30.08542526 38.44228141 27.93516566 29.05667302 20.76862968 35.65496286\n",
            " 32.22177222 39.28945282 23.99754254 20.35173741 49.90648989 28.49939864\n",
            " 34.90799475 14.80958336 24.75185758 19.43399851 42.21525227 38.15673988\n",
            " 18.0756705  49.43467106 17.80590691 26.59686493 19.538348   35.28033894\n",
            " 35.08283068 49.88170253 16.06721867 32.35393003 48.50167517 20.06043298\n",
            " 20.20378057 12.64139817 22.79779365 24.39008854 19.08821207 31.35023738\n",
            " 15.74224378 20.93732556 14.40862725 30.46170957 14.0426385  13.33035266\n",
            " 49.99998883 28.03835328 42.69535858 24.133158   26.24705091 45.00717801\n",
            " 25.29749663 16.95634628 14.28119385 49.95965489 27.82889393 15.29439281]\n",
            "selection [668 689 476 535 602 511 656 680   7 185] (10,) [10.94163219 11.40213754 11.47107025 11.65056203 11.6957348  11.69868809\n",
            " 11.87819834 12.23895838 12.34169953 12.34919535]\n",
            "trainset before adding uncertain samples (480, 10) (480,)\n",
            "trainset after adding uncertain samples (490, 10) (490,)\n",
            "updated train set: (490, 10) (490,) unique(labels): [196 294] [0 1]\n",
            "val set: (812, 10) (812,)\n",
            "\n",
            "Train set: (490, 10)\n",
            "Validation set: (812, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.001 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(C=0.10204081632653061, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.67      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "std (812,) [43.55882374 18.35998221 18.790838   22.90359488 41.37963753 27.13365171\n",
            " 26.66607053 19.82146602 47.92711481 47.96709873 44.45382938 32.05318351\n",
            " 20.34480343 27.23615231 19.78511717 18.9479866  43.8840036  16.9676563\n",
            " 47.0603777  23.0521461  37.89785597 23.47342628 35.42727811 19.29404643\n",
            " 27.21812816 32.99748792 41.27538036 21.28186239 45.92473945 14.1596088\n",
            " 14.14480104 22.22473796 19.04529855 25.32818528 29.1829592  17.48411659\n",
            " 34.13556995 19.19932793 23.44634032 20.58165822 30.72176414 38.28889751\n",
            " 14.8251774  48.73020636 33.79813255 28.91809548 17.68803771 22.01569185\n",
            " 21.75939644 19.94562887 17.49370412 43.25748279 29.80781998 47.01688299\n",
            " 31.21610808 31.80520836 36.27342728 22.07506916 17.72641989 27.4694576\n",
            " 18.50949071 40.42994901 31.3844966  31.49708284 17.10902852 18.18405706\n",
            " 28.73680691 49.9715028  21.94822629 42.35548889 21.92249026 29.71646103\n",
            " 28.28987269 33.14783528 17.24841453 16.44019544 32.73585603 20.82306973\n",
            " 29.87807598 31.67596957 21.2637539  21.61570128 22.55897536 40.54096174\n",
            " 39.72479226 17.03529753 38.39423823 34.92199128 49.98107583 40.53863299\n",
            " 14.28191851 27.31390598 49.99562189 42.49282104 40.24137312 36.10655103\n",
            " 21.75646237 29.75477751 25.21207004 27.04178351 15.94088652 28.38072207\n",
            " 38.74993375 34.47679462 34.03986861 22.98459004 20.66501437 34.76056433\n",
            " 21.1617834  31.25983862 47.31273538 26.68000762 44.26091731 24.39494658\n",
            " 17.98575872 34.99207783 49.97621584 35.11549041 16.54231599 19.51278377\n",
            " 49.99968236 22.42552008 35.32902169 33.62624364 23.47289071 28.12748808\n",
            " 26.51398235 37.08700183 17.20035278 31.07207665 19.24933468 20.43489085\n",
            " 13.53872869 34.43819924 22.6659552  19.70409198 38.73459402 20.96178342\n",
            " 17.37988504 23.65627191 19.98712923 14.81949889 33.5813692  32.12544479\n",
            " 25.81600843 17.20937918 21.68095415 16.4436544  47.91124354 18.44591349\n",
            " 27.45002905 27.98987404 35.40823787 19.36658118 42.03127188 17.04939054\n",
            " 32.38002304 25.63470794 49.68835345 20.95708155 27.79858055 23.76014496\n",
            " 21.45353176 32.29090792 17.27834134 25.14473051 36.35438264 43.5506372\n",
            " 47.78134396 28.83443841 20.3525563  14.04702069 34.41230556 49.50922672\n",
            " 17.22657677 37.01298683 17.32267198 18.78455993 38.98314901 22.79041473\n",
            " 29.495733   41.38204598 18.26795454 17.505002   46.33124379 18.36068366\n",
            " 34.92135863 43.15687856 15.69023285 27.04260655 37.78956143 24.7028736\n",
            " 20.49251699 41.47823027 32.34525546 20.64743895 36.56657913 20.82144987\n",
            " 26.53678294 35.7287336  16.52688186 19.25838504 21.86266346 28.19801973\n",
            " 26.21791225 22.41934385 28.84566302 37.44487849 28.50891821 35.07799473\n",
            " 31.49867104 17.62084886 29.48065701 49.60932428 15.66872473 18.16770837\n",
            " 32.66748122 15.87245789 27.2892405  18.27409962 39.9735439  37.12979517\n",
            " 35.24302756 18.90502208 45.35296764 32.07140662 32.45841318 49.90689015\n",
            " 21.41719686 28.44995705 30.7065177  21.67099009 29.06144884 27.13369931\n",
            " 25.8718397  49.99617268 45.57574349 34.11928886 45.89617783 28.04499361\n",
            " 34.93652038 45.98199537 32.05619142 48.69008245 20.06349664 20.63085968\n",
            " 23.79877976 25.34436086 32.23341848 19.2175193  27.42776751 24.84818851\n",
            " 46.11116059 49.90116135 26.54889789 20.9331302  29.29420415 18.01880871\n",
            " 20.20745552 21.39405351 39.67148011 18.81683736 19.72584001 23.45084214\n",
            " 29.25919381 23.65688748 31.3844966  34.99323632 18.9709952  15.57610477\n",
            " 34.01452133 25.04480187 25.60761804 32.37085156 31.20726446 28.03129954\n",
            " 22.41888141 37.67294781 32.63758436 32.76997216 12.89757498 33.84127847\n",
            " 22.43048179 41.03972627 24.88821663 37.13731608 19.69035068 25.2908296\n",
            " 33.81780188 30.38579052 29.74791132 20.45168146 25.34503045 19.53623632\n",
            " 33.67853802 18.19708857 16.74068037 25.49839858 38.88508065 21.03659514\n",
            " 26.77636372 17.39584779 24.12178617 24.58640626 27.33098953 17.37614307\n",
            " 30.00066515 16.97413971 31.90494918 30.76397577 20.66466238 39.40375579\n",
            " 17.19506224 21.19210365 29.81240722 23.58627624 49.97232398 23.72292521\n",
            " 24.58805092 29.03746816 16.44244709 33.99821879 49.00654809 27.71050083\n",
            " 45.2412179  27.268529   31.3133226  20.62284982 25.24668851 27.37499456\n",
            " 31.81378626 26.20066324 27.87788968 27.98225496 27.63669563 27.45775418\n",
            " 17.95017927 20.18887134 25.82384236 23.36696288 34.86923014 18.95379193\n",
            " 49.2182357  35.61495055 30.52068044 26.77255468 31.74867629 18.62197766\n",
            " 27.17970809 27.35200927 18.07773358 43.7512469  14.88712133 31.15297859\n",
            " 26.5468795  26.70900705 16.71715466 34.83844439 37.56214479 24.68667905\n",
            " 22.89625997 19.00464183 23.35253067 42.99133216 16.8666033  30.88292372\n",
            " 49.11779967 36.27055266 39.54168743 22.08649073 38.83451512 45.53473469\n",
            " 31.26707381 27.71619322 17.54224539 39.6782338  20.12453812 30.2464118\n",
            " 32.36672881 27.27229817 41.60586815 26.74491213 27.88505609 29.20584876\n",
            " 27.80574114 16.12510675 18.41777508 33.9043426  38.83599284 49.31346326\n",
            " 18.22168187 28.41054064 25.04953649 35.86189381 16.66036074 46.6866256\n",
            " 49.99871974 30.38741157 17.25842795 24.091512   32.42543708 19.01904983\n",
            " 35.85895789 15.99335791 22.35799913 49.5918831  37.06714463 46.08196136\n",
            " 34.98442272 19.2222265  15.18055746 23.59123469 27.19923873 21.81464384\n",
            " 24.06375978 44.18655531 40.08608787 25.55004139 36.46123411 31.71299114\n",
            " 17.59639187 30.57933297 14.6205519  25.59728403 18.59870478 16.27464957\n",
            " 19.29123433 23.62616346 24.62005565 16.87300406 34.06762886 39.82350525\n",
            " 29.35624767 13.84373632 14.89397455 27.05549665 44.31798868 37.67613884\n",
            " 33.05631687 33.55775173 31.88596359 21.30995322 22.05514841 38.40914603\n",
            " 44.51740985 29.30072835 27.9031264  18.86705692 44.21374617 25.93087888\n",
            " 23.29103609 26.79076764 29.23781661 37.01651798 34.00617263 21.70357568\n",
            " 43.67560112 21.46841135 26.51285521 14.84414166 26.84513897 30.27388885\n",
            " 37.57814796 48.90965367 19.50785007 25.54564132 16.87300406 14.176417\n",
            " 32.72540793 22.15501106 25.11410559 40.7751964  49.99806556 28.50596338\n",
            " 34.78510379 45.01186067 34.88922398 14.94228842 38.05148403 13.86989167\n",
            " 13.59032263 40.07874473 20.71678064 22.57680982 18.87099938 41.58439366\n",
            " 35.28324197 21.50375196 16.00460006 40.63111311 37.21760081 33.51134777\n",
            " 29.64490812 34.33572138 27.46405128 24.94961852 27.78580524 18.66320999\n",
            " 17.43033546 14.97677817 24.51985983 42.36644251 35.83471449 42.93699545\n",
            " 22.57423323 38.41775028 49.99915168 40.30278732 37.22958143 19.23215511\n",
            " 25.61373047 39.59175239 21.29015142 24.56198479 21.83002356 22.79339722\n",
            " 41.38145707 21.99768472 16.01316163 47.63421323 15.87603516 45.5857228\n",
            " 23.64544198 25.62094138 23.96347448 18.29077736 33.60708529 23.65154366\n",
            " 37.05534722 33.40413547 14.37057117 32.37807499 20.77757463 33.73053964\n",
            " 17.58120679 37.03829906 19.17624216 38.94550695 19.13123904 19.43626419\n",
            " 32.423355   17.10040066 18.8230338  43.89788254 24.53365655 48.83920627\n",
            " 31.97374981 36.2190928  28.80636294 21.39405351 49.94645823 29.6180716\n",
            " 20.83359997 28.61675533 44.09500238 32.71610576 38.60128261 38.6068871\n",
            " 22.1669979  47.68703575 22.59187025 22.12749834 37.11822011 20.58208113\n",
            " 31.33420329 26.92820939 20.45435444 35.94954085 17.08447851 36.21819846\n",
            " 43.92663432 30.40270568 41.43807945 42.1098469  48.39865769 16.41678772\n",
            " 31.12694231 30.12511444 21.18761733 25.98124123 19.13196527 16.00641634\n",
            " 19.36924053 47.29392682 18.38159675 21.69348448 36.30625138 16.92134948\n",
            " 14.43910094 16.4894226  26.77448464 33.99751139 27.05726646 19.6305197\n",
            " 41.22849613 36.48452979 33.48147435 40.98385695 22.10868567 17.63505492\n",
            " 18.53818389 49.99608272 49.10984147 39.37602083 26.9622793  49.02702423\n",
            " 31.76941072 22.50606381 23.90820625 24.10368619 18.95180813 40.09540752\n",
            " 34.54159873 30.7449337  29.2484598  17.94907361 29.99254387 29.17114893\n",
            " 30.49770366 23.26983572 38.37511769 31.55524697 23.5850632  23.47368982\n",
            " 39.14990721 19.97332932 35.05391738 49.98252671 30.72393174 24.16654037\n",
            " 38.69537535 35.13805422 26.91182144 43.86367062 29.59884384 46.21305546\n",
            " 24.34179254 32.43139971 45.45181392 15.52647565 16.43800233 27.19765164\n",
            " 35.8737459  33.45811955 41.04776065 26.72594251 49.61233493 18.95294621\n",
            " 20.66151111 33.98205066 30.73128007 21.28738299 14.34900864 31.10765598\n",
            " 23.51708926 37.26398551 25.59728403 40.67583954 39.17836308 28.68172361\n",
            " 32.33619398 35.59587077 39.91073751 26.45479151 31.50717137 37.90395619\n",
            " 45.8704022  49.03711765 26.81281699 31.34976545 49.61656699 17.37443797\n",
            " 13.70558128 30.73622657 48.14119887 36.42823948 26.42800096 27.22627792\n",
            " 24.31046731 41.56728826 49.50382908 31.58234632 48.31247188 18.9769016\n",
            " 36.10631212 33.13094979 37.78956143 18.79224701 46.58599346 46.0748\n",
            " 30.7363686  40.70785156 32.98386117 43.29880906 30.57424686 49.7110349\n",
            " 22.41083924 16.5461563  27.91865885 29.50584221 20.88632843 15.45587745\n",
            " 20.08639894 22.6650783  38.13257727 21.76376625 32.59945089 45.90887547\n",
            " 42.50659394 25.38828775 38.36558087 21.6796286  47.3526768  40.83514705\n",
            " 30.15587511 20.01808558 21.94014091 32.33667956 21.18761733 32.86837932\n",
            " 26.23949191 34.706554   34.93683841 20.49251699 25.93087888 18.95489848\n",
            " 24.56548218 30.14119645 41.82503605 17.45208833 17.24841453 23.51342003\n",
            " 18.73682552 36.06641192 29.77732373 14.39722889 25.8531506  18.04072737\n",
            " 42.93324426 29.35808621 22.86383083 22.80346866 24.12598849 22.40969767\n",
            " 20.71881275 22.42626349 21.39265108 27.43650337 34.30685853 40.3486403\n",
            " 23.13241433 38.81153649 28.12275578 32.02102031 29.61448338 31.58011465\n",
            " 36.69667902 39.32864706 31.8064613  40.30952894 30.68208993 31.38415739\n",
            " 22.76426253 37.24437002 33.73023283 40.16366731 27.13899624 21.52315552\n",
            " 49.88094826 31.53329732 37.21474297 16.20495066 26.78003984 20.46215851\n",
            " 41.04299473 37.34578208 18.86303048 49.58448208 18.62810481 29.38655143\n",
            " 20.58755158 36.41556138 37.06975583 49.86432936 18.19990294 31.90205678\n",
            " 48.58347818 20.99016156 20.81832651 14.30880887 23.40909479 25.98258604\n",
            " 20.33994453 31.22568573 16.5001802  21.91515841 16.06907429 31.987977\n",
            " 15.01657841 14.05065214 49.99999793 25.06355735 44.18990978 25.08815673\n",
            " 27.7842643  45.82609956 27.50412403 17.44028415 14.41156053 49.95563607\n",
            " 28.59897211 16.37386583]\n",
            "selection [280 132 480 672 433 479 171 799  30  29] (10,) [12.89757498 13.53872869 13.59032263 13.70558128 13.84373632 13.86989167\n",
            " 14.04702069 14.05065214 14.14480104 14.1596088 ]\n",
            "trainset before adding uncertain samples (490, 10) (490,)\n",
            "trainset after adding uncertain samples (500, 10) (500,)\n",
            "updated train set: (500, 10) (500,) unique(labels): [201 299] [0 1]\n",
            "val set: (802, 10) (802,)\n",
            "\n",
            "Train set: (500, 10)\n",
            "Validation set: (802, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.032258 \n",
            "Classification report for LogisticRegression(C=0.1, class_weight='balanced', penalty='l1',\n",
            "                   solver='liblinear', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.64      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.67      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "final active learning accuracies [69.12442396313364, 69.5852534562212, 72.35023041474655, 70.27649769585254, 71.88940092165899, 73.27188940092167, 72.81105990783409, 72.35023041474655, 79.03225806451613, 79.03225806451613, 78.11059907834101, 78.11059907834101, 78.11059907834101, 78.57142857142857, 79.03225806451613, 79.49308755760369, 78.80184331797236, 78.57142857142857, 79.49308755760369, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.49308755760369, 79.26267281105991, 78.57142857142857, 78.57142857142857, 80.18433179723502, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.26267281105991, 79.49308755760369, 79.72350230414746, 79.95391705069125, 81.33640552995391, 81.33640552995391, 80.18433179723502, 80.18433179723502, 79.95391705069125, 80.4147465437788, 80.4147465437788, 80.4147465437788, 79.95391705069125, 79.95391705069125, 80.18433179723502, 79.49308755760369, 80.18433179723502, 79.72350230414746, 79.03225806451613]\n",
            "saved /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-20.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['.DS_Store', 'Base classifiers', 'README.md', 'all_training.csv', 'Results', 'Active_learning.ipynb', 'Model_select.ipynb', 'Graphs', '.git', '.vscode']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.87557603686636,\n",
            "          80.18433179723502,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          75.11520737327189,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.57142857142857,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.58064516129032,\n",
            "          74.19354838709677,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          80.64516129032258,\n",
            "          78.80184331797236,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          76.49769585253456,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          81.33640552995391,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.12442396313364,\n",
            "          75.34562211981567,\n",
            "          76.26728110599078,\n",
            "          76.72811059907833,\n",
            "          77.41935483870968,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MinStdSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          69.12442396313364,\n",
            "          69.5852534562212,\n",
            "          72.35023041474655,\n",
            "          70.27649769585254,\n",
            "          71.88940092165899,\n",
            "          73.27188940092167,\n",
            "          72.81105990783409,\n",
            "          72.35023041474655,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          81.33640552995391,\n",
            "          81.33640552995391,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          72.11981566820278,\n",
            "          69.81566820276498,\n",
            "          69.35483870967742,\n",
            "          70.73732718894009,\n",
            "          72.58064516129032,\n",
            "          74.88479262672811,\n",
            "          71.42857142857143,\n",
            "          71.88940092165899,\n",
            "          71.88940092165899,\n",
            "          76.95852534562212,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          71.6589861751152,\n",
            "          77.41935483870968,\n",
            "          70.73732718894009,\n",
            "          73.04147465437788,\n",
            "          73.73271889400922,\n",
            "          74.65437788018433,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.26267281105991,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          81.5668202764977,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          79.72350230414746,\n",
            "          80.87557603686636,\n",
            "          79.49308755760369,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          81.5668202764977,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          80.4147465437788,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          71.42857142857143,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "{'LogModel': {'RandomSelection': {'250': [[80.4147465437788, 79.95391705069125]], '125': [[79.03225806451613, 79.03225806451613, 79.72350230414746, 79.03225806451613]], '50': [[76.95852534562212, 71.42857142857143, 78.3410138248848, 78.3410138248848, 78.80184331797236, 79.26267281105991, 79.72350230414746, 79.26267281105991, 79.72350230414746, 79.26267281105991]], '25': [[80.4147465437788, 79.95391705069125, 79.49308755760369, 80.64516129032258, 78.11059907834101, 77.64976958525345, 76.95852534562212, 79.95391705069125, 80.18433179723502, 79.26267281105991, 79.49308755760369, 79.49308755760369, 79.26267281105991, 79.03225806451613, 79.72350230414746, 79.95391705069125, 79.03225806451613, 79.03225806451613, 79.49308755760369, 79.03225806451613]], '10': [[77.18894009216591, 79.26267281105991, 80.64516129032258, 79.95391705069125, 78.80184331797236, 79.95391705069125, 80.64516129032258, 80.18433179723502, 81.5668202764977, 80.18433179723502, 80.87557603686636, 80.64516129032258, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.72350230414746, 80.18433179723502, 80.4147465437788, 79.72350230414746, 80.4147465437788, 81.10599078341014, 79.72350230414746, 80.87557603686636, 79.49308755760369, 80.87557603686636, 80.4147465437788, 79.95391705069125, 79.95391705069125, 79.49308755760369, 79.72350230414746, 80.64516129032258, 78.80184331797236, 79.95391705069125, 79.26267281105991, 80.64516129032258, 79.49308755760369, 80.18433179723502, 80.18433179723502, 80.64516129032258, 81.10599078341014, 81.5668202764977, 81.33640552995391, 80.87557603686636, 80.4147465437788, 80.64516129032258, 80.64516129032258, 79.49308755760369, 79.95391705069125, 80.64516129032258, 79.95391705069125]]}, 'MarginSamplingSelection': {'250': [[79.95391705069125, 80.4147465437788]], '125': [[80.64516129032258, 78.80184331797236, 80.4147465437788, 80.64516129032258]], '50': [[69.12442396313364, 75.34562211981567, 76.26728110599078, 76.72811059907833, 77.41935483870968, 78.11059907834101, 79.26267281105991, 79.49308755760369, 79.95391705069125, 80.4147465437788]], '25': [[76.26728110599078, 79.49308755760369, 78.80184331797236, 79.49308755760369, 78.57142857142857, 76.49769585253456, 79.26267281105991, 79.26267281105991, 80.4147465437788, 79.95391705069125, 80.64516129032258, 81.33640552995391, 80.18433179723502, 80.18433179723502, 80.4147465437788, 79.03225806451613, 79.49308755760369, 80.18433179723502, 79.95391705069125, 80.18433179723502]], '10': [[72.58064516129032, 74.19354838709677, 76.036866359447, 76.26728110599078, 76.26728110599078, 77.64976958525345, 78.57142857142857, 78.80184331797236, 79.95391705069125, 79.49308755760369, 79.95391705069125, 80.18433179723502, 80.18433179723502, 79.95391705069125, 79.95391705069125, 80.18433179723502, 79.95391705069125, 79.95391705069125, 79.95391705069125, 79.72350230414746, 80.4147465437788, 79.95391705069125, 80.4147465437788, 80.18433179723502, 80.87557603686636, 80.87557603686636, 80.64516129032258, 80.87557603686636, 80.87557603686636, 80.4147465437788, 80.4147465437788, 79.95391705069125, 80.18433179723502, 79.95391705069125, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.87557603686636, 80.64516129032258, 80.87557603686636, 80.64516129032258, 80.87557603686636, 81.10599078341014, 81.10599078341014, 81.10599078341014, 80.87557603686636, 80.87557603686636, 80.87557603686636, 80.87557603686636]]}, 'EntropySelection': {'250': [[78.11059907834101, 79.95391705069125]], '125': [[79.72350230414746, 80.18433179723502, 79.72350230414746, 79.95391705069125]], '50': [[78.57142857142857, 77.18894009216591, 78.3410138248848, 78.3410138248848, 79.95391705069125, 80.64516129032258, 80.87557603686636, 81.10599078341014, 81.10599078341014, 81.10599078341014]], '25': [[77.41935483870968, 75.11520737327189, 77.18894009216591, 78.3410138248848, 79.49308755760369, 80.87557603686636, 80.4147465437788, 79.26267281105991, 80.18433179723502, 80.64516129032258, 80.64516129032258, 80.64516129032258, 78.57142857142857, 78.80184331797236, 79.26267281105991, 79.26267281105991, 79.95391705069125, 80.4147465437788, 80.18433179723502, 80.18433179723502]], '10': [[76.72811059907833, 78.3410138248848, 79.95391705069125, 79.49308755760369, 79.03225806451613, 79.49308755760369, 79.03225806451613, 80.87557603686636, 80.64516129032258, 79.95391705069125, 80.87557603686636, 80.18433179723502, 81.10599078341014, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.4147465437788, 80.4147465437788, 80.18433179723502, 80.18433179723502, 80.4147465437788, 79.72350230414746, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.49308755760369, 79.72350230414746, 79.49308755760369, 79.72350230414746, 80.18433179723502, 80.4147465437788, 80.18433179723502, 79.95391705069125, 80.64516129032258, 80.18433179723502, 81.10599078341014, 80.87557603686636, 80.87557603686636, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.4147465437788, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.64516129032258, 80.4147465437788, 80.64516129032258, 79.95391705069125, 80.4147465437788]]}, 'MinStdSelection': {'250': [[78.11059907834101, 79.03225806451613]], '125': [[79.72350230414746, 79.72350230414746, 79.26267281105991, 80.18433179723502]], '50': [[71.6589861751152, 77.41935483870968, 70.73732718894009, 73.04147465437788, 73.73271889400922, 74.65437788018433, 79.49308755760369, 78.57142857142857, 78.80184331797236, 79.95391705069125]], '25': [[72.11981566820278, 69.81566820276498, 69.35483870967742, 70.73732718894009, 72.58064516129032, 74.88479262672811, 71.42857142857143, 71.88940092165899, 71.88940092165899, 76.95852534562212, 77.88018433179722, 77.88018433179722, 78.3410138248848, 78.57142857142857, 78.3410138248848, 79.26267281105991, 80.18433179723502, 79.95391705069125, 79.95391705069125, 79.72350230414746]], '10': [[69.12442396313364, 69.5852534562212, 72.35023041474655, 70.27649769585254, 71.88940092165899, 73.27188940092167, 72.81105990783409, 72.35023041474655, 79.03225806451613, 79.03225806451613, 78.11059907834101, 78.11059907834101, 78.11059907834101, 78.57142857142857, 79.03225806451613, 79.49308755760369, 78.80184331797236, 78.57142857142857, 79.49308755760369, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.49308755760369, 79.26267281105991, 78.57142857142857, 78.57142857142857, 80.18433179723502, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.26267281105991, 79.49308755760369, 79.72350230414746, 79.95391705069125, 81.33640552995391, 81.33640552995391, 80.18433179723502, 80.18433179723502, 79.95391705069125, 80.4147465437788, 80.4147465437788, 80.4147465437788, 79.95391705069125, 79.95391705069125, 80.18433179723502, 79.49308755760369, 80.18433179723502, 79.72350230414746, 79.03225806451613]]}}}\n",
            "{'LogModel': {'EntropySelection': {'10': [[76.72811059907833, 78.3410138248848, 79.95391705069125, 79.49308755760369, 79.03225806451613, 79.49308755760369, 79.03225806451613, 80.87557603686636, 80.64516129032258, 79.95391705069125, 80.87557603686636, 80.18433179723502, 81.10599078341014, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.4147465437788, 80.4147465437788, 80.18433179723502, 80.18433179723502, 80.4147465437788, 79.72350230414746, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.49308755760369, 79.72350230414746, 79.49308755760369, 79.72350230414746, 80.18433179723502, 80.4147465437788, 80.18433179723502, 79.95391705069125, 80.64516129032258, 80.18433179723502, 81.10599078341014, 80.87557603686636, 80.87557603686636, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.4147465437788, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.64516129032258, 80.4147465437788, 80.64516129032258, 79.95391705069125, 80.4147465437788]], '125': [[79.72350230414746, 80.18433179723502, 79.72350230414746, 79.95391705069125]], '25': [[77.41935483870968, 75.11520737327189, 77.18894009216591, 78.3410138248848, 79.49308755760369, 80.87557603686636, 80.4147465437788, 79.26267281105991, 80.18433179723502, 80.64516129032258, 80.64516129032258, 80.64516129032258, 78.57142857142857, 78.80184331797236, 79.26267281105991, 79.26267281105991, 79.95391705069125, 80.4147465437788, 80.18433179723502, 80.18433179723502]], '250': [[78.11059907834101, 79.95391705069125]], '50': [[78.57142857142857, 77.18894009216591, 78.3410138248848, 78.3410138248848, 79.95391705069125, 80.64516129032258, 80.87557603686636, 81.10599078341014, 81.10599078341014, 81.10599078341014]]}, 'MarginSamplingSelection': {'10': [[72.58064516129032, 74.19354838709677, 76.036866359447, 76.26728110599078, 76.26728110599078, 77.64976958525345, 78.57142857142857, 78.80184331797236, 79.95391705069125, 79.49308755760369, 79.95391705069125, 80.18433179723502, 80.18433179723502, 79.95391705069125, 79.95391705069125, 80.18433179723502, 79.95391705069125, 79.95391705069125, 79.95391705069125, 79.72350230414746, 80.4147465437788, 79.95391705069125, 80.4147465437788, 80.18433179723502, 80.87557603686636, 80.87557603686636, 80.64516129032258, 80.87557603686636, 80.87557603686636, 80.4147465437788, 80.4147465437788, 79.95391705069125, 80.18433179723502, 79.95391705069125, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.87557603686636, 80.64516129032258, 80.87557603686636, 80.64516129032258, 80.87557603686636, 81.10599078341014, 81.10599078341014, 81.10599078341014, 80.87557603686636, 80.87557603686636, 80.87557603686636, 80.87557603686636]], '125': [[80.64516129032258, 78.80184331797236, 80.4147465437788, 80.64516129032258]], '25': [[76.26728110599078, 79.49308755760369, 78.80184331797236, 79.49308755760369, 78.57142857142857, 76.49769585253456, 79.26267281105991, 79.26267281105991, 80.4147465437788, 79.95391705069125, 80.64516129032258, 81.33640552995391, 80.18433179723502, 80.18433179723502, 80.4147465437788, 79.03225806451613, 79.49308755760369, 80.18433179723502, 79.95391705069125, 80.18433179723502]], '250': [[79.95391705069125, 80.4147465437788]], '50': [[69.12442396313364, 75.34562211981567, 76.26728110599078, 76.72811059907833, 77.41935483870968, 78.11059907834101, 79.26267281105991, 79.49308755760369, 79.95391705069125, 80.4147465437788]]}, 'MinStdSelection': {'10': [[69.12442396313364, 69.5852534562212, 72.35023041474655, 70.27649769585254, 71.88940092165899, 73.27188940092167, 72.81105990783409, 72.35023041474655, 79.03225806451613, 79.03225806451613, 78.11059907834101, 78.11059907834101, 78.11059907834101, 78.57142857142857, 79.03225806451613, 79.49308755760369, 78.80184331797236, 78.57142857142857, 79.49308755760369, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.49308755760369, 79.26267281105991, 78.57142857142857, 78.57142857142857, 80.18433179723502, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.26267281105991, 79.49308755760369, 79.72350230414746, 79.95391705069125, 81.33640552995391, 81.33640552995391, 80.18433179723502, 80.18433179723502, 79.95391705069125, 80.4147465437788, 80.4147465437788, 80.4147465437788, 79.95391705069125, 79.95391705069125, 80.18433179723502, 79.49308755760369, 80.18433179723502, 79.72350230414746, 79.03225806451613]], '125': [[79.72350230414746, 79.72350230414746, 79.26267281105991, 80.18433179723502]], '25': [[72.11981566820278, 69.81566820276498, 69.35483870967742, 70.73732718894009, 72.58064516129032, 74.88479262672811, 71.42857142857143, 71.88940092165899, 71.88940092165899, 76.95852534562212, 77.88018433179722, 77.88018433179722, 78.3410138248848, 78.57142857142857, 78.3410138248848, 79.26267281105991, 80.18433179723502, 79.95391705069125, 79.95391705069125, 79.72350230414746]], '250': [[78.11059907834101, 79.03225806451613]], '50': [[71.6589861751152, 77.41935483870968, 70.73732718894009, 73.04147465437788, 73.73271889400922, 74.65437788018433, 79.49308755760369, 78.57142857142857, 78.80184331797236, 79.95391705069125]]}, 'RandomSelection': {'10': [[77.18894009216591, 79.26267281105991, 80.64516129032258, 79.95391705069125, 78.80184331797236, 79.95391705069125, 80.64516129032258, 80.18433179723502, 81.5668202764977, 80.18433179723502, 80.87557603686636, 80.64516129032258, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.72350230414746, 80.18433179723502, 80.4147465437788, 79.72350230414746, 80.4147465437788, 81.10599078341014, 79.72350230414746, 80.87557603686636, 79.49308755760369, 80.87557603686636, 80.4147465437788, 79.95391705069125, 79.95391705069125, 79.49308755760369, 79.72350230414746, 80.64516129032258, 78.80184331797236, 79.95391705069125, 79.26267281105991, 80.64516129032258, 79.49308755760369, 80.18433179723502, 80.18433179723502, 80.64516129032258, 81.10599078341014, 81.5668202764977, 81.33640552995391, 80.87557603686636, 80.4147465437788, 80.64516129032258, 80.64516129032258, 79.49308755760369, 79.95391705069125, 80.64516129032258, 79.95391705069125]], '125': [[79.03225806451613, 79.03225806451613, 79.72350230414746, 79.03225806451613]], '25': [[80.4147465437788, 79.95391705069125, 79.49308755760369, 80.64516129032258, 78.11059907834101, 77.64976958525345, 76.95852534562212, 79.95391705069125, 80.18433179723502, 79.26267281105991, 79.49308755760369, 79.49308755760369, 79.26267281105991, 79.03225806451613, 79.72350230414746, 79.95391705069125, 79.03225806451613, 79.03225806451613, 79.49308755760369, 79.03225806451613]], '250': [[80.4147465437788, 79.95391705069125]], '50': [[76.95852534562212, 71.42857142857143, 78.3410138248848, 78.3410138248848, 78.80184331797236, 79.26267281105991, 79.72350230414746, 79.26267281105991, 79.72350230414746, 79.26267281105991]]}}}\n"
          ]
        }
      ],
      "source": [
        "(X, y) = data_prep()\n",
        "(X_train_full, y_train_full, X_test, y_test) = split(trainset_size)\n",
        "print ('train:', X_train_full.shape, y_train_full.shape)\n",
        "print ('test :', X_test.shape, y_test.shape)\n",
        "classes = len(np.unique(y))\n",
        "print ('unique classes', classes)\n",
        "\n",
        "def pickle_save(fname, data):\n",
        "  filehandler = open(fname,\"wb\")\n",
        "  pickle.dump(data,filehandler)\n",
        "  filehandler.close() \n",
        "  print('saved', fname, os.getcwd(), os.listdir())\n",
        "\n",
        "def pickle_load(fname):\n",
        "  print(os.getcwd(), os.listdir())\n",
        "  file = open(fname,'rb')\n",
        "  data = pickle.load(file)\n",
        "  file.close()\n",
        "  print(data)\n",
        "  return data\n",
        "  \n",
        "def experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
        "    algos_temp = []\n",
        "    print ('stopping at:', max_queried)\n",
        "    count = 0\n",
        "    for model_object in models:\n",
        "      if model_object.__name__ not in d:\n",
        "          d[model_object.__name__] = {}\n",
        "      \n",
        "      for selection_function in selection_functions:\n",
        "        if selection_function.__name__ not in d[model_object.__name__]:\n",
        "            d[model_object.__name__][selection_function.__name__] = {}\n",
        "        \n",
        "        for k in Ks:\n",
        "            d[model_object.__name__][selection_function.__name__][str(k)] = []           \n",
        "            \n",
        "            for i in range(0, repeats):\n",
        "                count+=1\n",
        "                if count >= contfrom:\n",
        "                    print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
        "                    alg = TheAlgorithm(k, \n",
        "                                       model_object, \n",
        "                                       selection_function\n",
        "                                       )\n",
        "                    alg.run(X_train_full, y_train_full, X_test, y_test)\n",
        "                    d[model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
        "                    fname = '/Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-' + str(count) + '.pkl'\n",
        "                    pickle_save(fname, d)\n",
        "                    if count % 5 == 0:\n",
        "                        print(json.dumps(d, indent=2, sort_keys=True))\n",
        "                    print ()\n",
        "                    print ('---------------------------- FINISHED ---------------------------')\n",
        "                    print ()\n",
        "    return d\n",
        "\n",
        "\n",
        "max_queried = 500 \n",
        "\n",
        "repeats = 1\n",
        "\n",
        "models = [LogModel] \n",
        "# models = [SvmModel, RfModel, LogModel, GDBCModel, KnnModel] \n",
        "\n",
        "# selection_functions = [RandomSelection] \n",
        "selection_functions = [RandomSelection, MarginSamplingSelection, EntropySelection, MinStdSelection] \n",
        "\n",
        "Ks = [250,125,50,25,10] \n",
        "\n",
        "d = {}\n",
        "stopped_at = -1 \n",
        "\n",
        "# print('directory dump including pickle files:', os.getcwd(), np.sort(os.listdir()))  \n",
        "# d = pickle_load('Active-learning-experiment-' + str(stopped_at) + '.pkl')  \n",
        "# print(json.dumps(d, indent=2, sort_keys=True))\n",
        "\n",
        "d = experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
        "print (d)\n",
        "results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So which is the better model? under the stopping condition and hyper parameters\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f11eddc35366461a8389611af36955b8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%matplotlib widget\n",
        "# %matplotlib inline\n",
        "def performance_plot(fully_supervised_accuracy, dic, models, selection_functions, Ks, repeats):  \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot([0,500],[fully_supervised_accuracy, fully_supervised_accuracy],label = 'upper-bound')\n",
        "    for model_object in models:\n",
        "      for selection_function in selection_functions:\n",
        "        for idx, k in enumerate(Ks):\n",
        "            x = np.arange(float(Ks[idx]), 500 + float(Ks[idx]), float(Ks[idx]))            \n",
        "            Sum = np.array(dic[model_object][selection_function][k][0])\n",
        "            for i in range(1, repeats):\n",
        "                Sum = Sum + np.array(dic[model_object][selection_function][k][i])\n",
        "            mean = Sum / repeats\n",
        "            ax.plot(x, mean, label = model_object[0:3] + '-' + selection_function[0:3] + '-' + str(k))\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.set_xlim([50,500])\n",
        "    ax.set_ylim([70,83])\n",
        "    ax.grid(True)\n",
        "    mplcursors.cursor()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# models_str = ['SvmModel', 'RfModel', 'LogModel','GDBCModel','KnnModel']\n",
        "models_str = ['LogModel']\n",
        "selection_functions_str = ['RandomSelection', 'MarginSamplingSelection', 'EntropySelection', 'MinStdSelection']\n",
        "# selection_functions_str = ['RandomSelection']\n",
        "Ks_str = ['250','125','50','25','10'] \n",
        "repeats = 10\n",
        "# random_forest_upper_bound = 89.\n",
        "# svm_upper_bound = 87.\n",
        "log_upper_bound = 87.\n",
        "# gdbc_upper_bound = 86.\n",
        "# knn_upper_bound = 86.\n",
        "total_experiments = len(models_str) * len(selection_functions_str) * len(Ks_str) * repeats\n",
        "\n",
        "print('So which is the better model? under the stopping condition and hyper parameters')\n",
        "# performance_plot(random_forest_upper_bound, d, ['RfModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# performance_plot(svm_upper_bound, d, ['SvmModel'] , selection_functions_str    , Ks_str, 1)\n",
        "performance_plot(log_upper_bound, d, ['LogModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# performance_plot(gdbc_upper_bound, d, ['GDBCModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# performance_plot(log_upper_bound, d, ['KnnModel'] , selection_functions_str    , Ks_str, 1)"
      ]
    }
  ]
}