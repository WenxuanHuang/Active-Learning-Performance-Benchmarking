{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic.ipynb",
      "provenance": [],
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d"
    },
    "kernelspec": {
      "name": "python382jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
      "display_name": "Python 3.8.2 64-bit"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from scipy.special import expit\n",
        "from scipy import stats\n",
        "from pylab import rcParams\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, \\\n",
        "    GradientBoostingClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "# pd.options.display.max_rows = 20\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "max_queried = 500\n",
        "trainset_size = 1302"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        " def download():\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/ML-for-COVID-19-dataset/main/all_training.csv\", sep=',')\n",
        "    # Column selection\n",
        "    df = data.iloc[:,np.r_[3:34]].copy()\n",
        "\n",
        "    # define row and column index\n",
        "    col = df.columns\n",
        "    row = [i for i in range(df.shape[0])]\n",
        "\n",
        "    # define imputer\n",
        "    imputer = IterativeImputer(estimator=linear_model.BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
        "    # fit on the dataset\n",
        "    imputer.fit(df)\n",
        "    # transform the dataset\n",
        "    df_imputed = imputer.transform(df)\n",
        "    # convert back to pandas dataframe and rename back to df_normalized\n",
        "    df = pd.DataFrame(data=df_imputed, index=row, columns=col)\n",
        "\n",
        "    # Data preparation\n",
        "    X = df\n",
        "    y = data.target\n",
        "    X = X.to_numpy()    \n",
        "    print ('df:', X.shape, y.shape)\n",
        "    return (X, y)\n",
        "\n",
        "\n",
        "def split(train_size):\n",
        "    X_train_full = X[:train_size]\n",
        "    y_train_full = y[:train_size]\n",
        "    X_test = X[train_size:]\n",
        "    y_test = y[train_size:]\n",
        "    return (X_train_full, y_train_full, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5wm-nCFojyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a3f18c-5c4a-4fc5-f5fc-9b699e485fdd"
      },
      "source": [
        "# Using robust scaling as normalization method\n",
        "\n",
        "# # create a scaler object\n",
        "# scaler = RobustScaler()\n",
        "# # fit and transform the data\n",
        "# df_normalized = pd.DataFrame(scaler.fit_transform(df_features), columns=df_features.columns)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTChXqRGYTBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc129e7a-062e-4b20-951d-474768d269e8"
      },
      "source": [
        "# Cross validation\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SvmModel(BaseModel):\n",
        "\n",
        "    model_type = 'Support Vector Machine with linear Kernel'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training svm...')\n",
        "        self.classifier = SVC(C=1, kernel='linear', probability=True,\n",
        "                              class_weight=c_weight)\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    model_type = 'Multinominal Logistic Regression' \n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training multinomial logistic regression...')\n",
        "        train_samples = X_train.shape[0]\n",
        "        self.classifier = LogisticRegression(\n",
        "            C=50. / train_samples,\n",
        "            multi_class='multinomial',\n",
        "            penalty='l1',\n",
        "            solver='saga',\n",
        "            tol=0.1,\n",
        "            class_weight=c_weight,\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class RfModel(BaseModel):\n",
        "\n",
        "    model_type = 'Random Forest'\n",
        "    \n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training random forest...')\n",
        "        self.classifier = RandomForestClassifier(n_estimators=500, class_weight=c_weight)\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    # we train normally and get probabilities for the validation set. i.e., we use the probabilities to select the most uncertain samples\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('Train set:', X_train.shape, 'y:', y_train.shape)\n",
        "        print ('Val   set:', X_val.shape)\n",
        "        print ('Test  set:', X_test.shape)\n",
        "        t0 = time.time()\n",
        "        (X_train, X_val, X_test, self.val_y_predicted,\n",
        "         self.test_y_predicted) = \\\n",
        "            self.model_object.fit_predict(X_train, y_train, X_val, X_test, c_weight)\n",
        "        self.run_time = time.time() - t0\n",
        "        return (X_train, X_val, X_test)  # we return them in case we use PCA, with all the other algorithms, this is not needed.\n",
        "\n",
        "    # we want accuracy only for the test set\n",
        "\n",
        "    def get_test_accuracy(self, i, y_test):\n",
        "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        print('--------------------------------')\n",
        "        print('y-test set:',y_test.shape)\n",
        "        print('Example run in %.3f s' % self.run_time,'\\n')\n",
        "        print(\"Accuracy rate for %f \" % (classif_rate))    \n",
        "        print(\"Classification report for classifier %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
        "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
        "        print('--------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseSelectionFunction(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def select(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomSelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        random_state = check_random_state(0)\n",
        "        selection = np.random.choice(probas_val.shape[0], initial_labeled_samples, replace=False)\n",
        "\n",
        "#     print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',initial_labeled_samples)\n",
        "\n",
        "        return selection\n",
        "\n",
        "\n",
        "class EntropySelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "        selection = (np.argsort(e)[::-1])[:initial_labeled_samples]\n",
        "        return selection\n",
        "      \n",
        "      \n",
        "class MarginSamplingSelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        rev = np.sort(probas_val, axis=1)[:, ::-1]\n",
        "        values = rev[:, 0] - rev[:, 1]\n",
        "        selection = np.argsort(values)[:initial_labeled_samples]\n",
        "        return selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Normalize(object):\n",
        "    \n",
        "    def normalize(self, X_train, X_val, X_test):\n",
        "        self.scaler = RobustScaler()\n",
        "        X_train = self.scaler.fit_transform(X_train)\n",
        "        X_val   = self.scaler.transform(X_val)\n",
        "        X_test  = self.scaler.transform(X_test)\n",
        "        return (X_train, X_val, X_test) \n",
        "    \n",
        "    def inverse(self, X_train, X_val, X_test):\n",
        "        X_train = self.scaler.inverse_transform(X_train)\n",
        "        X_val   = self.scaler.inverse_transform(X_val)\n",
        "        X_test  = self.scaler.inverse_transform(X_test)\n",
        "        return (X_train, X_val, X_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_k_random_samples(initial_labeled_samples, X_train_full,\n",
        "                         y_train_full):\n",
        "    random_state = check_random_state(0)\n",
        "    permutation = np.random.choice(trainset_size,\n",
        "                                   initial_labeled_samples,\n",
        "                                   replace=False)\n",
        "    print ()\n",
        "    print ('initial random chosen samples', permutation.shape),\n",
        "#            permutation)\n",
        "    X_train = X_train_full[permutation]\n",
        "    y_train = y_train_full[permutation]\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "    bin_count = np.bincount(y_train.astype('int64'))\n",
        "    unique = np.unique(y_train.astype('int64'))\n",
        "    print (\n",
        "        'initial train set:',\n",
        "        X_train.shape,\n",
        "        y_train.shape,\n",
        "        'unique(labels):',\n",
        "        bin_count,\n",
        "        unique,\n",
        "        )\n",
        "    return (permutation, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TheAlgorithm(object):\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    def __init__(self, initial_labeled_samples, model_object, selection_function):\n",
        "        self.initial_labeled_samples = initial_labeled_samples\n",
        "        self.model_object = model_object\n",
        "        self.sample_selection_function = selection_function\n",
        "\n",
        "    def run(self, X_train_full, y_train_full, X_test, y_test):\n",
        "\n",
        "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
        "\n",
        "        (permutation, X_train, y_train) = \\\n",
        "            get_k_random_samples(self.initial_labeled_samples,\n",
        "                                 X_train_full, y_train_full)\n",
        "        self.queried = self.initial_labeled_samples\n",
        "        self.samplecount = [self.initial_labeled_samples]\n",
        "\n",
        "        # permutation, X_train, y_train = get_equally_k_random_samples(self.initial_labeled_samples,classes)\n",
        "\n",
        "        # assign the val set the rest of the 'unlabelled' training data\n",
        "\n",
        "        X_val = np.array([])\n",
        "        y_val = np.array([])\n",
        "        X_val = np.copy(X_train_full)\n",
        "        X_val = np.delete(X_val, permutation, axis=0)\n",
        "        y_val = np.copy(y_train_full)\n",
        "        y_val = np.delete(y_val, permutation, axis=0)\n",
        "        print ('val set:', X_val.shape, y_val.shape, permutation.shape)\n",
        "        print ()\n",
        "\n",
        "        # normalize data\n",
        "\n",
        "        normalizer = Normalize()\n",
        "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)   \n",
        "        \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(1, y_test)\n",
        "\n",
        "        # fpfn = self.clf_model.test_y_predicted.ravel() != y_val.ravel()\n",
        "        # print(fpfn)\n",
        "        # self.fpfncount = []\n",
        "        # self.fpfncount.append(fpfn.sum() / y_test.shape[0] * 100)\n",
        "\n",
        "        while self.queried < max_queried:\n",
        "\n",
        "            active_iteration += 1\n",
        "\n",
        "            # get validation probabilities\n",
        "\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
        "            print ('val predicted:',\n",
        "                   self.clf_model.val_y_predicted.shape,\n",
        "                   self.clf_model.val_y_predicted)\n",
        "            print ('probabilities:', probas_val.shape, '\\n',\n",
        "                   np.argmax(probas_val, axis=1))\n",
        "\n",
        "            # select samples using a selection function\n",
        "\n",
        "            uncertain_samples = \\\n",
        "                self.sample_selection_function.select(probas_val, self.initial_labeled_samples)\n",
        "\n",
        "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
        " \n",
        "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
        "\n",
        "            # get the uncertain samples from the validation set\n",
        "\n",
        "            print ('trainset before', X_train.shape, y_train.shape)\n",
        "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
        "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
        "            print ('trainset after', X_train.shape, y_train.shape)\n",
        "            self.samplecount.append(X_train.shape[0])\n",
        "\n",
        "            bin_count = np.bincount(y_train.astype('int64'))\n",
        "            unique = np.unique(y_train.astype('int64'))\n",
        "            print (\n",
        "                'updated train set:',\n",
        "                X_train.shape,\n",
        "                y_train.shape,\n",
        "                'unique(labels):',\n",
        "                bin_count,\n",
        "                unique,\n",
        "                )\n",
        "\n",
        "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
        "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
        "            print ('val set:', X_val.shape, y_val.shape)\n",
        "            print ()\n",
        "\n",
        "            # normalize again after creating the 'new' train/test sets\n",
        "            normalizer = Normalize()\n",
        "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
        "\n",
        "            self.queried += self.initial_labeled_samples\n",
        "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
        "\n",
        "        print ('final active learning accuracies',\n",
        "               self.clf_model.accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [163 187] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.058 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.86       321\n",
            "           1       0.62      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.73      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [169 191] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.202 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86       321\n",
            "           1       0.60      0.59      0.60       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.73      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [175 195] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.133 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86       321\n",
            "           1       0.60      0.60      0.60       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.73      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1\n",
            " 0 1 1 1 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [180 200] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.099 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85       321\n",
            "           1       0.58      0.59      0.59       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [183 207] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 638.396 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.66      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0]\n",
            "probabilities: (912, 2) \n",
            " [1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [186 214] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.073 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85       321\n",
            "           1       0.58      0.60      0.59       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [190 220] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.083 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1\n",
            " 1 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [198 222] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.116 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [204 226] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.112 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [208 232] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.088 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.72      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [213 237] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.110 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [218 242] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.096 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1\n",
            " 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1\n",
            " 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [222 248] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.112 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [228 252] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.129 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [234 256] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.124 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0\n",
            " 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0\n",
            " 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [240 260] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.139 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "final active learning accuracies [64.0552995391705, 71.19815668202764, 71.19815668202764, 70.27649769585254, 72.58064516129032, 71.88940092165899, 74.88479262672811, 74.65437788018433, 77.18894009216591, 76.49769585253456, 75.80645161290323, 77.41935483870968, 78.57142857142857, 77.64976958525345, 79.03225806451613, 78.3410138248848, 78.80184331797236, 77.64976958525345, 78.3410138248848, 77.18894009216591, 76.95852534562212, 78.3410138248848, 78.57142857142857, 78.11059907834101, 77.88018433179722, 78.57142857142857, 73.963133640553, 74.19354838709677, 79.72350230414746, 80.4147465437788, 81.10599078341014, 75.11520737327189, 79.03225806451613, 79.49308755760369, 79.72350230414746, 79.26267281105991, 79.03225806451613, 78.11059907834101, 79.49308755760369, 78.3410138248848, 78.3410138248848, 78.11059907834101, 78.11059907834101, 77.88018433179722, 78.3410138248848, 79.03225806451613, 78.3410138248848, 79.49308755760369, 79.49308755760369, 78.80184331797236]\n",
            "saved Active-learning-experiment-5.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Decision_tree.ipynb', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Logistic_Scikit.ipynb', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "{\n",
            "  \"SvmModel\": {\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.0552995391705,\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          72.58064516129032,\n",
            "          71.88940092165899,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          75.80645161290323,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          74.19354838709677,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          75.11520737327189,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          39.1705069124424,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          74.65437788018433,\n",
            "          74.19354838709677,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.18894009216591,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          64.28571428571429,\n",
            "          63.133640552995395,\n",
            "          65.2073732718894,\n",
            "          71.42857142857143,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 6, using model = SvmModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [109 141] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.046 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83       321\n",
            "           1       0.52      0.49      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.67      0.67       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [276 224] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 3.945 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.11520737327189, 77.64976958525345]\n",
            "saved Active-learning-experiment-6.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Decision_tree.ipynb', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Logistic_Scikit.ipynb', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 7, using model = SvmModel, selection_function = MarginSamplingSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [48 77] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 71.198157 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.79       321\n",
            "           1       0.46      0.65      0.54       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.69      0.66       434\n",
            "weighted avg       0.75      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[236  85]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 1 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [125 125] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.036 s \n",
            "\n",
            "Accuracy rate for 70.276498 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.70      0.78       321\n",
            "           1       0.46      0.72      0.56       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.67      0.71      0.67       434\n",
            "weighted avg       0.77      0.70      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[224  97]\n",
            " [ 32  81]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 1 ... 1 1 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 1 ... 1 1 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [202 173] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.114 s \n",
            "\n",
            "Accuracy rate for 71.428571 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.72      0.79       321\n",
            "           1       0.47      0.70      0.56       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.67      0.71      0.67       434\n",
            "weighted avg       0.77      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[231  90]\n",
            " [ 34  79]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [266 234] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.183 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.80       321\n",
            "           1       0.47      0.65      0.55       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.67      0.70      0.67       434\n",
            "weighted avg       0.76      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[239  82]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "final active learning accuracies [71.19815668202764, 70.27649769585254, 71.42857142857143, 72.11981566820278]\n",
            "saved Active-learning-experiment-7.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Decision_tree.ipynb', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Logistic_Scikit.ipynb', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 8, using model = SvmModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [20 30] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 55.299539 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.50      0.62       321\n",
            "           1       0.33      0.72      0.46       113\n",
            "\n",
            "    accuracy                           0.55       434\n",
            "   macro avg       0.58      0.61      0.54       434\n",
            "weighted avg       0.70      0.55      0.58       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[159 162]\n",
            " [ 32  81]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [57 43] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 63.594470 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.63      0.72       321\n",
            "           1       0.38      0.66      0.49       113\n",
            "\n",
            "    accuracy                           0.64       434\n",
            "   macro avg       0.61      0.64      0.60       434\n",
            "weighted avg       0.72      0.64      0.66       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[201 120]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [82 68] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.018 s \n",
            "\n",
            "Accuracy rate for 66.820276 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.69      0.76       321\n",
            "           1       0.41      0.60      0.49       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.62      0.65      0.62       434\n",
            "weighted avg       0.72      0.67      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[222  99]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [109  91] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.031 s \n",
            "\n",
            "Accuracy rate for 68.433180 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.71      0.77       321\n",
            "           1       0.42      0.60      0.50       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.63      0.66      0.63       434\n",
            "weighted avg       0.73      0.68      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[229  92]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [132 118] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.051 s \n",
            "\n",
            "Accuracy rate for 67.511521 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.69      0.76       321\n",
            "           1       0.42      0.62      0.50       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.63      0.66      0.63       434\n",
            "weighted avg       0.73      0.68      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[223  98]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [160 140] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.059 s \n",
            "\n",
            "Accuracy rate for 70.046083 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.72      0.78       321\n",
            "           1       0.45      0.64      0.53       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.65      0.68      0.65       434\n",
            "weighted avg       0.74      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[232  89]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [199 151] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.080 s \n",
            "\n",
            "Accuracy rate for 71.658986 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.76      0.80       321\n",
            "           1       0.47      0.60      0.53       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[243  78]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [218 182] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.134 s \n",
            "\n",
            "Accuracy rate for 70.737327 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.74      0.79       321\n",
            "           1       0.45      0.61      0.52       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[238  83]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [243 207] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.135 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.76      0.80       321\n",
            "           1       0.47      0.61      0.53       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[244  77]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [269 231] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.195 s \n",
            "\n",
            "Accuracy rate for 73.041475 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.49      0.62      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "final active learning accuracies [55.29953917050692, 63.594470046082954, 66.82027649769586, 68.4331797235023, 67.51152073732719, 70.04608294930875, 71.6589861751152, 70.73732718894009, 72.11981566820278, 73.04147465437788]\n",
            "saved Active-learning-experiment-8.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Decision_tree.ipynb', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 9, using model = SvmModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [11 14] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 71.889401 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.81      0.81       321\n",
            "           1       0.46      0.45      0.46       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.63      0.63      0.63       434\n",
            "weighted avg       0.72      0.72      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[261  60]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [1 1 1 ... 0 1 1]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [29 21] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 70.276498 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.78      0.79       321\n",
            "           1       0.44      0.50      0.46       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.63      0.64      0.63       434\n",
            "weighted avg       0.72      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[249  72]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [42 33] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [49 51] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.85       321\n",
            "           1       0.56      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [63 62] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.023 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [72 78] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.020 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86       321\n",
            "           1       0.60      0.60      0.60       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.73      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [94 81] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.028 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [105  95] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.034 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [118 107] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.049 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [127 123] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.122 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [136 139] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.635 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [151 149] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.063 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.66      0.56      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [163 162] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.174 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [174 176] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.196 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [191 184] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.168 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [198 202] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.269 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [209 216] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.301 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [225 225] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.199 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [242 233] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.413 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [253 247] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.197 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.86       321\n",
            "           1       0.62      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.73      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "final active learning accuracies [71.88940092165899, 70.27649769585254, 77.88018433179722, 77.18894009216591, 78.3410138248848, 79.03225806451613, 77.64976958525345, 79.26267281105991, 79.95391705069125, 80.18433179723502, 79.95391705069125, 81.10599078341014, 80.64516129032258, 80.4147465437788, 80.64516129032258, 80.64516129032258, 80.64516129032258, 80.18433179723502, 79.72350230414746, 79.72350230414746]\n",
            "saved Active-learning-experiment-9.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Decision_tree.ipynb', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 10, using model = SvmModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [3 7] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 73.502304 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.82       321\n",
            "           1       0.49      0.54      0.51       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.67      0.67       434\n",
            "weighted avg       0.74      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[258  63]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [1 1 1 ... 1 1 1]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [ 4 16] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 62.211982 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.66      0.72       321\n",
            "           1       0.35      0.52      0.42       113\n",
            "\n",
            "    accuracy                           0.62       434\n",
            "   macro avg       0.57      0.59      0.57       434\n",
            "weighted avg       0.68      0.62      0.64       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[211 110]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 1 0]\n",
            "probabilities: (1282, 2) \n",
            " [1 1 1 ... 1 1 1]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [ 4 26] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 66.589862 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.73      0.76       321\n",
            "           1       0.39      0.50      0.44       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.60      0.61      0.60       434\n",
            "weighted avg       0.70      0.67      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[233  88]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [1 1 1 ... 0 1 1]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [12 28] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 59.216590 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.63      0.70       321\n",
            "           1       0.31      0.48      0.38       113\n",
            "\n",
            "    accuracy                           0.59       434\n",
            "   macro avg       0.54      0.56      0.54       434\n",
            "weighted avg       0.65      0.59      0.61       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[203 118]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 1 0]\n",
            "probabilities: (1262, 2) \n",
            " [1 1 1 ... 1 1 1]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [20 30] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 53.917051 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.50      0.62       321\n",
            "           1       0.31      0.65      0.43       113\n",
            "\n",
            "    accuracy                           0.54       434\n",
            "   macro avg       0.56      0.58      0.52       434\n",
            "weighted avg       0.68      0.54      0.57       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[160 161]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 1 0]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 1 ... 0 1 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [28 32] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 45.161290 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.35      0.49       321\n",
            "           1       0.29      0.73      0.41       113\n",
            "\n",
            "    accuracy                           0.45       434\n",
            "   macro avg       0.54      0.54      0.45       434\n",
            "weighted avg       0.66      0.45      0.47       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[113 208]\n",
            " [ 30  83]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [32 38] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 51.612903 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.44      0.57       321\n",
            "           1       0.31      0.73      0.44       113\n",
            "\n",
            "    accuracy                           0.52       434\n",
            "   macro avg       0.57      0.58      0.51       434\n",
            "weighted avg       0.69      0.52      0.54       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[142 179]\n",
            " [ 31  82]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [39 41] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 59.677419 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.54      0.67       321\n",
            "           1       0.37      0.75      0.49       113\n",
            "\n",
            "    accuracy                           0.60       434\n",
            "   macro avg       0.61      0.65      0.58       434\n",
            "weighted avg       0.73      0.60      0.62       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[174 147]\n",
            " [ 28  85]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [47 43] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 57.142857 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.51      0.64       321\n",
            "           1       0.35      0.74      0.47       113\n",
            "\n",
            "    accuracy                           0.57       434\n",
            "   macro avg       0.60      0.63      0.56       434\n",
            "weighted avg       0.72      0.57      0.60       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[164 157]\n",
            " [ 29  84]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [50 50] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.011 s \n",
            "\n",
            "Accuracy rate for 59.447005 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.54      0.66       321\n",
            "           1       0.36      0.74      0.49       113\n",
            "\n",
            "    accuracy                           0.59       434\n",
            "   macro avg       0.61      0.64      0.58       434\n",
            "weighted avg       0.73      0.59      0.62       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[174 147]\n",
            " [ 29  84]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [54 56] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 58.064516 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.52      0.65       321\n",
            "           1       0.36      0.75      0.48       113\n",
            "\n",
            "    accuracy                           0.58       434\n",
            "   macro avg       0.61      0.64      0.57       434\n",
            "weighted avg       0.73      0.58      0.60       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[167 154]\n",
            " [ 28  85]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [62 58] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 62.442396 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.59      0.70       321\n",
            "           1       0.38      0.72      0.50       113\n",
            "\n",
            "    accuracy                           0.62       434\n",
            "   macro avg       0.62      0.65      0.60       434\n",
            "weighted avg       0.73      0.62      0.65       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[190 131]\n",
            " [ 32  81]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [65 65] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.014 s \n",
            "\n",
            "Accuracy rate for 61.290323 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.57      0.68       321\n",
            "           1       0.38      0.74      0.50       113\n",
            "\n",
            "    accuracy                           0.61       434\n",
            "   macro avg       0.62      0.66      0.59       434\n",
            "weighted avg       0.74      0.61      0.64       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[182 139]\n",
            " [ 29  84]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [69 71] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.018 s \n",
            "\n",
            "Accuracy rate for 60.829493 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.55      0.68       321\n",
            "           1       0.38      0.76      0.50       113\n",
            "\n",
            "    accuracy                           0.61       434\n",
            "   macro avg       0.62      0.66      0.59       434\n",
            "weighted avg       0.74      0.61      0.63       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[178 143]\n",
            " [ 27  86]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [74 76] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.017 s \n",
            "\n",
            "Accuracy rate for 63.133641 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.58      0.70       321\n",
            "           1       0.40      0.79      0.53       113\n",
            "\n",
            "    accuracy                           0.63       434\n",
            "   macro avg       0.64      0.68      0.61       434\n",
            "weighted avg       0.76      0.63      0.65       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[185 136]\n",
            " [ 24  89]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [78 82] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.023 s \n",
            "\n",
            "Accuracy rate for 64.516129 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.61      0.72       321\n",
            "           1       0.40      0.74      0.52       113\n",
            "\n",
            "    accuracy                           0.65       434\n",
            "   macro avg       0.64      0.68      0.62       434\n",
            "weighted avg       0.75      0.65      0.67       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[196 125]\n",
            " [ 29  84]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [83 87] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.042 s \n",
            "\n",
            "Accuracy rate for 67.741935 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.66      0.75       321\n",
            "           1       0.43      0.73      0.54       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.65      0.70      0.65       434\n",
            "weighted avg       0.76      0.68      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[211 110]\n",
            " [ 30  83]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [90 90] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.026 s \n",
            "\n",
            "Accuracy rate for 66.359447 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.66      0.74       321\n",
            "           1       0.41      0.67      0.51       113\n",
            "\n",
            "    accuracy                           0.66       434\n",
            "   macro avg       0.63      0.67      0.63       434\n",
            "weighted avg       0.74      0.66      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[212 109]\n",
            " [ 37  76]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [93 97] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.032 s \n",
            "\n",
            "Accuracy rate for 66.359447 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.66      0.74       321\n",
            "           1       0.41      0.68      0.51       113\n",
            "\n",
            "    accuracy                           0.66       434\n",
            "   macro avg       0.63      0.67      0.63       434\n",
            "weighted avg       0.74      0.66      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[211 110]\n",
            " [ 36  77]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 99 101] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.035 s \n",
            "\n",
            "Accuracy rate for 67.741935 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.68      0.76       321\n",
            "           1       0.43      0.68      0.52       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.64      0.68      0.64       434\n",
            "weighted avg       0.75      0.68      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[217 104]\n",
            " [ 36  77]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [102 108] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.032 s \n",
            "\n",
            "Accuracy rate for 68.202765 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.68      0.76       321\n",
            "           1       0.43      0.70      0.53       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.65      0.69      0.65       434\n",
            "weighted avg       0.75      0.68      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[217 104]\n",
            " [ 34  79]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [107 113] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.045 s \n",
            "\n",
            "Accuracy rate for 67.281106 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.67      0.75       321\n",
            "           1       0.42      0.68      0.52       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.64      0.68      0.64       434\n",
            "weighted avg       0.74      0.67      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[215 106]\n",
            " [ 36  77]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [116 114] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.041 s \n",
            "\n",
            "Accuracy rate for 69.815668 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.70      0.78       321\n",
            "           1       0.45      0.68      0.54       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.66      0.69      0.66       434\n",
            "weighted avg       0.75      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[226  95]\n",
            " [ 36  77]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [123 117] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.039 s \n",
            "\n",
            "Accuracy rate for 71.198157 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.73      0.79       321\n",
            "           1       0.46      0.66      0.55       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.70      0.67       434\n",
            "weighted avg       0.76      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[234  87]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [129 121] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.050 s \n",
            "\n",
            "Accuracy rate for 71.889401 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.75      0.80       321\n",
            "           1       0.47      0.63      0.54       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[241  80]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [134 126] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.062 s \n",
            "\n",
            "Accuracy rate for 73.732719 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.76      0.81       321\n",
            "           1       0.50      0.66      0.57       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.68      0.71      0.69       434\n",
            "weighted avg       0.77      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[245  76]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [140 130] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.055 s \n",
            "\n",
            "Accuracy rate for 73.732719 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.76      0.81       321\n",
            "           1       0.50      0.68      0.57       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.68      0.72      0.69       434\n",
            "weighted avg       0.77      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[243  78]\n",
            " [ 36  77]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [147 133] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.075 s \n",
            "\n",
            "Accuracy rate for 73.963134 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.76      0.81       321\n",
            "           1       0.50      0.67      0.57       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.68      0.72      0.69       434\n",
            "weighted avg       0.77      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[245  76]\n",
            " [ 37  76]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [154 136] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.057 s \n",
            "\n",
            "Accuracy rate for 72.350230 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.80       321\n",
            "           1       0.48      0.66      0.56       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.72      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[239  82]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [161 139] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.071 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.84       321\n",
            "           1       0.56      0.62      0.59       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.72      0.72       434\n",
            "weighted avg       0.78      0.77      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [167 143] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.088 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85       321\n",
            "           1       0.59      0.59      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [172 148] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.103 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85       321\n",
            "           1       0.59      0.59      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [176 154] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.089 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85       321\n",
            "           1       0.58      0.59      0.59       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [180 160] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.087 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.57      0.59      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.72      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [189 161] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.106 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86       321\n",
            "           1       0.59      0.59      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [196 164] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.128 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [200 170] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.109 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [209 171] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.133 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [213 177] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.128 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [217 183] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.123 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [220 190] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.136 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [221 199] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.158 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [231 199] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.145 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [234 206] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.154 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [238 212] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.208 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [241 219] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.195 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [249 221] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.162 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [252 228] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.198 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [255 235] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.169 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [261 239] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.222 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "final active learning accuracies [73.50230414746544, 62.21198156682027, 66.58986175115207, 59.21658986175116, 53.91705069124424, 45.16129032258064, 51.61290322580645, 59.67741935483871, 57.14285714285714, 59.44700460829493, 58.06451612903226, 62.44239631336406, 61.29032258064516, 60.82949308755761, 63.133640552995395, 64.51612903225806, 67.74193548387096, 66.3594470046083, 66.3594470046083, 67.74193548387096, 68.20276497695853, 67.2811059907834, 69.81566820276498, 71.19815668202764, 71.88940092165899, 73.73271889400922, 73.73271889400922, 73.963133640553, 72.35023041474655, 77.41935483870968, 78.57142857142857, 78.57142857142857, 78.11059907834101, 77.64976958525345, 78.80184331797236, 79.03225806451613, 78.3410138248848, 78.80184331797236, 78.3410138248848, 78.80184331797236, 78.57142857142857, 79.03225806451613, 79.03225806451613, 79.03225806451613, 78.80184331797236, 78.80184331797236, 79.03225806451613, 79.03225806451613, 79.26267281105991, 79.03225806451613]\n",
            "saved Active-learning-experiment-10.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Decision_tree.ipynb', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "{\n",
            "  \"SvmModel\": {\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          62.21198156682027,\n",
            "          66.58986175115207,\n",
            "          59.21658986175116,\n",
            "          53.91705069124424,\n",
            "          45.16129032258064,\n",
            "          51.61290322580645,\n",
            "          59.67741935483871,\n",
            "          57.14285714285714,\n",
            "          59.44700460829493,\n",
            "          58.06451612903226,\n",
            "          62.44239631336406,\n",
            "          61.29032258064516,\n",
            "          60.82949308755761,\n",
            "          63.133640552995395,\n",
            "          64.51612903225806,\n",
            "          67.74193548387096,\n",
            "          66.3594470046083,\n",
            "          66.3594470046083,\n",
            "          67.74193548387096,\n",
            "          68.20276497695853,\n",
            "          67.2811059907834,\n",
            "          69.81566820276498,\n",
            "          71.19815668202764,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          73.73271889400922,\n",
            "          73.963133640553,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          71.42857142857143,\n",
            "          72.11981566820278\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          70.27649769585254,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.29953917050692,\n",
            "          63.594470046082954,\n",
            "          66.82027649769586,\n",
            "          68.4331797235023,\n",
            "          67.51152073732719,\n",
            "          70.04608294930875,\n",
            "          71.6589861751152,\n",
            "          70.73732718894009,\n",
            "          72.11981566820278,\n",
            "          73.04147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.0552995391705,\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          72.58064516129032,\n",
            "          71.88940092165899,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          75.80645161290323,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          74.19354838709677,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          75.11520737327189,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          39.1705069124424,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          74.65437788018433,\n",
            "          74.19354838709677,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.18894009216591,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          64.28571428571429,\n",
            "          63.133640552995395,\n",
            "          65.2073732718894,\n",
            "          71.42857142857143,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 11, using model = SvmModel, selection_function = EntropySelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [114 136] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.073 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.55      0.52      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.68      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [259 241] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.153 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.26728110599078, 79.03225806451613]\n",
            "saved Active-learning-experiment-11.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Decision_tree.ipynb', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 12, using model = SvmModel, selection_function = EntropySelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [63 62] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.013 s \n",
            "\n",
            "Accuracy rate for 71.889401 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.81      0.81       321\n",
            "           1       0.46      0.45      0.46       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.63      0.63      0.63       434\n",
            "weighted avg       0.72      0.72      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[261  60]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [141 109] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.047 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [199 176] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.235 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [245 255] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.160 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "final active learning accuracies [71.88940092165899, 78.57142857142857, 80.18433179723502, 79.49308755760369]\n",
            "saved Active-learning-experiment-12.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 13, using model = SvmModel, selection_function = EntropySelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [27 23] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 69.585253 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.77      0.79       321\n",
            "           1       0.43      0.49      0.45       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.62      0.63      0.62       434\n",
            "weighted avg       0.71      0.70      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [40 60] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.013 s \n",
            "\n",
            "Accuracy rate for 72.350230 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.86      0.82       321\n",
            "           1       0.46      0.35      0.39       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.62      0.60      0.61       434\n",
            "weighted avg       0.70      0.72      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 74  39]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [63 87] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.023 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.59      0.44      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 84 116] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.032 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [113 137] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.048 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [134 166] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.069 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [157 193] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.109 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [184 216] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.115 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [214 236] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.189 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [235 265] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.193 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87       321\n",
            "           1       0.64      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "final active learning accuracies [69.5852534562212, 72.35023041474655, 77.41935483870968, 78.3410138248848, 80.64516129032258, 79.26267281105991, 79.72350230414746, 80.64516129032258, 79.26267281105991, 80.4147465437788]\n",
            "saved Active-learning-experiment-13.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 14, using model = SvmModel, selection_function = EntropySelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [14 11] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.90      0.85       321\n",
            "           1       0.56      0.36      0.44       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.63      0.64       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 72  41]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [23 27] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.84       321\n",
            "           1       0.53      0.44      0.48       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [32 43] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83       321\n",
            "           1       0.51      0.48      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [47 53] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.582 s \n",
            "\n",
            "Accuracy rate for 69.585253 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.74      0.78       321\n",
            "           1       0.44      0.58      0.50       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.64      0.66      0.64       434\n",
            "weighted avg       0.73      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[236  85]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [61 64] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.014 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [81 69] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.024 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84       321\n",
            "           1       0.56      0.52      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [91 84] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.025 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.53      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [105  95] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.043 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [113 112] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.051 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [131 119] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.083 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [143 132] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.088 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [153 147] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.080 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [166 159] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.122 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [176 174] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.117 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [192 183] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.137 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [200 200] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.204 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [213 212] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.155 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [220 230] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.358 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [235 240] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.215 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [245 255] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.287 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.036866359447, 75.11520737327189, 74.65437788018433, 69.5852534562212, 77.41935483870968, 76.72811059907833, 77.18894009216591, 77.88018433179722, 77.88018433179722, 77.41935483870968, 77.64976958525345, 77.88018433179722, 77.64976958525345, 78.11059907834101, 79.03225806451613, 78.57142857142857, 79.26267281105991, 79.26267281105991, 79.72350230414746, 79.95391705069125]\n",
            "saved Active-learning-experiment-14.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-14.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 15, using model = SvmModel, selection_function = EntropySelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [3 7] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 62.903226 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.66      0.73       321\n",
            "           1       0.36      0.53      0.43       113\n",
            "\n",
            "    accuracy                           0.63       434\n",
            "   macro avg       0.58      0.60      0.58       434\n",
            "weighted avg       0.69      0.63      0.65       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[213 108]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [ 7 13] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 65.668203 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.66      0.74       321\n",
            "           1       0.40      0.64      0.49       113\n",
            "\n",
            "    accuracy                           0.66       434\n",
            "   macro avg       0.62      0.65      0.62       434\n",
            "weighted avg       0.72      0.66      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[213 108]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [12 18] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 64.746544 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.65      0.73       321\n",
            "           1       0.39      0.65      0.49       113\n",
            "\n",
            "    accuracy                           0.65       434\n",
            "   macro avg       0.62      0.65      0.61       434\n",
            "weighted avg       0.72      0.65      0.67       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[208 113]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [18 22] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 67.050691 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.68      0.75       321\n",
            "           1       0.42      0.65      0.51       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.63      0.67      0.63       434\n",
            "weighted avg       0.74      0.67      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[217 104]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [22 28] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 67.050691 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.68      0.75       321\n",
            "           1       0.42      0.65      0.51       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.63      0.67      0.63       434\n",
            "weighted avg       0.74      0.67      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[217 104]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [29 31] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 69.354839 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.72      0.78       321\n",
            "           1       0.44      0.63      0.52       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.64      0.67      0.65       434\n",
            "weighted avg       0.74      0.69      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[230  91]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [33 37] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.83       321\n",
            "           1       0.51      0.57      0.54       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.69      0.68       434\n",
            "weighted avg       0.76      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[260  61]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [35 45] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.83       321\n",
            "           1       0.52      0.57      0.54       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.69      0.68       434\n",
            "weighted avg       0.76      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[261  60]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [40 50] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.54      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[265  56]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [43 57] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 71.658986 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.79       321\n",
            "           1       0.47      0.65      0.55       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.70      0.67       434\n",
            "weighted avg       0.76      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[237  84]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [46 64] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 71.428571 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.79       321\n",
            "           1       0.47      0.65      0.54       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.70      0.67       434\n",
            "weighted avg       0.76      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[236  85]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [48 72] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.013 s \n",
            "\n",
            "Accuracy rate for 72.350230 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.75      0.80       321\n",
            "           1       0.48      0.65      0.55       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.67      0.70      0.67       434\n",
            "weighted avg       0.76      0.72      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[241  80]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [55 75] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.018 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.83       321\n",
            "           1       0.53      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.70      0.70       434\n",
            "weighted avg       0.77      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[263  58]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [59 81] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.020 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.54      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.70      0.70       434\n",
            "weighted avg       0.77      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [66 84] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.020 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.58      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [73 87] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.029 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.57      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [83 87] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.026 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [86 94] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.036 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [91 99] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.036 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 95 105] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.044 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.58      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [100 110] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.044 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [105 115] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.054 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [113 117] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.065 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [117 123] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.060 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.58      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [123 127] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.092 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.58      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [131 129] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.083 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [136 134] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.118 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [139 141] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.112 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.58      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [146 144] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.116 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [151 149] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.122 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.58      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [155 155] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.158 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.59      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [159 161] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.128 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [166 164] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.111 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [171 169] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.140 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [176 174] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.129 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [177 183] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.138 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [180 190] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.159 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [184 196] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.159 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [190 200] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.164 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [195 205] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.154 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [202 208] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.162 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [204 216] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.159 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [209 221] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.159 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [212 228] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.202 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [213 237] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.231 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [219 241] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.200 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [228 242] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.182 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [229 251] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.191 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [235 255] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.184 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [237 263] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.184 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "final active learning accuracies [62.903225806451616, 65.66820276497695, 64.74654377880185, 67.05069124423963, 67.05069124423963, 69.35483870967742, 74.65437788018433, 74.88479262672811, 76.26728110599078, 71.6589861751152, 71.42857142857143, 72.35023041474655, 75.80645161290323, 76.26728110599078, 77.64976958525345, 76.49769585253456, 77.88018433179722, 78.11059907834101, 77.64976958525345, 77.88018433179722, 77.64976958525345, 78.11059907834101, 78.57142857142857, 78.11059907834101, 78.11059907834101, 79.03225806451613, 78.3410138248848, 78.11059907834101, 78.3410138248848, 78.11059907834101, 78.57142857142857, 78.3410138248848, 78.80184331797236, 78.80184331797236, 78.80184331797236, 79.03225806451613, 78.80184331797236, 79.49308755760369, 79.03225806451613, 78.57142857142857, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.26267281105991, 78.80184331797236, 79.26267281105991, 79.03225806451613, 79.03225806451613]\n",
            "saved Active-learning-experiment-15.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "{\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          62.903225806451616,\n",
            "          65.66820276497695,\n",
            "          64.74654377880185,\n",
            "          67.05069124423963,\n",
            "          67.05069124423963,\n",
            "          69.35483870967742,\n",
            "          74.65437788018433,\n",
            "          74.88479262672811,\n",
            "          76.26728110599078,\n",
            "          71.6589861751152,\n",
            "          71.42857142857143,\n",
            "          72.35023041474655,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          69.5852534562212,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.5852534562212,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          62.21198156682027,\n",
            "          66.58986175115207,\n",
            "          59.21658986175116,\n",
            "          53.91705069124424,\n",
            "          45.16129032258064,\n",
            "          51.61290322580645,\n",
            "          59.67741935483871,\n",
            "          57.14285714285714,\n",
            "          59.44700460829493,\n",
            "          58.06451612903226,\n",
            "          62.44239631336406,\n",
            "          61.29032258064516,\n",
            "          60.82949308755761,\n",
            "          63.133640552995395,\n",
            "          64.51612903225806,\n",
            "          67.74193548387096,\n",
            "          66.3594470046083,\n",
            "          66.3594470046083,\n",
            "          67.74193548387096,\n",
            "          68.20276497695853,\n",
            "          67.2811059907834,\n",
            "          69.81566820276498,\n",
            "          71.19815668202764,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          73.73271889400922,\n",
            "          73.963133640553,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          71.42857142857143,\n",
            "          72.11981566820278\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          70.27649769585254,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.29953917050692,\n",
            "          63.594470046082954,\n",
            "          66.82027649769586,\n",
            "          68.4331797235023,\n",
            "          67.51152073732719,\n",
            "          70.04608294930875,\n",
            "          71.6589861751152,\n",
            "          70.73732718894009,\n",
            "          72.11981566820278,\n",
            "          73.04147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.0552995391705,\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          72.58064516129032,\n",
            "          71.88940092165899,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          75.80645161290323,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          74.19354838709677,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          75.11520737327189,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          39.1705069124424,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          74.65437788018433,\n",
            "          74.19354838709677,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.18894009216591,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          64.28571428571429,\n",
            "          63.133640552995395,\n",
            "          65.2073732718894,\n",
            "          71.42857142857143,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 16, using model = RfModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [126 124] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.966 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.92      0.86       321\n",
            "           1       0.63      0.40      0.49       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.66      0.68       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [240 260] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.130 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.45      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "final active learning accuracies [78.3410138248848, 79.95391705069125]\n",
            "saved Active-learning-experiment-16.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 17, using model = RfModel, selection_function = RandomSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [53 72] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.771 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83       321\n",
            "           1       0.53      0.57      0.55       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[264  57]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [111 139] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.891 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.85       321\n",
            "           1       0.57      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [169 206] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.020 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 0 1]\n",
            "probabilities: (927, 2) \n",
            " [0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 0 1]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [223 277] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.098 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.57603686635944, 77.41935483870968, 77.64976958525345, 77.41935483870968]\n",
            "saved Active-learning-experiment-17.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 18, using model = RfModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [28 22] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.750 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [49 51] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.884 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1202, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [69 81] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.799 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 91 109] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.849 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [116 134] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.866 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [140 160] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.903 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 0 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 0 ... 1 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [165 185] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.951 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [185 215] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.988 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.62      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [208 242] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.043 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [221 279] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.098 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [78.80184331797236, 79.03225806451613, 79.72350230414746, 79.49308755760369, 80.64516129032258, 79.49308755760369, 79.03225806451613, 79.03225806451613, 79.95391705069125, 79.26267281105991]\n",
            "saved Active-learning-experiment-18.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', '.git', '.vscode', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 19, using model = RfModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [ 5 20] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.690 s \n",
            "\n",
            "Accuracy rate for 52.073733 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.41      0.56       321\n",
            "           1       0.33      0.83      0.47       113\n",
            "\n",
            "    accuracy                           0.52       434\n",
            "   macro avg       0.60      0.62      0.52       434\n",
            "weighted avg       0.73      0.52      0.54       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[132 189]\n",
            " [ 19  94]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1277, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [17 33] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.725 s \n",
            "\n",
            "Accuracy rate for 66.820276 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.67      0.75       321\n",
            "           1       0.41      0.66      0.51       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.63      0.67      0.63       434\n",
            "weighted avg       0.74      0.67      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[215 106]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 0 0 ... 0 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [1 0 0 ... 0 0 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [27 48] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.753 s \n",
            "\n",
            "Accuracy rate for 69.585253 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.71      0.77       321\n",
            "           1       0.44      0.66      0.53       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.65      0.69      0.65       434\n",
            "weighted avg       0.75      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[227  94]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [41 59] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.742 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83       321\n",
            "           1       0.52      0.58      0.55       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.69      0.70      0.69       434\n",
            "weighted avg       0.76      0.75      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[261  60]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [56 69] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.814 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84       321\n",
            "           1       0.54      0.57      0.55       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.70      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [66 84] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.791 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.84       321\n",
            "           1       0.54      0.60      0.57       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[263  58]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [ 75 100] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.816 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84       321\n",
            "           1       0.55      0.62      0.58       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.72      0.71       434\n",
            "weighted avg       0.78      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[264  57]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 88 112] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.000 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.57      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [102 123] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.959 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [119 131] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.976 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [134 141] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.011 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [147 153] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.012 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [159 166] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.966 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [170 180] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.034 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [180 195] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.051 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [193 207] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.108 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [203 222] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.110 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [211 239] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.088 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [221 254] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.053 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0\n",
            " 0 0 1 1 1 0 1 0 1 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0\n",
            " 0 0 1 1 1 0 1 0 1 1 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [236 264] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.112 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [52.07373271889401, 66.82027649769586, 69.5852534562212, 75.34562211981567, 76.036866359447, 76.26728110599078, 76.95852534562212, 77.64976958525345, 78.80184331797236, 79.26267281105991, 79.95391705069125, 80.18433179723502, 79.72350230414746, 79.95391705069125, 79.95391705069125, 79.26267281105991, 79.95391705069125, 80.4147465437788, 79.72350230414746, 79.72350230414746]\n",
            "saved Active-learning-experiment-19.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', 'Active-learning-experiment-19.pkl', '.git', '.vscode', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 20, using model = RfModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [4 6] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.835 s \n",
            "\n",
            "Accuracy rate for 53.225806 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.49      0.61       321\n",
            "           1       0.31      0.66      0.42       113\n",
            "\n",
            "    accuracy                           0.53       434\n",
            "   macro avg       0.56      0.57      0.52       434\n",
            "weighted avg       0.68      0.53      0.56       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[156 165]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [ 8 12] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.793 s \n",
            "\n",
            "Accuracy rate for 61.059908 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.64      0.71       321\n",
            "           1       0.34      0.53      0.42       113\n",
            "\n",
            "    accuracy                           0.61       434\n",
            "   macro avg       0.57      0.58      0.56       434\n",
            "weighted avg       0.68      0.61      0.63       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[205 116]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [14 16] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.727 s \n",
            "\n",
            "Accuracy rate for 67.281106 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.71      0.76       321\n",
            "           1       0.41      0.56      0.47       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.61      0.64      0.62       434\n",
            "weighted avg       0.71      0.67      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[229  92]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [22 18] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.804 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.93      0.85       321\n",
            "           1       0.59      0.31      0.41       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.62      0.63       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 78  35]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [25 25] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.719 s \n",
            "\n",
            "Accuracy rate for 74.193548 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.87      0.83       321\n",
            "           1       0.51      0.38      0.43       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.65      0.62      0.63       434\n",
            "weighted avg       0.72      0.74      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [31 29] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.716 s \n",
            "\n",
            "Accuracy rate for 73.271889 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82       321\n",
            "           1       0.48      0.42      0.45       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.64      0.63      0.64       434\n",
            "weighted avg       0.72      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [35 35] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.761 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [40 40] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.735 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.49      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [46 44] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.747 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [52 48] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.746 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.64      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [56 54] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.751 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.63      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [60 60] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.799 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.48      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [63 67] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.762 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.52      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [68 72] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.787 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.57      0.48      0.52       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [75 75] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.814 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [80 80] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.791 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [84 86] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.836 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [89 91] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.822 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [94 96] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.815 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 98 102] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.867 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [102 108] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.853 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [106 114] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.838 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [111 119] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.852 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [116 124] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.883 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [122 128] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.902 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [130 130] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.018 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [136 134] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.898 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [141 139] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.927 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [148 142] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.898 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [153 147] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.925 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [159 151] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.958 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [163 157] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.920 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [166 164] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.944 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [169 171] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.987 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [172 178] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.978 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [177 183] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.970 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [181 189] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.994 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [185 195] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.970 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [190 200] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.999 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [195 205] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.997 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [198 212] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.036 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [202 218] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.019 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88       321\n",
            "           1       0.67      0.57      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.81      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [205 225] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.021 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.87       321\n",
            "           1       0.65      0.58      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [207 233] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.036 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [212 238] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.058 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [215 245] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.033 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [220 250] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.051 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.62      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [225 255] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.084 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [227 263] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.109 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.59      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [232 268] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.099 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.57      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "final active learning accuracies [53.2258064516129, 61.05990783410138, 67.2811059907834, 76.49769585253456, 74.19354838709677, 73.27188940092167, 77.64976958525345, 78.3410138248848, 79.95391705069125, 79.49308755760369, 79.26267281105991, 77.88018433179722, 77.18894009216591, 77.18894009216591, 78.11059907834101, 79.49308755760369, 78.3410138248848, 79.72350230414746, 79.03225806451613, 79.72350230414746, 79.03225806451613, 79.72350230414746, 79.49308755760369, 80.4147465437788, 78.57142857142857, 79.49308755760369, 80.18433179723502, 79.49308755760369, 80.64516129032258, 79.95391705069125, 79.72350230414746, 79.72350230414746, 80.64516129032258, 80.18433179723502, 80.18433179723502, 80.64516129032258, 80.64516129032258, 79.72350230414746, 80.64516129032258, 80.4147465437788, 80.18433179723502, 81.33640552995391, 80.87557603686636, 79.03225806451613, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.03225806451613, 78.57142857142857, 79.95391705069125]\n",
            "saved Active-learning-experiment-20.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', 'Active-learning-experiment-19.pkl', '.git', '.vscode', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Logit_default_f8(std_removal).pdf']\n",
            "{\n",
            "  \"RfModel\": {\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          53.2258064516129,\n",
            "          61.05990783410138,\n",
            "          67.2811059907834,\n",
            "          76.49769585253456,\n",
            "          74.19354838709677,\n",
            "          73.27188940092167,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          66.82027649769586,\n",
            "          69.5852534562212,\n",
            "          75.34562211981567,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.3410138248848,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          62.903225806451616,\n",
            "          65.66820276497695,\n",
            "          64.74654377880185,\n",
            "          67.05069124423963,\n",
            "          67.05069124423963,\n",
            "          69.35483870967742,\n",
            "          74.65437788018433,\n",
            "          74.88479262672811,\n",
            "          76.26728110599078,\n",
            "          71.6589861751152,\n",
            "          71.42857142857143,\n",
            "          72.35023041474655,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          69.5852534562212,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.5852534562212,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          62.21198156682027,\n",
            "          66.58986175115207,\n",
            "          59.21658986175116,\n",
            "          53.91705069124424,\n",
            "          45.16129032258064,\n",
            "          51.61290322580645,\n",
            "          59.67741935483871,\n",
            "          57.14285714285714,\n",
            "          59.44700460829493,\n",
            "          58.06451612903226,\n",
            "          62.44239631336406,\n",
            "          61.29032258064516,\n",
            "          60.82949308755761,\n",
            "          63.133640552995395,\n",
            "          64.51612903225806,\n",
            "          67.74193548387096,\n",
            "          66.3594470046083,\n",
            "          66.3594470046083,\n",
            "          67.74193548387096,\n",
            "          68.20276497695853,\n",
            "          67.2811059907834,\n",
            "          69.81566820276498,\n",
            "          71.19815668202764,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          73.73271889400922,\n",
            "          73.963133640553,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          71.42857142857143,\n",
            "          72.11981566820278\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          70.27649769585254,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.29953917050692,\n",
            "          63.594470046082954,\n",
            "          66.82027649769586,\n",
            "          68.4331797235023,\n",
            "          67.51152073732719,\n",
            "          70.04608294930875,\n",
            "          71.6589861751152,\n",
            "          70.73732718894009,\n",
            "          72.11981566820278,\n",
            "          73.04147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.0552995391705,\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          72.58064516129032,\n",
            "          71.88940092165899,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          75.80645161290323,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          74.19354838709677,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          75.11520737327189,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          39.1705069124424,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          74.65437788018433,\n",
            "          74.19354838709677,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.18894009216591,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          64.28571428571429,\n",
            "          63.133640552995395,\n",
            "          65.2073732718894,\n",
            "          71.42857142857143,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 21, using model = RfModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [136 114] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.886 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [227 273] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.136 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.88       321\n",
            "           1       0.67      0.52      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "final active learning accuracies [80.64516129032258, 80.87557603686636]\n",
            "saved Active-learning-experiment-21.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', 'Active-learning-experiment-19.pkl', '.git', '.vscode', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Logit_default_f8(std_removal).pdf']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 22, using model = RfModel, selection_function = MarginSamplingSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [45 80] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.811 s \n",
            "\n",
            "Accuracy rate for 69.815668 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.72      0.78       321\n",
            "           1       0.45      0.65      0.53       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.65      0.68      0.65       434\n",
            "weighted avg       0.75      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[230  91]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 1 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 1 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [128 122] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.932 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84       321\n",
            "           1       0.54      0.56      0.55       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [200 175] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.981 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0\n",
            " 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [264 236] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.126 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [69.81566820276498, 76.036866359447, 78.3410138248848, 80.64516129032258]\n",
            "saved Active-learning-experiment-22.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', 'Active-learning-experiment-19.pkl', '.git', '.vscode', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 23, using model = RfModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [23 27] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.805 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.56      0.52      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [55 45] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.776 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.72      0.43      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [75 75] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.813 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 97 103] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.891 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [119 131] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.887 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [147 153] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.980 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.52      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [176 174] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.983 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [200 200] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.042 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [214 236] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.101 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [249 251] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.125 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.95852534562212, 80.87557603686636, 79.95391705069125, 79.03225806451613, 77.64976958525345, 77.18894009216591, 79.72350230414746, 79.72350230414746, 79.03225806451613, 79.49308755760369]\n",
            "saved Active-learning-experiment-23.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', 'Active-learning-experiment-19.pkl', '.git', '.vscode', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 24, using model = RfModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [ 8 17] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.772 s \n",
            "\n",
            "Accuracy rate for 51.843318 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.40      0.55       321\n",
            "           1       0.33      0.84      0.48       113\n",
            "\n",
            "    accuracy                           0.52       434\n",
            "   macro avg       0.61      0.62      0.52       434\n",
            "weighted avg       0.74      0.52      0.53       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[130 191]\n",
            " [ 18  95]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1277, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [25 25] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.775 s \n",
            "\n",
            "Accuracy rate for 73.041475 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.76      0.81       321\n",
            "           1       0.49      0.64      0.55       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[245  76]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [38 37] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.883 s \n",
            "\n",
            "Accuracy rate for 73.041475 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.49      0.62      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [54 46] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.780 s \n",
            "\n",
            "Accuracy rate for 70.967742 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79       321\n",
            "           1       0.46      0.64      0.53       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.69      0.66       434\n",
            "weighted avg       0.75      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[236  85]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [63 62] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.816 s \n",
            "\n",
            "Accuracy rate for 70.737327 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.72      0.78       321\n",
            "           1       0.46      0.67      0.54       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.70      0.66       434\n",
            "weighted avg       0.76      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[231  90]\n",
            " [ 37  76]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [79 71] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.866 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.58      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [92 83] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.860 s \n",
            "\n",
            "Accuracy rate for 73.502304 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.79      0.82       321\n",
            "           1       0.49      0.57      0.53       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.68      0.67       434\n",
            "weighted avg       0.75      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[255  66]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [102  98] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.892 s \n",
            "\n",
            "Accuracy rate for 72.811060 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.81       321\n",
            "           1       0.48      0.61      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.67       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [120 105] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.879 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.88       321\n",
            "           1       0.67      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [130 120] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.925 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [139 136] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.019 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.57      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.73      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [151 149] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.970 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.86       321\n",
            "           1       0.61      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.73      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [171 154] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.983 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.57      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [180 170] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.001 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [197 178] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.112 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.67      0.55      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [208 192] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.075 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.67      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [219 206] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.183 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [228 222] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.084 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.67      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [240 235] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.194 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.68      0.53      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.74       434\n",
            "weighted avg       0.80      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [254 246] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.158 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.65      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "final active learning accuracies [51.843317972350235, 73.04147465437788, 73.04147465437788, 70.96774193548387, 70.73732718894009, 77.88018433179722, 73.50230414746544, 72.81105990783409, 80.87557603686636, 79.72350230414746, 80.18433179723502, 79.49308755760369, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.87557603686636, 79.72350230414746, 81.10599078341014, 81.33640552995391, 79.95391705069125]\n",
            "saved Active-learning-experiment-24.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', 'Active-learning-experiment-19.pkl', '.git', '.vscode', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 25, using model = RfModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [7 3] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.697 s \n",
            "\n",
            "Accuracy rate for 74.423963 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.98      0.85       321\n",
            "           1       0.58      0.06      0.11       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.52      0.48       434\n",
            "weighted avg       0.71      0.74      0.66       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[316   5]\n",
            " [106   7]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [ 7 13] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.761 s \n",
            "\n",
            "Accuracy rate for 71.889401 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.77      0.80       321\n",
            "           1       0.47      0.58      0.52       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.74      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[246  75]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 1 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 1 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [12 18] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.745 s \n",
            "\n",
            "Accuracy rate for 73.732719 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.77      0.81       321\n",
            "           1       0.50      0.65      0.56       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.68      0.71      0.69       434\n",
            "weighted avg       0.77      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [1 1 1 ... 0 1 1]\n",
            "probabilities: (1272, 2) \n",
            " [1 1 1 ... 0 1 1]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [20 20] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.723 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.47      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [26 24] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.714 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.62      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.67      0.68       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [33 27] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.975 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.67      0.43      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [36 34] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.883 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [39 41] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.894 s \n",
            "\n",
            "Accuracy rate for 73.271889 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.81      0.82       321\n",
            "           1       0.49      0.51      0.50       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.66      0.66       434\n",
            "weighted avg       0.74      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[260  61]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [45 45] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.861 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.60      0.45      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.69       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [51 49] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.878 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.61      0.42      0.50       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.66      0.68       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [55 55] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.888 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.62      0.45      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [58 62] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.894 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [61 69] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.994 s \n",
            "\n",
            "Accuracy rate for 73.041475 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.78      0.81       321\n",
            "           1       0.49      0.58      0.53       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.68      0.67       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[251  70]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [66 74] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.918 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [74 76] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.902 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [77 83] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.084 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [81 89] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.919 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [85 95] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.868 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [ 88 102] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.853 s \n",
            "\n",
            "Accuracy rate for 72.811060 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.78      0.81       321\n",
            "           1       0.48      0.58      0.53       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.68      0.67       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[250  71]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 93 107] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.924 s \n",
            "\n",
            "Accuracy rate for 73.271889 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.78      0.81       321\n",
            "           1       0.49      0.61      0.54       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.69      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[249  72]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [100 110] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.926 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [106 114] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.907 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [112 118] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.896 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [117 123] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.966 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [121 129] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.954 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.60      0.49      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [128 132] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.994 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [132 138] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.947 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [136 144] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.920 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.67      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [143 147] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.990 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [149 151] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.937 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [155 155] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.951 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [156 164] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.995 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [160 170] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.971 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [166 174] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.048 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [170 180] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.159 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [177 183] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.137 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [181 189] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.167 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [185 195] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.105 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [191 199] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.171 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [196 204] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.215 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [201 209] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.187 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [211 209] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.133 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [215 215] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.075 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [219 221] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.285 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [225 225] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.244 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [231 229] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.208 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [239 231] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.302 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [243 237] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.158 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0\n",
            " 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0\n",
            " 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [248 242] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.164 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0\n",
            " 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0\n",
            " 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [252 248] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.179 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [74.42396313364056, 71.88940092165899, 73.73271889400922, 77.88018433179722, 78.3410138248848, 79.72350230414746, 79.95391705069125, 73.27188940092167, 77.88018433179722, 77.88018433179722, 78.57142857142857, 78.80184331797236, 73.04147465437788, 78.57142857142857, 79.95391705069125, 79.03225806451613, 79.49308755760369, 79.03225806451613, 72.81105990783409, 73.27188940092167, 79.03225806451613, 77.88018433179722, 78.80184331797236, 80.18433179723502, 78.11059907834101, 79.72350230414746, 79.49308755760369, 80.64516129032258, 80.4147465437788, 80.18433179723502, 80.18433179723502, 80.64516129032258, 80.18433179723502, 79.95391705069125, 78.3410138248848, 80.64516129032258, 80.18433179723502, 77.88018433179722, 79.49308755760369, 79.26267281105991, 79.26267281105991, 79.49308755760369, 78.3410138248848, 79.26267281105991, 78.3410138248848, 79.95391705069125, 80.64516129032258, 80.4147465437788, 79.49308755760369, 79.95391705069125]\n",
            "saved Active-learning-experiment-25.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Model_select.ipynb', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', '.git', '.vscode', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "{\n",
            "  \"RfModel\": {\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          73.27188940092167,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          73.04147465437788,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          79.03225806451613,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          78.11059907834101,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          69.81566820276498,\n",
            "          76.036866359447,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          51.843317972350235,\n",
            "          73.04147465437788,\n",
            "          73.04147465437788,\n",
            "          70.96774193548387,\n",
            "          70.73732718894009,\n",
            "          77.88018433179722,\n",
            "          73.50230414746544,\n",
            "          72.81105990783409,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          80.64516129032258,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          53.2258064516129,\n",
            "          61.05990783410138,\n",
            "          67.2811059907834,\n",
            "          76.49769585253456,\n",
            "          74.19354838709677,\n",
            "          73.27188940092167,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          66.82027649769586,\n",
            "          69.5852534562212,\n",
            "          75.34562211981567,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.3410138248848,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          62.903225806451616,\n",
            "          65.66820276497695,\n",
            "          64.74654377880185,\n",
            "          67.05069124423963,\n",
            "          67.05069124423963,\n",
            "          69.35483870967742,\n",
            "          74.65437788018433,\n",
            "          74.88479262672811,\n",
            "          76.26728110599078,\n",
            "          71.6589861751152,\n",
            "          71.42857142857143,\n",
            "          72.35023041474655,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          69.5852534562212,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.5852534562212,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          62.21198156682027,\n",
            "          66.58986175115207,\n",
            "          59.21658986175116,\n",
            "          53.91705069124424,\n",
            "          45.16129032258064,\n",
            "          51.61290322580645,\n",
            "          59.67741935483871,\n",
            "          57.14285714285714,\n",
            "          59.44700460829493,\n",
            "          58.06451612903226,\n",
            "          62.44239631336406,\n",
            "          61.29032258064516,\n",
            "          60.82949308755761,\n",
            "          63.133640552995395,\n",
            "          64.51612903225806,\n",
            "          67.74193548387096,\n",
            "          66.3594470046083,\n",
            "          66.3594470046083,\n",
            "          67.74193548387096,\n",
            "          68.20276497695853,\n",
            "          67.2811059907834,\n",
            "          69.81566820276498,\n",
            "          71.19815668202764,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          73.73271889400922,\n",
            "          73.963133640553,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          71.42857142857143,\n",
            "          72.11981566820278\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          70.27649769585254,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.29953917050692,\n",
            "          63.594470046082954,\n",
            "          66.82027649769586,\n",
            "          68.4331797235023,\n",
            "          67.51152073732719,\n",
            "          70.04608294930875,\n",
            "          71.6589861751152,\n",
            "          70.73732718894009,\n",
            "          72.11981566820278,\n",
            "          73.04147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.0552995391705,\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          72.58064516129032,\n",
            "          71.88940092165899,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          75.80645161290323,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          74.19354838709677,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          75.11520737327189,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          39.1705069124424,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          74.65437788018433,\n",
            "          74.19354838709677,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.18894009216591,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          64.28571428571429,\n",
            "          63.133640552995395,\n",
            "          65.2073732718894,\n",
            "          71.42857142857143,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 26, using model = RfModel, selection_function = EntropySelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [107 143] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.950 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.84       321\n",
            "           1       0.54      0.52      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.68      0.68       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [225 275] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.227 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.80645161290323, 79.95391705069125]\n",
            "saved Active-learning-experiment-26.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', '.git', '.vscode', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 27, using model = RfModel, selection_function = EntropySelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [55 70] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.829 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.80      0.82       321\n",
            "           1       0.51      0.61      0.56       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.70      0.69       434\n",
            "weighted avg       0.77      0.75      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[256  65]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [125 125] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.934 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87       321\n",
            "           1       0.64      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [185 190] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.021 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [255 245] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.189 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [74.88479262672811, 80.4147465437788, 78.3410138248848, 78.57142857142857]\n",
            "saved Active-learning-experiment-27.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', '.git', '.vscode', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 28, using model = RfModel, selection_function = EntropySelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [20 30] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.764 s \n",
            "\n",
            "Accuracy rate for 73.963134 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.77      0.81       321\n",
            "           1       0.50      0.65      0.57       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.68      0.71      0.69       434\n",
            "weighted avg       0.77      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 1 1]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 1 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [53 47] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.859 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.67      0.56      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.81      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [79 71] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.867 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 98 102] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.909 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [121 129] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.967 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [147 153] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.269 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [167 183] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.046 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [202 198] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.135 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [237 213] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.082 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [257 243] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.281 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "final active learning accuracies [73.963133640553, 81.33640552995391, 79.72350230414746, 79.26267281105991, 79.49308755760369, 80.4147465437788, 78.3410138248848, 78.80184331797236, 80.64516129032258, 80.64516129032258]\n",
            "saved Active-learning-experiment-28.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', '.git', '.vscode', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 29, using model = RfModel, selection_function = EntropySelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [ 9 16] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.784 s \n",
            "\n",
            "Accuracy rate for 62.672811 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.59      0.70       321\n",
            "           1       0.39      0.74      0.51       113\n",
            "\n",
            "    accuracy                           0.63       434\n",
            "   macro avg       0.63      0.66      0.60       434\n",
            "weighted avg       0.74      0.63      0.65       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[188 133]\n",
            " [ 29  84]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [1 0 1 ... 0 0 1]\n",
            "probabilities: (1277, 2) \n",
            " [1 0 1 ... 0 0 1]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [23 27] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.766 s \n",
            "\n",
            "Accuracy rate for 66.359447 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.64      0.74       321\n",
            "           1       0.42      0.73      0.53       113\n",
            "\n",
            "    accuracy                           0.66       434\n",
            "   macro avg       0.64      0.68      0.63       434\n",
            "weighted avg       0.75      0.66      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[206 115]\n",
            " [ 31  82]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [40 35] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.819 s \n",
            "\n",
            "Accuracy rate for 72.811060 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.76      0.80       321\n",
            "           1       0.48      0.65      0.55       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[243  78]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1227, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [56 44] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.765 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [63 62] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.797 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.84       321\n",
            "           1       0.54      0.59      0.57       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[264  57]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [76 74] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.812 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.56      0.57      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [92 83] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.834 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       321\n",
            "           1       0.55      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.68      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [106  94] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.871 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [114 111] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.866 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [129 121] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.899 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [140 135] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.014 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [151 149] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.993 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [165 160] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.124 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [179 171] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.154 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [193 182] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.157 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [200 200] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.193 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.60      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [212 213] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.249 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [224 226] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.322 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [239 236] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.328 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1\n",
            " 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1\n",
            " 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [245 255] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.297 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "final active learning accuracies [62.67281105990783, 66.3594470046083, 72.81105990783409, 79.72350230414746, 76.26728110599078, 77.18894009216591, 76.49769585253456, 79.95391705069125, 77.64976958525345, 78.57142857142857, 78.11059907834101, 79.03225806451613, 78.80184331797236, 80.64516129032258, 80.18433179723502, 78.3410138248848, 79.49308755760369, 79.26267281105991, 79.72350230414746, 80.4147465437788]\n",
            "saved Active-learning-experiment-29.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', '.git', '.vscode', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 30, using model = RfModel, selection_function = EntropySelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [5 5] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.797 s \n",
            "\n",
            "Accuracy rate for 68.202765 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       321\n",
            "           1       0.41      0.49      0.44       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.61      0.62      0.61       434\n",
            "weighted avg       0.70      0.68      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[241  80]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [ 7 13] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.780 s \n",
            "\n",
            "Accuracy rate for 59.677419 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.53      0.66       321\n",
            "           1       0.37      0.78      0.50       113\n",
            "\n",
            "    accuracy                           0.60       434\n",
            "   macro avg       0.62      0.66      0.58       434\n",
            "weighted avg       0.74      0.60      0.62       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[171 150]\n",
            " [ 25  88]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [1 1 1 ... 0 1 1]\n",
            "probabilities: (1282, 2) \n",
            " [1 1 1 ... 0 1 1]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [16 14] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.761 s \n",
            "\n",
            "Accuracy rate for 70.046083 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.74      0.79       321\n",
            "           1       0.44      0.58      0.50       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.64      0.66      0.64       434\n",
            "weighted avg       0.73      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[238  83]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [21 19] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.753 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.57      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [26 24] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.771 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84       321\n",
            "           1       0.54      0.61      0.57       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.71      0.71       434\n",
            "weighted avg       0.78      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[263  58]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [28 32] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.750 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.78      0.82       321\n",
            "           1       0.51      0.65      0.58       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.69      0.72      0.70       434\n",
            "weighted avg       0.77      0.75      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[251  70]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [32 38] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.753 s \n",
            "\n",
            "Accuracy rate for 73.271889 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.77      0.81       321\n",
            "           1       0.49      0.64      0.55       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[246  75]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [1 1 1 ... 1 1 0]\n",
            "probabilities: (1232, 2) \n",
            " [1 1 1 ... 1 1 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [38 42] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.735 s \n",
            "\n",
            "Accuracy rate for 73.963134 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.77      0.81       321\n",
            "           1       0.50      0.65      0.57       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.68      0.71      0.69       434\n",
            "weighted avg       0.77      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[247  74]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [1 1 1 ... 1 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [1 1 1 ... 1 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [46 44] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.880 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.57      0.59      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [54 46] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.806 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 0 0 ... 1 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 0 0 ... 1 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [59 51] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.815 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [63 57] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.911 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [68 62] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.903 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [72 68] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.838 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [76 74] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.949 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [1 0 0 ... 0 0 1]\n",
            "probabilities: (1152, 2) \n",
            " [1 0 0 ... 0 0 1]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [82 78] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.069 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [87 83] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.830 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [1 0 0 ... 0 0 1]\n",
            "probabilities: (1132, 2) \n",
            " [1 0 0 ... 0 0 1]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [92 88] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.861 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [99 91] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.836 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [1 0 0 ... 0 0 1]\n",
            "probabilities: (1112, 2) \n",
            " [1 0 0 ... 0 0 1]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [104  96] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.910 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.67      0.54      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 0 0 ... 0 0 1]\n",
            "probabilities: (1102, 2) \n",
            " [1 0 0 ... 0 0 1]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [108 102] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.870 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [1 0 0 ... 0 0 1]\n",
            "probabilities: (1092, 2) \n",
            " [1 0 0 ... 0 0 1]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [112 108] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.865 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.67      0.55      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [1 0 0 ... 0 0 1]\n",
            "probabilities: (1082, 2) \n",
            " [1 0 0 ... 0 0 1]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [117 113] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.874 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.67      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [121 119] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.899 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [124 126] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.912 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.88       321\n",
            "           1       0.66      0.58      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [127 133] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.907 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.57      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [130 140] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.932 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87       321\n",
            "           1       0.64      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [136 144] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.918 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.55      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [140 150] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.967 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.55      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [145 155] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.952 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 0 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 0 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [152 158] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.976 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [156 164] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.001 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [159 171] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.015 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [164 176] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.003 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [170 180] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.027 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [179 181] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.009 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [186 184] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.004 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.62      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [191 189] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.013 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [195 195] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.065 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [201 199] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.032 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [206 204] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.049 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [211 209] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.073 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [216 214] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.107 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [225 215] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.105 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [228 222] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.097 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1\n",
            " 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1\n",
            " 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [237 223] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.093 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.67      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [246 224] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "<ipython-input-22-5d00dd11dc5d>:26: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-22-5d00dd11dc5d>:26: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.125 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [249 231] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.140 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.67      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [254 236] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.140 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (812, 2) \n",
            " [1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [261 239] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.169 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.67      0.55      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "final active learning accuracies [68.20276497695853, 59.67741935483871, 70.04608294930875, 76.49769585253456, 76.49769585253456, 74.88479262672811, 73.27188940092167, 73.963133640553, 77.88018433179722, 79.72350230414746, 79.72350230414746, 80.64516129032258, 79.03225806451613, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.49308755760369, 80.4147465437788, 78.80184331797236, 81.10599078341014, 80.18433179723502, 81.10599078341014, 81.10599078341014, 80.4147465437788, 81.10599078341014, 80.4147465437788, 80.4147465437788, 80.87557603686636, 80.64516129032258, 79.95391705069125, 79.26267281105991, 78.57142857142857, 78.57142857142857, 78.11059907834101, 78.11059907834101, 79.26267281105991, 79.03225806451613, 79.72350230414746, 79.95391705069125, 79.95391705069125, 79.95391705069125, 79.26267281105991, 80.18433179723502, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.64516129032258, 80.87557603686636, 80.64516129032258, 81.10599078341014]\n",
            "saved Active-learning-experiment-30.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "{\n",
            "  \"RfModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          68.20276497695853,\n",
            "          59.67741935483871,\n",
            "          70.04608294930875,\n",
            "          76.49769585253456,\n",
            "          76.49769585253456,\n",
            "          74.88479262672811,\n",
            "          73.27188940092167,\n",
            "          73.963133640553,\n",
            "          77.88018433179722,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          81.10599078341014,\n",
            "          80.18433179723502,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.88479262672811,\n",
            "          80.4147465437788,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          62.67281105990783,\n",
            "          66.3594470046083,\n",
            "          72.81105990783409,\n",
            "          79.72350230414746,\n",
            "          76.26728110599078,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          79.95391705069125,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          81.33640552995391,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          73.27188940092167,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          73.04147465437788,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          79.03225806451613,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          78.11059907834101,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          69.81566820276498,\n",
            "          76.036866359447,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          51.843317972350235,\n",
            "          73.04147465437788,\n",
            "          73.04147465437788,\n",
            "          70.96774193548387,\n",
            "          70.73732718894009,\n",
            "          77.88018433179722,\n",
            "          73.50230414746544,\n",
            "          72.81105990783409,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          80.64516129032258,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          53.2258064516129,\n",
            "          61.05990783410138,\n",
            "          67.2811059907834,\n",
            "          76.49769585253456,\n",
            "          74.19354838709677,\n",
            "          73.27188940092167,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          66.82027649769586,\n",
            "          69.5852534562212,\n",
            "          75.34562211981567,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.3410138248848,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          62.903225806451616,\n",
            "          65.66820276497695,\n",
            "          64.74654377880185,\n",
            "          67.05069124423963,\n",
            "          67.05069124423963,\n",
            "          69.35483870967742,\n",
            "          74.65437788018433,\n",
            "          74.88479262672811,\n",
            "          76.26728110599078,\n",
            "          71.6589861751152,\n",
            "          71.42857142857143,\n",
            "          72.35023041474655,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          69.5852534562212,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.5852534562212,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          62.21198156682027,\n",
            "          66.58986175115207,\n",
            "          59.21658986175116,\n",
            "          53.91705069124424,\n",
            "          45.16129032258064,\n",
            "          51.61290322580645,\n",
            "          59.67741935483871,\n",
            "          57.14285714285714,\n",
            "          59.44700460829493,\n",
            "          58.06451612903226,\n",
            "          62.44239631336406,\n",
            "          61.29032258064516,\n",
            "          60.82949308755761,\n",
            "          63.133640552995395,\n",
            "          64.51612903225806,\n",
            "          67.74193548387096,\n",
            "          66.3594470046083,\n",
            "          66.3594470046083,\n",
            "          67.74193548387096,\n",
            "          68.20276497695853,\n",
            "          67.2811059907834,\n",
            "          69.81566820276498,\n",
            "          71.19815668202764,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          73.73271889400922,\n",
            "          73.963133640553,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          71.42857142857143,\n",
            "          72.11981566820278\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          70.27649769585254,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.29953917050692,\n",
            "          63.594470046082954,\n",
            "          66.82027649769586,\n",
            "          68.4331797235023,\n",
            "          67.51152073732719,\n",
            "          70.04608294930875,\n",
            "          71.6589861751152,\n",
            "          70.73732718894009,\n",
            "          72.11981566820278,\n",
            "          73.04147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.0552995391705,\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          72.58064516129032,\n",
            "          71.88940092165899,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          75.80645161290323,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          74.19354838709677,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          75.11520737327189,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          39.1705069124424,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          74.65437788018433,\n",
            "          74.19354838709677,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.18894009216591,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          64.28571428571429,\n",
            "          63.133640552995395,\n",
            "          65.2073732718894,\n",
            "          71.42857142857143,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 31, using model = LogModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [115 135] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 73.963134 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.83       321\n",
            "           1       0.50      0.44      0.47       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.64      0.65       434\n",
            "weighted avg       0.73      0.74      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [221 279] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.54      0.45      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "final active learning accuracies [73.963133640553, 75.57603686635944]\n",
            "saved Active-learning-experiment-31.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 32, using model = LogModel, selection_function = RandomSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [49 76] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [114 136] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       321\n",
            "           1       0.56      0.47      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [181 194] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.84       321\n",
            "           1       0.53      0.48      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.67      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [233 267] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.95852534562212, 76.72811059907833, 75.57603686635944, 76.26728110599078]\n",
            "saved Active-learning-experiment-32.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 33, using model = LogModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [23 27] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 72.811060 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82       321\n",
            "           1       0.48      0.43      0.45       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.64      0.63      0.64       434\n",
            "weighted avg       0.72      0.73      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [41 59] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 73.041475 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82       321\n",
            "           1       0.48      0.48      0.48       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.65      0.65      0.65       434\n",
            "weighted avg       0.73      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[263  58]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [65 85] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.47      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 90 110] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [120 130] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [143 157] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 71.658986 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.77      0.80       321\n",
            "           1       0.46      0.58      0.51       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.65      0.67      0.66       434\n",
            "weighted avg       0.74      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[246  75]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [165 185] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [188 212] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.54      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [211 239] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [238 262] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.60      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "final active learning accuracies [72.81105990783409, 73.04147465437788, 76.26728110599078, 76.95852534562212, 77.18894009216591, 71.6589861751152, 78.11059907834101, 77.41935483870968, 79.72350230414746, 78.11059907834101]\n",
            "saved Active-learning-experiment-33.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 34, using model = LogModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [12 13] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 74.193548 \n",
            "Classification report for classifier LogisticRegression(C=2.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.83       321\n",
            "           1       0.50      0.49      0.50       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.66      0.66       434\n",
            "weighted avg       0.74      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [24 26] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.53      0.46      0.49       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.67       434\n",
            "weighted avg       0.74      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [34 41] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=0.6666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83       321\n",
            "           1       0.52      0.45      0.48       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [44 56] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.56      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [56 69] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.47      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [68 82] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [79 96] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.2857142857142857, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.49      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 93 107] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.59      0.41      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.65      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [106 119] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.2222222222222222, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [117 133] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.57      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [131 144] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.18181818181818182, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [140 160] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [151 174] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.15384615384615385, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.54      0.53      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1\n",
            " 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1\n",
            " 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [166 184] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.55      0.53      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [179 196] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.54      0.51      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.68      0.68       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [192 208] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.54      0.50      0.52       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.68      0.68       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1\n",
            " 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1\n",
            " 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [205 220] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.11764705882352941, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.84       321\n",
            "           1       0.53      0.52      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.68      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[269  52]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [222 228] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.54      0.50      0.52       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0\n",
            " 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [230 245] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.10526315789473684, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.54      0.50      0.52       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [242 258] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "final active learning accuracies [74.19354838709677, 75.34562211981567, 74.88479262672811, 76.49769585253456, 77.88018433179722, 79.49308755760369, 77.88018433179722, 77.18894009216591, 76.95852534562212, 77.18894009216591, 76.72811059907833, 76.72811059907833, 76.036866359447, 76.26728110599078, 76.036866359447, 76.036866359447, 75.57603686635944, 76.036866359447, 75.80645161290323, 76.95852534562212]\n",
            "saved Active-learning-experiment-34.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 35, using model = LogModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [4 6] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 73.502304 \n",
            "Classification report for classifier LogisticRegression(C=5.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.85      0.83       321\n",
            "           1       0.49      0.40      0.44       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.65      0.63      0.63       434\n",
            "weighted avg       0.72      0.74      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [10 10] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=2.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83       321\n",
            "           1       0.52      0.45      0.48       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [18 12] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=1.6666666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.59      0.42      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [25 15] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=1.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.39      0.48       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.65      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [30 20] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.41      0.49       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.66      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [35 25] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.8333333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.60      0.44      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.68       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [42 28] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.7142857142857143, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.40      0.48       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.65      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [43 37] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.625, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.62      0.42      0.50       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.67      0.68       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [46 44] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.5555555555555556, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.92      0.86       321\n",
            "           1       0.64      0.39      0.48       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.66      0.67       434\n",
            "weighted avg       0.77      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [50 50] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.58      0.39      0.47       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.64      0.66       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1202, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [56 54] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.45454545454545453, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.84       321\n",
            "           1       0.55      0.39      0.46       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [60 60] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.4166666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.61      0.43      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.67      0.68       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [67 63] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.38461538461538464, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.40      0.48       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.65      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [73 67] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.35714285714285715, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.55      0.42      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [76 74] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.56      0.43      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [81 79] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.3125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.58      0.45      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [87 83] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.29411764705882354, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.86       321\n",
            "           1       0.60      0.42      0.49       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.66      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [91 89] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.2777777777777778, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.57      0.41      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [97 93] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.2631578947368421, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       321\n",
            "           1       0.56      0.46      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 99 101] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.58      0.43      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [105 105] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.23809523809523808, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.54      0.39      0.45       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [109 111] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.22727272727272727, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.58      0.41      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.65      0.67       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [115 115] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.21739130434782608, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.56      0.41      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [120 120] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.20833333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.56      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [124 126] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.56      0.40      0.46       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [128 132] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.19230769230769232, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.55      0.40      0.46       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [132 138] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.18518518518518517, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.54      0.41      0.46       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [136 144] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.17857142857142858, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.56      0.41      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [140 150] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.017 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.1724137931034483, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.55      0.42      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [147 153] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.54      0.44      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [153 157] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.16129032258064516, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.85       321\n",
            "           1       0.56      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [155 165] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.15625, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.60      0.43      0.50       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.68       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [160 170] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.15151515151515152, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.85       321\n",
            "           1       0.59      0.42      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [163 177] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.14705882352941177, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.61      0.42      0.50       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.66      0.68       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [170 180] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.57      0.42      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [173 187] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.1388888888888889, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.85       321\n",
            "           1       0.56      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0\n",
            " 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [178 192] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.13513513513513514, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.56      0.41      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1\n",
            " 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1\n",
            " 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 1 0 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [184 196] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.13157894736842105, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.56      0.40      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.64      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [186 204] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.1282051282051282, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.85       321\n",
            "           1       0.55      0.41      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [191 209] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.40      0.48       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.65      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1\n",
            " 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1\n",
            " 0 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1\n",
            " 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [197 213] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.12195121951219512, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.58      0.41      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.65      0.67       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 0 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [203 217] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.11904761904761904, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.57      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1\n",
            " 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1\n",
            " 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [207 223] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.11627906976744186, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.44      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.68       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [211 229] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.11363636363636363, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.56      0.43      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [212 238] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.023 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.44      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [219 241] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.10869565217391304, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.58      0.44      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [223 247] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.10638297872340426, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.66      0.68       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [227 253] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.015 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.10416666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.56      0.40      0.46       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 0 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 0 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [230 260] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.10204081632653061, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.57      0.41      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [234 266] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.85       321\n",
            "           1       0.59      0.42      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "final active learning accuracies [73.50230414746544, 74.88479262672811, 77.18894009216591, 77.64976958525345, 77.64976958525345, 77.88018433179722, 77.64976958525345, 78.11059907834101, 78.3410138248848, 76.72811059907833, 75.80645161290323, 78.11059907834101, 77.64976958525345, 75.80645161290323, 76.26728110599078, 77.18894009216591, 77.64976958525345, 76.72811059907833, 76.49769585253456, 76.95852534562212, 75.57603686635944, 76.95852534562212, 76.26728110599078, 76.49769585253456, 76.036866359447, 75.80645161290323, 75.57603686635944, 76.26728110599078, 75.80645161290323, 75.57603686635944, 76.49769585253456, 77.64976958525345, 77.41935483870968, 77.88018433179722, 76.72811059907833, 76.26728110599078, 76.26728110599078, 76.26728110599078, 76.036866359447, 77.64976958525345, 76.95852534562212, 76.49769585253456, 76.95852534562212, 76.26728110599078, 76.72811059907833, 77.18894009216591, 76.72811059907833, 76.036866359447, 76.49769585253456, 77.41935483870968]\n",
            "saved Active-learning-experiment-35.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          74.88479262672811,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          76.72811059907833,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          76.72811059907833,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          75.57603686635944,\n",
            "          76.95852534562212,\n",
            "          76.26728110599078,\n",
            "          76.49769585253456,\n",
            "          76.036866359447,\n",
            "          75.80645161290323,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078,\n",
            "          75.80645161290323,\n",
            "          75.57603686635944,\n",
            "          76.49769585253456,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.26728110599078,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          76.26728110599078,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          76.72811059907833,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          76.72811059907833,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          74.19354838709677,\n",
            "          75.34562211981567,\n",
            "          74.88479262672811,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.036866359447,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          75.80645161290323,\n",
            "          76.95852534562212\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          75.57603686635944\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.81105990783409,\n",
            "          73.04147465437788,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          71.6589861751152,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"RfModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          68.20276497695853,\n",
            "          59.67741935483871,\n",
            "          70.04608294930875,\n",
            "          76.49769585253456,\n",
            "          76.49769585253456,\n",
            "          74.88479262672811,\n",
            "          73.27188940092167,\n",
            "          73.963133640553,\n",
            "          77.88018433179722,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          81.10599078341014,\n",
            "          80.18433179723502,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.88479262672811,\n",
            "          80.4147465437788,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          62.67281105990783,\n",
            "          66.3594470046083,\n",
            "          72.81105990783409,\n",
            "          79.72350230414746,\n",
            "          76.26728110599078,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          79.95391705069125,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          81.33640552995391,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          73.27188940092167,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          73.04147465437788,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          79.03225806451613,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          78.11059907834101,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          69.81566820276498,\n",
            "          76.036866359447,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          51.843317972350235,\n",
            "          73.04147465437788,\n",
            "          73.04147465437788,\n",
            "          70.96774193548387,\n",
            "          70.73732718894009,\n",
            "          77.88018433179722,\n",
            "          73.50230414746544,\n",
            "          72.81105990783409,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          80.64516129032258,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          53.2258064516129,\n",
            "          61.05990783410138,\n",
            "          67.2811059907834,\n",
            "          76.49769585253456,\n",
            "          74.19354838709677,\n",
            "          73.27188940092167,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          66.82027649769586,\n",
            "          69.5852534562212,\n",
            "          75.34562211981567,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.3410138248848,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          62.903225806451616,\n",
            "          65.66820276497695,\n",
            "          64.74654377880185,\n",
            "          67.05069124423963,\n",
            "          67.05069124423963,\n",
            "          69.35483870967742,\n",
            "          74.65437788018433,\n",
            "          74.88479262672811,\n",
            "          76.26728110599078,\n",
            "          71.6589861751152,\n",
            "          71.42857142857143,\n",
            "          72.35023041474655,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          69.5852534562212,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.5852534562212,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          62.21198156682027,\n",
            "          66.58986175115207,\n",
            "          59.21658986175116,\n",
            "          53.91705069124424,\n",
            "          45.16129032258064,\n",
            "          51.61290322580645,\n",
            "          59.67741935483871,\n",
            "          57.14285714285714,\n",
            "          59.44700460829493,\n",
            "          58.06451612903226,\n",
            "          62.44239631336406,\n",
            "          61.29032258064516,\n",
            "          60.82949308755761,\n",
            "          63.133640552995395,\n",
            "          64.51612903225806,\n",
            "          67.74193548387096,\n",
            "          66.3594470046083,\n",
            "          66.3594470046083,\n",
            "          67.74193548387096,\n",
            "          68.20276497695853,\n",
            "          67.2811059907834,\n",
            "          69.81566820276498,\n",
            "          71.19815668202764,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          73.73271889400922,\n",
            "          73.963133640553,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          71.42857142857143,\n",
            "          72.11981566820278\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          70.27649769585254,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.29953917050692,\n",
            "          63.594470046082954,\n",
            "          66.82027649769586,\n",
            "          68.4331797235023,\n",
            "          67.51152073732719,\n",
            "          70.04608294930875,\n",
            "          71.6589861751152,\n",
            "          70.73732718894009,\n",
            "          72.11981566820278,\n",
            "          73.04147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.0552995391705,\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          72.58064516129032,\n",
            "          71.88940092165899,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          75.80645161290323,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          74.19354838709677,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          75.11520737327189,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          39.1705069124424,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          74.65437788018433,\n",
            "          74.19354838709677,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.18894009216591,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          64.28571428571429,\n",
            "          63.133640552995395,\n",
            "          65.2073732718894,\n",
            "          71.42857142857143,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 36, using model = LogModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [121 129] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.60      0.45      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.69       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [212 288] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.72      0.47      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.88018433179722, 81.33640552995391]\n",
            "saved Active-learning-experiment-36.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 37, using model = LogModel, selection_function = MarginSamplingSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [54 71] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 74.423963 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83       321\n",
            "           1       0.51      0.47      0.49       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.66      0.66       434\n",
            "weighted avg       0.74      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [ 96 154] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.65      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [143 232] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [212 288] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.44      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "final active learning accuracies [74.42396313364056, 79.49308755760369, 79.49308755760369, 79.03225806451613]\n",
            "saved Active-learning-experiment-37.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 38, using model = LogModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [18 32] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.84       321\n",
            "           1       0.53      0.50      0.51       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.67      0.67       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [40 60] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.86       321\n",
            "           1       0.64      0.37      0.47       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.65      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [55 95] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 74 126] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.86       321\n",
            "           1       0.64      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.67      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [ 91 159] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.62      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.67      0.68       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [110 190] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [138 212] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.65      0.41      0.50       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.66      0.68       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [162 238] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.46      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [185 265] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 82.027650 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89       321\n",
            "           1       0.74      0.48      0.58       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [203 297] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.029 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.74      0.45      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.34562211981567, 78.11059907834101, 79.95391705069125, 78.80184331797236, 78.3410138248848, 78.57142857142857, 78.80184331797236, 81.5668202764977, 82.02764976958525, 81.5668202764977]\n",
            "saved Active-learning-experiment-38.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 39, using model = LogModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [12 13] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.022 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=2.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.93      0.85       321\n",
            "           1       0.60      0.32      0.42       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.62      0.64       434\n",
            "weighted avg       0.74      0.77      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 77  36]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [20 30] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.86       321\n",
            "           1       0.65      0.39      0.49       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.66      0.68       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [30 45] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.6666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87       321\n",
            "           1       0.70      0.37      0.49       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.76      0.66      0.68       434\n",
            "weighted avg       0.78      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [40 60] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.67      0.39      0.49       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.66      0.68       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [47 78] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.67      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [56 94] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87       321\n",
            "           1       0.68      0.37      0.48       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.65      0.67       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [ 61 114] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.2857142857142857, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.66      0.40      0.50       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.66      0.68       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 65 135] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87       321\n",
            "           1       0.69      0.36      0.48       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.75      0.65      0.67       434\n",
            "weighted avg       0.78      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 72  41]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [ 72 153] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.2222222222222222, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.40      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [ 80 170] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.87       321\n",
            "           1       0.69      0.40      0.51       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [ 90 185] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.18181818181818182, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.94      0.87       321\n",
            "           1       0.67      0.35      0.46       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.65      0.66       434\n",
            "weighted avg       0.77      0.79      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 73  40]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [ 95 205] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.96      0.88       321\n",
            "           1       0.75      0.37      0.50       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.78      0.66      0.69       434\n",
            "weighted avg       0.80      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[307  14]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [106 219] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.15384615384615385, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.96      0.88       321\n",
            "           1       0.76      0.36      0.49       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.78      0.66      0.68       434\n",
            "weighted avg       0.80      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[308  13]\n",
            " [ 72  41]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [109 241] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.41      0.52       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [117 258] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.95      0.88       321\n",
            "           1       0.73      0.38      0.50       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [122 278] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [134 291] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier LogisticRegression(C=0.11764705882352941, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.75      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [144 306] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.72      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [161 314] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 82.718894 \n",
            "Classification report for classifier LogisticRegression(C=0.10526315789473684, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.97      0.89       321\n",
            "           1       0.82      0.43      0.57       113\n",
            "\n",
            "    accuracy                           0.83       434\n",
            "   macro avg       0.82      0.70      0.73       434\n",
            "weighted avg       0.83      0.83      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[310  11]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [170 330] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.74      0.45      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.72811059907833, 78.57142857142857, 79.49308755760369, 79.03225806451613, 80.18433179723502, 79.03225806451613, 79.03225806451613, 79.26267281105991, 80.18433179723502, 79.72350230414746, 78.57142857142857, 80.4147465437788, 80.4147465437788, 80.64516129032258, 80.18433179723502, 80.87557603686636, 81.10599078341014, 81.33640552995391, 82.7188940092166, 81.5668202764977]\n",
            "saved Active-learning-experiment-39.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 40, using model = LogModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [6 4] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 72.350230 \n",
            "Classification report for classifier LogisticRegression(C=5.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.81      0.81       321\n",
            "           1       0.47      0.48      0.47       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.64      0.64      0.64       434\n",
            "weighted avg       0.73      0.72      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[260  61]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 1 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 1 0]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [12  8] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.002 s \n",
            "\n",
            "Accuracy rate for 73.271889 \n",
            "Classification report for classifier LogisticRegression(C=2.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.82       321\n",
            "           1       0.49      0.43      0.46       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.65      0.64      0.64       434\n",
            "weighted avg       0.72      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[269  52]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [17 13] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=1.6666666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       321\n",
            "           1       0.56      0.46      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [23 17] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier LogisticRegression(C=1.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83       321\n",
            "           1       0.52      0.44      0.48       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.66      0.65      0.65       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [27 23] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [28 32] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 68.433180 \n",
            "Classification report for classifier LogisticRegression(C=0.8333333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.71      0.77       321\n",
            "           1       0.42      0.60      0.50       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.63      0.66      0.63       434\n",
            "weighted avg       0.73      0.68      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[229  92]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [31 39] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=0.7142857142857143, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83       321\n",
            "           1       0.52      0.51      0.52       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.67      0.67       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [37 43] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=0.625, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83       321\n",
            "           1       0.52      0.51      0.52       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.67      0.67       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [41 49] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 70.506912 \n",
            "Classification report for classifier LogisticRegression(C=0.5555555555555556, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.74      0.79       321\n",
            "           1       0.45      0.59      0.51       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.64      0.67      0.65       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[239  82]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [45 55] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.54      0.53      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [48 62] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.45454545454545453, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.55      0.55      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [53 67] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.029 s \n",
            "\n",
            "Accuracy rate for 70.737327 \n",
            "Classification report for classifier LogisticRegression(C=0.4166666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.74      0.79       321\n",
            "           1       0.45      0.60      0.52       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.67      0.65       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[239  82]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [59 71] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 70.737327 \n",
            "Classification report for classifier LogisticRegression(C=0.38461538461538464, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79       321\n",
            "           1       0.45      0.62      0.52       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[237  84]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [62 78] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 70.737327 \n",
            "Classification report for classifier LogisticRegression(C=0.35714285714285715, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.75      0.79       321\n",
            "           1       0.45      0.58      0.51       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.64      0.67      0.65       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[241  80]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [70 80] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       321\n",
            "           1       0.56      0.57      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [77 83] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.3125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [83 87] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.29411764705882354, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [86 94] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.2777777777777778, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [91 99] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.2631578947368421, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 97 103] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [101 109] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.23809523809523808, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.51      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [107 113] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.22727272727272727, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       321\n",
            "           1       0.68      0.54      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.74       434\n",
            "weighted avg       0.80      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [114 116] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier LogisticRegression(C=0.21739130434782608, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.66      0.56      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [118 122] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier LogisticRegression(C=0.20833333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.79      0.82       321\n",
            "           1       0.51      0.62      0.56       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.71      0.69       434\n",
            "weighted avg       0.77      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[254  67]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [122 128] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87       321\n",
            "           1       0.64      0.59      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.74      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [129 131] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.19230769230769232, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [131 139] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.18518518518518517, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [137 143] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.17857142857142858, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [144 146] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.1724137931034483, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [147 153] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [152 158] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.16129032258064516, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.64      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.67      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [155 165] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.15625, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [158 172] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.15151515151515152, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0\n",
            " 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [161 179] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier LogisticRegression(C=0.14705882352941177, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.66      0.57      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [167 183] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [173 187] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.1388888888888889, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.56      0.43      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [177 193] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.13513513513513514, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [183 197] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.13157894736842105, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [188 202] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.1282051282051282, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [192 208] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [200 210] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.12195121951219512, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [204 216] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.11904761904761904, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.65      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [212 218] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.11627906976744186, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [214 226] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.11363636363636363, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [217 233] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [224 236] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.10869565217391304, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.58      0.45      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [231 239] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.10638297872340426, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.60      0.40      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.65      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [232 248] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.020 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.10416666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [241 249] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.10204081632653061, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.62      0.45      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [243 257] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "final active learning accuracies [72.35023041474655, 73.27188940092167, 76.49769585253456, 74.65437788018433, 76.95852534562212, 68.4331797235023, 74.88479262672811, 74.88479262672811, 70.50691244239631, 76.036866359447, 76.72811059907833, 70.73732718894009, 70.73732718894009, 70.73732718894009, 76.95852534562212, 78.3410138248848, 79.26267281105991, 80.18433179723502, 79.95391705069125, 79.49308755760369, 80.4147465437788, 81.33640552995391, 81.10599078341014, 74.65437788018433, 80.64516129032258, 78.11059907834101, 78.11059907834101, 80.4147465437788, 79.95391705069125, 79.95391705069125, 78.80184331797236, 78.80184331797236, 79.49308755760369, 81.10599078341014, 79.95391705069125, 76.49769585253456, 79.03225806451613, 80.18433179723502, 79.03225806451613, 79.26267281105991, 78.11059907834101, 79.26267281105991, 77.64976958525345, 78.57142857142857, 78.80184331797236, 77.18894009216591, 77.41935483870968, 78.11059907834101, 78.57142857142857, 79.49308755760369]\n",
            "saved Active-learning-experiment-40.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.35023041474655,\n",
            "          73.27188940092167,\n",
            "          76.49769585253456,\n",
            "          74.65437788018433,\n",
            "          76.95852534562212,\n",
            "          68.4331797235023,\n",
            "          74.88479262672811,\n",
            "          74.88479262672811,\n",
            "          70.50691244239631,\n",
            "          76.036866359447,\n",
            "          76.72811059907833,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          81.33640552995391,\n",
            "          81.10599078341014,\n",
            "          74.65437788018433,\n",
            "          80.64516129032258,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          81.10599078341014,\n",
            "          79.95391705069125,\n",
            "          76.49769585253456,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          77.18894009216591,\n",
            "          77.41935483870968,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          82.7188940092166,\n",
            "          81.5668202764977\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          81.33640552995391\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          75.34562211981567,\n",
            "          78.11059907834101,\n",
            "          79.95391705069125,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          81.5668202764977,\n",
            "          82.02764976958525,\n",
            "          81.5668202764977\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          74.88479262672811,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          76.72811059907833,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          76.72811059907833,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          75.57603686635944,\n",
            "          76.95852534562212,\n",
            "          76.26728110599078,\n",
            "          76.49769585253456,\n",
            "          76.036866359447,\n",
            "          75.80645161290323,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078,\n",
            "          75.80645161290323,\n",
            "          75.57603686635944,\n",
            "          76.49769585253456,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.26728110599078,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          76.26728110599078,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          76.72811059907833,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          76.72811059907833,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          74.19354838709677,\n",
            "          75.34562211981567,\n",
            "          74.88479262672811,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.036866359447,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          75.80645161290323,\n",
            "          76.95852534562212\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          75.57603686635944\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.81105990783409,\n",
            "          73.04147465437788,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          71.6589861751152,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"RfModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          68.20276497695853,\n",
            "          59.67741935483871,\n",
            "          70.04608294930875,\n",
            "          76.49769585253456,\n",
            "          76.49769585253456,\n",
            "          74.88479262672811,\n",
            "          73.27188940092167,\n",
            "          73.963133640553,\n",
            "          77.88018433179722,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          81.10599078341014,\n",
            "          80.18433179723502,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.88479262672811,\n",
            "          80.4147465437788,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          62.67281105990783,\n",
            "          66.3594470046083,\n",
            "          72.81105990783409,\n",
            "          79.72350230414746,\n",
            "          76.26728110599078,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          79.95391705069125,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          81.33640552995391,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          73.27188940092167,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          73.04147465437788,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          79.03225806451613,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          78.11059907834101,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          69.81566820276498,\n",
            "          76.036866359447,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          51.843317972350235,\n",
            "          73.04147465437788,\n",
            "          73.04147465437788,\n",
            "          70.96774193548387,\n",
            "          70.73732718894009,\n",
            "          77.88018433179722,\n",
            "          73.50230414746544,\n",
            "          72.81105990783409,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          80.64516129032258,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          53.2258064516129,\n",
            "          61.05990783410138,\n",
            "          67.2811059907834,\n",
            "          76.49769585253456,\n",
            "          74.19354838709677,\n",
            "          73.27188940092167,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          66.82027649769586,\n",
            "          69.5852534562212,\n",
            "          75.34562211981567,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.3410138248848,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          62.903225806451616,\n",
            "          65.66820276497695,\n",
            "          64.74654377880185,\n",
            "          67.05069124423963,\n",
            "          67.05069124423963,\n",
            "          69.35483870967742,\n",
            "          74.65437788018433,\n",
            "          74.88479262672811,\n",
            "          76.26728110599078,\n",
            "          71.6589861751152,\n",
            "          71.42857142857143,\n",
            "          72.35023041474655,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          69.5852534562212,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.5852534562212,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          62.21198156682027,\n",
            "          66.58986175115207,\n",
            "          59.21658986175116,\n",
            "          53.91705069124424,\n",
            "          45.16129032258064,\n",
            "          51.61290322580645,\n",
            "          59.67741935483871,\n",
            "          57.14285714285714,\n",
            "          59.44700460829493,\n",
            "          58.06451612903226,\n",
            "          62.44239631336406,\n",
            "          61.29032258064516,\n",
            "          60.82949308755761,\n",
            "          63.133640552995395,\n",
            "          64.51612903225806,\n",
            "          67.74193548387096,\n",
            "          66.3594470046083,\n",
            "          66.3594470046083,\n",
            "          67.74193548387096,\n",
            "          68.20276497695853,\n",
            "          67.2811059907834,\n",
            "          69.81566820276498,\n",
            "          71.19815668202764,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          73.73271889400922,\n",
            "          73.963133640553,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          71.42857142857143,\n",
            "          72.11981566820278\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          70.27649769585254,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.29953917050692,\n",
            "          63.594470046082954,\n",
            "          66.82027649769586,\n",
            "          68.4331797235023,\n",
            "          67.51152073732719,\n",
            "          70.04608294930875,\n",
            "          71.6589861751152,\n",
            "          70.73732718894009,\n",
            "          72.11981566820278,\n",
            "          73.04147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.0552995391705,\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          72.58064516129032,\n",
            "          71.88940092165899,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          75.80645161290323,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          74.19354838709677,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          75.11520737327189,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          39.1705069124424,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          74.65437788018433,\n",
            "          74.19354838709677,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.18894009216591,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          64.28571428571429,\n",
            "          63.133640552995395,\n",
            "          65.2073732718894,\n",
            "          71.42857142857143,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 41, using model = LogModel, selection_function = EntropySelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [116 134] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.48      0.52       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [234 266] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.86       321\n",
            "           1       0.59      0.42      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.95852534562212, 77.41935483870968]\n",
            "saved Active-learning-experiment-41.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 42, using model = LogModel, selection_function = EntropySelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [55 70] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [ 97 153] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [136 239] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [182 318] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "final active learning accuracies [79.26267281105991, 80.4147465437788, 80.4147465437788, 78.57142857142857]\n",
            "saved Active-learning-experiment-42.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 43, using model = LogModel, selection_function = EntropySelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [28 22] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 73.041475 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81       321\n",
            "           1       0.48      0.55      0.51       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.67      0.66       434\n",
            "weighted avg       0.74      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[255  66]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [56 44] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [83 67] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85       321\n",
            "           1       0.58      0.60      0.59       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [107  93] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.46      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [135 115] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 71.428571 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.76      0.80       321\n",
            "           1       0.46      0.59      0.52       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.67      0.66       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[243  78]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [170 130] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.48      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.68      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [193 157] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.58      0.43      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1]\n",
            "probabilities: (952, 2) \n",
            " [1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [216 184] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [243 207] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [269 231] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.015 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.44      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "final active learning accuracies [73.04147465437788, 76.95852534562212, 78.3410138248848, 76.036866359447, 71.42857142857143, 78.3410138248848, 77.18894009216591, 78.57142857142857, 78.11059907834101, 79.03225806451613]\n",
            "saved Active-learning-experiment-43.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 44, using model = LogModel, selection_function = EntropySelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [14 11] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.016 s \n",
            "\n",
            "Accuracy rate for 70.737327 \n",
            "Classification report for classifier LogisticRegression(C=2.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.79      0.80       321\n",
            "           1       0.44      0.47      0.45       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.63      0.63      0.63       434\n",
            "weighted avg       0.71      0.71      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[254  67]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [28 22] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 72.811060 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81       321\n",
            "           1       0.48      0.56      0.52       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.67      0.66       434\n",
            "weighted avg       0.74      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[253  68]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [40 35] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 74.193548 \n",
            "Classification report for classifier LogisticRegression(C=0.6666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.81      0.82       321\n",
            "           1       0.50      0.54      0.52       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.68      0.67       434\n",
            "weighted avg       0.75      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[261  60]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [58 42] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.002 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [71 54] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [83 67] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.85       321\n",
            "           1       0.56      0.56      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [94 81] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.2857142857142857, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [106  94] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [118 107] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.2222222222222222, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [125 125] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [135 140] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.18181818181818182, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [149 151] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [162 163] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 73.963134 \n",
            "Classification report for classifier LogisticRegression(C=0.15384615384615385, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83       321\n",
            "           1       0.50      0.43      0.46       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.65      0.64      0.65       434\n",
            "weighted avg       0.73      0.74      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [170 180] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [185 190] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [199 201] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [208 217] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.11764705882352941, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [220 230] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [233 242] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.10526315789473684, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [241 259] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.41      0.46       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "final active learning accuracies [70.73732718894009, 72.81105990783409, 74.19354838709677, 77.88018433179722, 78.57142857142857, 77.18894009216591, 78.3410138248848, 77.64976958525345, 77.64976958525345, 77.64976958525345, 79.03225806451613, 78.57142857142857, 73.963133640553, 78.57142857142857, 77.64976958525345, 78.57142857142857, 79.49308755760369, 79.95391705069125, 80.18433179723502, 75.34562211981567]\n",
            "saved Active-learning-experiment-44.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 45, using model = LogModel, selection_function = EntropySelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [6 4] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 59.907834 \n",
            "Classification report for classifier LogisticRegression(C=5.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.58      0.68       321\n",
            "           1       0.36      0.66      0.46       113\n",
            "\n",
            "    accuracy                           0.60       434\n",
            "   macro avg       0.59      0.62      0.57       434\n",
            "weighted avg       0.71      0.60      0.62       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[185 136]\n",
            " [ 38  75]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [13  7] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 57.834101 \n",
            "Classification report for classifier LogisticRegression(C=2.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.58      0.67       321\n",
            "           1       0.33      0.58      0.42       113\n",
            "\n",
            "    accuracy                           0.58       434\n",
            "   macro avg       0.56      0.58      0.54       434\n",
            "weighted avg       0.67      0.58      0.60       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[186 135]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [1 0 1 ... 0 0 1]\n",
            "probabilities: (1282, 2) \n",
            " [1 0 1 ... 0 0 1]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [16 14] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier LogisticRegression(C=1.6666666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.78      0.81       321\n",
            "           1       0.47      0.56      0.51       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.65      0.67      0.66       434\n",
            "weighted avg       0.74      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[250  71]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [21 19] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=1.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.49      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [27 23] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [31 29] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.8333333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       321\n",
            "           1       0.56      0.46      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [33 37] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.7142857142857143, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       321\n",
            "           1       0.56      0.58      0.57       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[269  52]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [37 43] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier LogisticRegression(C=0.625, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83       321\n",
            "           1       0.52      0.45      0.48       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [43 47] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.022 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.5555555555555556, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.56      0.48      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [47 53] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 73.963134 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.83       321\n",
            "           1       0.50      0.48      0.49       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.65      0.66       434\n",
            "weighted avg       0.74      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [51 59] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.45454545454545453, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83       321\n",
            "           1       0.52      0.49      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.67      0.67       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [54 66] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=0.4166666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83       321\n",
            "           1       0.52      0.49      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.67       434\n",
            "weighted avg       0.74      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [58 72] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.38461538461538464, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.49      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [62 78] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.35714285714285715, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [63 87] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.63      0.44      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [67 93] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier LogisticRegression(C=0.3125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.52      0.38      0.44       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.66      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [72 98] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.29411764705882354, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.44      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [ 74 106] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.2777777777777778, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.59      0.44      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [ 76 114] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.2631578947368421, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.54      0.43      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 78 122] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [ 82 128] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.23809523809523808, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.54      0.44      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [ 87 133] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.22727272727272727, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.55      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [ 90 140] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.21739130434782608, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.56      0.44      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [ 93 147] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.20833333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [ 97 153] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.60      0.45      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.69       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [100 160] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.19230769230769232, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.59      0.45      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.68       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [103 167] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.18518518518518517, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.59      0.43      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [106 174] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.17857142857142858, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.41      0.49       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.66      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [109 181] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.1724137931034483, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.58      0.46      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [115 185] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.62      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.67      0.68       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [120 190] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.16129032258064516, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.41      0.49       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.66      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [123 197] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.15625, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.58      0.46      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [129 201] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.15151515151515152, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.41      0.49       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.66      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [134 206] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.14705882352941177, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.92      0.86       321\n",
            "           1       0.63      0.39      0.48       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.65      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [136 214] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       321\n",
            "           1       0.56      0.47      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [143 217] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.1388888888888889, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.67      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [144 226] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.13513513513513514, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.67      0.40      0.50       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.66      0.68       434\n",
            "weighted avg       0.78      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 1 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [149 231] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.13157894736842105, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.66      0.41      0.50       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.67      0.68       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [151 239] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.1282051282051282, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.73      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1\n",
            " 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1\n",
            " 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [156 244] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.45      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [161 249] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.12195121951219512, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.74      0.44      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [163 257] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.11904761904761904, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88       321\n",
            "           1       0.75      0.44      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.69      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [168 262] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier LogisticRegression(C=0.11627906976744186, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.43      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [170 270] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.11363636363636363, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [171 279] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [172 288] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.10869565217391304, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [175 295] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.10638297872340426, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.75      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.79      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [177 303] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.015 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.10416666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [181 309] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.10204081632653061, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.88       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [187 313] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.45      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "final active learning accuracies [59.907834101382484, 57.83410138248848, 72.11981566820278, 77.18894009216591, 78.57142857142857, 76.49769585253456, 76.95852534562212, 74.65437788018433, 76.49769585253456, 73.963133640553, 75.11520737327189, 74.88479262672811, 77.88018433179722, 78.80184331797236, 78.80184331797236, 74.65437788018433, 79.03225806451613, 77.41935483870968, 75.57603686635944, 76.26728110599078, 75.57603686635944, 75.80645161290323, 76.26728110599078, 78.57142857142857, 77.88018433179722, 77.64976958525345, 77.41935483870968, 77.88018433179722, 77.18894009216591, 78.3410138248848, 77.88018433179722, 77.41935483870968, 77.64976958525345, 78.11059907834101, 76.72811059907833, 79.49308755760369, 79.26267281105991, 79.03225806451613, 80.87557603686636, 81.33640552995391, 81.33640552995391, 81.5668202764977, 81.10599078341014, 80.64516129032258, 80.87557603686636, 80.64516129032258, 81.33640552995391, 80.87557603686636, 80.64516129032258, 80.87557603686636]\n",
            "saved Active-learning-experiment-45.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          59.907834101382484,\n",
            "          57.83410138248848,\n",
            "          72.11981566820278,\n",
            "          77.18894009216591,\n",
            "          78.57142857142857,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          74.65437788018433,\n",
            "          76.49769585253456,\n",
            "          73.963133640553,\n",
            "          75.11520737327189,\n",
            "          74.88479262672811,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          74.65437788018433,\n",
            "          79.03225806451613,\n",
            "          77.41935483870968,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078,\n",
            "          75.57603686635944,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          77.88018433179722,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          76.72811059907833,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          80.87557603686636,\n",
            "          81.33640552995391,\n",
            "          81.33640552995391,\n",
            "          81.5668202764977,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          70.73732718894009,\n",
            "          72.81105990783409,\n",
            "          74.19354838709677,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          77.64976958525345,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          75.34562211981567\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          73.04147465437788,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          76.036866359447,\n",
            "          71.42857142857143,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.35023041474655,\n",
            "          73.27188940092167,\n",
            "          76.49769585253456,\n",
            "          74.65437788018433,\n",
            "          76.95852534562212,\n",
            "          68.4331797235023,\n",
            "          74.88479262672811,\n",
            "          74.88479262672811,\n",
            "          70.50691244239631,\n",
            "          76.036866359447,\n",
            "          76.72811059907833,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          81.33640552995391,\n",
            "          81.10599078341014,\n",
            "          74.65437788018433,\n",
            "          80.64516129032258,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          81.10599078341014,\n",
            "          79.95391705069125,\n",
            "          76.49769585253456,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          77.18894009216591,\n",
            "          77.41935483870968,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.87557603686636,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          82.7188940092166,\n",
            "          81.5668202764977\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          81.33640552995391\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          75.34562211981567,\n",
            "          78.11059907834101,\n",
            "          79.95391705069125,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          81.5668202764977,\n",
            "          82.02764976958525,\n",
            "          81.5668202764977\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          74.88479262672811,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          76.72811059907833,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          76.72811059907833,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          75.57603686635944,\n",
            "          76.95852534562212,\n",
            "          76.26728110599078,\n",
            "          76.49769585253456,\n",
            "          76.036866359447,\n",
            "          75.80645161290323,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078,\n",
            "          75.80645161290323,\n",
            "          75.57603686635944,\n",
            "          76.49769585253456,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.26728110599078,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          76.26728110599078,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          76.72811059907833,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          76.72811059907833,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          74.19354838709677,\n",
            "          75.34562211981567,\n",
            "          74.88479262672811,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.036866359447,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          75.80645161290323,\n",
            "          76.95852534562212\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          75.57603686635944\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.81105990783409,\n",
            "          73.04147465437788,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          71.6589861751152,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"RfModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          68.20276497695853,\n",
            "          59.67741935483871,\n",
            "          70.04608294930875,\n",
            "          76.49769585253456,\n",
            "          76.49769585253456,\n",
            "          74.88479262672811,\n",
            "          73.27188940092167,\n",
            "          73.963133640553,\n",
            "          77.88018433179722,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          81.10599078341014,\n",
            "          80.18433179723502,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.88479262672811,\n",
            "          80.4147465437788,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          62.67281105990783,\n",
            "          66.3594470046083,\n",
            "          72.81105990783409,\n",
            "          79.72350230414746,\n",
            "          76.26728110599078,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          79.95391705069125,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          81.33640552995391,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          73.27188940092167,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          73.04147465437788,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          79.03225806451613,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          78.11059907834101,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          69.81566820276498,\n",
            "          76.036866359447,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          51.843317972350235,\n",
            "          73.04147465437788,\n",
            "          73.04147465437788,\n",
            "          70.96774193548387,\n",
            "          70.73732718894009,\n",
            "          77.88018433179722,\n",
            "          73.50230414746544,\n",
            "          72.81105990783409,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          79.72350230414746,\n",
            "          81.10599078341014,\n",
            "          81.33640552995391,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          80.64516129032258,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          53.2258064516129,\n",
            "          61.05990783410138,\n",
            "          67.2811059907834,\n",
            "          76.49769585253456,\n",
            "          74.19354838709677,\n",
            "          73.27188940092167,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          81.33640552995391,\n",
            "          80.87557603686636,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          66.82027649769586,\n",
            "          69.5852534562212,\n",
            "          75.34562211981567,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.3410138248848,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          62.903225806451616,\n",
            "          65.66820276497695,\n",
            "          64.74654377880185,\n",
            "          67.05069124423963,\n",
            "          67.05069124423963,\n",
            "          69.35483870967742,\n",
            "          74.65437788018433,\n",
            "          74.88479262672811,\n",
            "          76.26728110599078,\n",
            "          71.6589861751152,\n",
            "          71.42857142857143,\n",
            "          72.35023041474655,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          77.64976958525345,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          78.57142857142857,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          69.5852534562212,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.26728110599078,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          69.5852534562212,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          73.50230414746544,\n",
            "          62.21198156682027,\n",
            "          66.58986175115207,\n",
            "          59.21658986175116,\n",
            "          53.91705069124424,\n",
            "          45.16129032258064,\n",
            "          51.61290322580645,\n",
            "          59.67741935483871,\n",
            "          57.14285714285714,\n",
            "          59.44700460829493,\n",
            "          58.06451612903226,\n",
            "          62.44239631336406,\n",
            "          61.29032258064516,\n",
            "          60.82949308755761,\n",
            "          63.133640552995395,\n",
            "          64.51612903225806,\n",
            "          67.74193548387096,\n",
            "          66.3594470046083,\n",
            "          66.3594470046083,\n",
            "          67.74193548387096,\n",
            "          68.20276497695853,\n",
            "          67.2811059907834,\n",
            "          69.81566820276498,\n",
            "          71.19815668202764,\n",
            "          71.88940092165899,\n",
            "          73.73271889400922,\n",
            "          73.73271889400922,\n",
            "          73.963133640553,\n",
            "          72.35023041474655,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          71.42857142857143,\n",
            "          72.11981566820278\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.88940092165899,\n",
            "          70.27649769585254,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.29953917050692,\n",
            "          63.594470046082954,\n",
            "          66.82027649769586,\n",
            "          68.4331797235023,\n",
            "          67.51152073732719,\n",
            "          70.04608294930875,\n",
            "          71.6589861751152,\n",
            "          70.73732718894009,\n",
            "          72.11981566820278,\n",
            "          73.04147465437788\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.0552995391705,\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          70.27649769585254,\n",
            "          72.58064516129032,\n",
            "          71.88940092165899,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          76.49769585253456,\n",
            "          75.80645161290323,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.80184331797236,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          77.18894009216591,\n",
            "          76.95852534562212,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          73.963133640553,\n",
            "          74.19354838709677,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          75.11520737327189,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          39.1705069124424,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          74.65437788018433,\n",
            "          74.19354838709677,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          77.18894009216591,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          64.28571428571429,\n",
            "          63.133640552995395,\n",
            "          65.2073732718894,\n",
            "          71.42857142857143,\n",
            "          76.95852534562212,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "{'SvmModel': {'RandomSelection': {'250': [[77.88018433179722, 79.72350230414746]], '125': [[76.95852534562212, 39.1705069124424, 76.72811059907833, 76.72811059907833]], '50': [[64.28571428571429, 63.133640552995395, 65.2073732718894, 71.42857142857143, 76.95852534562212, 77.18894009216591, 78.11059907834101, 79.26267281105991, 79.49308755760369, 79.72350230414746]], '25': [[75.57603686635944, 75.57603686635944, 76.036866359447, 74.65437788018433, 74.19354838709677, 74.88479262672811, 76.036866359447, 76.49769585253456, 77.18894009216591, 75.80645161290323, 78.11059907834101, 79.49308755760369, 78.57142857142857, 78.11059907834101, 79.26267281105991, 78.80184331797236, 79.49308755760369, 80.18433179723502, 79.72350230414746, 79.95391705069125]], '10': [[64.0552995391705, 71.19815668202764, 71.19815668202764, 70.27649769585254, 72.58064516129032, 71.88940092165899, 74.88479262672811, 74.65437788018433, 77.18894009216591, 76.49769585253456, 75.80645161290323, 77.41935483870968, 78.57142857142857, 77.64976958525345, 79.03225806451613, 78.3410138248848, 78.80184331797236, 77.64976958525345, 78.3410138248848, 77.18894009216591, 76.95852534562212, 78.3410138248848, 78.57142857142857, 78.11059907834101, 77.88018433179722, 78.57142857142857, 73.963133640553, 74.19354838709677, 79.72350230414746, 80.4147465437788, 81.10599078341014, 75.11520737327189, 79.03225806451613, 79.49308755760369, 79.72350230414746, 79.26267281105991, 79.03225806451613, 78.11059907834101, 79.49308755760369, 78.3410138248848, 78.3410138248848, 78.11059907834101, 78.11059907834101, 77.88018433179722, 78.3410138248848, 79.03225806451613, 78.3410138248848, 79.49308755760369, 79.49308755760369, 78.80184331797236]]}, 'MarginSamplingSelection': {'250': [[75.11520737327189, 77.64976958525345]], '125': [[71.19815668202764, 70.27649769585254, 71.42857142857143, 72.11981566820278]], '50': [[55.29953917050692, 63.594470046082954, 66.82027649769586, 68.4331797235023, 67.51152073732719, 70.04608294930875, 71.6589861751152, 70.73732718894009, 72.11981566820278, 73.04147465437788]], '25': [[71.88940092165899, 70.27649769585254, 77.88018433179722, 77.18894009216591, 78.3410138248848, 79.03225806451613, 77.64976958525345, 79.26267281105991, 79.95391705069125, 80.18433179723502, 79.95391705069125, 81.10599078341014, 80.64516129032258, 80.4147465437788, 80.64516129032258, 80.64516129032258, 80.64516129032258, 80.18433179723502, 79.72350230414746, 79.72350230414746]], '10': [[73.50230414746544, 62.21198156682027, 66.58986175115207, 59.21658986175116, 53.91705069124424, 45.16129032258064, 51.61290322580645, 59.67741935483871, 57.14285714285714, 59.44700460829493, 58.06451612903226, 62.44239631336406, 61.29032258064516, 60.82949308755761, 63.133640552995395, 64.51612903225806, 67.74193548387096, 66.3594470046083, 66.3594470046083, 67.74193548387096, 68.20276497695853, 67.2811059907834, 69.81566820276498, 71.19815668202764, 71.88940092165899, 73.73271889400922, 73.73271889400922, 73.963133640553, 72.35023041474655, 77.41935483870968, 78.57142857142857, 78.57142857142857, 78.11059907834101, 77.64976958525345, 78.80184331797236, 79.03225806451613, 78.3410138248848, 78.80184331797236, 78.3410138248848, 78.80184331797236, 78.57142857142857, 79.03225806451613, 79.03225806451613, 79.03225806451613, 78.80184331797236, 78.80184331797236, 79.03225806451613, 79.03225806451613, 79.26267281105991, 79.03225806451613]]}, 'EntropySelection': {'250': [[76.26728110599078, 79.03225806451613]], '125': [[71.88940092165899, 78.57142857142857, 80.18433179723502, 79.49308755760369]], '50': [[69.5852534562212, 72.35023041474655, 77.41935483870968, 78.3410138248848, 80.64516129032258, 79.26267281105991, 79.72350230414746, 80.64516129032258, 79.26267281105991, 80.4147465437788]], '25': [[76.036866359447, 75.11520737327189, 74.65437788018433, 69.5852534562212, 77.41935483870968, 76.72811059907833, 77.18894009216591, 77.88018433179722, 77.88018433179722, 77.41935483870968, 77.64976958525345, 77.88018433179722, 77.64976958525345, 78.11059907834101, 79.03225806451613, 78.57142857142857, 79.26267281105991, 79.26267281105991, 79.72350230414746, 79.95391705069125]], '10': [[62.903225806451616, 65.66820276497695, 64.74654377880185, 67.05069124423963, 67.05069124423963, 69.35483870967742, 74.65437788018433, 74.88479262672811, 76.26728110599078, 71.6589861751152, 71.42857142857143, 72.35023041474655, 75.80645161290323, 76.26728110599078, 77.64976958525345, 76.49769585253456, 77.88018433179722, 78.11059907834101, 77.64976958525345, 77.88018433179722, 77.64976958525345, 78.11059907834101, 78.57142857142857, 78.11059907834101, 78.11059907834101, 79.03225806451613, 78.3410138248848, 78.11059907834101, 78.3410138248848, 78.11059907834101, 78.57142857142857, 78.3410138248848, 78.80184331797236, 78.80184331797236, 78.80184331797236, 79.03225806451613, 78.80184331797236, 79.49308755760369, 79.03225806451613, 78.57142857142857, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.26267281105991, 78.80184331797236, 79.26267281105991, 79.03225806451613, 79.03225806451613]]}}, 'RfModel': {'RandomSelection': {'250': [[78.3410138248848, 79.95391705069125]], '125': [[75.57603686635944, 77.41935483870968, 77.64976958525345, 77.41935483870968]], '50': [[78.80184331797236, 79.03225806451613, 79.72350230414746, 79.49308755760369, 80.64516129032258, 79.49308755760369, 79.03225806451613, 79.03225806451613, 79.95391705069125, 79.26267281105991]], '25': [[52.07373271889401, 66.82027649769586, 69.5852534562212, 75.34562211981567, 76.036866359447, 76.26728110599078, 76.95852534562212, 77.64976958525345, 78.80184331797236, 79.26267281105991, 79.95391705069125, 80.18433179723502, 79.72350230414746, 79.95391705069125, 79.95391705069125, 79.26267281105991, 79.95391705069125, 80.4147465437788, 79.72350230414746, 79.72350230414746]], '10': [[53.2258064516129, 61.05990783410138, 67.2811059907834, 76.49769585253456, 74.19354838709677, 73.27188940092167, 77.64976958525345, 78.3410138248848, 79.95391705069125, 79.49308755760369, 79.26267281105991, 77.88018433179722, 77.18894009216591, 77.18894009216591, 78.11059907834101, 79.49308755760369, 78.3410138248848, 79.72350230414746, 79.03225806451613, 79.72350230414746, 79.03225806451613, 79.72350230414746, 79.49308755760369, 80.4147465437788, 78.57142857142857, 79.49308755760369, 80.18433179723502, 79.49308755760369, 80.64516129032258, 79.95391705069125, 79.72350230414746, 79.72350230414746, 80.64516129032258, 80.18433179723502, 80.18433179723502, 80.64516129032258, 80.64516129032258, 79.72350230414746, 80.64516129032258, 80.4147465437788, 80.18433179723502, 81.33640552995391, 80.87557603686636, 79.03225806451613, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.03225806451613, 78.57142857142857, 79.95391705069125]]}, 'MarginSamplingSelection': {'250': [[80.64516129032258, 80.87557603686636]], '125': [[69.81566820276498, 76.036866359447, 78.3410138248848, 80.64516129032258]], '50': [[76.95852534562212, 80.87557603686636, 79.95391705069125, 79.03225806451613, 77.64976958525345, 77.18894009216591, 79.72350230414746, 79.72350230414746, 79.03225806451613, 79.49308755760369]], '25': [[51.843317972350235, 73.04147465437788, 73.04147465437788, 70.96774193548387, 70.73732718894009, 77.88018433179722, 73.50230414746544, 72.81105990783409, 80.87557603686636, 79.72350230414746, 80.18433179723502, 79.49308755760369, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.87557603686636, 79.72350230414746, 81.10599078341014, 81.33640552995391, 79.95391705069125]], '10': [[74.42396313364056, 71.88940092165899, 73.73271889400922, 77.88018433179722, 78.3410138248848, 79.72350230414746, 79.95391705069125, 73.27188940092167, 77.88018433179722, 77.88018433179722, 78.57142857142857, 78.80184331797236, 73.04147465437788, 78.57142857142857, 79.95391705069125, 79.03225806451613, 79.49308755760369, 79.03225806451613, 72.81105990783409, 73.27188940092167, 79.03225806451613, 77.88018433179722, 78.80184331797236, 80.18433179723502, 78.11059907834101, 79.72350230414746, 79.49308755760369, 80.64516129032258, 80.4147465437788, 80.18433179723502, 80.18433179723502, 80.64516129032258, 80.18433179723502, 79.95391705069125, 78.3410138248848, 80.64516129032258, 80.18433179723502, 77.88018433179722, 79.49308755760369, 79.26267281105991, 79.26267281105991, 79.49308755760369, 78.3410138248848, 79.26267281105991, 78.3410138248848, 79.95391705069125, 80.64516129032258, 80.4147465437788, 79.49308755760369, 79.95391705069125]]}, 'EntropySelection': {'250': [[75.80645161290323, 79.95391705069125]], '125': [[74.88479262672811, 80.4147465437788, 78.3410138248848, 78.57142857142857]], '50': [[73.963133640553, 81.33640552995391, 79.72350230414746, 79.26267281105991, 79.49308755760369, 80.4147465437788, 78.3410138248848, 78.80184331797236, 80.64516129032258, 80.64516129032258]], '25': [[62.67281105990783, 66.3594470046083, 72.81105990783409, 79.72350230414746, 76.26728110599078, 77.18894009216591, 76.49769585253456, 79.95391705069125, 77.64976958525345, 78.57142857142857, 78.11059907834101, 79.03225806451613, 78.80184331797236, 80.64516129032258, 80.18433179723502, 78.3410138248848, 79.49308755760369, 79.26267281105991, 79.72350230414746, 80.4147465437788]], '10': [[68.20276497695853, 59.67741935483871, 70.04608294930875, 76.49769585253456, 76.49769585253456, 74.88479262672811, 73.27188940092167, 73.963133640553, 77.88018433179722, 79.72350230414746, 79.72350230414746, 80.64516129032258, 79.03225806451613, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.49308755760369, 80.4147465437788, 78.80184331797236, 81.10599078341014, 80.18433179723502, 81.10599078341014, 81.10599078341014, 80.4147465437788, 81.10599078341014, 80.4147465437788, 80.4147465437788, 80.87557603686636, 80.64516129032258, 79.95391705069125, 79.26267281105991, 78.57142857142857, 78.57142857142857, 78.11059907834101, 78.11059907834101, 79.26267281105991, 79.03225806451613, 79.72350230414746, 79.95391705069125, 79.95391705069125, 79.95391705069125, 79.26267281105991, 80.18433179723502, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.64516129032258, 80.87557603686636, 80.64516129032258, 81.10599078341014]]}}, 'LogModel': {'RandomSelection': {'250': [[73.963133640553, 75.57603686635944]], '125': [[76.95852534562212, 76.72811059907833, 75.57603686635944, 76.26728110599078]], '50': [[72.81105990783409, 73.04147465437788, 76.26728110599078, 76.95852534562212, 77.18894009216591, 71.6589861751152, 78.11059907834101, 77.41935483870968, 79.72350230414746, 78.11059907834101]], '25': [[74.19354838709677, 75.34562211981567, 74.88479262672811, 76.49769585253456, 77.88018433179722, 79.49308755760369, 77.88018433179722, 77.18894009216591, 76.95852534562212, 77.18894009216591, 76.72811059907833, 76.72811059907833, 76.036866359447, 76.26728110599078, 76.036866359447, 76.036866359447, 75.57603686635944, 76.036866359447, 75.80645161290323, 76.95852534562212]], '10': [[73.50230414746544, 74.88479262672811, 77.18894009216591, 77.64976958525345, 77.64976958525345, 77.88018433179722, 77.64976958525345, 78.11059907834101, 78.3410138248848, 76.72811059907833, 75.80645161290323, 78.11059907834101, 77.64976958525345, 75.80645161290323, 76.26728110599078, 77.18894009216591, 77.64976958525345, 76.72811059907833, 76.49769585253456, 76.95852534562212, 75.57603686635944, 76.95852534562212, 76.26728110599078, 76.49769585253456, 76.036866359447, 75.80645161290323, 75.57603686635944, 76.26728110599078, 75.80645161290323, 75.57603686635944, 76.49769585253456, 77.64976958525345, 77.41935483870968, 77.88018433179722, 76.72811059907833, 76.26728110599078, 76.26728110599078, 76.26728110599078, 76.036866359447, 77.64976958525345, 76.95852534562212, 76.49769585253456, 76.95852534562212, 76.26728110599078, 76.72811059907833, 77.18894009216591, 76.72811059907833, 76.036866359447, 76.49769585253456, 77.41935483870968]]}, 'MarginSamplingSelection': {'250': [[77.88018433179722, 81.33640552995391]], '125': [[74.42396313364056, 79.49308755760369, 79.49308755760369, 79.03225806451613]], '50': [[75.34562211981567, 78.11059907834101, 79.95391705069125, 78.80184331797236, 78.3410138248848, 78.57142857142857, 78.80184331797236, 81.5668202764977, 82.02764976958525, 81.5668202764977]], '25': [[76.72811059907833, 78.57142857142857, 79.49308755760369, 79.03225806451613, 80.18433179723502, 79.03225806451613, 79.03225806451613, 79.26267281105991, 80.18433179723502, 79.72350230414746, 78.57142857142857, 80.4147465437788, 80.4147465437788, 80.64516129032258, 80.18433179723502, 80.87557603686636, 81.10599078341014, 81.33640552995391, 82.7188940092166, 81.5668202764977]], '10': [[72.35023041474655, 73.27188940092167, 76.49769585253456, 74.65437788018433, 76.95852534562212, 68.4331797235023, 74.88479262672811, 74.88479262672811, 70.50691244239631, 76.036866359447, 76.72811059907833, 70.73732718894009, 70.73732718894009, 70.73732718894009, 76.95852534562212, 78.3410138248848, 79.26267281105991, 80.18433179723502, 79.95391705069125, 79.49308755760369, 80.4147465437788, 81.33640552995391, 81.10599078341014, 74.65437788018433, 80.64516129032258, 78.11059907834101, 78.11059907834101, 80.4147465437788, 79.95391705069125, 79.95391705069125, 78.80184331797236, 78.80184331797236, 79.49308755760369, 81.10599078341014, 79.95391705069125, 76.49769585253456, 79.03225806451613, 80.18433179723502, 79.03225806451613, 79.26267281105991, 78.11059907834101, 79.26267281105991, 77.64976958525345, 78.57142857142857, 78.80184331797236, 77.18894009216591, 77.41935483870968, 78.11059907834101, 78.57142857142857, 79.49308755760369]]}, 'EntropySelection': {'250': [[76.95852534562212, 77.41935483870968]], '125': [[79.26267281105991, 80.4147465437788, 80.4147465437788, 78.57142857142857]], '50': [[73.04147465437788, 76.95852534562212, 78.3410138248848, 76.036866359447, 71.42857142857143, 78.3410138248848, 77.18894009216591, 78.57142857142857, 78.11059907834101, 79.03225806451613]], '25': [[70.73732718894009, 72.81105990783409, 74.19354838709677, 77.88018433179722, 78.57142857142857, 77.18894009216591, 78.3410138248848, 77.64976958525345, 77.64976958525345, 77.64976958525345, 79.03225806451613, 78.57142857142857, 73.963133640553, 78.57142857142857, 77.64976958525345, 78.57142857142857, 79.49308755760369, 79.95391705069125, 80.18433179723502, 75.34562211981567]], '10': [[59.907834101382484, 57.83410138248848, 72.11981566820278, 77.18894009216591, 78.57142857142857, 76.49769585253456, 76.95852534562212, 74.65437788018433, 76.49769585253456, 73.963133640553, 75.11520737327189, 74.88479262672811, 77.88018433179722, 78.80184331797236, 78.80184331797236, 74.65437788018433, 79.03225806451613, 77.41935483870968, 75.57603686635944, 76.26728110599078, 75.57603686635944, 75.80645161290323, 76.26728110599078, 78.57142857142857, 77.88018433179722, 77.64976958525345, 77.41935483870968, 77.88018433179722, 77.18894009216591, 78.3410138248848, 77.88018433179722, 77.41935483870968, 77.64976958525345, 78.11059907834101, 76.72811059907833, 79.49308755760369, 79.26267281105991, 79.03225806451613, 80.87557603686636, 81.33640552995391, 81.33640552995391, 81.5668202764977, 81.10599078341014, 80.64516129032258, 80.87557603686636, 80.64516129032258, 81.33640552995391, 80.87557603686636, 80.64516129032258, 80.87557603686636]]}}}\n",
            "{'LogModel': {'EntropySelection': {'10': [[59.907834101382484, 57.83410138248848, 72.11981566820278, 77.18894009216591, 78.57142857142857, 76.49769585253456, 76.95852534562212, 74.65437788018433, 76.49769585253456, 73.963133640553, 75.11520737327189, 74.88479262672811, 77.88018433179722, 78.80184331797236, 78.80184331797236, 74.65437788018433, 79.03225806451613, 77.41935483870968, 75.57603686635944, 76.26728110599078, 75.57603686635944, 75.80645161290323, 76.26728110599078, 78.57142857142857, 77.88018433179722, 77.64976958525345, 77.41935483870968, 77.88018433179722, 77.18894009216591, 78.3410138248848, 77.88018433179722, 77.41935483870968, 77.64976958525345, 78.11059907834101, 76.72811059907833, 79.49308755760369, 79.26267281105991, 79.03225806451613, 80.87557603686636, 81.33640552995391, 81.33640552995391, 81.5668202764977, 81.10599078341014, 80.64516129032258, 80.87557603686636, 80.64516129032258, 81.33640552995391, 80.87557603686636, 80.64516129032258, 80.87557603686636]], '125': [[79.26267281105991, 80.4147465437788, 80.4147465437788, 78.57142857142857]], '25': [[70.73732718894009, 72.81105990783409, 74.19354838709677, 77.88018433179722, 78.57142857142857, 77.18894009216591, 78.3410138248848, 77.64976958525345, 77.64976958525345, 77.64976958525345, 79.03225806451613, 78.57142857142857, 73.963133640553, 78.57142857142857, 77.64976958525345, 78.57142857142857, 79.49308755760369, 79.95391705069125, 80.18433179723502, 75.34562211981567]], '250': [[76.95852534562212, 77.41935483870968]], '50': [[73.04147465437788, 76.95852534562212, 78.3410138248848, 76.036866359447, 71.42857142857143, 78.3410138248848, 77.18894009216591, 78.57142857142857, 78.11059907834101, 79.03225806451613]]}, 'MarginSamplingSelection': {'10': [[72.35023041474655, 73.27188940092167, 76.49769585253456, 74.65437788018433, 76.95852534562212, 68.4331797235023, 74.88479262672811, 74.88479262672811, 70.50691244239631, 76.036866359447, 76.72811059907833, 70.73732718894009, 70.73732718894009, 70.73732718894009, 76.95852534562212, 78.3410138248848, 79.26267281105991, 80.18433179723502, 79.95391705069125, 79.49308755760369, 80.4147465437788, 81.33640552995391, 81.10599078341014, 74.65437788018433, 80.64516129032258, 78.11059907834101, 78.11059907834101, 80.4147465437788, 79.95391705069125, 79.95391705069125, 78.80184331797236, 78.80184331797236, 79.49308755760369, 81.10599078341014, 79.95391705069125, 76.49769585253456, 79.03225806451613, 80.18433179723502, 79.03225806451613, 79.26267281105991, 78.11059907834101, 79.26267281105991, 77.64976958525345, 78.57142857142857, 78.80184331797236, 77.18894009216591, 77.41935483870968, 78.11059907834101, 78.57142857142857, 79.49308755760369]], '125': [[74.42396313364056, 79.49308755760369, 79.49308755760369, 79.03225806451613]], '25': [[76.72811059907833, 78.57142857142857, 79.49308755760369, 79.03225806451613, 80.18433179723502, 79.03225806451613, 79.03225806451613, 79.26267281105991, 80.18433179723502, 79.72350230414746, 78.57142857142857, 80.4147465437788, 80.4147465437788, 80.64516129032258, 80.18433179723502, 80.87557603686636, 81.10599078341014, 81.33640552995391, 82.7188940092166, 81.5668202764977]], '250': [[77.88018433179722, 81.33640552995391]], '50': [[75.34562211981567, 78.11059907834101, 79.95391705069125, 78.80184331797236, 78.3410138248848, 78.57142857142857, 78.80184331797236, 81.5668202764977, 82.02764976958525, 81.5668202764977]]}, 'RandomSelection': {'10': [[73.50230414746544, 74.88479262672811, 77.18894009216591, 77.64976958525345, 77.64976958525345, 77.88018433179722, 77.64976958525345, 78.11059907834101, 78.3410138248848, 76.72811059907833, 75.80645161290323, 78.11059907834101, 77.64976958525345, 75.80645161290323, 76.26728110599078, 77.18894009216591, 77.64976958525345, 76.72811059907833, 76.49769585253456, 76.95852534562212, 75.57603686635944, 76.95852534562212, 76.26728110599078, 76.49769585253456, 76.036866359447, 75.80645161290323, 75.57603686635944, 76.26728110599078, 75.80645161290323, 75.57603686635944, 76.49769585253456, 77.64976958525345, 77.41935483870968, 77.88018433179722, 76.72811059907833, 76.26728110599078, 76.26728110599078, 76.26728110599078, 76.036866359447, 77.64976958525345, 76.95852534562212, 76.49769585253456, 76.95852534562212, 76.26728110599078, 76.72811059907833, 77.18894009216591, 76.72811059907833, 76.036866359447, 76.49769585253456, 77.41935483870968]], '125': [[76.95852534562212, 76.72811059907833, 75.57603686635944, 76.26728110599078]], '25': [[74.19354838709677, 75.34562211981567, 74.88479262672811, 76.49769585253456, 77.88018433179722, 79.49308755760369, 77.88018433179722, 77.18894009216591, 76.95852534562212, 77.18894009216591, 76.72811059907833, 76.72811059907833, 76.036866359447, 76.26728110599078, 76.036866359447, 76.036866359447, 75.57603686635944, 76.036866359447, 75.80645161290323, 76.95852534562212]], '250': [[73.963133640553, 75.57603686635944]], '50': [[72.81105990783409, 73.04147465437788, 76.26728110599078, 76.95852534562212, 77.18894009216591, 71.6589861751152, 78.11059907834101, 77.41935483870968, 79.72350230414746, 78.11059907834101]]}}, 'RfModel': {'EntropySelection': {'10': [[68.20276497695853, 59.67741935483871, 70.04608294930875, 76.49769585253456, 76.49769585253456, 74.88479262672811, 73.27188940092167, 73.963133640553, 77.88018433179722, 79.72350230414746, 79.72350230414746, 80.64516129032258, 79.03225806451613, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.49308755760369, 80.4147465437788, 78.80184331797236, 81.10599078341014, 80.18433179723502, 81.10599078341014, 81.10599078341014, 80.4147465437788, 81.10599078341014, 80.4147465437788, 80.4147465437788, 80.87557603686636, 80.64516129032258, 79.95391705069125, 79.26267281105991, 78.57142857142857, 78.57142857142857, 78.11059907834101, 78.11059907834101, 79.26267281105991, 79.03225806451613, 79.72350230414746, 79.95391705069125, 79.95391705069125, 79.95391705069125, 79.26267281105991, 80.18433179723502, 80.64516129032258, 80.64516129032258, 80.87557603686636, 80.64516129032258, 80.87557603686636, 80.64516129032258, 81.10599078341014]], '125': [[74.88479262672811, 80.4147465437788, 78.3410138248848, 78.57142857142857]], '25': [[62.67281105990783, 66.3594470046083, 72.81105990783409, 79.72350230414746, 76.26728110599078, 77.18894009216591, 76.49769585253456, 79.95391705069125, 77.64976958525345, 78.57142857142857, 78.11059907834101, 79.03225806451613, 78.80184331797236, 80.64516129032258, 80.18433179723502, 78.3410138248848, 79.49308755760369, 79.26267281105991, 79.72350230414746, 80.4147465437788]], '250': [[75.80645161290323, 79.95391705069125]], '50': [[73.963133640553, 81.33640552995391, 79.72350230414746, 79.26267281105991, 79.49308755760369, 80.4147465437788, 78.3410138248848, 78.80184331797236, 80.64516129032258, 80.64516129032258]]}, 'MarginSamplingSelection': {'10': [[74.42396313364056, 71.88940092165899, 73.73271889400922, 77.88018433179722, 78.3410138248848, 79.72350230414746, 79.95391705069125, 73.27188940092167, 77.88018433179722, 77.88018433179722, 78.57142857142857, 78.80184331797236, 73.04147465437788, 78.57142857142857, 79.95391705069125, 79.03225806451613, 79.49308755760369, 79.03225806451613, 72.81105990783409, 73.27188940092167, 79.03225806451613, 77.88018433179722, 78.80184331797236, 80.18433179723502, 78.11059907834101, 79.72350230414746, 79.49308755760369, 80.64516129032258, 80.4147465437788, 80.18433179723502, 80.18433179723502, 80.64516129032258, 80.18433179723502, 79.95391705069125, 78.3410138248848, 80.64516129032258, 80.18433179723502, 77.88018433179722, 79.49308755760369, 79.26267281105991, 79.26267281105991, 79.49308755760369, 78.3410138248848, 79.26267281105991, 78.3410138248848, 79.95391705069125, 80.64516129032258, 80.4147465437788, 79.49308755760369, 79.95391705069125]], '125': [[69.81566820276498, 76.036866359447, 78.3410138248848, 80.64516129032258]], '25': [[51.843317972350235, 73.04147465437788, 73.04147465437788, 70.96774193548387, 70.73732718894009, 77.88018433179722, 73.50230414746544, 72.81105990783409, 80.87557603686636, 79.72350230414746, 80.18433179723502, 79.49308755760369, 80.64516129032258, 80.64516129032258, 81.10599078341014, 80.87557603686636, 79.72350230414746, 81.10599078341014, 81.33640552995391, 79.95391705069125]], '250': [[80.64516129032258, 80.87557603686636]], '50': [[76.95852534562212, 80.87557603686636, 79.95391705069125, 79.03225806451613, 77.64976958525345, 77.18894009216591, 79.72350230414746, 79.72350230414746, 79.03225806451613, 79.49308755760369]]}, 'RandomSelection': {'10': [[53.2258064516129, 61.05990783410138, 67.2811059907834, 76.49769585253456, 74.19354838709677, 73.27188940092167, 77.64976958525345, 78.3410138248848, 79.95391705069125, 79.49308755760369, 79.26267281105991, 77.88018433179722, 77.18894009216591, 77.18894009216591, 78.11059907834101, 79.49308755760369, 78.3410138248848, 79.72350230414746, 79.03225806451613, 79.72350230414746, 79.03225806451613, 79.72350230414746, 79.49308755760369, 80.4147465437788, 78.57142857142857, 79.49308755760369, 80.18433179723502, 79.49308755760369, 80.64516129032258, 79.95391705069125, 79.72350230414746, 79.72350230414746, 80.64516129032258, 80.18433179723502, 80.18433179723502, 80.64516129032258, 80.64516129032258, 79.72350230414746, 80.64516129032258, 80.4147465437788, 80.18433179723502, 81.33640552995391, 80.87557603686636, 79.03225806451613, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.03225806451613, 78.57142857142857, 79.95391705069125]], '125': [[75.57603686635944, 77.41935483870968, 77.64976958525345, 77.41935483870968]], '25': [[52.07373271889401, 66.82027649769586, 69.5852534562212, 75.34562211981567, 76.036866359447, 76.26728110599078, 76.95852534562212, 77.64976958525345, 78.80184331797236, 79.26267281105991, 79.95391705069125, 80.18433179723502, 79.72350230414746, 79.95391705069125, 79.95391705069125, 79.26267281105991, 79.95391705069125, 80.4147465437788, 79.72350230414746, 79.72350230414746]], '250': [[78.3410138248848, 79.95391705069125]], '50': [[78.80184331797236, 79.03225806451613, 79.72350230414746, 79.49308755760369, 80.64516129032258, 79.49308755760369, 79.03225806451613, 79.03225806451613, 79.95391705069125, 79.26267281105991]]}}, 'SvmModel': {'EntropySelection': {'10': [[62.903225806451616, 65.66820276497695, 64.74654377880185, 67.05069124423963, 67.05069124423963, 69.35483870967742, 74.65437788018433, 74.88479262672811, 76.26728110599078, 71.6589861751152, 71.42857142857143, 72.35023041474655, 75.80645161290323, 76.26728110599078, 77.64976958525345, 76.49769585253456, 77.88018433179722, 78.11059907834101, 77.64976958525345, 77.88018433179722, 77.64976958525345, 78.11059907834101, 78.57142857142857, 78.11059907834101, 78.11059907834101, 79.03225806451613, 78.3410138248848, 78.11059907834101, 78.3410138248848, 78.11059907834101, 78.57142857142857, 78.3410138248848, 78.80184331797236, 78.80184331797236, 78.80184331797236, 79.03225806451613, 78.80184331797236, 79.49308755760369, 79.03225806451613, 78.57142857142857, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.26267281105991, 78.80184331797236, 79.26267281105991, 79.03225806451613, 79.03225806451613]], '125': [[71.88940092165899, 78.57142857142857, 80.18433179723502, 79.49308755760369]], '25': [[76.036866359447, 75.11520737327189, 74.65437788018433, 69.5852534562212, 77.41935483870968, 76.72811059907833, 77.18894009216591, 77.88018433179722, 77.88018433179722, 77.41935483870968, 77.64976958525345, 77.88018433179722, 77.64976958525345, 78.11059907834101, 79.03225806451613, 78.57142857142857, 79.26267281105991, 79.26267281105991, 79.72350230414746, 79.95391705069125]], '250': [[76.26728110599078, 79.03225806451613]], '50': [[69.5852534562212, 72.35023041474655, 77.41935483870968, 78.3410138248848, 80.64516129032258, 79.26267281105991, 79.72350230414746, 80.64516129032258, 79.26267281105991, 80.4147465437788]]}, 'MarginSamplingSelection': {'10': [[73.50230414746544, 62.21198156682027, 66.58986175115207, 59.21658986175116, 53.91705069124424, 45.16129032258064, 51.61290322580645, 59.67741935483871, 57.14285714285714, 59.44700460829493, 58.06451612903226, 62.44239631336406, 61.29032258064516, 60.82949308755761, 63.133640552995395, 64.51612903225806, 67.74193548387096, 66.3594470046083, 66.3594470046083, 67.74193548387096, 68.20276497695853, 67.2811059907834, 69.81566820276498, 71.19815668202764, 71.88940092165899, 73.73271889400922, 73.73271889400922, 73.963133640553, 72.35023041474655, 77.41935483870968, 78.57142857142857, 78.57142857142857, 78.11059907834101, 77.64976958525345, 78.80184331797236, 79.03225806451613, 78.3410138248848, 78.80184331797236, 78.3410138248848, 78.80184331797236, 78.57142857142857, 79.03225806451613, 79.03225806451613, 79.03225806451613, 78.80184331797236, 78.80184331797236, 79.03225806451613, 79.03225806451613, 79.26267281105991, 79.03225806451613]], '125': [[71.19815668202764, 70.27649769585254, 71.42857142857143, 72.11981566820278]], '25': [[71.88940092165899, 70.27649769585254, 77.88018433179722, 77.18894009216591, 78.3410138248848, 79.03225806451613, 77.64976958525345, 79.26267281105991, 79.95391705069125, 80.18433179723502, 79.95391705069125, 81.10599078341014, 80.64516129032258, 80.4147465437788, 80.64516129032258, 80.64516129032258, 80.64516129032258, 80.18433179723502, 79.72350230414746, 79.72350230414746]], '250': [[75.11520737327189, 77.64976958525345]], '50': [[55.29953917050692, 63.594470046082954, 66.82027649769586, 68.4331797235023, 67.51152073732719, 70.04608294930875, 71.6589861751152, 70.73732718894009, 72.11981566820278, 73.04147465437788]]}, 'RandomSelection': {'10': [[64.0552995391705, 71.19815668202764, 71.19815668202764, 70.27649769585254, 72.58064516129032, 71.88940092165899, 74.88479262672811, 74.65437788018433, 77.18894009216591, 76.49769585253456, 75.80645161290323, 77.41935483870968, 78.57142857142857, 77.64976958525345, 79.03225806451613, 78.3410138248848, 78.80184331797236, 77.64976958525345, 78.3410138248848, 77.18894009216591, 76.95852534562212, 78.3410138248848, 78.57142857142857, 78.11059907834101, 77.88018433179722, 78.57142857142857, 73.963133640553, 74.19354838709677, 79.72350230414746, 80.4147465437788, 81.10599078341014, 75.11520737327189, 79.03225806451613, 79.49308755760369, 79.72350230414746, 79.26267281105991, 79.03225806451613, 78.11059907834101, 79.49308755760369, 78.3410138248848, 78.3410138248848, 78.11059907834101, 78.11059907834101, 77.88018433179722, 78.3410138248848, 79.03225806451613, 78.3410138248848, 79.49308755760369, 79.49308755760369, 78.80184331797236]], '125': [[76.95852534562212, 39.1705069124424, 76.72811059907833, 76.72811059907833]], '25': [[75.57603686635944, 75.57603686635944, 76.036866359447, 74.65437788018433, 74.19354838709677, 74.88479262672811, 76.036866359447, 76.49769585253456, 77.18894009216591, 75.80645161290323, 78.11059907834101, 79.49308755760369, 78.57142857142857, 78.11059907834101, 79.26267281105991, 78.80184331797236, 79.49308755760369, 80.18433179723502, 79.72350230414746, 79.95391705069125]], '250': [[77.88018433179722, 79.72350230414746]], '50': [[64.28571428571429, 63.133640552995395, 65.2073732718894, 71.42857142857143, 76.95852534562212, 77.18894009216591, 78.11059907834101, 79.26267281105991, 79.49308755760369, 79.72350230414746]]}}}\n"
          ]
        }
      ],
      "source": [
        "(X, y) = download()\n",
        "(X_train_full, y_train_full, X_test, y_test) = split(trainset_size)\n",
        "print ('train:', X_train_full.shape, y_train_full.shape)\n",
        "print ('test :', X_test.shape, y_test.shape)\n",
        "classes = len(np.unique(y))\n",
        "print ('unique classes', classes)\n",
        "\n",
        "def pickle_save(fname, data):\n",
        "  filehandler = open(fname,\"wb\")\n",
        "  pickle.dump(data,filehandler)\n",
        "  filehandler.close() \n",
        "  print('saved', fname, os.getcwd(), os.listdir())\n",
        "\n",
        "def pickle_load(fname):\n",
        "  print(os.getcwd(), os.listdir())\n",
        "  file = open(fname,'rb')\n",
        "  data = pickle.load(file)\n",
        "  file.close()\n",
        "  print(data)\n",
        "  return data\n",
        "  \n",
        "def experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
        "    algos_temp = []\n",
        "    print ('stopping at:', max_queried)\n",
        "    count = 0\n",
        "    for model_object in models:\n",
        "      if model_object.__name__ not in d:\n",
        "          d[model_object.__name__] = {}\n",
        "      \n",
        "      for selection_function in selection_functions:\n",
        "        if selection_function.__name__ not in d[model_object.__name__]:\n",
        "            d[model_object.__name__][selection_function.__name__] = {}\n",
        "        \n",
        "        for k in Ks:\n",
        "            d[model_object.__name__][selection_function.__name__][str(k)] = []           \n",
        "            \n",
        "            for i in range(0, repeats):\n",
        "                count+=1\n",
        "                if count >= contfrom:\n",
        "                    print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
        "                    alg = TheAlgorithm(k, \n",
        "                                       model_object, \n",
        "                                       selection_function\n",
        "                                       )\n",
        "                    alg.run(X_train_full, y_train_full, X_test, y_test)\n",
        "                    d[model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
        "                    fname = 'Active-learning-experiment-' + str(count) + '.pkl'\n",
        "                    pickle_save(fname, d)\n",
        "                    if count % 5 == 0:\n",
        "                        print(json.dumps(d, indent=2, sort_keys=True))\n",
        "                    print ()\n",
        "                    print ('---------------------------- FINISHED ---------------------------')\n",
        "                    print ()\n",
        "    return d\n",
        "\n",
        "\n",
        "max_queried = 500 \n",
        "\n",
        "repeats = 1\n",
        "\n",
        "models = [SvmModel, RfModel, LogModel] \n",
        "\n",
        "selection_functions = [RandomSelection, MarginSamplingSelection, EntropySelection] \n",
        "\n",
        "Ks = [250,125,50,25,10] \n",
        "\n",
        "d = {}\n",
        "stopped_at = -1 \n",
        "\n",
        "# print('directory dump including pickle files:', os.getcwd(), np.sort(os.listdir()))  \n",
        "# d = pickle_load('Active-learning-experiment-' + str(stopped_at) + '.pkl')  \n",
        "# print(json.dumps(d, indent=2, sort_keys=True))\n",
        "\n",
        "d = experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
        "print (d)\n",
        "results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So which is the better model? under the stopping condition and hyper parameters - random forest is the winner!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"261.049219pt\" version=\"1.1\" viewBox=\"0 0 384.83125 261.049219\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-17T14:00:05.821119</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 261.049219 \nL 384.83125 261.049219 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \nL 368.0875 10.999219 \nL 33.2875 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m40e7b0f20c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 70.4875 228.439219 \nL 70.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 107.6875 228.439219 \nL 107.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 144.8875 228.439219 \nL 144.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 182.0875 228.439219 \nL 182.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 219.2875 228.439219 \nL 219.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 256.4875 228.439219 \nL 256.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 293.6875 228.439219 \nL 293.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 330.8875 228.439219 \nL 330.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#m40e7b0f20c\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mff127dd484\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mff127dd484\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 232.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 192.199219 \nL 368.0875 192.199219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mff127dd484\" y=\"192.199219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 195.998438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 155.959219 \nL 368.0875 155.959219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mff127dd484\" y=\"155.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 159.758438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 119.719219 \nL 368.0875 119.719219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mff127dd484\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 123.518438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 83.479219 \nL 368.0875 83.479219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mff127dd484\" y=\"83.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 87.278438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 47.239219 \nL 368.0875 47.239219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mff127dd484\" y=\"47.239219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 51.038438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mff127dd484\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M -1 21.871219 \nL 368.0875 21.871219 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 182.0875 89.491385 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 89.0875 99.511661 \nL 182.0875 92.831477 \nL 275.0875 91.996454 \nL 368.0875 92.831477 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 87.821339 \nL 70.4875 86.986316 \nL 107.6875 84.481246 \nL 144.8875 85.316269 \nL 182.0875 81.141154 \nL 219.2875 85.316269 \nL 256.4875 86.986316 \nL 293.6875 86.986316 \nL 330.8875 83.646223 \nL 368.0875 86.151292 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 14.6875 184.684011 \nL 33.2875 131.242537 \nL 51.8875 121.22226 \nL 70.4875 100.346684 \nL 89.0875 97.841615 \nL 107.6875 97.006592 \nL 126.2875 94.501523 \nL 144.8875 91.996454 \nL 163.4875 87.821339 \nL 182.0875 86.151292 \nL 200.6875 83.646223 \nL 219.2875 82.8112 \nL 237.8875 84.481246 \nL 256.4875 83.646223 \nL 275.0875 83.646223 \nL 293.6875 86.151292 \nL 312.2875 83.646223 \nL 330.8875 81.976177 \nL 349.4875 84.481246 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 3.5275 180.508896 \nL 10.9675 152.118113 \nL 18.4075 129.572491 \nL 25.8475 96.171569 \nL 33.2875 104.521799 \nL 40.7275 107.861892 \nL 48.1675 91.996454 \nL 55.6075 89.491385 \nL 63.0475 83.646223 \nL 70.4875 85.316269 \nL 77.9275 86.151292 \nL 85.3675 91.161431 \nL 92.8075 93.6665 \nL 100.2475 93.6665 \nL 107.6875 90.326408 \nL 115.1275 85.316269 \nL 122.5675 89.491385 \nL 130.0075 84.481246 \nL 137.4475 86.986316 \nL 144.8875 84.481246 \nL 152.3275 86.986316 \nL 159.7675 84.481246 \nL 167.2075 85.316269 \nL 174.6475 81.976177 \nL 182.0875 88.656362 \nL 189.5275 85.316269 \nL 196.9675 82.8112 \nL 204.4075 85.316269 \nL 211.8475 81.141154 \nL 219.2875 83.646223 \nL 226.7275 84.481246 \nL 234.1675 84.481246 \nL 241.6075 81.141154 \nL 249.0475 82.8112 \nL 256.4875 82.8112 \nL 263.9275 81.141154 \nL 271.3675 81.141154 \nL 278.8075 84.481246 \nL 286.2475 81.141154 \nL 293.6875 81.976177 \nL 301.1275 82.8112 \nL 308.5675 78.636085 \nL 316.0075 80.306131 \nL 323.4475 86.986316 \nL 330.8875 83.646223 \nL 338.3275 84.481246 \nL 345.7675 84.481246 \nL 353.2075 86.986316 \nL 360.6475 88.656362 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 182.0875 81.141154 \nL 368.0875 80.306131 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 89.0875 120.387237 \nL 182.0875 97.841615 \nL 275.0875 89.491385 \nL 368.0875 81.141154 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 94.501523 \nL 70.4875 80.306131 \nL 107.6875 83.646223 \nL 144.8875 86.986316 \nL 182.0875 91.996454 \nL 219.2875 93.6665 \nL 256.4875 84.481246 \nL 293.6875 84.481246 \nL 330.8875 86.986316 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 14.6875 185.519034 \nL 33.2875 108.696915 \nL 51.8875 108.696915 \nL 70.4875 116.212122 \nL 89.0875 117.047145 \nL 107.6875 91.161431 \nL 126.2875 107.026869 \nL 144.8875 109.531938 \nL 163.4875 80.306131 \nL 182.0875 84.481246 \nL 200.6875 82.8112 \nL 219.2875 85.316269 \nL 237.8875 81.141154 \nL 256.4875 81.141154 \nL 275.0875 79.471108 \nL 293.6875 80.306131 \nL 312.2875 84.481246 \nL 330.8875 79.471108 \nL 349.4875 78.636085 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 3.5275 103.686776 \nL 10.9675 112.87203 \nL 18.4075 106.191845 \nL 25.8475 91.161431 \nL 33.2875 89.491385 \nL 40.7275 84.481246 \nL 48.1675 83.646223 \nL 55.6075 107.861892 \nL 63.0475 91.161431 \nL 70.4875 91.161431 \nL 77.9275 88.656362 \nL 85.3675 87.821339 \nL 92.8075 108.696915 \nL 100.2475 88.656362 \nL 107.6875 83.646223 \nL 115.1275 86.986316 \nL 122.5675 85.316269 \nL 130.0075 86.986316 \nL 137.4475 109.531938 \nL 144.8875 107.861892 \nL 152.3275 86.986316 \nL 159.7675 91.161431 \nL 167.2075 87.821339 \nL 174.6475 82.8112 \nL 182.0875 90.326408 \nL 189.5275 84.481246 \nL 196.9675 85.316269 \nL 204.4075 81.141154 \nL 211.8475 81.976177 \nL 219.2875 82.8112 \nL 226.7275 82.8112 \nL 234.1675 81.141154 \nL 241.6075 82.8112 \nL 249.0475 83.646223 \nL 256.4875 89.491385 \nL 263.9275 81.141154 \nL 271.3675 82.8112 \nL 278.8075 91.161431 \nL 286.2475 85.316269 \nL 293.6875 86.151292 \nL 301.1275 86.151292 \nL 308.5675 85.316269 \nL 316.0075 89.491385 \nL 323.4475 86.151292 \nL 330.8875 89.491385 \nL 338.3275 83.646223 \nL 345.7675 81.141154 \nL 353.2075 81.976177 \nL 360.6475 85.316269 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 182.0875 98.676638 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 89.0875 102.01673 \nL 182.0875 81.976177 \nL 275.0875 89.491385 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 33.2875 105.356822 \nL 70.4875 78.636085 \nL 107.6875 84.481246 \nL 144.8875 86.151292 \nL 182.0875 85.316269 \nL 219.2875 81.976177 \nL 256.4875 89.491385 \nL 293.6875 87.821339 \nL 330.8875 81.141154 \nL 368.0875 81.141154 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 14.6875 146.272951 \nL 33.2875 132.912583 \nL 51.8875 109.531938 \nL 70.4875 84.481246 \nL 89.0875 97.006592 \nL 107.6875 93.6665 \nL 126.2875 96.171569 \nL 144.8875 83.646223 \nL 163.4875 91.996454 \nL 182.0875 88.656362 \nL 200.6875 90.326408 \nL 219.2875 86.986316 \nL 237.8875 87.821339 \nL 256.4875 81.141154 \nL 275.0875 82.8112 \nL 293.6875 89.491385 \nL 312.2875 85.316269 \nL 330.8875 86.151292 \nL 349.4875 84.481246 \nL 368.0875 81.976177 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path clip-path=\"url(#p7c448048e0)\" d=\"M 3.5275 126.232398 \nL 10.9675 157.128251 \nL 18.4075 119.552214 \nL 25.8475 96.171569 \nL 33.2875 96.171569 \nL 40.7275 102.01673 \nL 48.1675 107.861892 \nL 55.6075 105.356822 \nL 63.0475 91.161431 \nL 70.4875 84.481246 \nL 77.9275 84.481246 \nL 85.3675 81.141154 \nL 92.8075 86.986316 \nL 100.2475 83.646223 \nL 107.6875 84.481246 \nL 115.1275 83.646223 \nL 122.5675 85.316269 \nL 130.0075 81.976177 \nL 137.4475 87.821339 \nL 144.8875 79.471108 \nL 152.3275 82.8112 \nL 159.7675 79.471108 \nL 167.2075 79.471108 \nL 174.6475 81.976177 \nL 182.0875 79.471108 \nL 189.5275 81.976177 \nL 196.9675 81.976177 \nL 204.4075 80.306131 \nL 211.8475 81.141154 \nL 219.2875 83.646223 \nL 226.7275 86.151292 \nL 234.1675 88.656362 \nL 241.6075 88.656362 \nL 249.0475 90.326408 \nL 256.4875 90.326408 \nL 263.9275 86.151292 \nL 271.3675 86.986316 \nL 278.8075 84.481246 \nL 286.2475 83.646223 \nL 293.6875 83.646223 \nL 301.1275 83.646223 \nL 308.5675 86.151292 \nL 316.0075 82.8112 \nL 323.4475 81.141154 \nL 330.8875 81.141154 \nL 338.3275 80.306131 \nL 345.7675 81.141154 \nL 353.2075 80.306131 \nL 360.6475 81.141154 \nL 368.0875 79.471108 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 135.239062 253.849219 \nL 361.0875 253.849219 \nQ 363.0875 253.849219 363.0875 251.849219 \nL 363.0875 17.999219 \nQ 363.0875 15.999219 361.0875 15.999219 \nL 135.239062 15.999219 \nQ 133.239062 15.999219 133.239062 17.999219 \nL 133.239062 251.849219 \nQ 133.239062 253.849219 135.239062 253.849219 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 137.239062 24.097656 \nL 157.239062 24.097656 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(165.239062 27.597656)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_53\">\n     <path d=\"M 137.239062 38.775781 \nL 157.239062 38.775781 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_54\"/>\n    <g id=\"text_19\">\n     <!-- RfModel-RandomSelection-250 -->\n     <g transform=\"translate(165.239062 42.275781)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1479.146484\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_55\">\n     <path d=\"M 137.239062 53.453906 \nL 157.239062 53.453906 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_56\"/>\n    <g id=\"text_20\">\n     <!-- RfModel-RandomSelection-125 -->\n     <g transform=\"translate(165.239062 56.953906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1479.146484\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_57\">\n     <path d=\"M 137.239062 68.132031 \nL 157.239062 68.132031 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_58\"/>\n    <g id=\"text_21\">\n     <!-- RfModel-RandomSelection-50 -->\n     <g transform=\"translate(165.239062 71.632031)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_59\">\n     <path d=\"M 137.239062 82.810156 \nL 157.239062 82.810156 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_60\"/>\n    <g id=\"text_22\">\n     <!-- RfModel-RandomSelection-25 -->\n     <g transform=\"translate(165.239062 86.310156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_61\">\n     <path d=\"M 137.239062 97.488281 \nL 157.239062 97.488281 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_62\"/>\n    <g id=\"text_23\">\n     <!-- RfModel-RandomSelection-10 -->\n     <g transform=\"translate(165.239062 100.988281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_63\">\n     <path d=\"M 137.239062 112.166406 \nL 157.239062 112.166406 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_64\"/>\n    <g id=\"text_24\">\n     <!-- RfModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(165.239062 115.666406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_65\">\n     <path d=\"M 137.239062 126.844531 \nL 157.239062 126.844531 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_66\"/>\n    <g id=\"text_25\">\n     <!-- RfModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(165.239062 130.344531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_67\">\n     <path d=\"M 137.239062 141.522656 \nL 157.239062 141.522656 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_68\"/>\n    <g id=\"text_26\">\n     <!-- RfModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(165.239062 145.022656)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_69\">\n     <path d=\"M 137.239062 156.200781 \nL 157.239062 156.200781 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_70\"/>\n    <g id=\"text_27\">\n     <!-- RfModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(165.239062 159.700781)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_71\">\n     <path d=\"M 137.239062 170.878906 \nL 157.239062 170.878906 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_72\"/>\n    <g id=\"text_28\">\n     <!-- RfModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(165.239062 174.378906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_73\">\n     <path d=\"M 137.239062 185.557031 \nL 157.239062 185.557031 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_74\"/>\n    <g id=\"text_29\">\n     <!-- RfModel-EntropySelection-250 -->\n     <g transform=\"translate(165.239062 189.057031)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1453.658203\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_75\">\n     <path d=\"M 137.239062 200.235156 \nL 157.239062 200.235156 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_76\"/>\n    <g id=\"text_30\">\n     <!-- RfModel-EntropySelection-125 -->\n     <g transform=\"translate(165.239062 203.735156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1453.658203\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_77\">\n     <path d=\"M 137.239062 214.913281 \nL 157.239062 214.913281 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_78\"/>\n    <g id=\"text_31\">\n     <!-- RfModel-EntropySelection-50 -->\n     <g transform=\"translate(165.239062 218.413281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_79\">\n     <path d=\"M 137.239062 229.591406 \nL 157.239062 229.591406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_80\"/>\n    <g id=\"text_32\">\n     <!-- RfModel-EntropySelection-25 -->\n     <g transform=\"translate(165.239062 233.091406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_81\">\n     <path d=\"M 137.239062 244.269531 \nL 157.239062 244.269531 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_82\"/>\n    <g id=\"text_33\">\n     <!-- RfModel-EntropySelection-10 -->\n     <g transform=\"translate(165.239062 247.769531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7c448048e0\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"273.928125pt\" version=\"1.1\" viewBox=\"0 0 384.83125 273.928125\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-17T14:00:06.516799</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 273.928125 \nL 384.83125 273.928125 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 250.05 \nL 368.0875 250.05 \nL 368.0875 32.61 \nL 33.2875 32.61 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 250.05 \nL 33.2875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m9e19453d2c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 264.648437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 70.4875 250.05 \nL 70.4875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 264.648437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 107.6875 250.05 \nL 107.6875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 264.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 144.8875 250.05 \nL 144.8875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 264.648437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 182.0875 250.05 \nL 182.0875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 264.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 219.2875 250.05 \nL 219.2875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 264.648437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 256.4875 250.05 \nL 256.4875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 264.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 293.6875 250.05 \nL 293.6875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 264.648437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 330.8875 250.05 \nL 330.8875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 264.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 368.0875 250.05 \nL 368.0875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#m9e19453d2c\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 264.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 250.05 \nL 368.0875 250.05 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m3f2cf44d72\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m3f2cf44d72\" y=\"250.05\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 253.849219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 213.81 \nL 368.0875 213.81 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m3f2cf44d72\" y=\"213.81\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 217.609219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 177.57 \nL 368.0875 177.57 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m3f2cf44d72\" y=\"177.57\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 181.369219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 141.33 \nL 368.0875 141.33 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m3f2cf44d72\" y=\"141.33\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 145.129219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 105.09 \nL 368.0875 105.09 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m3f2cf44d72\" y=\"105.09\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 108.889219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 68.85 \nL 368.0875 68.85 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m3f2cf44d72\" y=\"68.85\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 72.649219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 32.61 \nL 368.0875 32.61 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m3f2cf44d72\" y=\"32.61\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 36.409219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M -1 54.354 \nL 368.0875 54.354 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 182.0875 112.772212 \nL 368.0875 106.092028 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 89.0875 116.112304 \nL 182.0875 253.056083 \nL 275.0875 116.947327 \nL 368.0875 116.947327 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 162.038571 \nL 70.4875 166.213687 \nL 107.6875 158.698479 \nL 144.8875 136.152857 \nL 182.0875 116.112304 \nL 219.2875 115.277281 \nL 256.4875 111.937189 \nL 293.6875 107.762074 \nL 330.8875 106.927051 \nL 368.0875 106.092028 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 14.6875 121.122442 \nL 33.2875 121.122442 \nL 51.8875 119.452396 \nL 70.4875 124.462535 \nL 89.0875 126.132581 \nL 107.6875 123.627512 \nL 126.2875 119.452396 \nL 144.8875 117.78235 \nL 163.4875 115.277281 \nL 182.0875 120.287419 \nL 200.6875 111.937189 \nL 219.2875 106.927051 \nL 237.8875 110.267143 \nL 256.4875 111.937189 \nL 275.0875 107.762074 \nL 293.6875 109.43212 \nL 312.2875 106.927051 \nL 330.8875 104.421982 \nL 349.4875 106.092028 \nL 368.0875 105.257005 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 3.5275 162.873594 \nL 10.9675 136.98788 \nL 18.4075 136.98788 \nL 25.8475 140.327972 \nL 33.2875 131.977742 \nL 40.7275 134.482811 \nL 48.1675 123.627512 \nL 55.6075 124.462535 \nL 63.0475 115.277281 \nL 70.4875 117.78235 \nL 77.9275 120.287419 \nL 85.3675 114.442258 \nL 92.8075 110.267143 \nL 100.2475 113.607235 \nL 107.6875 108.597097 \nL 115.1275 111.102166 \nL 122.5675 109.43212 \nL 130.0075 113.607235 \nL 137.4475 111.102166 \nL 144.8875 115.277281 \nL 152.3275 116.112304 \nL 159.7675 111.102166 \nL 167.2075 110.267143 \nL 174.6475 111.937189 \nL 182.0875 112.772212 \nL 189.5275 110.267143 \nL 196.9675 126.967604 \nL 204.4075 126.132581 \nL 211.8475 106.092028 \nL 219.2875 103.586959 \nL 226.7275 101.081889 \nL 234.1675 122.792488 \nL 241.6075 108.597097 \nL 249.0475 106.927051 \nL 256.4875 106.092028 \nL 263.9275 107.762074 \nL 271.3675 108.597097 \nL 278.8075 111.937189 \nL 286.2475 106.927051 \nL 293.6875 111.102166 \nL 301.1275 111.102166 \nL 308.5675 111.937189 \nL 316.0075 111.937189 \nL 323.4475 112.772212 \nL 330.8875 111.102166 \nL 338.3275 108.597097 \nL 345.7675 111.102166 \nL 353.2075 106.927051 \nL 360.6475 106.927051 \nL 368.0875 109.43212 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 182.0875 122.792488 \nL 368.0875 113.607235 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 89.0875 136.98788 \nL 182.0875 140.327972 \nL 275.0875 136.152857 \nL 368.0875 133.647788 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 194.60447 \nL 70.4875 164.543641 \nL 107.6875 152.853318 \nL 144.8875 147.008157 \nL 182.0875 150.348249 \nL 219.2875 141.162995 \nL 256.4875 135.317834 \nL 293.6875 138.657926 \nL 330.8875 133.647788 \nL 368.0875 130.307696 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 14.6875 134.482811 \nL 33.2875 140.327972 \nL 51.8875 112.772212 \nL 70.4875 115.277281 \nL 89.0875 111.102166 \nL 107.6875 108.597097 \nL 126.2875 113.607235 \nL 144.8875 107.762074 \nL 163.4875 105.257005 \nL 182.0875 104.421982 \nL 200.6875 105.257005 \nL 219.2875 101.081889 \nL 237.8875 102.751935 \nL 256.4875 103.586959 \nL 275.0875 102.751935 \nL 293.6875 102.751935 \nL 312.2875 102.751935 \nL 330.8875 104.421982 \nL 349.4875 106.092028 \nL 368.0875 106.092028 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 3.5275 128.63765 \nL 10.9675 169.553779 \nL 18.4075 153.688341 \nL 25.8475 180.409078 \nL 33.2875 199.614608 \nL 40.7275 231.345484 \nL 48.1675 207.964839 \nL 55.6075 178.739032 \nL 63.0475 187.924286 \nL 70.4875 179.574055 \nL 77.9275 184.584194 \nL 85.3675 168.718756 \nL 92.8075 172.893871 \nL 100.2475 174.563917 \nL 107.6875 166.213687 \nL 115.1275 161.203548 \nL 122.5675 149.513226 \nL 130.0075 154.523364 \nL 137.4475 154.523364 \nL 144.8875 149.513226 \nL 152.3275 147.84318 \nL 159.7675 151.183272 \nL 167.2075 141.998018 \nL 174.6475 136.98788 \nL 182.0875 134.482811 \nL 189.5275 127.802627 \nL 196.9675 127.802627 \nL 204.4075 126.967604 \nL 211.8475 132.812765 \nL 219.2875 114.442258 \nL 226.7275 110.267143 \nL 234.1675 110.267143 \nL 241.6075 111.937189 \nL 249.0475 113.607235 \nL 256.4875 109.43212 \nL 263.9275 108.597097 \nL 271.3675 111.102166 \nL 278.8075 109.43212 \nL 286.2475 111.102166 \nL 293.6875 109.43212 \nL 301.1275 110.267143 \nL 308.5675 108.597097 \nL 316.0075 108.597097 \nL 323.4475 108.597097 \nL 330.8875 109.43212 \nL 338.3275 109.43212 \nL 345.7675 108.597097 \nL 353.2075 108.597097 \nL 360.6475 107.762074 \nL 368.0875 108.597097 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 182.0875 118.617373 \nL 368.0875 108.597097 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 89.0875 134.482811 \nL 182.0875 110.267143 \nL 275.0875 104.421982 \nL 368.0875 106.927051 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 33.2875 142.833041 \nL 70.4875 132.812765 \nL 107.6875 114.442258 \nL 144.8875 111.102166 \nL 182.0875 102.751935 \nL 219.2875 107.762074 \nL 256.4875 106.092028 \nL 293.6875 102.751935 \nL 330.8875 107.762074 \nL 368.0875 103.586959 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 14.6875 119.452396 \nL 33.2875 122.792488 \nL 51.8875 124.462535 \nL 70.4875 142.833041 \nL 89.0875 114.442258 \nL 107.6875 116.947327 \nL 126.2875 115.277281 \nL 144.8875 112.772212 \nL 163.4875 112.772212 \nL 182.0875 114.442258 \nL 200.6875 113.607235 \nL 219.2875 112.772212 \nL 237.8875 113.607235 \nL 256.4875 111.937189 \nL 275.0875 108.597097 \nL 293.6875 110.267143 \nL 312.2875 107.762074 \nL 330.8875 107.762074 \nL 349.4875 106.092028 \nL 368.0875 105.257005 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path clip-path=\"url(#p9f6beacd54)\" d=\"M 3.5275 167.04871 \nL 10.9675 157.028433 \nL 18.4075 160.368525 \nL 25.8475 152.018295 \nL 33.2875 152.018295 \nL 40.7275 143.668065 \nL 48.1675 124.462535 \nL 55.6075 123.627512 \nL 63.0475 118.617373 \nL 70.4875 135.317834 \nL 77.9275 136.152857 \nL 85.3675 132.812765 \nL 92.8075 120.287419 \nL 100.2475 118.617373 \nL 107.6875 113.607235 \nL 115.1275 117.78235 \nL 122.5675 112.772212 \nL 130.0075 111.937189 \nL 137.4475 113.607235 \nL 144.8875 112.772212 \nL 152.3275 113.607235 \nL 159.7675 111.937189 \nL 167.2075 110.267143 \nL 174.6475 111.937189 \nL 182.0875 111.937189 \nL 189.5275 108.597097 \nL 196.9675 111.102166 \nL 204.4075 111.937189 \nL 211.8475 111.102166 \nL 219.2875 111.937189 \nL 226.7275 110.267143 \nL 234.1675 111.102166 \nL 241.6075 109.43212 \nL 249.0475 109.43212 \nL 256.4875 109.43212 \nL 263.9275 108.597097 \nL 271.3675 109.43212 \nL 278.8075 106.927051 \nL 286.2475 108.597097 \nL 293.6875 110.267143 \nL 301.1275 108.597097 \nL 308.5675 108.597097 \nL 316.0075 108.597097 \nL 323.4475 108.597097 \nL 330.8875 108.597097 \nL 338.3275 107.762074 \nL 345.7675 109.43212 \nL 353.2075 107.762074 \nL 360.6475 108.597097 \nL 368.0875 108.597097 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 250.05 \nL 33.2875 32.61 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 250.05 \nL 368.0875 32.61 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 250.05 \nL 368.0875 250.05 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 32.61 \nL 368.0875 32.61 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 123.7 245.05 \nL 361.0875 245.05 \nQ 363.0875 245.05 363.0875 243.05 \nL 363.0875 9.2 \nQ 363.0875 7.2 361.0875 7.2 \nL 123.7 7.2 \nQ 121.7 7.2 121.7 9.2 \nL 121.7 243.05 \nQ 121.7 245.05 123.7 245.05 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 125.7 15.298438 \nL 145.7 15.298438 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(153.7 18.798438)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_53\">\n     <path d=\"M 125.7 29.976563 \nL 145.7 29.976563 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_54\"/>\n    <g id=\"text_19\">\n     <!-- SvmModel-RandomSelection-250 -->\n     <g transform=\"translate(153.7 33.476563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1594.527344\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_55\">\n     <path d=\"M 125.7 44.654688 \nL 145.7 44.654688 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_56\"/>\n    <g id=\"text_20\">\n     <!-- SvmModel-RandomSelection-125 -->\n     <g transform=\"translate(153.7 48.154688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1594.527344\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_57\">\n     <path d=\"M 125.7 59.332813 \nL 145.7 59.332813 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_58\"/>\n    <g id=\"text_21\">\n     <!-- SvmModel-RandomSelection-50 -->\n     <g transform=\"translate(153.7 62.832813)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_59\">\n     <path d=\"M 125.7 74.010938 \nL 145.7 74.010938 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_60\"/>\n    <g id=\"text_22\">\n     <!-- SvmModel-RandomSelection-25 -->\n     <g transform=\"translate(153.7 77.510938)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_61\">\n     <path d=\"M 125.7 88.689063 \nL 145.7 88.689063 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_62\"/>\n    <g id=\"text_23\">\n     <!-- SvmModel-RandomSelection-10 -->\n     <g transform=\"translate(153.7 92.189063)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_63\">\n     <path d=\"M 125.7 103.367188 \nL 145.7 103.367188 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_64\"/>\n    <g id=\"text_24\">\n     <!-- SvmModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(153.7 106.867188)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1990.193359\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_65\">\n     <path d=\"M 125.7 118.045313 \nL 145.7 118.045313 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_66\"/>\n    <g id=\"text_25\">\n     <!-- SvmModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(153.7 121.545313)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1990.193359\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_67\">\n     <path d=\"M 125.7 132.723437 \nL 145.7 132.723437 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_68\"/>\n    <g id=\"text_26\">\n     <!-- SvmModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(153.7 136.223437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_69\">\n     <path d=\"M 125.7 147.401562 \nL 145.7 147.401562 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_70\"/>\n    <g id=\"text_27\">\n     <!-- SvmModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(153.7 150.901562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_71\">\n     <path d=\"M 125.7 162.079687 \nL 145.7 162.079687 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_72\"/>\n    <g id=\"text_28\">\n     <!-- SvmModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(153.7 165.579687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_73\">\n     <path d=\"M 125.7 176.757812 \nL 145.7 176.757812 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_74\"/>\n    <g id=\"text_29\">\n     <!-- SvmModel-EntropySelection-250 -->\n     <g transform=\"translate(153.7 180.257812)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1569.039062\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_75\">\n     <path d=\"M 125.7 191.435937 \nL 145.7 191.435937 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_76\"/>\n    <g id=\"text_30\">\n     <!-- SvmModel-EntropySelection-125 -->\n     <g transform=\"translate(153.7 194.935937)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1569.039062\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_77\">\n     <path d=\"M 125.7 206.114062 \nL 145.7 206.114062 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_78\"/>\n    <g id=\"text_31\">\n     <!-- SvmModel-EntropySelection-50 -->\n     <g transform=\"translate(153.7 209.614062)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_79\">\n     <path d=\"M 125.7 220.792187 \nL 145.7 220.792187 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_80\"/>\n    <g id=\"text_32\">\n     <!-- SvmModel-EntropySelection-25 -->\n     <g transform=\"translate(153.7 224.292187)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_81\">\n     <path d=\"M 125.7 235.470312 \nL 145.7 235.470312 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_82\"/>\n    <g id=\"text_33\">\n     <!-- SvmModel-EntropySelection-10 -->\n     <g transform=\"translate(153.7 238.970312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9f6beacd54\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"32.61\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"261.049219pt\" version=\"1.1\" viewBox=\"0 0 384.83125 261.049219\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-17T14:00:07.303131</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 261.049219 \nL 384.83125 261.049219 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \nL 368.0875 10.999219 \nL 33.2875 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m89f2726cb3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 70.4875 228.439219 \nL 70.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 107.6875 228.439219 \nL 107.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 144.8875 228.439219 \nL 144.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 182.0875 228.439219 \nL 182.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 219.2875 228.439219 \nL 219.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 256.4875 228.439219 \nL 256.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 293.6875 228.439219 \nL 293.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 330.8875 228.439219 \nL 330.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#m89f2726cb3\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m2c2738d378\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2c2738d378\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 232.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 192.199219 \nL 368.0875 192.199219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2c2738d378\" y=\"192.199219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 195.998438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 155.959219 \nL 368.0875 155.959219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2c2738d378\" y=\"155.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 159.758438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 119.719219 \nL 368.0875 119.719219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2c2738d378\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 123.518438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 83.479219 \nL 368.0875 83.479219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2c2738d378\" y=\"83.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 87.278438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 47.239219 \nL 368.0875 47.239219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2c2738d378\" y=\"47.239219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 51.038438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2c2738d378\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M -1 38.287939 \nL 368.0875 38.287939 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 182.0875 105.356822 \nL 368.0875 99.511661 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 89.0875 94.501523 \nL 182.0875 95.336546 \nL 275.0875 99.511661 \nL 368.0875 97.006592 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 109.531938 \nL 70.4875 108.696915 \nL 107.6875 97.006592 \nL 144.8875 94.501523 \nL 182.0875 93.6665 \nL 219.2875 113.707053 \nL 256.4875 90.326408 \nL 293.6875 92.831477 \nL 330.8875 84.481246 \nL 368.0875 90.326408 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 14.6875 104.521799 \nL 33.2875 100.346684 \nL 51.8875 102.01673 \nL 70.4875 96.171569 \nL 89.0875 91.161431 \nL 107.6875 85.316269 \nL 126.2875 91.161431 \nL 144.8875 93.6665 \nL 163.4875 94.501523 \nL 182.0875 93.6665 \nL 200.6875 95.336546 \nL 219.2875 95.336546 \nL 237.8875 97.841615 \nL 256.4875 97.006592 \nL 275.0875 97.841615 \nL 293.6875 97.841615 \nL 312.2875 99.511661 \nL 330.8875 97.841615 \nL 349.4875 98.676638 \nL 368.0875 94.501523 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 3.5275 107.026869 \nL 10.9675 102.01673 \nL 18.4075 93.6665 \nL 25.8475 91.996454 \nL 33.2875 91.996454 \nL 40.7275 91.161431 \nL 48.1675 91.996454 \nL 55.6075 90.326408 \nL 63.0475 89.491385 \nL 70.4875 95.336546 \nL 77.9275 98.676638 \nL 85.3675 90.326408 \nL 92.8075 91.996454 \nL 100.2475 98.676638 \nL 107.6875 97.006592 \nL 115.1275 93.6665 \nL 122.5675 91.996454 \nL 130.0075 95.336546 \nL 137.4475 96.171569 \nL 144.8875 94.501523 \nL 152.3275 99.511661 \nL 159.7675 94.501523 \nL 167.2075 97.006592 \nL 174.6475 96.171569 \nL 182.0875 97.841615 \nL 189.5275 98.676638 \nL 196.9675 99.511661 \nL 204.4075 97.006592 \nL 211.8475 98.676638 \nL 219.2875 99.511661 \nL 226.7275 96.171569 \nL 234.1675 91.996454 \nL 241.6075 92.831477 \nL 249.0475 91.161431 \nL 256.4875 95.336546 \nL 263.9275 97.006592 \nL 271.3675 97.006592 \nL 278.8075 97.006592 \nL 286.2475 97.841615 \nL 293.6875 91.996454 \nL 301.1275 94.501523 \nL 308.5675 96.171569 \nL 316.0075 94.501523 \nL 323.4475 97.006592 \nL 330.8875 95.336546 \nL 338.3275 93.6665 \nL 345.7675 95.336546 \nL 353.2075 97.841615 \nL 360.6475 96.171569 \nL 368.0875 92.831477 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 182.0875 91.161431 \nL 368.0875 78.636085 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 89.0875 103.686776 \nL 182.0875 85.316269 \nL 275.0875 85.316269 \nL 368.0875 86.986316 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 100.346684 \nL 70.4875 90.326408 \nL 107.6875 83.646223 \nL 144.8875 87.821339 \nL 182.0875 89.491385 \nL 219.2875 88.656362 \nL 256.4875 87.821339 \nL 293.6875 77.801062 \nL 330.8875 76.131016 \nL 368.0875 77.801062 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 14.6875 95.336546 \nL 33.2875 88.656362 \nL 51.8875 85.316269 \nL 70.4875 86.986316 \nL 89.0875 82.8112 \nL 107.6875 86.986316 \nL 126.2875 86.986316 \nL 144.8875 86.151292 \nL 163.4875 82.8112 \nL 182.0875 84.481246 \nL 200.6875 88.656362 \nL 219.2875 81.976177 \nL 237.8875 81.976177 \nL 256.4875 81.141154 \nL 275.0875 82.8112 \nL 293.6875 80.306131 \nL 312.2875 79.471108 \nL 330.8875 78.636085 \nL 349.4875 73.625947 \nL 368.0875 77.801062 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 3.5275 111.201984 \nL 10.9675 107.861892 \nL 18.4075 96.171569 \nL 25.8475 102.851753 \nL 33.2875 94.501523 \nL 40.7275 125.397375 \nL 48.1675 102.01673 \nL 55.6075 102.01673 \nL 63.0475 117.882168 \nL 70.4875 97.841615 \nL 77.9275 95.336546 \nL 85.3675 117.047145 \nL 92.8075 117.047145 \nL 100.2475 117.047145 \nL 107.6875 94.501523 \nL 115.1275 89.491385 \nL 122.5675 86.151292 \nL 130.0075 82.8112 \nL 137.4475 83.646223 \nL 144.8875 85.316269 \nL 152.3275 81.976177 \nL 159.7675 78.636085 \nL 167.2075 79.471108 \nL 174.6475 102.851753 \nL 182.0875 81.141154 \nL 189.5275 90.326408 \nL 196.9675 90.326408 \nL 204.4075 81.976177 \nL 211.8475 83.646223 \nL 219.2875 83.646223 \nL 226.7275 87.821339 \nL 234.1675 87.821339 \nL 241.6075 85.316269 \nL 249.0475 79.471108 \nL 256.4875 83.646223 \nL 263.9275 96.171569 \nL 271.3675 86.986316 \nL 278.8075 82.8112 \nL 286.2475 86.986316 \nL 293.6875 86.151292 \nL 301.1275 90.326408 \nL 308.5675 86.151292 \nL 316.0075 91.996454 \nL 323.4475 88.656362 \nL 330.8875 87.821339 \nL 338.3275 93.6665 \nL 345.7675 92.831477 \nL 353.2075 90.326408 \nL 360.6475 88.656362 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 182.0875 94.501523 \nL 368.0875 92.831477 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 89.0875 86.151292 \nL 182.0875 81.976177 \nL 275.0875 81.976177 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 33.2875 108.696915 \nL 70.4875 94.501523 \nL 107.6875 89.491385 \nL 144.8875 97.841615 \nL 182.0875 114.542076 \nL 219.2875 89.491385 \nL 256.4875 93.6665 \nL 293.6875 88.656362 \nL 330.8875 90.326408 \nL 368.0875 86.986316 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 14.6875 117.047145 \nL 33.2875 109.531938 \nL 51.8875 104.521799 \nL 70.4875 91.161431 \nL 89.0875 88.656362 \nL 107.6875 93.6665 \nL 126.2875 89.491385 \nL 144.8875 91.996454 \nL 163.4875 91.996454 \nL 182.0875 91.996454 \nL 200.6875 86.986316 \nL 219.2875 88.656362 \nL 237.8875 105.356822 \nL 256.4875 88.656362 \nL 275.0875 91.996454 \nL 293.6875 88.656362 \nL 312.2875 85.316269 \nL 330.8875 83.646223 \nL 349.4875 82.8112 \nL 368.0875 100.346684 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path clip-path=\"url(#p4d92d6717a)\" d=\"M 3.5275 156.293228 \nL 10.9675 163.808435 \nL 18.4075 112.037007 \nL 25.8475 93.6665 \nL 33.2875 88.656362 \nL 40.7275 96.171569 \nL 48.1675 94.501523 \nL 55.6075 102.851753 \nL 63.0475 96.171569 \nL 70.4875 105.356822 \nL 77.9275 101.181707 \nL 85.3675 102.01673 \nL 92.8075 91.161431 \nL 100.2475 87.821339 \nL 107.6875 87.821339 \nL 115.1275 102.851753 \nL 122.5675 86.986316 \nL 130.0075 92.831477 \nL 137.4475 99.511661 \nL 144.8875 97.006592 \nL 152.3275 99.511661 \nL 159.7675 98.676638 \nL 167.2075 97.006592 \nL 174.6475 88.656362 \nL 182.0875 91.161431 \nL 189.5275 91.996454 \nL 196.9675 92.831477 \nL 204.4075 91.161431 \nL 211.8475 93.6665 \nL 219.2875 89.491385 \nL 226.7275 91.161431 \nL 234.1675 92.831477 \nL 241.6075 91.996454 \nL 249.0475 90.326408 \nL 256.4875 95.336546 \nL 263.9275 85.316269 \nL 271.3675 86.151292 \nL 278.8075 86.986316 \nL 286.2475 80.306131 \nL 293.6875 78.636085 \nL 301.1275 78.636085 \nL 308.5675 77.801062 \nL 316.0075 79.471108 \nL 323.4475 81.141154 \nL 330.8875 80.306131 \nL 338.3275 81.141154 \nL 345.7675 78.636085 \nL 353.2075 80.306131 \nL 360.6475 81.141154 \nL 368.0875 80.306131 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 127.84375 253.849219 \nL 361.0875 253.849219 \nQ 363.0875 253.849219 363.0875 251.849219 \nL 363.0875 17.999219 \nQ 363.0875 15.999219 361.0875 15.999219 \nL 127.84375 15.999219 \nQ 125.84375 15.999219 125.84375 17.999219 \nL 125.84375 251.849219 \nQ 125.84375 253.849219 127.84375 253.849219 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 129.84375 24.097656 \nL 149.84375 24.097656 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(157.84375 27.597656)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_53\">\n     <path d=\"M 129.84375 38.775781 \nL 149.84375 38.775781 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_54\"/>\n    <g id=\"text_19\">\n     <!-- LogModel-RandomSelection-250 -->\n     <g transform=\"translate(157.84375 42.275781)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1553.080078\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_55\">\n     <path d=\"M 129.84375 53.453906 \nL 149.84375 53.453906 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_56\"/>\n    <g id=\"text_20\">\n     <!-- LogModel-RandomSelection-125 -->\n     <g transform=\"translate(157.84375 56.953906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1553.080078\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_57\">\n     <path d=\"M 129.84375 68.132031 \nL 149.84375 68.132031 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_58\"/>\n    <g id=\"text_21\">\n     <!-- LogModel-RandomSelection-50 -->\n     <g transform=\"translate(157.84375 71.632031)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_59\">\n     <path d=\"M 129.84375 82.810156 \nL 149.84375 82.810156 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_60\"/>\n    <g id=\"text_22\">\n     <!-- LogModel-RandomSelection-25 -->\n     <g transform=\"translate(157.84375 86.310156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_61\">\n     <path d=\"M 129.84375 97.488281 \nL 149.84375 97.488281 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_62\"/>\n    <g id=\"text_23\">\n     <!-- LogModel-RandomSelection-10 -->\n     <g transform=\"translate(157.84375 100.988281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_63\">\n     <path d=\"M 129.84375 112.166406 \nL 149.84375 112.166406 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_64\"/>\n    <g id=\"text_24\">\n     <!-- LogModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(157.84375 115.666406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1948.746094\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_65\">\n     <path d=\"M 129.84375 126.844531 \nL 149.84375 126.844531 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_66\"/>\n    <g id=\"text_25\">\n     <!-- LogModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(157.84375 130.344531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1948.746094\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_67\">\n     <path d=\"M 129.84375 141.522656 \nL 149.84375 141.522656 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_68\"/>\n    <g id=\"text_26\">\n     <!-- LogModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(157.84375 145.022656)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_69\">\n     <path d=\"M 129.84375 156.200781 \nL 149.84375 156.200781 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_70\"/>\n    <g id=\"text_27\">\n     <!-- LogModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(157.84375 159.700781)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_71\">\n     <path d=\"M 129.84375 170.878906 \nL 149.84375 170.878906 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_72\"/>\n    <g id=\"text_28\">\n     <!-- LogModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(157.84375 174.378906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_73\">\n     <path d=\"M 129.84375 185.557031 \nL 149.84375 185.557031 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_74\"/>\n    <g id=\"text_29\">\n     <!-- LogModel-EntropySelection-250 -->\n     <g transform=\"translate(157.84375 189.057031)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1527.591797\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_75\">\n     <path d=\"M 129.84375 200.235156 \nL 149.84375 200.235156 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_76\"/>\n    <g id=\"text_30\">\n     <!-- LogModel-EntropySelection-125 -->\n     <g transform=\"translate(157.84375 203.735156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1527.591797\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_77\">\n     <path d=\"M 129.84375 214.913281 \nL 149.84375 214.913281 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_78\"/>\n    <g id=\"text_31\">\n     <!-- LogModel-EntropySelection-50 -->\n     <g transform=\"translate(157.84375 218.413281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_79\">\n     <path d=\"M 129.84375 229.591406 \nL 149.84375 229.591406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_80\"/>\n    <g id=\"text_32\">\n     <!-- LogModel-EntropySelection-25 -->\n     <g transform=\"translate(157.84375 233.091406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_81\">\n     <path d=\"M 129.84375 244.269531 \nL 149.84375 244.269531 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_82\"/>\n    <g id=\"text_33\">\n     <!-- LogModel-EntropySelection-10 -->\n     <g transform=\"translate(157.84375 247.769531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4d92d6717a\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def performance_plot(fully_supervised_accuracy, dic, models, selection_functions, Ks, repeats):  \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot([0,500],[fully_supervised_accuracy, fully_supervised_accuracy],label = 'algorithm-upper-bound')\n",
        "    for model_object in models:\n",
        "      for selection_function in selection_functions:\n",
        "        for idx, k in enumerate(Ks):\n",
        "            x = np.arange(float(Ks[idx]), 500 + float(Ks[idx]), float(Ks[idx]))            \n",
        "            Sum = np.array(dic[model_object][selection_function][k][0])\n",
        "            for i in range(1, repeats):\n",
        "                Sum = Sum + np.array(dic[model_object][selection_function][k][i])\n",
        "            mean = Sum / repeats\n",
        "            ax.plot(x, mean ,label = model_object + '-' + selection_function + '-' + str(k))\n",
        "    ax.legend()\n",
        "    ax.set_xlim([50,500])\n",
        "    ax.set_ylim([40,100])\n",
        "    ax.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "models_str = ['SvmModel', 'RfModel', 'LogModel']\n",
        "selection_functions_str = ['RandomSelection', 'MarginSamplingSelection', 'EntropySelection']\n",
        "Ks_str = ['250','125','50','25','10'] \n",
        "repeats = 1\n",
        "random_forest_upper_bound = 97.\n",
        "svm_upper_bound = 94.\n",
        "log_upper_bound = 92.47\n",
        "total_experiments = len(models_str) * len(selection_functions_str) * len(Ks_str) * repeats\n",
        "\n",
        "print('So which is the better model? under the stopping condition and hyper parameters - random forest is the winner!')\n",
        "performance_plot(random_forest_upper_bound, d, ['RfModel'] , selection_functions_str    , Ks_str, 1)\n",
        "performance_plot(svm_upper_bound, d, ['SvmModel'] , selection_functions_str    , Ks_str, 1)\n",
        "performance_plot(log_upper_bound, d, ['LogModel'] , selection_functions_str    , Ks_str, 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So which is the best sample selection function? margin sampling is the winner!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"261.049219pt\" version=\"1.1\" viewBox=\"0 0 384.83125 261.049219\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-17T14:05:18.041621</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 261.049219 \nL 384.83125 261.049219 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \nL 368.0875 10.999219 \nL 33.2875 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m7686426fa6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 70.4875 228.439219 \nL 70.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 107.6875 228.439219 \nL 107.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 144.8875 228.439219 \nL 144.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 182.0875 228.439219 \nL 182.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 219.2875 228.439219 \nL 219.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 256.4875 228.439219 \nL 256.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 293.6875 228.439219 \nL 293.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 330.8875 228.439219 \nL 330.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#m7686426fa6\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m0c7eb681a4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0c7eb681a4\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 232.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 192.199219 \nL 368.0875 192.199219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0c7eb681a4\" y=\"192.199219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 195.998438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 155.959219 \nL 368.0875 155.959219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0c7eb681a4\" y=\"155.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 159.758438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 119.719219 \nL 368.0875 119.719219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0c7eb681a4\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 123.518438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 83.479219 \nL 368.0875 83.479219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0c7eb681a4\" y=\"83.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 87.278438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 47.239219 \nL 368.0875 47.239219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0c7eb681a4\" y=\"47.239219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 51.038438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m0c7eb681a4\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M -1 21.871219 \nL 368.0875 21.871219 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 182.0875 89.491385 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 89.0875 99.511661 \nL 182.0875 92.831477 \nL 275.0875 91.996454 \nL 368.0875 92.831477 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 87.821339 \nL 70.4875 86.986316 \nL 107.6875 84.481246 \nL 144.8875 85.316269 \nL 182.0875 81.141154 \nL 219.2875 85.316269 \nL 256.4875 86.986316 \nL 293.6875 86.986316 \nL 330.8875 83.646223 \nL 368.0875 86.151292 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 14.6875 184.684011 \nL 33.2875 131.242537 \nL 51.8875 121.22226 \nL 70.4875 100.346684 \nL 89.0875 97.841615 \nL 107.6875 97.006592 \nL 126.2875 94.501523 \nL 144.8875 91.996454 \nL 163.4875 87.821339 \nL 182.0875 86.151292 \nL 200.6875 83.646223 \nL 219.2875 82.8112 \nL 237.8875 84.481246 \nL 256.4875 83.646223 \nL 275.0875 83.646223 \nL 293.6875 86.151292 \nL 312.2875 83.646223 \nL 330.8875 81.976177 \nL 349.4875 84.481246 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 3.5275 180.508896 \nL 10.9675 152.118113 \nL 18.4075 129.572491 \nL 25.8475 96.171569 \nL 33.2875 104.521799 \nL 40.7275 107.861892 \nL 48.1675 91.996454 \nL 55.6075 89.491385 \nL 63.0475 83.646223 \nL 70.4875 85.316269 \nL 77.9275 86.151292 \nL 85.3675 91.161431 \nL 92.8075 93.6665 \nL 100.2475 93.6665 \nL 107.6875 90.326408 \nL 115.1275 85.316269 \nL 122.5675 89.491385 \nL 130.0075 84.481246 \nL 137.4475 86.986316 \nL 144.8875 84.481246 \nL 152.3275 86.986316 \nL 159.7675 84.481246 \nL 167.2075 85.316269 \nL 174.6475 81.976177 \nL 182.0875 88.656362 \nL 189.5275 85.316269 \nL 196.9675 82.8112 \nL 204.4075 85.316269 \nL 211.8475 81.141154 \nL 219.2875 83.646223 \nL 226.7275 84.481246 \nL 234.1675 84.481246 \nL 241.6075 81.141154 \nL 249.0475 82.8112 \nL 256.4875 82.8112 \nL 263.9275 81.141154 \nL 271.3675 81.141154 \nL 278.8075 84.481246 \nL 286.2475 81.141154 \nL 293.6875 81.976177 \nL 301.1275 82.8112 \nL 308.5675 78.636085 \nL 316.0075 80.306131 \nL 323.4475 86.986316 \nL 330.8875 83.646223 \nL 338.3275 84.481246 \nL 345.7675 84.481246 \nL 353.2075 86.986316 \nL 360.6475 88.656362 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 182.0875 81.141154 \nL 368.0875 80.306131 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 89.0875 120.387237 \nL 182.0875 97.841615 \nL 275.0875 89.491385 \nL 368.0875 81.141154 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 94.501523 \nL 70.4875 80.306131 \nL 107.6875 83.646223 \nL 144.8875 86.986316 \nL 182.0875 91.996454 \nL 219.2875 93.6665 \nL 256.4875 84.481246 \nL 293.6875 84.481246 \nL 330.8875 86.986316 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 14.6875 185.519034 \nL 33.2875 108.696915 \nL 51.8875 108.696915 \nL 70.4875 116.212122 \nL 89.0875 117.047145 \nL 107.6875 91.161431 \nL 126.2875 107.026869 \nL 144.8875 109.531938 \nL 163.4875 80.306131 \nL 182.0875 84.481246 \nL 200.6875 82.8112 \nL 219.2875 85.316269 \nL 237.8875 81.141154 \nL 256.4875 81.141154 \nL 275.0875 79.471108 \nL 293.6875 80.306131 \nL 312.2875 84.481246 \nL 330.8875 79.471108 \nL 349.4875 78.636085 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 3.5275 103.686776 \nL 10.9675 112.87203 \nL 18.4075 106.191845 \nL 25.8475 91.161431 \nL 33.2875 89.491385 \nL 40.7275 84.481246 \nL 48.1675 83.646223 \nL 55.6075 107.861892 \nL 63.0475 91.161431 \nL 70.4875 91.161431 \nL 77.9275 88.656362 \nL 85.3675 87.821339 \nL 92.8075 108.696915 \nL 100.2475 88.656362 \nL 107.6875 83.646223 \nL 115.1275 86.986316 \nL 122.5675 85.316269 \nL 130.0075 86.986316 \nL 137.4475 109.531938 \nL 144.8875 107.861892 \nL 152.3275 86.986316 \nL 159.7675 91.161431 \nL 167.2075 87.821339 \nL 174.6475 82.8112 \nL 182.0875 90.326408 \nL 189.5275 84.481246 \nL 196.9675 85.316269 \nL 204.4075 81.141154 \nL 211.8475 81.976177 \nL 219.2875 82.8112 \nL 226.7275 82.8112 \nL 234.1675 81.141154 \nL 241.6075 82.8112 \nL 249.0475 83.646223 \nL 256.4875 89.491385 \nL 263.9275 81.141154 \nL 271.3675 82.8112 \nL 278.8075 91.161431 \nL 286.2475 85.316269 \nL 293.6875 86.151292 \nL 301.1275 86.151292 \nL 308.5675 85.316269 \nL 316.0075 89.491385 \nL 323.4475 86.151292 \nL 330.8875 89.491385 \nL 338.3275 83.646223 \nL 345.7675 81.141154 \nL 353.2075 81.976177 \nL 360.6475 85.316269 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 182.0875 98.676638 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 89.0875 102.01673 \nL 182.0875 81.976177 \nL 275.0875 89.491385 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 33.2875 105.356822 \nL 70.4875 78.636085 \nL 107.6875 84.481246 \nL 144.8875 86.151292 \nL 182.0875 85.316269 \nL 219.2875 81.976177 \nL 256.4875 89.491385 \nL 293.6875 87.821339 \nL 330.8875 81.141154 \nL 368.0875 81.141154 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 14.6875 146.272951 \nL 33.2875 132.912583 \nL 51.8875 109.531938 \nL 70.4875 84.481246 \nL 89.0875 97.006592 \nL 107.6875 93.6665 \nL 126.2875 96.171569 \nL 144.8875 83.646223 \nL 163.4875 91.996454 \nL 182.0875 88.656362 \nL 200.6875 90.326408 \nL 219.2875 86.986316 \nL 237.8875 87.821339 \nL 256.4875 81.141154 \nL 275.0875 82.8112 \nL 293.6875 89.491385 \nL 312.2875 85.316269 \nL 330.8875 86.151292 \nL 349.4875 84.481246 \nL 368.0875 81.976177 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path clip-path=\"url(#p5eff31d08f)\" d=\"M 3.5275 126.232398 \nL 10.9675 157.128251 \nL 18.4075 119.552214 \nL 25.8475 96.171569 \nL 33.2875 96.171569 \nL 40.7275 102.01673 \nL 48.1675 107.861892 \nL 55.6075 105.356822 \nL 63.0475 91.161431 \nL 70.4875 84.481246 \nL 77.9275 84.481246 \nL 85.3675 81.141154 \nL 92.8075 86.986316 \nL 100.2475 83.646223 \nL 107.6875 84.481246 \nL 115.1275 83.646223 \nL 122.5675 85.316269 \nL 130.0075 81.976177 \nL 137.4475 87.821339 \nL 144.8875 79.471108 \nL 152.3275 82.8112 \nL 159.7675 79.471108 \nL 167.2075 79.471108 \nL 174.6475 81.976177 \nL 182.0875 79.471108 \nL 189.5275 81.976177 \nL 196.9675 81.976177 \nL 204.4075 80.306131 \nL 211.8475 81.141154 \nL 219.2875 83.646223 \nL 226.7275 86.151292 \nL 234.1675 88.656362 \nL 241.6075 88.656362 \nL 249.0475 90.326408 \nL 256.4875 90.326408 \nL 263.9275 86.151292 \nL 271.3675 86.986316 \nL 278.8075 84.481246 \nL 286.2475 83.646223 \nL 293.6875 83.646223 \nL 301.1275 83.646223 \nL 308.5675 86.151292 \nL 316.0075 82.8112 \nL 323.4475 81.141154 \nL 330.8875 81.141154 \nL 338.3275 80.306131 \nL 345.7675 81.141154 \nL 353.2075 80.306131 \nL 360.6475 81.141154 \nL 368.0875 79.471108 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 135.239062 253.849219 \nL 361.0875 253.849219 \nQ 363.0875 253.849219 363.0875 251.849219 \nL 363.0875 17.999219 \nQ 363.0875 15.999219 361.0875 15.999219 \nL 135.239062 15.999219 \nQ 133.239062 15.999219 133.239062 17.999219 \nL 133.239062 251.849219 \nQ 133.239062 253.849219 135.239062 253.849219 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 137.239062 24.097656 \nL 157.239062 24.097656 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(165.239062 27.597656)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_53\">\n     <path d=\"M 137.239062 38.775781 \nL 157.239062 38.775781 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_54\"/>\n    <g id=\"text_19\">\n     <!-- RfModel-RandomSelection-250 -->\n     <g transform=\"translate(165.239062 42.275781)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1479.146484\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_55\">\n     <path d=\"M 137.239062 53.453906 \nL 157.239062 53.453906 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_56\"/>\n    <g id=\"text_20\">\n     <!-- RfModel-RandomSelection-125 -->\n     <g transform=\"translate(165.239062 56.953906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1479.146484\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_57\">\n     <path d=\"M 137.239062 68.132031 \nL 157.239062 68.132031 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_58\"/>\n    <g id=\"text_21\">\n     <!-- RfModel-RandomSelection-50 -->\n     <g transform=\"translate(165.239062 71.632031)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_59\">\n     <path d=\"M 137.239062 82.810156 \nL 157.239062 82.810156 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_60\"/>\n    <g id=\"text_22\">\n     <!-- RfModel-RandomSelection-25 -->\n     <g transform=\"translate(165.239062 86.310156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_61\">\n     <path d=\"M 137.239062 97.488281 \nL 157.239062 97.488281 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_62\"/>\n    <g id=\"text_23\">\n     <!-- RfModel-RandomSelection-10 -->\n     <g transform=\"translate(165.239062 100.988281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_63\">\n     <path d=\"M 137.239062 112.166406 \nL 157.239062 112.166406 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_64\"/>\n    <g id=\"text_24\">\n     <!-- RfModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(165.239062 115.666406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_65\">\n     <path d=\"M 137.239062 126.844531 \nL 157.239062 126.844531 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_66\"/>\n    <g id=\"text_25\">\n     <!-- RfModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(165.239062 130.344531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_67\">\n     <path d=\"M 137.239062 141.522656 \nL 157.239062 141.522656 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_68\"/>\n    <g id=\"text_26\">\n     <!-- RfModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(165.239062 145.022656)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_69\">\n     <path d=\"M 137.239062 156.200781 \nL 157.239062 156.200781 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_70\"/>\n    <g id=\"text_27\">\n     <!-- RfModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(165.239062 159.700781)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_71\">\n     <path d=\"M 137.239062 170.878906 \nL 157.239062 170.878906 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_72\"/>\n    <g id=\"text_28\">\n     <!-- RfModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(165.239062 174.378906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_73\">\n     <path d=\"M 137.239062 185.557031 \nL 157.239062 185.557031 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_74\"/>\n    <g id=\"text_29\">\n     <!-- RfModel-EntropySelection-250 -->\n     <g transform=\"translate(165.239062 189.057031)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1453.658203\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_75\">\n     <path d=\"M 137.239062 200.235156 \nL 157.239062 200.235156 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_76\"/>\n    <g id=\"text_30\">\n     <!-- RfModel-EntropySelection-125 -->\n     <g transform=\"translate(165.239062 203.735156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1453.658203\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_77\">\n     <path d=\"M 137.239062 214.913281 \nL 157.239062 214.913281 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_78\"/>\n    <g id=\"text_31\">\n     <!-- RfModel-EntropySelection-50 -->\n     <g transform=\"translate(165.239062 218.413281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_79\">\n     <path d=\"M 137.239062 229.591406 \nL 157.239062 229.591406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_80\"/>\n    <g id=\"text_32\">\n     <!-- RfModel-EntropySelection-25 -->\n     <g transform=\"translate(165.239062 233.091406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_81\">\n     <path d=\"M 137.239062 244.269531 \nL 157.239062 244.269531 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_82\"/>\n    <g id=\"text_33\">\n     <!-- RfModel-EntropySelection-10 -->\n     <g transform=\"translate(165.239062 247.769531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5eff31d08f\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nSo which is the best k? k=10 is the winner\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"252.317344pt\" version=\"1.1\" viewBox=\"0 0 384.83125 252.317344\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-17T14:05:18.577451</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 252.317344 \nL 384.83125 252.317344 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \nL 368.0875 10.999219 \nL 33.2875 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m2cbae95960\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 70.4875 228.439219 \nL 70.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 107.6875 228.439219 \nL 107.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 144.8875 228.439219 \nL 144.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 182.0875 228.439219 \nL 182.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 219.2875 228.439219 \nL 219.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 256.4875 228.439219 \nL 256.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 293.6875 228.439219 \nL 293.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 330.8875 228.439219 \nL 330.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#m2cbae95960\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"ma52ae30ce4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma52ae30ce4\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 232.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 33.2875 192.199219 \nL 368.0875 192.199219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma52ae30ce4\" y=\"192.199219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 195.998438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 33.2875 155.959219 \nL 368.0875 155.959219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma52ae30ce4\" y=\"155.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 159.758438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 33.2875 119.719219 \nL 368.0875 119.719219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma52ae30ce4\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 123.518438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 33.2875 83.479219 \nL 368.0875 83.479219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma52ae30ce4\" y=\"83.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 87.278438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 33.2875 47.239219 \nL 368.0875 47.239219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma52ae30ce4\" y=\"47.239219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 51.038438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma52ae30ce4\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M -1 21.871219 \nL 368.0875 21.871219 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 182.0875 81.141154 \nL 368.0875 80.306131 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 89.0875 120.387237 \nL 182.0875 97.841615 \nL 275.0875 89.491385 \nL 368.0875 81.141154 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 33.2875 94.501523 \nL 70.4875 80.306131 \nL 107.6875 83.646223 \nL 144.8875 86.986316 \nL 182.0875 91.996454 \nL 219.2875 93.6665 \nL 256.4875 84.481246 \nL 293.6875 84.481246 \nL 330.8875 86.986316 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 14.6875 185.519034 \nL 33.2875 108.696915 \nL 51.8875 108.696915 \nL 70.4875 116.212122 \nL 89.0875 117.047145 \nL 107.6875 91.161431 \nL 126.2875 107.026869 \nL 144.8875 109.531938 \nL 163.4875 80.306131 \nL 182.0875 84.481246 \nL 200.6875 82.8112 \nL 219.2875 85.316269 \nL 237.8875 81.141154 \nL 256.4875 81.141154 \nL 275.0875 79.471108 \nL 293.6875 80.306131 \nL 312.2875 84.481246 \nL 330.8875 79.471108 \nL 349.4875 78.636085 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p8ae8ca6f0c)\" d=\"M 3.5275 103.686776 \nL 10.9675 112.87203 \nL 18.4075 106.191845 \nL 25.8475 91.161431 \nL 33.2875 89.491385 \nL 40.7275 84.481246 \nL 48.1675 83.646223 \nL 55.6075 107.861892 \nL 63.0475 91.161431 \nL 70.4875 91.161431 \nL 77.9275 88.656362 \nL 85.3675 87.821339 \nL 92.8075 108.696915 \nL 100.2475 88.656362 \nL 107.6875 83.646223 \nL 115.1275 86.986316 \nL 122.5675 85.316269 \nL 130.0075 86.986316 \nL 137.4475 109.531938 \nL 144.8875 107.861892 \nL 152.3275 86.986316 \nL 159.7675 91.161431 \nL 167.2075 87.821339 \nL 174.6475 82.8112 \nL 182.0875 90.326408 \nL 189.5275 84.481246 \nL 196.9675 85.316269 \nL 204.4075 81.141154 \nL 211.8475 81.976177 \nL 219.2875 82.8112 \nL 226.7275 82.8112 \nL 234.1675 81.141154 \nL 241.6075 82.8112 \nL 249.0475 83.646223 \nL 256.4875 89.491385 \nL 263.9275 81.141154 \nL 271.3675 82.8112 \nL 278.8075 91.161431 \nL 286.2475 85.316269 \nL 293.6875 86.151292 \nL 301.1275 86.151292 \nL 308.5675 85.316269 \nL 316.0075 89.491385 \nL 323.4475 86.151292 \nL 330.8875 89.491385 \nL 338.3275 83.646223 \nL 345.7675 81.141154 \nL 353.2075 81.976177 \nL 360.6475 85.316269 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 40.2875 223.439219 \nL 266.135938 223.439219 \nQ 268.135938 223.439219 268.135938 221.439219 \nL 268.135938 134.370469 \nQ 268.135938 132.370469 266.135938 132.370469 \nL 40.2875 132.370469 \nQ 38.2875 132.370469 38.2875 134.370469 \nL 38.2875 221.439219 \nQ 38.2875 223.439219 40.2875 223.439219 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_41\">\n     <path d=\"M 42.2875 140.468906 \nL 62.2875 140.468906 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_42\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(70.2875 143.968906)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_43\">\n     <path d=\"M 42.2875 155.147031 \nL 62.2875 155.147031 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_44\"/>\n    <g id=\"text_19\">\n     <!-- RfModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(70.2875 158.647031)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_45\">\n     <path d=\"M 42.2875 169.825156 \nL 62.2875 169.825156 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_46\"/>\n    <g id=\"text_20\">\n     <!-- RfModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(70.2875 173.325156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_47\">\n     <path d=\"M 42.2875 184.503281 \nL 62.2875 184.503281 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_48\"/>\n    <g id=\"text_21\">\n     <!-- RfModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(70.2875 188.003281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_49\">\n     <path d=\"M 42.2875 199.181406 \nL 62.2875 199.181406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_50\"/>\n    <g id=\"text_22\">\n     <!-- RfModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(70.2875 202.681406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 42.2875 213.859531 \nL 62.2875 213.859531 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_23\">\n     <!-- RfModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(70.2875 217.359531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p8ae8ca6f0c\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print('So which is the best sample selection function? margin sampling is the winner!')\n",
        "performance_plot(random_forest_upper_bound, d, ['RfModel'], selection_functions_str    , Ks_str, 1)\n",
        "print()\n",
        "print('So which is the best k? k=10 is the winner')\n",
        "performance_plot(random_forest_upper_bound, d, ['RfModel'] , ['MarginSamplingSelection'], Ks_str, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}