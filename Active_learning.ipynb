{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic.ipynb",
      "provenance": [],
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d"
    },
    "kernelspec": {
      "name": "python382jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
      "display_name": "Python 3.8.2 64-bit"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from scipy.special import expit\n",
        "from scipy import stats\n",
        "from pylab import rcParams\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, \\\n",
        "    GradientBoostingClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "# pd.options.display.max_rows = 20\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "max_queried = 500\n",
        "trainset_size = 1302"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        " def download():\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/ML-for-COVID-19-dataset/main/all_training.csv\", sep=',')\n",
        "    # Column selection\n",
        "    df = data.iloc[:,np.r_[3:34]].copy()\n",
        "\n",
        "    # define row and column index\n",
        "    col = df.columns\n",
        "    row = [i for i in range(df.shape[0])]\n",
        "\n",
        "    # define imputer\n",
        "    imputer = IterativeImputer(estimator=linear_model.BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
        "    # fit on the dataset\n",
        "    imputer.fit(df)\n",
        "    # transform the dataset\n",
        "    df_imputed = imputer.transform(df)\n",
        "    # convert back to pandas dataframe and rename back to df_normalized\n",
        "    df = pd.DataFrame(data=df_imputed, index=row, columns=col)\n",
        "\n",
        "    # Data preparation\n",
        "    X = df\n",
        "    y = data.target\n",
        "    X = X.to_numpy()    \n",
        "    print ('df:', X.shape, y.shape)\n",
        "    return (X, y)\n",
        "\n",
        "\n",
        "def split(train_size):\n",
        "    X_train_full = X[:train_size]\n",
        "    y_train_full = y[:train_size]\n",
        "    X_test = X[train_size:]\n",
        "    y_test = y[train_size:]\n",
        "    return (X_train_full, y_train_full, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5wm-nCFojyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a3f18c-5c4a-4fc5-f5fc-9b699e485fdd"
      },
      "source": [
        "# Using robust scaling as normalization method\n",
        "\n",
        "# # create a scaler object\n",
        "# scaler = RobustScaler()\n",
        "# # fit and transform the data\n",
        "# df_normalized = pd.DataFrame(scaler.fit_transform(df_features), columns=df_features.columns)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTChXqRGYTBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc129e7a-062e-4b20-951d-474768d269e8"
      },
      "source": [
        "# Cross validation\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SvmModel(BaseModel):\n",
        "\n",
        "    model_type = 'Support Vector Machine with linear Kernel'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training svm...')\n",
        "        self.classifier = SVC(C=1, kernel='linear', probability=True,\n",
        "                              class_weight=c_weight)\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    model_type = 'Multinominal Logistic Regression' \n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training multinomial logistic regression...')\n",
        "        train_samples = X_train.shape[0]\n",
        "        self.classifier = LogisticRegression(\n",
        "            C=50. / train_samples,\n",
        "            multi_class='multinomial',\n",
        "            penalty='l1',\n",
        "            solver='saga',\n",
        "            tol=0.1,\n",
        "            class_weight=c_weight,\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class RfModel(BaseModel):\n",
        "\n",
        "    model_type = 'Random Forest'\n",
        "    \n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training random forest...')\n",
        "        self.classifier = RandomForestClassifier(n_estimators=500, class_weight=c_weight)\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    # we train normally and get probabilities for the validation set. i.e., we use the probabilities to select the most uncertain samples\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('Train set:', X_train.shape, 'y:', y_train.shape)\n",
        "        print ('Val   set:', X_val.shape)\n",
        "        print ('Test  set:', X_test.shape)\n",
        "        t0 = time.time()\n",
        "        (X_train, X_val, X_test, self.val_y_predicted,\n",
        "         self.test_y_predicted) = \\\n",
        "            self.model_object.fit_predict(X_train, y_train, X_val, X_test, c_weight)\n",
        "        self.run_time = time.time() - t0\n",
        "        return (X_train, X_val, X_test)  # we return them in case we use PCA, with all the other algorithms, this is not needed.\n",
        "\n",
        "    # we want accuracy only for the test set\n",
        "\n",
        "    def get_test_accuracy(self, i, y_test):\n",
        "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        print('--------------------------------')\n",
        "        print('y-test set:',y_test.shape)\n",
        "        print('Example run in %.3f s' % self.run_time,'\\n')\n",
        "        print(\"Accuracy rate for %f \" % (classif_rate))    \n",
        "        print(\"Classification report for classifier %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
        "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
        "        print('--------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseSelectionFunction(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def select(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomSelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        random_state = check_random_state(0)\n",
        "        selection = np.random.choice(probas_val.shape[0], initial_labeled_samples, replace=False)\n",
        "\n",
        "#     print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',initial_labeled_samples)\n",
        "\n",
        "        return selection\n",
        "\n",
        "\n",
        "class EntropySelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "        selection = (np.argsort(e)[::-1])[:initial_labeled_samples]\n",
        "        return selection\n",
        "      \n",
        "      \n",
        "class MarginSamplingSelection(BaseSelectionFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def select(probas_val, initial_labeled_samples):\n",
        "        rev = np.sort(probas_val, axis=1)[:, ::-1]\n",
        "        values = rev[:, 0] - rev[:, 1]\n",
        "        selection = np.argsort(values)[:initial_labeled_samples]\n",
        "        return selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Normalize(object):\n",
        "    \n",
        "    def normalize(self, X_train, X_val, X_test):\n",
        "        self.scaler = RobustScaler()\n",
        "        X_train = self.scaler.fit_transform(X_train)\n",
        "        X_val   = self.scaler.transform(X_val)\n",
        "        X_test  = self.scaler.transform(X_test)\n",
        "        return (X_train, X_val, X_test) \n",
        "    \n",
        "    def inverse(self, X_train, X_val, X_test):\n",
        "        X_train = self.scaler.inverse_transform(X_train)\n",
        "        X_val   = self.scaler.inverse_transform(X_val)\n",
        "        X_test  = self.scaler.inverse_transform(X_test)\n",
        "        return (X_train, X_val, X_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_k_random_samples(initial_labeled_samples, X_train_full,\n",
        "                         y_train_full):\n",
        "    random_state = check_random_state(0)\n",
        "    permutation = np.random.choice(trainset_size,\n",
        "                                   initial_labeled_samples,\n",
        "                                   replace=False)\n",
        "    print ()\n",
        "    print ('initial random chosen samples', permutation.shape),\n",
        "#            permutation)\n",
        "    X_train = X_train_full[permutation]\n",
        "    y_train = y_train_full[permutation]\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "    bin_count = np.bincount(y_train.astype('int64'))\n",
        "    unique = np.unique(y_train.astype('int64'))\n",
        "    print (\n",
        "        'initial train set:',\n",
        "        X_train.shape,\n",
        "        y_train.shape,\n",
        "        'unique(labels):',\n",
        "        bin_count,\n",
        "        unique,\n",
        "        )\n",
        "    return (permutation, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TheAlgorithm(object):\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    def __init__(self, initial_labeled_samples, model_object, selection_function):\n",
        "        self.initial_labeled_samples = initial_labeled_samples\n",
        "        self.model_object = model_object\n",
        "        self.sample_selection_function = selection_function\n",
        "\n",
        "    def run(self, X_train_full, y_train_full, X_test, y_test):\n",
        "\n",
        "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
        "\n",
        "        (permutation, X_train, y_train) = \\\n",
        "            get_k_random_samples(self.initial_labeled_samples,\n",
        "                                 X_train_full, y_train_full)\n",
        "        self.queried = self.initial_labeled_samples\n",
        "        self.samplecount = [self.initial_labeled_samples]\n",
        "\n",
        "        # permutation, X_train, y_train = get_equally_k_random_samples(self.initial_labeled_samples,classes)\n",
        "\n",
        "        # assign the val set the rest of the 'unlabelled' training data\n",
        "\n",
        "        X_val = np.array([])\n",
        "        y_val = np.array([])\n",
        "        X_val = np.copy(X_train_full)\n",
        "        X_val = np.delete(X_val, permutation, axis=0)\n",
        "        y_val = np.copy(y_train_full)\n",
        "        y_val = np.delete(y_val, permutation, axis=0)\n",
        "        print ('val set:', X_val.shape, y_val.shape, permutation.shape)\n",
        "        print ()\n",
        "\n",
        "        # normalize data\n",
        "\n",
        "        normalizer = Normalize()\n",
        "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)   \n",
        "        \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(1, y_test)\n",
        "\n",
        "        # fpfn = self.clf_model.test_y_predicted.ravel() != y_val.ravel()\n",
        "        # print(fpfn)\n",
        "        # self.fpfncount = []\n",
        "        # self.fpfncount.append(fpfn.sum() / y_test.shape[0] * 100)\n",
        "\n",
        "        while self.queried < max_queried:\n",
        "\n",
        "            active_iteration += 1\n",
        "\n",
        "            # get validation probabilities\n",
        "\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
        "            print ('val predicted:',\n",
        "                   self.clf_model.val_y_predicted.shape,\n",
        "                   self.clf_model.val_y_predicted)\n",
        "            print ('probabilities:', probas_val.shape, '\\n',\n",
        "                   np.argmax(probas_val, axis=1))\n",
        "\n",
        "            # select samples using a selection function\n",
        "\n",
        "            uncertain_samples = \\\n",
        "                self.sample_selection_function.select(probas_val, self.initial_labeled_samples)\n",
        "\n",
        "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
        " \n",
        "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
        "\n",
        "            # get the uncertain samples from the validation set\n",
        "\n",
        "            print ('trainset before', X_train.shape, y_train.shape)\n",
        "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
        "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
        "            print ('trainset after', X_train.shape, y_train.shape)\n",
        "            self.samplecount.append(X_train.shape[0])\n",
        "\n",
        "            bin_count = np.bincount(y_train.astype('int64'))\n",
        "            unique = np.unique(y_train.astype('int64'))\n",
        "            print (\n",
        "                'updated train set:',\n",
        "                X_train.shape,\n",
        "                y_train.shape,\n",
        "                'unique(labels):',\n",
        "                bin_count,\n",
        "                unique,\n",
        "                )\n",
        "\n",
        "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
        "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
        "            print ('val set:', X_val.shape, y_val.shape)\n",
        "            print ()\n",
        "\n",
        "            # normalize again after creating the 'new' train/test sets\n",
        "            normalizer = Normalize()\n",
        "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
        "\n",
        "            self.queried += self.initial_labeled_samples\n",
        "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
        "\n",
        "        print ('final active learning accuracies',\n",
        "               self.clf_model.accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df: (1736, 31) (1736,)\n",
            "train: (1302, 31) (1302,)\n",
            "test : (434, 31) (434,)\n",
            "unique classes 2\n",
            "stopping at: 500\n",
            "Count = 1, using model = SvmModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [125 125] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.034 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.83       321\n",
            "           1       0.53      0.58      0.55       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.70      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[262  59]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [227 273] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.241 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.57603686635944, 77.64976958525345]\n",
            "saved Active-learning-experiment-1.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 2, using model = SvmModel, selection_function = RandomSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [61 64] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.017 s \n",
            "\n",
            "Accuracy rate for 73.963134 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.82       321\n",
            "           1       0.50      0.52      0.51       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.67      0.67       434\n",
            "weighted avg       0.74      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[262  59]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [114 136] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.062 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 0 0 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 0 0 ... 1 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [172 203] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.146 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [226 274] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.146 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [73.963133640553, 77.64976958525345, 79.26267281105991, 78.11059907834101]\n",
            "saved Active-learning-experiment-2.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 3, using model = SvmModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [24 26] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 67.741935 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.72      0.77       321\n",
            "           1       0.41      0.56      0.47       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.62      0.64      0.62       434\n",
            "weighted avg       0.72      0.68      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[231  90]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 1 ... 0 1 1]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 1 ... 1 1 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [46 54] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.015 s \n",
            "\n",
            "Accuracy rate for 62.672811 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.60      0.70       321\n",
            "           1       0.38      0.70      0.49       113\n",
            "\n",
            "    accuracy                           0.63       434\n",
            "   macro avg       0.62      0.65      0.60       434\n",
            "weighted avg       0.73      0.63      0.65       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[193 128]\n",
            " [ 34  79]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [68 82] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1248.921 s \n",
            "\n",
            "Accuracy rate for 72.580645 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.81      0.81       321\n",
            "           1       0.47      0.49      0.48       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.65      0.65      0.65       434\n",
            "weighted avg       0.73      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[260  61]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [1 0 1 ... 0 0 1]\n",
            "probabilities: (1152, 2) \n",
            " [1 0 1 ... 1 1 1]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 87 113] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 970.435 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.56      0.43      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 0 1 ... 0 0 1]\n",
            "probabilities: (1102, 2) \n",
            " [1 0 1 ... 1 1 1]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [109 141] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.185 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.85       321\n",
            "           1       0.56      0.54      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [127 173] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.059 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [156 194] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.172 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.49      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [181 219] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.098 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.48      0.52       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 1 1 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [200 250] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.137 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [221 279] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.181 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [67.74193548387096, 62.67281105990783, 72.58064516129032, 76.26728110599078, 76.95852534562212, 79.03225806451613, 77.64976958525345, 76.95852534562212, 76.95852534562212, 77.41935483870968]\n",
            "saved Active-learning-experiment-3.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 4, using model = SvmModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [12 13] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 73.732719 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82       321\n",
            "           1       0.50      0.48      0.49       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.65      0.66       434\n",
            "weighted avg       0.73      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [19 31] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.49      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [30 45] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 73.502304 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82       321\n",
            "           1       0.49      0.48      0.48       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.65      0.65      0.65       434\n",
            "weighted avg       0.73      0.74      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[265  56]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [1 1 1 ... 1 0 1]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [39 61] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.81       321\n",
            "           1       0.47      0.51      0.49       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.65      0.65      0.65       434\n",
            "weighted avg       0.73      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[255  66]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 1 ... 1 1 1]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [51 74] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.019 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84       321\n",
            "           1       0.56      0.53      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [57 93] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.018 s \n",
            "\n",
            "Accuracy rate for 69.815668 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.73      0.78       321\n",
            "           1       0.44      0.60      0.51       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.64      0.67      0.65       434\n",
            "weighted avg       0.74      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[235  86]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 1 0 1]\n",
            "probabilities: (1152, 2) \n",
            " [1 1 1 ... 1 1 1]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [ 69 106] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.025 s \n",
            "\n",
            "Accuracy rate for 70.506912 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.73      0.79       321\n",
            "           1       0.45      0.63      0.53       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.68      0.66       434\n",
            "weighted avg       0.75      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[235  86]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 1 0 1]\n",
            "probabilities: (1127, 2) \n",
            " [1 1 1 ... 1 0 1]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 79 121] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.032 s \n",
            "\n",
            "Accuracy rate for 66.820276 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.69      0.76       321\n",
            "           1       0.41      0.60      0.49       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.62      0.65      0.62       434\n",
            "weighted avg       0.72      0.67      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[222  99]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 1 0 ... 1 0 1]\n",
            "probabilities: (1102, 2) \n",
            " [1 1 0 ... 1 0 1]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [ 89 136] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.064 s \n",
            "\n",
            "Accuracy rate for 68.663594 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.71      0.77       321\n",
            "           1       0.43      0.61      0.50       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.63      0.66      0.64       434\n",
            "weighted avg       0.73      0.69      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[229  92]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [1 1 0 ... 1 0 1]\n",
            "probabilities: (1077, 2) \n",
            " [1 1 0 ... 1 0 1]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [104 146] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.089 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.55      0.54      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 1 0 1]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [117 158] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.121 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.55      0.55      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [1 1 0 ... 1 0 1]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [125 175] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.355 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.55      0.52      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.68      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 1 0 ... 1 0 1]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [139 186] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.075 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.84       321\n",
            "           1       0.54      0.52      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.68      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0\n",
            " 1 0 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 0 0]\n",
            "probabilities: (977, 2) \n",
            " [1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 1 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [151 199] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.678 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84       321\n",
            "           1       0.56      0.55      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [164 211] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.116 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0\n",
            " 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [174 226] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.108 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [184 241] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.115 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0]\n",
            "probabilities: (877, 2) \n",
            " [1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [196 254] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.123 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.58      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 1]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [209 266] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.148 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0\n",
            " 1 1 1 0 0 0 0 1 1 0 1 1 0]\n",
            "probabilities: (827, 2) \n",
            " [1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0\n",
            " 1 1 1 0 0 0 0 1 1 0 1 1 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [220 280] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.165 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.86       321\n",
            "           1       0.61      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.73      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "final active learning accuracies [73.73271889400922, 77.18894009216591, 73.50230414746544, 72.11981566820278, 76.72811059907833, 69.81566820276498, 70.50691244239631, 66.82027649769586, 68.66359447004609, 76.49769585253456, 76.72811059907833, 76.26728110599078, 76.036866359447, 76.95852534562212, 77.64976958525345, 77.88018433179722, 78.3410138248848, 78.11059907834101, 79.03225806451613, 79.49308755760369]\n",
            "saved Active-learning-experiment-4.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 5, using model = SvmModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [5 5] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 67.281106 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.78      0.78       321\n",
            "           1       0.37      0.38      0.38       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.58      0.58      0.58       434\n",
            "weighted avg       0.67      0.67      0.67       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[249  72]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 1 1]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 1 1]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [12  8] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 67.972350 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.76      0.78       321\n",
            "           1       0.40      0.44      0.42       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.60      0.60      0.60       434\n",
            "weighted avg       0.69      0.68      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[245  76]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [17 13] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 69.815668 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.78      0.79       321\n",
            "           1       0.43      0.47      0.45       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.62      0.62      0.62       434\n",
            "weighted avg       0.71      0.70      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[250  71]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [22 18] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 74.193548 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83       321\n",
            "           1       0.51      0.43      0.47       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.64      0.65       434\n",
            "weighted avg       0.73      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [29 21] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 70.737327 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.80       321\n",
            "           1       0.44      0.42      0.43       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.62      0.62      0.62       434\n",
            "weighted avg       0.70      0.71      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[259  62]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [35 25] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 70.737327 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.79      0.80       321\n",
            "           1       0.44      0.46      0.45       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.62      0.63      0.63       434\n",
            "weighted avg       0.71      0.71      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[255  66]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [37 33] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 73.963134 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82       321\n",
            "           1       0.50      0.50      0.50       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.66      0.66       434\n",
            "weighted avg       0.74      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[265  56]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [43 37] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [50 40] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.84       321\n",
            "           1       0.53      0.52      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.68      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[269  52]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [54 46] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.77      0.80       321\n",
            "           1       0.47      0.58      0.52       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.65      0.67      0.66       434\n",
            "weighted avg       0.74      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[248  73]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 1 1 ... 1 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 1 ... 1 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [59 51] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83       321\n",
            "           1       0.52      0.53      0.53       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.68      0.68       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [64 56] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.82      0.83       321\n",
            "           1       0.53      0.58      0.55       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.70      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[263  58]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [68 62] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 73.502304 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.82       321\n",
            "           1       0.49      0.55      0.52       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.67      0.67       434\n",
            "weighted avg       0.75      0.74      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[257  64]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [72 68] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84       321\n",
            "           1       0.54      0.54      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [77 73] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.019 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [81 79] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.028 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.54      0.49      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [85 85] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.020 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [90 90] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.026 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.85       321\n",
            "           1       0.56      0.54      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [95 95] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.026 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.84       321\n",
            "           1       0.53      0.52      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.68      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[269  52]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [101  99] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.032 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.57      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [105 105] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.029 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.84       321\n",
            "           1       0.53      0.52      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.68      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[269  52]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [110 110] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.037 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [115 115] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.036 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [123 117] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.036 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [126 124] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.037 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [132 128] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.045 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [136 134] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.036 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [142 138] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.045 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [146 144] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.042 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [152 148] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.044 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [157 153] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.050 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [161 159] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.049 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
            " 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [163 167] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.061 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [167 173] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.067 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [172 178] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.081 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [179 181] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.070 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0\n",
            " 0 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0\n",
            " 0 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [184 186] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.084 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [189 191] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.080 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [193 197] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.102 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [197 203] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.099 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
            " 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [200 210] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.108 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [206 214] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.088 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0\n",
            " 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [214 216] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.105 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1\n",
            " 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [219 221] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.113 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 0 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [224 226] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.110 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [227 233] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.122 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [235 235] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.135 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [238 242] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.136 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [242 248] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.139 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [247 253] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.126 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [67.2811059907834, 67.97235023041475, 69.81566820276498, 74.19354838709677, 70.73732718894009, 70.73732718894009, 73.963133640553, 77.64976958525345, 75.57603686635944, 72.11981566820278, 75.11520737327189, 75.57603686635944, 73.50230414746544, 75.80645161290323, 77.88018433179722, 75.80645161290323, 77.64976958525345, 77.18894009216591, 75.57603686635944, 77.64976958525345, 75.57603686635944, 78.57142857142857, 77.64976958525345, 78.57142857142857, 78.80184331797236, 78.57142857142857, 79.26267281105991, 78.80184331797236, 77.88018433179722, 79.03225806451613, 79.03225806451613, 78.57142857142857, 79.49308755760369, 78.80184331797236, 77.88018433179722, 77.64976958525345, 78.11059907834101, 77.41935483870968, 77.41935483870968, 77.41935483870968, 77.64976958525345, 78.11059907834101, 78.11059907834101, 78.11059907834101, 77.88018433179722, 78.11059907834101, 78.11059907834101, 78.57142857142857, 78.11059907834101, 77.41935483870968]\n",
            "saved Active-learning-experiment-5.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"SvmModel\": {\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          67.2811059907834,\n",
            "          67.97235023041475,\n",
            "          69.81566820276498,\n",
            "          74.19354838709677,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          72.11981566820278,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          73.50230414746544,\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.18894009216591,\n",
            "          73.50230414746544,\n",
            "          72.11981566820278,\n",
            "          76.72811059907833,\n",
            "          69.81566820276498,\n",
            "          70.50691244239631,\n",
            "          66.82027649769586,\n",
            "          68.66359447004609,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          67.74193548387096,\n",
            "          62.67281105990783,\n",
            "          72.58064516129032,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 6, using model = SvmModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [121 129] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.039 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84       321\n",
            "           1       0.54      0.56      0.55       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [243 257] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.165 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.036866359447, 78.80184331797236]\n",
            "saved Active-learning-experiment-6.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 7, using model = SvmModel, selection_function = MarginSamplingSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [67 58] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.013 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.54      0.50      0.52       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.68      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [126 124] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.044 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [193 182] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.160 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [239 261] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.271 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.80645161290323, 77.88018433179722, 78.3410138248848, 79.03225806451613]\n",
            "saved Active-learning-experiment-7.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 8, using model = SvmModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [25 25] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 62.211982 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.64      0.71       321\n",
            "           1       0.36      0.58      0.45       113\n",
            "\n",
            "    accuracy                           0.62       434\n",
            "   macro avg       0.59      0.61      0.58       434\n",
            "weighted avg       0.70      0.62      0.64       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[204 117]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 1 ... 1 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 0 ... 1 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [42 58] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.011 s \n",
            "\n",
            "Accuracy rate for 65.437788 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.68      0.74       321\n",
            "           1       0.39      0.58      0.46       113\n",
            "\n",
            "    accuracy                           0.65       434\n",
            "   macro avg       0.60      0.63      0.60       434\n",
            "weighted avg       0.71      0.65      0.67       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[219 102]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 1 ... 1 0 1]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [78 72] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.020 s \n",
            "\n",
            "Accuracy rate for 65.437788 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.63      0.73       321\n",
            "           1       0.41      0.73      0.52       113\n",
            "\n",
            "    accuracy                           0.65       434\n",
            "   macro avg       0.64      0.68      0.63       434\n",
            "weighted avg       0.75      0.65      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[202 119]\n",
            " [ 31  82]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [109  91] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.046 s \n",
            "\n",
            "Accuracy rate for 70.046083 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.71      0.78       321\n",
            "           1       0.45      0.68      0.54       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.66      0.69      0.66       434\n",
            "weighted avg       0.76      0.70      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[227  94]\n",
            " [ 36  77]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 1 0 ... 0 0 1]\n",
            "probabilities: (1102, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [140 110] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.097 s \n",
            "\n",
            "Accuracy rate for 70.967742 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.74      0.79       321\n",
            "           1       0.46      0.64      0.53       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.69      0.66       434\n",
            "weighted avg       0.75      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[236  85]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [156 144] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.071 s \n",
            "\n",
            "Accuracy rate for 72.811060 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.76      0.81       321\n",
            "           1       0.48      0.64      0.55       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[244  77]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [186 164] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.101 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.57      0.59      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.72      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [207 193] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.129 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85       321\n",
            "           1       0.59      0.59      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [231 219] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.143 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.86       321\n",
            "           1       0.61      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.73      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [258 242] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.186 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "final active learning accuracies [62.21198156682027, 65.43778801843318, 65.43778801843318, 70.04608294930875, 70.96774193548387, 72.81105990783409, 77.64976958525345, 78.57142857142857, 79.49308755760369, 79.26267281105991]\n",
            "saved Active-learning-experiment-8.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 9, using model = SvmModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [13 12] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 71.198157 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.80       321\n",
            "           1       0.45      0.50      0.47       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.63      0.64      0.64       434\n",
            "weighted avg       0.72      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[253  68]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1277, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [24 26] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 71.198157 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.80      0.80       321\n",
            "           1       0.45      0.47      0.46       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.63      0.63      0.63       434\n",
            "weighted avg       0.72      0.71      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[256  65]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [35 40] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 71.428571 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.80      0.81       321\n",
            "           1       0.45      0.47      0.46       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.63      0.63      0.63       434\n",
            "weighted avg       0.72      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[257  64]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [52 48] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 70.967742 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.78      0.80       321\n",
            "           1       0.45      0.51      0.48       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.63      0.65      0.64       434\n",
            "weighted avg       0.72      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[250  71]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [63 62] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.016 s \n",
            "\n",
            "Accuracy rate for 66.589862 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.70      0.76       321\n",
            "           1       0.40      0.56      0.46       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.61      0.63      0.61       434\n",
            "weighted avg       0.71      0.67      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[226  95]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [77 73] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.029 s \n",
            "\n",
            "Accuracy rate for 67.741935 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.71      0.77       321\n",
            "           1       0.42      0.58      0.49       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.62      0.65      0.63       434\n",
            "weighted avg       0.72      0.68      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[228  93]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [84 91] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.031 s \n",
            "\n",
            "Accuracy rate for 69.124424 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.72      0.78       321\n",
            "           1       0.43      0.61      0.51       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.64      0.67      0.64       434\n",
            "weighted avg       0.73      0.69      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[231  90]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 94 106] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.047 s \n",
            "\n",
            "Accuracy rate for 69.354839 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.72      0.78       321\n",
            "           1       0.44      0.62      0.51       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.64      0.67      0.64       434\n",
            "weighted avg       0.74      0.69      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[231  90]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [109 116] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.047 s \n",
            "\n",
            "Accuracy rate for 69.585253 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.72      0.78       321\n",
            "           1       0.44      0.64      0.52       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.65      0.68      0.65       434\n",
            "weighted avg       0.74      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[230  91]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [123 127] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.066 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.76      0.80       321\n",
            "           1       0.47      0.61      0.53       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.69      0.67       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[244  77]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [145 130] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.064 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.76      0.80       321\n",
            "           1       0.47      0.60      0.53       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.66      0.68      0.67       434\n",
            "weighted avg       0.75      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[245  76]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [162 138] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.081 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 1 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 1 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [171 154] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.088 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [181 169] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.119 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [192 183] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.133 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [206 194] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.128 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [215 210] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.184 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [230 220] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.151 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [236 239] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.190 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
            " 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [246 254] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.324 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "final active learning accuracies [71.19815668202764, 71.19815668202764, 71.42857142857143, 70.96774193548387, 66.58986175115207, 67.74193548387096, 69.12442396313364, 69.35483870967742, 69.5852534562212, 72.11981566820278, 72.11981566820278, 78.11059907834101, 78.80184331797236, 79.49308755760369, 79.49308755760369, 78.3410138248848, 78.11059907834101, 77.88018433179722, 77.88018433179722, 78.11059907834101]\n",
            "saved Active-learning-experiment-9.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 10, using model = SvmModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [3 7] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 61.751152 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.63      0.71       321\n",
            "           1       0.36      0.58      0.44       113\n",
            "\n",
            "    accuracy                           0.62       434\n",
            "   macro avg       0.58      0.61      0.58       434\n",
            "weighted avg       0.69      0.62      0.64       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[202 119]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 1 0]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [ 9 11] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 68.894009 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       321\n",
            "           1       0.42      0.50      0.46       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.62      0.63      0.62       434\n",
            "weighted avg       0.71      0.69      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[242  79]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 0 1 ... 0 1 1]\n",
            "probabilities: (1282, 2) \n",
            " [0 0 1 ... 0 1 1]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [12 18] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 65.898618 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.68      0.75       321\n",
            "           1       0.40      0.60      0.48       113\n",
            "\n",
            "    accuracy                           0.66       434\n",
            "   macro avg       0.61      0.64      0.61       434\n",
            "weighted avg       0.72      0.66      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[218 103]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [18 22] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 66.359447 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.67      0.75       321\n",
            "           1       0.41      0.65      0.50       113\n",
            "\n",
            "    accuracy                           0.66       434\n",
            "   macro avg       0.63      0.66      0.62       434\n",
            "weighted avg       0.73      0.66      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[215 106]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [24 26] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 69.815668 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.71      0.78       321\n",
            "           1       0.45      0.67      0.54       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.65      0.69      0.66       434\n",
            "weighted avg       0.75      0.70      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[227  94]\n",
            " [ 37  76]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [31 29] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.79      0.82       321\n",
            "           1       0.51      0.62      0.56       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.69      0.71      0.69       434\n",
            "weighted avg       0.77      0.75      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[255  66]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [38 32] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 69.354839 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.71      0.77       321\n",
            "           1       0.44      0.65      0.53       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.65      0.68      0.65       434\n",
            "weighted avg       0.75      0.69      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[227  94]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [43 37] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 71.428571 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.79       321\n",
            "           1       0.47      0.65      0.54       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.70      0.67       434\n",
            "weighted avg       0.76      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[236  85]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [47 43] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.028 s \n",
            "\n",
            "Accuracy rate for 71.428571 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.79       321\n",
            "           1       0.47      0.65      0.54       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.66      0.70      0.67       434\n",
            "weighted avg       0.76      0.71      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[236  85]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [52 48] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.025 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.57       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [57 53] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 72.580645 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.75      0.80       321\n",
            "           1       0.48      0.65      0.55       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.67      0.70      0.68       434\n",
            "weighted avg       0.76      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[242  79]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [65 55] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.022 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.73      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [71 59] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.018 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.62      0.57      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [76 64] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.024 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.58      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [79 71] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.037 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [84 76] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.137 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [89 81] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.026 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [94 86] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.166 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.58      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [97 93] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.030 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 99 101] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.034 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85       321\n",
            "           1       0.58      0.59      0.59       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [105 105] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.035 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [112 108] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.043 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [116 114] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.040 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [120 120] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.050 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [125 125] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.051 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [129 131] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.048 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [135 135] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.063 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [137 143] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.061 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [145 145] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.066 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [153 147] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.204 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [161 149] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.112 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [168 152] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.108 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [169 161] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.134 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [173 167] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.130 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [178 172] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.113 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [181 179] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.186 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [187 183] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.137 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [190 190] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.167 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [197 193] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.163 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [203 197] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.170 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [210 200] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.169 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [213 207] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.186 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [217 213] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.204 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [222 218] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.169 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [225 225] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.260 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [230 230] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.757 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [232 238] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.148 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [240 240] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.242 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [246 244] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.393 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [247 253] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 10.978 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "final active learning accuracies [61.75115207373272, 68.89400921658986, 65.89861751152074, 66.3594470046083, 69.81566820276498, 74.88479262672811, 69.35483870967742, 71.42857142857143, 71.42857142857143, 76.72811059907833, 72.58064516129032, 79.72350230414746, 79.72350230414746, 78.11059907834101, 79.03225806451613, 78.3410138248848, 78.3410138248848, 78.11059907834101, 78.3410138248848, 78.3410138248848, 78.3410138248848, 79.03225806451613, 79.49308755760369, 79.72350230414746, 80.4147465437788, 79.95391705069125, 79.95391705069125, 80.18433179723502, 79.95391705069125, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.49308755760369, 79.49308755760369, 79.49308755760369, 79.49308755760369, 79.72350230414746, 79.95391705069125, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.4147465437788, 79.95391705069125, 80.18433179723502, 79.26267281105991, 79.49308755760369, 79.72350230414746]\n",
            "saved Active-learning-experiment-10.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"SvmModel\": {\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          61.75115207373272,\n",
            "          68.89400921658986,\n",
            "          65.89861751152074,\n",
            "          66.3594470046083,\n",
            "          69.81566820276498,\n",
            "          74.88479262672811,\n",
            "          69.35483870967742,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          76.72811059907833,\n",
            "          72.58064516129032,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          71.42857142857143,\n",
            "          70.96774193548387,\n",
            "          66.58986175115207,\n",
            "          67.74193548387096,\n",
            "          69.12442396313364,\n",
            "          69.35483870967742,\n",
            "          69.5852534562212,\n",
            "          72.11981566820278,\n",
            "          72.11981566820278,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          62.21198156682027,\n",
            "          65.43778801843318,\n",
            "          65.43778801843318,\n",
            "          70.04608294930875,\n",
            "          70.96774193548387,\n",
            "          72.81105990783409,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          67.2811059907834,\n",
            "          67.97235023041475,\n",
            "          69.81566820276498,\n",
            "          74.19354838709677,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          72.11981566820278,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          73.50230414746544,\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.18894009216591,\n",
            "          73.50230414746544,\n",
            "          72.11981566820278,\n",
            "          76.72811059907833,\n",
            "          69.81566820276498,\n",
            "          70.50691244239631,\n",
            "          66.82027649769586,\n",
            "          68.66359447004609,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          67.74193548387096,\n",
            "          62.67281105990783,\n",
            "          72.58064516129032,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 11, using model = SvmModel, selection_function = EntropySelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [115 135] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.042 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [232 268] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.185 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.88018433179722, 79.72350230414746]\n",
            "saved Active-learning-experiment-11.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 12, using model = SvmModel, selection_function = EntropySelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [52 73] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 74.423963 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.83       321\n",
            "           1       0.51      0.54      0.52       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.68      0.67       434\n",
            "weighted avg       0.75      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[262  59]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [135 115] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.051 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.72      0.79       321\n",
            "           1       0.48      0.72      0.57       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.68      0.72      0.68       434\n",
            "weighted avg       0.77      0.72      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[232  89]\n",
            " [ 32  81]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [205 170] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.098 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [257 243] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.150 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "final active learning accuracies [74.42396313364056, 72.11981566820278, 79.03225806451613, 78.57142857142857]\n",
            "saved Active-learning-experiment-12.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 13, using model = SvmModel, selection_function = EntropySelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [23 27] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81       321\n",
            "           1       0.47      0.53      0.50       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.65      0.66      0.65       434\n",
            "weighted avg       0.73      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[253  68]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 1 1]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 1 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [43 57] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.83       321\n",
            "           1       0.51      0.54      0.53       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.68      0.68       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[263  58]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [78 72] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.018 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84       321\n",
            "           1       0.54      0.55      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [117  83] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.035 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [137 113] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.065 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [152 148] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.094 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [185 165] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.139 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [200 200] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.165 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [223 227] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.211 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [251 249] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.258 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "final active learning accuracies [72.11981566820278, 74.65437788018433, 76.036866359447, 77.41935483870968, 77.88018433179722, 78.57142857142857, 79.49308755760369, 79.49308755760369, 79.95391705069125, 79.49308755760369]\n",
            "saved Active-learning-experiment-13.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 14, using model = SvmModel, selection_function = EntropySelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [14 11] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.84      0.83       321\n",
            "           1       0.52      0.49      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.67      0.67       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [23 27] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 67.972350 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.67      0.76       321\n",
            "           1       0.43      0.71      0.54       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.65      0.69      0.65       434\n",
            "weighted avg       0.75      0.68      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[215 106]\n",
            " [ 33  80]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [39 36] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.57       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.71       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [45 55] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 72.119816 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.74      0.80       321\n",
            "           1       0.47      0.65      0.55       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.67      0.70      0.67       434\n",
            "weighted avg       0.76      0.72      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[239  82]\n",
            " [ 39  74]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [67 58] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.013 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.54      0.54      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [79 71] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.020 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [92 83] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.036 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [105  95] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.037 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [114 111] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.052 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [129 121] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.069 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [143 132] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.069 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.86       321\n",
            "           1       0.61      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.73      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [151 149] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.068 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.57      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 1 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [167 158] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.089 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [176 174] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.123 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [191 184] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.144 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [200 200] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.226 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [213 212] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.225 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [225 225] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.159 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.62      0.57      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [237 238] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.253 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [246 254] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.192 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.11520737327189, 67.97235023041475, 76.95852534562212, 72.11981566820278, 76.26728110599078, 79.03225806451613, 78.3410138248848, 79.03225806451613, 78.3410138248848, 78.11059907834101, 79.49308755760369, 79.26267281105991, 79.26267281105991, 79.95391705069125, 79.49308755760369, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.49308755760369]\n",
            "saved Active-learning-experiment-14.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 15, using model = SvmModel, selection_function = EntropySelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [7 3] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 72.350230 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.88      0.83       321\n",
            "           1       0.45      0.27      0.34       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.61      0.58      0.58       434\n",
            "weighted avg       0.69      0.72      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 82  31]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [ 9 11] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 73.732719 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.91      0.84       321\n",
            "           1       0.49      0.25      0.33       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.63      0.58      0.58       434\n",
            "weighted avg       0.70      0.74      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 85  28]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [12 18] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 72.350230 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.86      0.82       321\n",
            "           1       0.46      0.34      0.39       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.62      0.60      0.60       434\n",
            "weighted avg       0.70      0.72      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 75  38]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 0 0 ... 0 1 0]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [17 23] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 72.580645 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82       321\n",
            "           1       0.47      0.44      0.46       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.64      0.63      0.64       434\n",
            "weighted avg       0.72      0.73      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[265  56]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [23 27] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 73.041475 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.85      0.82       321\n",
            "           1       0.48      0.39      0.43       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.64      0.62      0.63       434\n",
            "weighted avg       0.71      0.73      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 0 ... 0 1 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 0 ... 0 1 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [26 34] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       321\n",
            "           1       0.52      0.44      0.48       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 1 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [33 37] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.54      0.46      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [36 44] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.84       321\n",
            "           1       0.53      0.48      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 1 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [41 49] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.011 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [44 56] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.57       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [50 60] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [57 63] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.014 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [61 69] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.015 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [69 71] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.022 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.62      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [74 76] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.020 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [79 81] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.034 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.60      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [83 87] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.037 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [86 94] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.066 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [ 89 101] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.052 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 92 108] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.050 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [ 94 116] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.124 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.62      0.57      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [103 117] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.053 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.57      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [106 124] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.103 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [114 126] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.095 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [118 132] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.072 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [122 138] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.100 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [130 140] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.121 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [136 144] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.107 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [144 146] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.144 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [149 151] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.132 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [154 156] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.112 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [157 163] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.118 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [163 167] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.128 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [168 172] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.128 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [171 179] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.186 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [176 184] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.163 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0\n",
            " 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0\n",
            " 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [181 189] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.158 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [187 193] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.184 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [191 199] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.153 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [198 202] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.189 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [202 208] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.197 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [204 216] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.191 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [212 218] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.210 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [216 224] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.192 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [220 230] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.222 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [223 237] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.226 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [230 240] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.278 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [236 244] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.198 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0\n",
            " 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 1 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [242 248] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.200 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [245 255] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training svm...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.231 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier SVC(C=1, class_weight='balanced', kernel='linear', probability=True):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [72.35023041474655, 73.73271889400922, 72.35023041474655, 72.58064516129032, 73.04147465437788, 74.88479262672811, 75.80645161290323, 75.34562211981567, 76.72811059907833, 76.72811059907833, 78.11059907834101, 78.57142857142857, 77.64976958525345, 79.72350230414746, 78.57142857142857, 78.11059907834101, 77.64976958525345, 78.3410138248848, 78.57142857142857, 79.95391705069125, 79.72350230414746, 80.4147465437788, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.72350230414746, 79.49308755760369, 79.72350230414746, 79.49308755760369, 79.49308755760369, 79.26267281105991, 79.49308755760369, 78.57142857142857, 78.57142857142857, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.26267281105991, 79.26267281105991, 78.80184331797236, 79.03225806451613, 79.03225806451613, 79.49308755760369, 79.26267281105991, 79.03225806451613, 78.80184331797236, 78.80184331797236, 79.03225806451613, 79.26267281105991, 79.03225806451613]\n",
            "saved Active-learning-experiment-15.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.35023041474655,\n",
            "          73.73271889400922,\n",
            "          72.35023041474655,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          74.88479262672811,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          72.11981566820278,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          67.97235023041475,\n",
            "          76.95852534562212,\n",
            "          72.11981566820278,\n",
            "          76.26728110599078,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.11981566820278,\n",
            "          74.65437788018433,\n",
            "          76.036866359447,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          61.75115207373272,\n",
            "          68.89400921658986,\n",
            "          65.89861751152074,\n",
            "          66.3594470046083,\n",
            "          69.81566820276498,\n",
            "          74.88479262672811,\n",
            "          69.35483870967742,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          76.72811059907833,\n",
            "          72.58064516129032,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          71.42857142857143,\n",
            "          70.96774193548387,\n",
            "          66.58986175115207,\n",
            "          67.74193548387096,\n",
            "          69.12442396313364,\n",
            "          69.35483870967742,\n",
            "          69.5852534562212,\n",
            "          72.11981566820278,\n",
            "          72.11981566820278,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          62.21198156682027,\n",
            "          65.43778801843318,\n",
            "          65.43778801843318,\n",
            "          70.04608294930875,\n",
            "          70.96774193548387,\n",
            "          72.81105990783409,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          67.2811059907834,\n",
            "          67.97235023041475,\n",
            "          69.81566820276498,\n",
            "          74.19354838709677,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          72.11981566820278,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          73.50230414746544,\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.18894009216591,\n",
            "          73.50230414746544,\n",
            "          72.11981566820278,\n",
            "          76.72811059907833,\n",
            "          69.81566820276498,\n",
            "          70.50691244239631,\n",
            "          66.82027649769586,\n",
            "          68.66359447004609,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          67.74193548387096,\n",
            "          62.67281105990783,\n",
            "          72.58064516129032,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 16, using model = RfModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [114 136] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.019 s \n",
            "\n",
            "Accuracy rate for 73.732719 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.78      0.81       321\n",
            "           1       0.50      0.63      0.55       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.68      0.70      0.68       434\n",
            "weighted avg       0.76      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[249  72]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [225 275] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.167 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.57      0.58      0.57       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "final active learning accuracies [73.73271889400922, 77.41935483870968]\n",
            "saved Active-learning-experiment-16.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 17, using model = RfModel, selection_function = RandomSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [64 61] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.815 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [125 125] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.927 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [184 191] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.058 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.63      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1\n",
            " 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [236 264] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.261 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.64      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.18894009216591, 79.49308755760369, 79.26267281105991, 79.26267281105991]\n",
            "saved Active-learning-experiment-17.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 18, using model = RfModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [19 31] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.818 s \n",
            "\n",
            "Accuracy rate for 55.760369 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.50      0.63       321\n",
            "           1       0.34      0.72      0.46       113\n",
            "\n",
            "    accuracy                           0.56       434\n",
            "   macro avg       0.59      0.61      0.54       434\n",
            "weighted avg       0.70      0.56      0.58       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[161 160]\n",
            " [ 32  81]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [44 56] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.836 s \n",
            "\n",
            "Accuracy rate for 71.658986 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.80       321\n",
            "           1       0.46      0.51      0.49       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.64      0.65      0.64       434\n",
            "weighted avg       0.73      0.72      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[253  68]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [61 89] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.853 s \n",
            "\n",
            "Accuracy rate for 73.041475 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.79      0.81       321\n",
            "           1       0.49      0.58      0.53       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.68      0.67       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[252  69]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 85 115] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.875 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83       321\n",
            "           1       0.51      0.55      0.53       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.68      0.68       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[262  59]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [109 141] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.916 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.85       321\n",
            "           1       0.56      0.56      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [130 170] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.977 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.57      0.59      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [153 197] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.007 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1\n",
            " 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [182 218] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.062 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.57      0.57       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [205 245] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.127 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [232 268] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.160 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "final active learning accuracies [55.76036866359447, 71.6589861751152, 73.04147465437788, 74.65437788018433, 77.18894009216591, 77.88018433179722, 76.49769585253456, 77.41935483870968, 79.26267281105991, 78.57142857142857]\n",
            "saved Active-learning-experiment-18.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 19, using model = RfModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [ 9 16] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.797 s \n",
            "\n",
            "Accuracy rate for 58.755760 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.55      0.66       321\n",
            "           1       0.35      0.71      0.47       113\n",
            "\n",
            "    accuracy                           0.59       434\n",
            "   macro avg       0.60      0.63      0.57       434\n",
            "weighted avg       0.71      0.59      0.61       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[175 146]\n",
            " [ 33  80]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1277, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [19 31] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.775 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.80      0.82       321\n",
            "           1       0.51      0.60      0.55       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.70      0.69       434\n",
            "weighted avg       0.76      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[256  65]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [28 47] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.770 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.80      0.83       321\n",
            "           1       0.52      0.62      0.56       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.69      0.71      0.70       434\n",
            "weighted avg       0.77      0.75      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[256  65]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1227, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [39 61] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.798 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.55      0.55      0.55       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [48 77] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.839 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.82       321\n",
            "           1       0.51      0.58      0.54       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.69      0.68       434\n",
            "weighted avg       0.76      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[259  62]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [1 1 0 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [1 1 0 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [60 90] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.857 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.80      0.83       321\n",
            "           1       0.53      0.63      0.57       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.71      0.70       434\n",
            "weighted avg       0.77      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[257  64]\n",
            " [ 42  71]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [1 1 0 ... 1 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [ 74 101] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.892 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [1 1 0 ... 1 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 86 114] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.900 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85       321\n",
            "           1       0.57      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [1 1 0 ... 1 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [ 99 126] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.905 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.55      0.53      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [1 1 0 ... 1 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [107 143] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.930 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.57       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.71       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 1 0 ... 1 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [116 159] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.967 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.83       321\n",
            "           1       0.52      0.58      0.55       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.69      0.69       434\n",
            "weighted avg       0.76      0.75      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[261  60]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [1 1 0 ... 1 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [1 1 0 ... 1 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [128 172] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.009 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [1 0 1 ... 1 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [1 0 1 ... 1 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [142 183] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.995 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "probabilities: (977, 2) \n",
            " [1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0\n",
            " 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [157 193] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.034 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [166 209] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.046 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1\n",
            " 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [176 224] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.084 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [191 234] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.111 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [204 246] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.131 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86       321\n",
            "           1       0.59      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [215 260] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.189 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.57      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.72      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [223 277] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.182 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "final active learning accuracies [58.75576036866359, 74.65437788018433, 75.11520737327189, 76.49769585253456, 74.65437788018433, 75.57603686635944, 78.3410138248848, 77.64976958525345, 76.26728110599078, 76.95852534562212, 75.11520737327189, 76.49769585253456, 78.57142857142857, 78.80184331797236, 77.88018433179722, 76.49769585253456, 78.11059907834101, 78.57142857142857, 77.88018433179722, 77.88018433179722]\n",
            "saved Active-learning-experiment-19.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 20, using model = RfModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [3 7] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.726 s \n",
            "\n",
            "Accuracy rate for 52.073733 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.42      0.56       321\n",
            "           1       0.33      0.81      0.47       113\n",
            "\n",
            "    accuracy                           0.52       434\n",
            "   macro avg       0.59      0.61      0.52       434\n",
            "weighted avg       0.72      0.52      0.54       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[135 186]\n",
            " [ 22  91]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 1 0 1]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 1 0 1]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [ 4 16] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.721 s \n",
            "\n",
            "Accuracy rate for 39.631336 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.22      0.35       321\n",
            "           1       0.29      0.89      0.44       113\n",
            "\n",
            "    accuracy                           0.40       434\n",
            "   macro avg       0.57      0.56      0.39       434\n",
            "weighted avg       0.71      0.40      0.37       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[ 71 250]\n",
            " [ 12 101]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [1 1 1 ... 1 0 1]\n",
            "probabilities: (1282, 2) \n",
            " [1 1 1 ... 1 0 1]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [ 6 24] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.076 s \n",
            "\n",
            "Accuracy rate for 52.764977 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.40      0.56       321\n",
            "           1       0.34      0.88      0.49       113\n",
            "\n",
            "    accuracy                           0.53       434\n",
            "   macro avg       0.62      0.64      0.53       434\n",
            "weighted avg       0.76      0.53      0.54       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[130 191]\n",
            " [ 14  99]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [1 1 1 ... 1 0 1]\n",
            "probabilities: (1272, 2) \n",
            " [1 1 1 ... 1 0 1]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [10 30] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.773 s \n",
            "\n",
            "Accuracy rate for 51.612903 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.39      0.55       321\n",
            "           1       0.33      0.87      0.48       113\n",
            "\n",
            "    accuracy                           0.52       434\n",
            "   macro avg       0.61      0.63      0.51       434\n",
            "weighted avg       0.75      0.52      0.53       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[126 195]\n",
            " [ 15  98]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [1 1 1 ... 1 0 1]\n",
            "probabilities: (1262, 2) \n",
            " [1 1 1 ... 1 0 1]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [15 35] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.749 s \n",
            "\n",
            "Accuracy rate for 66.129032 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.61      0.73       321\n",
            "           1       0.42      0.81      0.56       113\n",
            "\n",
            "    accuracy                           0.66       434\n",
            "   macro avg       0.66      0.71      0.64       434\n",
            "weighted avg       0.78      0.66      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[195 126]\n",
            " [ 21  92]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [1 1 1 ... 1 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [1 1 1 ... 1 0 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [21 39] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.773 s \n",
            "\n",
            "Accuracy rate for 70.046083 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.69      0.77       321\n",
            "           1       0.45      0.74      0.56       113\n",
            "\n",
            "    accuracy                           0.70       434\n",
            "   macro avg       0.67      0.71      0.67       434\n",
            "weighted avg       0.77      0.70      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[220 101]\n",
            " [ 29  84]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1242, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [27 43] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.808 s \n",
            "\n",
            "Accuracy rate for 68.663594 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.69      0.76       321\n",
            "           1       0.44      0.69      0.53       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.65      0.69      0.65       434\n",
            "weighted avg       0.75      0.69      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[220 101]\n",
            " [ 35  78]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1232, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [35 45] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.783 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.79      0.82       321\n",
            "           1       0.52      0.65      0.57       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.69      0.72      0.70       434\n",
            "weighted avg       0.77      0.75      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[253  68]\n",
            " [ 40  73]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [40 50] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.789 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.81      0.83       321\n",
            "           1       0.53      0.61      0.57       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.76      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[261  60]\n",
            " [ 44  69]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [46 54] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.790 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84       321\n",
            "           1       0.56      0.58      0.57       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[269  52]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [52 58] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.832 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [58 62] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.841 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [64 66] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.811 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [67 73] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.823 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.55      0.55      0.55       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [70 80] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.884 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [76 84] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.850 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [83 87] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.880 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [85 95] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.868 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [92 98] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.905 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 98 102] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.874 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [103 107] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.894 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [105 115] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.897 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.57      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [110 120] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.927 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [114 126] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.911 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [120 130] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.905 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.88      0.87       321\n",
            "           1       0.63      0.58      0.61       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [122 138] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.914 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.57      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [127 143] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.976 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [133 147] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.940 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [137 153] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.955 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.85       321\n",
            "           1       0.56      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [143 157] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.965 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [149 161] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.042 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.56      0.52      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [154 166] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.983 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       321\n",
            "           1       0.56      0.49      0.52       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [158 172] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.987 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.60      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [165 175] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.007 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [167 183] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.043 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0\n",
            " 0 1 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [171 189] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.027 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [175 195] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.017 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [180 200] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.041 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 0\n",
            " 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [186 204] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.098 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0\n",
            " 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [191 209] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.107 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0\n",
            " 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0\n",
            " 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [197 213] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.073 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [203 217] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.089 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [208 222] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.099 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [212 228] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.125 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [220 230] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.093 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.54      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [223 237] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.129 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [226 244] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.154 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0\n",
            " 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [230 250] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.112 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 1 1 1 1 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [234 256] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.127 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.60      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [241 259] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.139 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "final active learning accuracies [52.07373271889401, 39.63133640552996, 52.764976958525345, 51.61290322580645, 66.12903225806451, 70.04608294930875, 68.66359447004609, 75.11520737327189, 76.036866359447, 77.18894009216591, 78.3410138248848, 77.64976958525345, 78.3410138248848, 76.49769585253456, 76.72811059907833, 76.72811059907833, 77.18894009216591, 78.11059907834101, 79.49308755760369, 79.49308755760369, 79.03225806451613, 79.49308755760369, 79.49308755760369, 78.3410138248848, 80.4147465437788, 78.80184331797236, 77.88018433179722, 77.64976958525345, 77.18894009216591, 77.64976958525345, 76.95852534562212, 76.49769585253456, 78.3410138248848, 79.49308755760369, 78.57142857142857, 79.26267281105991, 78.57142857142857, 79.03225806451613, 78.11059907834101, 78.3410138248848, 79.26267281105991, 79.03225806451613, 79.03225806451613, 78.57142857142857, 78.80184331797236, 79.49308755760369, 78.11059907834101, 78.57142857142857, 78.80184331797236, 79.49308755760369]\n",
            "saved Active-learning-experiment-20.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"RfModel\": {\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          39.63133640552996,\n",
            "          52.764976958525345,\n",
            "          51.61290322580645,\n",
            "          66.12903225806451,\n",
            "          70.04608294930875,\n",
            "          68.66359447004609,\n",
            "          75.11520737327189,\n",
            "          76.036866359447,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.49769585253456,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          58.75576036866359,\n",
            "          74.65437788018433,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          74.65437788018433,\n",
            "          75.57603686635944,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.76036866359447,\n",
            "          71.6589861751152,\n",
            "          73.04147465437788,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          77.41935483870968,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.35023041474655,\n",
            "          73.73271889400922,\n",
            "          72.35023041474655,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          74.88479262672811,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          72.11981566820278,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          67.97235023041475,\n",
            "          76.95852534562212,\n",
            "          72.11981566820278,\n",
            "          76.26728110599078,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.11981566820278,\n",
            "          74.65437788018433,\n",
            "          76.036866359447,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          61.75115207373272,\n",
            "          68.89400921658986,\n",
            "          65.89861751152074,\n",
            "          66.3594470046083,\n",
            "          69.81566820276498,\n",
            "          74.88479262672811,\n",
            "          69.35483870967742,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          76.72811059907833,\n",
            "          72.58064516129032,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          71.42857142857143,\n",
            "          70.96774193548387,\n",
            "          66.58986175115207,\n",
            "          67.74193548387096,\n",
            "          69.12442396313364,\n",
            "          69.35483870967742,\n",
            "          69.5852534562212,\n",
            "          72.11981566820278,\n",
            "          72.11981566820278,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          62.21198156682027,\n",
            "          65.43778801843318,\n",
            "          65.43778801843318,\n",
            "          70.04608294930875,\n",
            "          70.96774193548387,\n",
            "          72.81105990783409,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          67.2811059907834,\n",
            "          67.97235023041475,\n",
            "          69.81566820276498,\n",
            "          74.19354838709677,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          72.11981566820278,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          73.50230414746544,\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.18894009216591,\n",
            "          73.50230414746544,\n",
            "          72.11981566820278,\n",
            "          76.72811059907833,\n",
            "          69.81566820276498,\n",
            "          70.50691244239631,\n",
            "          66.82027649769586,\n",
            "          68.66359447004609,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          67.74193548387096,\n",
            "          62.67281105990783,\n",
            "          72.58064516129032,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 21, using model = RfModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [112 138] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.935 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [241 259] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.204 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "final active learning accuracies [78.11059907834101, 78.57142857142857]\n",
            "saved Active-learning-experiment-21.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 22, using model = RfModel, selection_function = MarginSamplingSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [63 62] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.834 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.55      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [126 124] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.985 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.61      0.44      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.67      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [182 193] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.119 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [1 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [244 256] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.184 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "final active learning accuracies [78.11059907834101, 78.11059907834101, 79.49308755760369, 79.72350230414746]\n",
            "saved Active-learning-experiment-22.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 23, using model = RfModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [16 34] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.753 s \n",
            "\n",
            "Accuracy rate for 68.433180 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.68      0.76       321\n",
            "           1       0.43      0.71      0.54       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.65      0.69      0.65       434\n",
            "weighted avg       0.76      0.68      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[217 104]\n",
            " [ 33  80]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [53 47] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.813 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84       321\n",
            "           1       0.54      0.56      0.55       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.70      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [83 67] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.916 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [110  90] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.897 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.86       321\n",
            "           1       0.64      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.67      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [132 118] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.947 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [151 149] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.985 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [175 175] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.058 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [207 193] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.116 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
            " 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [232 218] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.142 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [267 233] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.175 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [68.4331797235023, 76.26728110599078, 80.4147465437788, 78.80184331797236, 79.49308755760369, 79.95391705069125, 80.4147465437788, 79.72350230414746, 79.03225806451613, 80.18433179723502]\n",
            "saved Active-learning-experiment-23.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 24, using model = RfModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [11 14] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.747 s \n",
            "\n",
            "Accuracy rate for 72.811060 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.78      0.81       321\n",
            "           1       0.48      0.58      0.53       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.68      0.67       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[250  71]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [22 28] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.755 s \n",
            "\n",
            "Accuracy rate for 73.271889 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.79      0.81       321\n",
            "           1       0.49      0.57      0.52       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.66      0.68      0.67       434\n",
            "weighted avg       0.75      0.73      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[254  67]\n",
            " [ 49  64]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 1 1]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 1 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [36 39] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.829 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.84       321\n",
            "           1       0.53      0.55      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1227, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [50 50] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.817 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       321\n",
            "           1       0.55      0.49      0.52       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [66 59] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.886 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.64      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [73 77] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.874 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [87 88] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.430 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 98 102] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.013 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [106 119] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.943 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.85       321\n",
            "           1       0.56      0.55      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [117 133] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.929 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [132 143] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.977 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.59      0.57       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.71       434\n",
            "weighted avg       0.78      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[267  54]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [149 151] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.984 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [156 169] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.013 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.85       321\n",
            "           1       0.59      0.56      0.57       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.71      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [173 177] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.049 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.62      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [188 187] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.025 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [201 199] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.107 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 1 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 1 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [216 209] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.084 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.53      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [225 225] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.142 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [237 238] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.128 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [253 247] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.209 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.67      0.55      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "final active learning accuracies [72.81105990783409, 73.27188940092167, 75.80645161290323, 76.26728110599078, 79.26267281105991, 77.64976958525345, 78.11059907834101, 77.88018433179722, 77.18894009216591, 77.88018433179722, 76.95852534562212, 79.26267281105991, 78.3410138248848, 79.72350230414746, 80.4147465437788, 80.18433179723502, 80.64516129032258, 80.4147465437788, 80.4147465437788, 81.10599078341014]\n",
            "saved Active-learning-experiment-24.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 25, using model = RfModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [3 7] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.714 s \n",
            "\n",
            "Accuracy rate for 30.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.08      0.14       321\n",
            "           1       0.26      0.94      0.41       113\n",
            "\n",
            "    accuracy                           0.30       434\n",
            "   macro avg       0.52      0.51      0.28       434\n",
            "weighted avg       0.65      0.30      0.21       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[ 25 296]\n",
            " [  7 106]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 1 1 1]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 1 1 1]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [13  7] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.804 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.64      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.67      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [16 14] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.849 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.55      0.51      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.68      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [22 18] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.710 s \n",
            "\n",
            "Accuracy rate for 70.506912 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.74      0.79       321\n",
            "           1       0.45      0.60      0.52       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.65      0.67      0.65       434\n",
            "weighted avg       0.74      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[238  83]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [28 22] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.936 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.53      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [33 27] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 6.031 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.44      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [40 30] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 2.450 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.86       321\n",
            "           1       0.64      0.38      0.48       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.73      0.65      0.67       434\n",
            "weighted avg       0.77      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [44 36] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 5.361 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.67      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [50 40] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.143 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.65      0.40      0.49       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.66      0.68       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [56 44] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 2.994 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87       321\n",
            "           1       0.71      0.39      0.50       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [60 50] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.517 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.41      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [64 56] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.122 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [67 63] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.022 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [71 69] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.133 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [73 77] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.378 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [77 83] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.036 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.65      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [83 87] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.059 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [85 95] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.990 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.86       321\n",
            "           1       0.59      0.53      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [91 99] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.068 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 96 104] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.059 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.59      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [101 109] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.040 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [106 114] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.150 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [110 120] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.213 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.58      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [113 127] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.111 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.54      0.53      0.54       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.69      0.69       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[270  51]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [120 130] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.144 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [126 134] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.016 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       321\n",
            "           1       0.57      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [135 135] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.124 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [142 138] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.120 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [143 147] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.305 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.52      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [149 151] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.380 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.55      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [152 158] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.378 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [158 162] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.175 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [162 168] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.120 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
            " 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 1 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
            " 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [167 173] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.637 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0\n",
            " 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [171 179] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.407 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [176 184] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.225 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [182 188] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.814 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [189 191] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.308 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [192 198] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.317 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1\n",
            " 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [198 202] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.536 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [205 205] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.435 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [208 212] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.374 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [217 213] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.189 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [222 218] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.348 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [226 224] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.372 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [231 229] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.186 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [238 232] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.167 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [242 238] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.241 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [247 243] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.214 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [251 249] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.168 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.65      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "final active learning accuracies [30.184331797235025, 78.80184331797236, 76.49769585253456, 70.50691244239631, 77.41935483870968, 76.72811059907833, 78.3410138248848, 79.49308755760369, 78.80184331797236, 79.95391705069125, 79.49308755760369, 80.18433179723502, 78.57142857142857, 79.03225806451613, 80.87557603686636, 79.95391705069125, 79.26267281105991, 78.3410138248848, 79.03225806451613, 78.11059907834101, 79.03225806451613, 78.57142857142857, 77.64976958525345, 76.036866359447, 77.64976958525345, 77.41935483870968, 78.57142857142857, 79.03225806451613, 78.11059907834101, 80.87557603686636, 79.26267281105991, 80.4147465437788, 79.26267281105991, 79.72350230414746, 79.03225806451613, 79.49308755760369, 79.26267281105991, 78.11059907834101, 78.80184331797236, 77.88018433179722, 79.72350230414746, 78.80184331797236, 79.26267281105991, 79.72350230414746, 79.95391705069125, 79.95391705069125, 80.18433179723502, 80.18433179723502, 79.72350230414746, 79.95391705069125]\n",
            "saved Active-learning-experiment-25.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"RfModel\": {\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          30.184331797235025,\n",
            "          78.80184331797236,\n",
            "          76.49769585253456,\n",
            "          70.50691244239631,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          76.036866359447,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          80.87557603686636,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.72350230414746,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          79.26267281105991,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.95852534562212,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          68.4331797235023,\n",
            "          76.26728110599078,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          39.63133640552996,\n",
            "          52.764976958525345,\n",
            "          51.61290322580645,\n",
            "          66.12903225806451,\n",
            "          70.04608294930875,\n",
            "          68.66359447004609,\n",
            "          75.11520737327189,\n",
            "          76.036866359447,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.49769585253456,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          58.75576036866359,\n",
            "          74.65437788018433,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          74.65437788018433,\n",
            "          75.57603686635944,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.76036866359447,\n",
            "          71.6589861751152,\n",
            "          73.04147465437788,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          77.41935483870968,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.35023041474655,\n",
            "          73.73271889400922,\n",
            "          72.35023041474655,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          74.88479262672811,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          72.11981566820278,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          67.97235023041475,\n",
            "          76.95852534562212,\n",
            "          72.11981566820278,\n",
            "          76.26728110599078,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.11981566820278,\n",
            "          74.65437788018433,\n",
            "          76.036866359447,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          61.75115207373272,\n",
            "          68.89400921658986,\n",
            "          65.89861751152074,\n",
            "          66.3594470046083,\n",
            "          69.81566820276498,\n",
            "          74.88479262672811,\n",
            "          69.35483870967742,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          76.72811059907833,\n",
            "          72.58064516129032,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          71.42857142857143,\n",
            "          70.96774193548387,\n",
            "          66.58986175115207,\n",
            "          67.74193548387096,\n",
            "          69.12442396313364,\n",
            "          69.35483870967742,\n",
            "          69.5852534562212,\n",
            "          72.11981566820278,\n",
            "          72.11981566820278,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          62.21198156682027,\n",
            "          65.43778801843318,\n",
            "          65.43778801843318,\n",
            "          70.04608294930875,\n",
            "          70.96774193548387,\n",
            "          72.81105990783409,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          67.2811059907834,\n",
            "          67.97235023041475,\n",
            "          69.81566820276498,\n",
            "          74.19354838709677,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          72.11981566820278,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          73.50230414746544,\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.18894009216591,\n",
            "          73.50230414746544,\n",
            "          72.11981566820278,\n",
            "          76.72811059907833,\n",
            "          69.81566820276498,\n",
            "          70.50691244239631,\n",
            "          66.82027649769586,\n",
            "          68.66359447004609,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          67.74193548387096,\n",
            "          62.67281105990783,\n",
            "          72.58064516129032,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 26, using model = RfModel, selection_function = EntropySelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [117 133] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.901 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.49      0.52       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [238 262] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.556 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.95852534562212, 79.49308755760369]\n",
            "saved Active-learning-experiment-26.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 27, using model = RfModel, selection_function = EntropySelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [54 71] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.872 s \n",
            "\n",
            "Accuracy rate for 74.193548 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82       321\n",
            "           1       0.50      0.58      0.54       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.67      0.69      0.68       434\n",
            "weighted avg       0.76      0.74      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[256  65]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [132 118] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.944 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [188 187] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.305 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [246 254] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.478 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [74.19354838709677, 79.03225806451613, 79.26267281105991, 79.72350230414746]\n",
            "saved Active-learning-experiment-27.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 28, using model = RfModel, selection_function = EntropySelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [26 24] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.062 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       321\n",
            "           1       0.54      0.49      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [50 50] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.396 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       321\n",
            "           1       0.56      0.56      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [77 73] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.078 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [100 100] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.193 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [126 124] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.046 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [147 153] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.172 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.73      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [177 173] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.260 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [204 196] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.340 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1\n",
            " 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [230 220] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.393 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [251 249] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.442 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.88       321\n",
            "           1       0.66      0.56      0.61       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.036866359447, 76.95852534562212, 77.41935483870968, 79.95391705069125, 79.49308755760369, 79.72350230414746, 79.49308755760369, 80.64516129032258, 80.4147465437788, 81.10599078341014]\n",
            "saved Active-learning-experiment-28.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 29, using model = RfModel, selection_function = EntropySelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [13 12] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.821 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.60      0.40      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.65      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [18 32] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.924 s \n",
            "\n",
            "Accuracy rate for 67.511521 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.69      0.76       321\n",
            "           1       0.42      0.64      0.51       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.63      0.66      0.63       434\n",
            "weighted avg       0.73      0.68      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[221 100]\n",
            " [ 41  72]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [40 35] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.067 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.61      0.44      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.67      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [54 46] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.059 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [62 63] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.946 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [72 78] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.830 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [87 88] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.821 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [100 100] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.888 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.56      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [111 114] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.938 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.86       321\n",
            "           1       0.61      0.58      0.60       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.73      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [121 129] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.951 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.85       321\n",
            "           1       0.57      0.62      0.59       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.73      0.72       434\n",
            "weighted avg       0.79      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 43  70]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [138 137] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.951 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       321\n",
            "           1       0.61      0.58      0.59       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.72      0.73       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [153 147] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.960 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [163 162] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.971 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [175 175] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.001 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.55      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [188 187] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.135 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       321\n",
            "           1       0.63      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [202 198] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.211 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.72      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 0 0 1 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1\n",
            " 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 0 0 1 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [210 215] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.365 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.64      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1\n",
            " 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [223 227] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.555 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.61      0.55      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
            " 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [241 234] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.240 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.65      0.56      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.73      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0\n",
            " 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0\n",
            " 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0\n",
            " 1 0 0 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 0 1 1 1 1 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [249 251] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.170 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.41935483870968, 67.51152073732719, 78.11059907834101, 79.49308755760369, 78.80184331797236, 79.95391705069125, 80.18433179723502, 79.26267281105991, 79.49308755760369, 77.88018433179722, 79.49308755760369, 79.72350230414746, 79.49308755760369, 80.4147465437788, 79.72350230414746, 80.18433179723502, 80.4147465437788, 79.03225806451613, 80.64516129032258, 79.72350230414746]\n",
            "saved Active-learning-experiment-29.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 30, using model = RfModel, selection_function = EntropySelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [5 5] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.790 s \n",
            "\n",
            "Accuracy rate for 54.147465 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.53      0.63       321\n",
            "           1       0.30      0.58      0.40       113\n",
            "\n",
            "    accuracy                           0.54       434\n",
            "   macro avg       0.54      0.55      0.51       434\n",
            "weighted avg       0.66      0.54      0.57       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[170 151]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1292, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [12  8] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.877 s \n",
            "\n",
            "Accuracy rate for 69.124424 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.78      0.79       321\n",
            "           1       0.41      0.43      0.42       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.60      0.61      0.61       434\n",
            "weighted avg       0.70      0.69      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[251  70]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1282, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [21  9] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "<ipython-input-35-5d00dd11dc5d>:26: RuntimeWarning: divide by zero encountered in log2\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "<ipython-input-35-5d00dd11dc5d>:26: RuntimeWarning: invalid value encountered in multiply\n",
            "  e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.876 s \n",
            "\n",
            "Accuracy rate for 67.511521 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.73      0.77       321\n",
            "           1       0.40      0.51      0.45       113\n",
            "\n",
            "    accuracy                           0.68       434\n",
            "   macro avg       0.61      0.62      0.61       434\n",
            "weighted avg       0.70      0.68      0.69       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[235  86]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1272, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [27 13] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.721 s \n",
            "\n",
            "Accuracy rate for 70.737327 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.80      0.80       321\n",
            "           1       0.44      0.44      0.44       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.62      0.62      0.62       434\n",
            "weighted avg       0.71      0.71      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[257  64]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [1 1 1 ... 0 0 1]\n",
            "probabilities: (1262, 2) \n",
            " [1 1 1 ... 0 0 1]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [32 18] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.701 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [33 27] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.907 s \n",
            "\n",
            "Accuracy rate for 71.428571 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.78      0.80       321\n",
            "           1       0.46      0.54      0.50       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.64      0.66      0.65       434\n",
            "weighted avg       0.73      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[249  72]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [1 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [1 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [41 29] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.857 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.84       321\n",
            "           1       0.53      0.50      0.51       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.67      0.67       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[271  50]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [48 32] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.758 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.60      0.45      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.69       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [50 40] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.829 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.55      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.68      0.68       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [58 42] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.801 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [62 48] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.862 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.62      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [66 54] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.912 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.58      0.46      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [73 57] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.023 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.60      0.44      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.68       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [77 63] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.966 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [80 70] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.975 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.48      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [87 73] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.043 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.86       321\n",
            "           1       0.60      0.46      0.52       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [91 79] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.919 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.62      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [96 84] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.853 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.62      0.49      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [99 91] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.886 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.73       434\n",
            "weighted avg       0.79      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [104  96] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.873 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [110 100] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.121 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.48      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [114 106] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.115 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [119 111] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.120 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.58      0.51      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [126 114] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.229 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [135 115] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.983 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [139 121] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.938 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [143 127] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.129 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [149 131] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.095 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [155 135] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.077 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [159 141] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.157 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [166 144] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.123 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1\n",
            " 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [168 152] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.970 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1\n",
            " 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [170 160] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.986 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [175 165] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.953 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [181 169] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.016 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1\n",
            " 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [186 174] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.146 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0\n",
            " 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [189 181] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.013 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1\n",
            " 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [196 184] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.027 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       321\n",
            "           1       0.62      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1\n",
            " 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [203 187] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.235 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.64      0.53      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0\n",
            " 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [207 193] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.078 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0\n",
            " 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [210 200] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.049 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [212 208] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.045 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.72       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1\n",
            " 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [219 211] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.227 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.52      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 0 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [226 214] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.221 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 0 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [228 222] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.243 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0\n",
            " 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [232 228] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.129 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0\n",
            " 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [239 231] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.097 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87       321\n",
            "           1       0.63      0.54      0.58       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0\n",
            " 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0\n",
            " 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [247 233] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.143 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0\n",
            " 1 1 1 1 1 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [253 237] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.131 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.65      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.80      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0\n",
            " 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [258 242] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training random forest...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 1.231 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier RandomForestClassifier(class_weight='balanced', n_estimators=500):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.55      0.60       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.74       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "final active learning accuracies [54.14746543778802, 69.12442396313364, 67.51152073732719, 70.73732718894009, 76.95852534562212, 71.42857142857143, 75.34562211981567, 77.88018433179722, 76.26728110599078, 78.57142857142857, 78.57142857142857, 77.18894009216591, 77.88018433179722, 78.80184331797236, 78.11059907834101, 78.11059907834101, 78.80184331797236, 78.80184331797236, 80.18433179723502, 79.49308755760369, 79.03225806451613, 78.57142857142857, 77.64976958525345, 79.72350230414746, 79.49308755760369, 80.64516129032258, 79.95391705069125, 80.18433179723502, 79.72350230414746, 80.64516129032258, 80.18433179723502, 79.72350230414746, 79.72350230414746, 79.95391705069125, 79.26267281105991, 80.18433179723502, 79.49308755760369, 79.49308755760369, 79.95391705069125, 79.26267281105991, 78.80184331797236, 79.26267281105991, 80.18433179723502, 79.49308755760369, 79.49308755760369, 79.72350230414746, 79.72350230414746, 79.72350230414746, 80.4147465437788, 80.87557603686636]\n",
            "saved Active-learning-experiment-30.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"RfModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          54.14746543778802,\n",
            "          69.12442396313364,\n",
            "          67.51152073732719,\n",
            "          70.73732718894009,\n",
            "          76.95852534562212,\n",
            "          71.42857142857143,\n",
            "          75.34562211981567,\n",
            "          77.88018433179722,\n",
            "          76.26728110599078,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.19354838709677,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          67.51152073732719,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.03225806451613,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          30.184331797235025,\n",
            "          78.80184331797236,\n",
            "          76.49769585253456,\n",
            "          70.50691244239631,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          76.036866359447,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          80.87557603686636,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.72350230414746,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          79.26267281105991,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.95852534562212,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          68.4331797235023,\n",
            "          76.26728110599078,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          39.63133640552996,\n",
            "          52.764976958525345,\n",
            "          51.61290322580645,\n",
            "          66.12903225806451,\n",
            "          70.04608294930875,\n",
            "          68.66359447004609,\n",
            "          75.11520737327189,\n",
            "          76.036866359447,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.49769585253456,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          58.75576036866359,\n",
            "          74.65437788018433,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          74.65437788018433,\n",
            "          75.57603686635944,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.76036866359447,\n",
            "          71.6589861751152,\n",
            "          73.04147465437788,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          77.41935483870968,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.35023041474655,\n",
            "          73.73271889400922,\n",
            "          72.35023041474655,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          74.88479262672811,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          72.11981566820278,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          67.97235023041475,\n",
            "          76.95852534562212,\n",
            "          72.11981566820278,\n",
            "          76.26728110599078,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.11981566820278,\n",
            "          74.65437788018433,\n",
            "          76.036866359447,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          61.75115207373272,\n",
            "          68.89400921658986,\n",
            "          65.89861751152074,\n",
            "          66.3594470046083,\n",
            "          69.81566820276498,\n",
            "          74.88479262672811,\n",
            "          69.35483870967742,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          76.72811059907833,\n",
            "          72.58064516129032,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          71.42857142857143,\n",
            "          70.96774193548387,\n",
            "          66.58986175115207,\n",
            "          67.74193548387096,\n",
            "          69.12442396313364,\n",
            "          69.35483870967742,\n",
            "          69.5852534562212,\n",
            "          72.11981566820278,\n",
            "          72.11981566820278,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          62.21198156682027,\n",
            "          65.43778801843318,\n",
            "          65.43778801843318,\n",
            "          70.04608294930875,\n",
            "          70.96774193548387,\n",
            "          72.81105990783409,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          67.2811059907834,\n",
            "          67.97235023041475,\n",
            "          69.81566820276498,\n",
            "          74.19354838709677,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          72.11981566820278,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          73.50230414746544,\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.18894009216591,\n",
            "          73.50230414746544,\n",
            "          72.11981566820278,\n",
            "          76.72811059907833,\n",
            "          69.81566820276498,\n",
            "          70.50691244239631,\n",
            "          66.82027649769586,\n",
            "          68.66359447004609,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          67.74193548387096,\n",
            "          62.67281105990783,\n",
            "          72.58064516129032,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 31, using model = LogModel, selection_function = RandomSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [119 131] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [225 275] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.66      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.88018433179722, 79.95391705069125]\n",
            "saved Active-learning-experiment-31.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 32, using model = LogModel, selection_function = RandomSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [63 62] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84       321\n",
            "           1       0.56      0.53      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [127 123] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.47      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [189 186] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.48      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1\n",
            " 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [240 260] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.62      0.49      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.72811059907833, 78.3410138248848, 78.11059907834101, 78.80184331797236]\n",
            "saved Active-learning-experiment-32.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 33, using model = LogModel, selection_function = RandomSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [20 30] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 66.589862 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.71      0.76       321\n",
            "           1       0.40      0.54      0.46       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.61      0.63      0.61       434\n",
            "weighted avg       0.71      0.67      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[228  93]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [44 56] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.58      0.43      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [67 83] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.49      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 88 112] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.46      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [107 143] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.54      0.47      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [133 167] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       321\n",
            "           1       0.55      0.50      0.52       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.68      0.68       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 0 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 0 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [150 200] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.47      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [174 226] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1\n",
            " 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [202 248] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.53      0.43      0.48       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1\n",
            " 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [225 275] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       321\n",
            "           1       0.56      0.49      0.52       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "final active learning accuracies [66.58986175115207, 77.18894009216591, 78.3410138248848, 76.26728110599078, 75.57603686635944, 76.26728110599078, 76.95852534562212, 76.26728110599078, 75.34562211981567, 76.49769585253456]\n",
            "saved Active-learning-experiment-33.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 34, using model = LogModel, selection_function = RandomSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [10 15] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=2.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.53      0.50      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [22 28] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.58      0.39      0.47       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.64      0.66       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [30 45] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.6666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.66      0.37      0.47       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.65      0.67       434\n",
            "weighted avg       0.77      0.79      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [43 57] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.011 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.92      0.86       321\n",
            "           1       0.62      0.38      0.47       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.65      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [57 68] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.85       321\n",
            "           1       0.56      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [66 84] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.57      0.41      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [77 98] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.2857142857142857, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.54      0.39      0.45       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1127, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 89 111] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.57      0.39      0.46       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.64      0.66       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [ 96 129] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=0.2222222222222222, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.52      0.38      0.44       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.66      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [110 140] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.043 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.44      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [116 159] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.18181818181818182, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.57      0.38      0.46       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [126 174] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.013 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.53      0.37      0.44       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [141 184] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.15384615384615385, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.54      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0\n",
            " 0 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [156 194] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.46      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 1 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
            " 1 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
            " 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [165 210] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.56      0.43      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0\n",
            " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1\n",
            " 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0\n",
            " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1\n",
            " 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [179 221] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       321\n",
            "           1       0.56      0.47      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0\n",
            " 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0\n",
            " 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
            " 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [189 236] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.11764705882352941, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0\n",
            " 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0\n",
            " 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [200 250] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.54      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1\n",
            " 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1\n",
            " 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1\n",
            " 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [212 263] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.10526315789473684, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.58      0.43      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0\n",
            " 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0\n",
            " 1 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [225 275] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.54      0.43      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.57603686635944, 76.72811059907833, 78.57142857142857, 77.88018433179722, 76.49769585253456, 76.49769585253456, 75.57603686635944, 76.49769585253456, 74.88479262672811, 76.036866359447, 76.49769585253456, 75.11520737327189, 75.57603686635944, 76.036866359447, 76.26728110599078, 76.72811059907833, 76.26728110599078, 75.57603686635944, 77.18894009216591, 75.57603686635944]\n",
            "saved Active-learning-experiment-34.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 35, using model = LogModel, selection_function = RandomSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [6 4] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 70.506912 \n",
            "Classification report for classifier LogisticRegression(C=5.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.80       321\n",
            "           1       0.43      0.42      0.42       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.61      0.61      0.61       434\n",
            "weighted avg       0.70      0.71      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[259  62]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [10 10] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=2.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.53      0.42      0.47       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.65       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 1]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 1]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [13 17] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=1.6666666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.84       321\n",
            "           1       0.53      0.49      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.67      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1272, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [14 26] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=1.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.59      0.41      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.65      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [17 33] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.93      0.86       321\n",
            "           1       0.65      0.35      0.45       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.73      0.64      0.66       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 74  39]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [22 38] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.8333333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.58      0.39      0.47       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.64      0.66       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [28 42] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.7142857142857143, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.92      0.86       321\n",
            "           1       0.61      0.37      0.46       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.64      0.66       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [32 48] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.625, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85       321\n",
            "           1       0.59      0.35      0.44       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.63      0.65       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 73  40]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [36 54] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.5555555555555556, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85       321\n",
            "           1       0.57      0.35      0.43       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.63      0.64       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 74  39]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [39 61] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.92      0.85       321\n",
            "           1       0.58      0.34      0.43       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.63      0.64       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 75  38]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [45 65] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.45454545454545453, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85       321\n",
            "           1       0.59      0.36      0.45       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.64      0.65       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 72  41]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [48 72] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.4166666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.89      0.84       321\n",
            "           1       0.55      0.36      0.44       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.67      0.63      0.64       434\n",
            "weighted avg       0.73      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 72  41]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [54 76] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.38461538461538464, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.57      0.40      0.47       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [61 79] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.036866 \n",
            "Classification report for classifier LogisticRegression(C=0.35714285714285715, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.55      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [65 85] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [71 89] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.3125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [74 96] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=0.29411764705882354, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.52      0.41      0.46       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.66      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [ 79 101] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.2777777777777778, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.53      0.42      0.47       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [ 81 109] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.2631578947368421, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.53      0.42      0.47       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.65       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 84 116] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [ 93 117] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.011 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.23809523809523808, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [ 99 121] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier LogisticRegression(C=0.22727272727272727, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       321\n",
            "           1       0.52      0.43      0.47       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.66      0.65      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [100 130] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.21739130434782608, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.53      0.46      0.49       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.67       434\n",
            "weighted avg       0.74      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [104 136] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=0.20833333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.52      0.39      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.66      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [111 139] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.87      0.84       321\n",
            "           1       0.52      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.66      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [113 147] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier LogisticRegression(C=0.19230769230769232, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.52      0.38      0.44       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.66      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [116 154] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.18518518518518517, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [122 158] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.17857142857142858, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.53      0.39      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [1 0 0 ... 0 0 1]\n",
            "probabilities: (1022, 2) \n",
            " [1 0 0 ... 0 0 1]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [128 162] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.1724137931034483, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.53      0.39      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1012, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [131 169] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.53      0.39      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1002, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [138 172] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.16129032258064516, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.53      0.41      0.46       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1]\n",
            "probabilities: (992, 2) \n",
            " [0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0\n",
            " 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [143 177] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.15625, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.54      0.39      0.45       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1]\n",
            "probabilities: (982, 2) \n",
            " [0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
            " 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [147 183] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.15151515151515152, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.54      0.40      0.46       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 1]\n",
            "probabilities: (972, 2) \n",
            " [0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1\n",
            " 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 0 1]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [152 188] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 74.423963 \n",
            "Classification report for classifier LogisticRegression(C=0.14705882352941177, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.87      0.83       321\n",
            "           1       0.51      0.38      0.44       113\n",
            "\n",
            "    accuracy                           0.74       434\n",
            "   macro avg       0.66      0.63      0.64       434\n",
            "weighted avg       0.72      0.74      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1]\n",
            "probabilities: (962, 2) \n",
            " [0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0\n",
            " 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0\n",
            " 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1\n",
            " 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [159 191] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0\n",
            " 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [161 199] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.1388888888888889, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1]\n",
            "probabilities: (942, 2) \n",
            " [0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [164 206] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.13513513513513514, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.40      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1\n",
            " 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [167 213] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.011 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=0.13157894736842105, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.84       321\n",
            "           1       0.52      0.42      0.47       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0\n",
            " 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [172 218] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 74.884793 \n",
            "Classification report for classifier LogisticRegression(C=0.1282051282051282, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.52      0.42      0.46       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0\n",
            " 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0\n",
            " 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [179 221] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.84       321\n",
            "           1       0.53      0.43      0.48       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1\n",
            " 1 1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [182 228] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.12195121951219512, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.84       321\n",
            "           1       0.53      0.47      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.74      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1\n",
            " 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1\n",
            " 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1\n",
            " 1 0 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0\n",
            " 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0\n",
            " 0 1 0 1]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [187 233] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.11904761904761904, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       321\n",
            "           1       0.56      0.46      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1\n",
            " 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 1 0 1]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [192 238] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.11627906976744186, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.53      0.42      0.47       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
            " 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1\n",
            " 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1\n",
            " 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [197 243] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.024 s \n",
            "\n",
            "Accuracy rate for 74.654378 \n",
            "Classification report for classifier LogisticRegression(C=0.11363636363636363, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       321\n",
            "           1       0.52      0.43      0.47       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.66      0.65      0.65       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 0\n",
            " 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1\n",
            " 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [204 246] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.028 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.55      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0\n",
            " 0 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1\n",
            " 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1\n",
            " 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 1 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 1]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [208 252] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.10869565217391304, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.54      0.43      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1\n",
            " 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0\n",
            " 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [215 255] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.011 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.10638297872340426, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       321\n",
            "           1       0.53      0.42      0.47       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1\n",
            " 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [217 263] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.10416666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.53      0.41      0.46       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.74      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 1 1 0\n",
            " 1 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
            " 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1\n",
            " 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0\n",
            " 0 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 0 0 0 1]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [223 267] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.10204081632653061, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.54      0.44      0.49       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.65      0.66       434\n",
            "weighted avg       0.74      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1\n",
            " 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1\n",
            " 0 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0\n",
            " 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1\n",
            " 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1\n",
            " 1 1 0 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [230 270] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.84       321\n",
            "           1       0.53      0.44      0.48       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.65      0.66       434\n",
            "weighted avg       0.74      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "final active learning accuracies [70.50691244239631, 75.11520737327189, 75.57603686635944, 77.18894009216591, 78.11059907834101, 76.72811059907833, 77.41935483870968, 76.72811059907833, 76.036866359447, 76.49769585253456, 76.95852534562212, 75.57603686635944, 76.49769585253456, 76.036866359447, 75.11520737327189, 75.11520737327189, 74.88479262672811, 75.34562211981567, 75.11520737327189, 75.11520737327189, 75.11520737327189, 74.65437788018433, 75.34562211981567, 74.88479262672811, 74.65437788018433, 74.65437788018433, 75.11520737327189, 75.11520737327189, 75.11520737327189, 75.11520737327189, 75.11520737327189, 75.57603686635944, 75.57603686635944, 74.42396313364056, 75.11520737327189, 75.11520737327189, 75.11520737327189, 74.88479262672811, 74.88479262672811, 75.11520737327189, 75.34562211981567, 76.49769585253456, 75.11520737327189, 74.65437788018433, 75.80645161290323, 75.80645161290323, 75.34562211981567, 75.34562211981567, 75.57603686635944, 75.34562211981567]\n",
            "saved Active-learning-experiment-35.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          70.50691244239631,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          76.72811059907833,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          75.57603686635944,\n",
            "          76.49769585253456,\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          74.88479262672811,\n",
            "          75.34562211981567,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          75.34562211981567,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          74.65437788018433,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          74.42396313364056,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          74.88479262672811,\n",
            "          74.88479262672811,\n",
            "          75.11520737327189,\n",
            "          75.34562211981567,\n",
            "          76.49769585253456,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          75.80645161290323,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          75.34562211981567,\n",
            "          75.57603686635944,\n",
            "          75.34562211981567\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          76.72811059907833,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          76.49769585253456,\n",
            "          75.57603686635944,\n",
            "          76.49769585253456,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          75.57603686635944,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          66.58986175115207,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          76.26728110599078,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          76.26728110599078,\n",
            "          75.34562211981567,\n",
            "          76.49769585253456\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"RfModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          54.14746543778802,\n",
            "          69.12442396313364,\n",
            "          67.51152073732719,\n",
            "          70.73732718894009,\n",
            "          76.95852534562212,\n",
            "          71.42857142857143,\n",
            "          75.34562211981567,\n",
            "          77.88018433179722,\n",
            "          76.26728110599078,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.19354838709677,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          67.51152073732719,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.03225806451613,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          30.184331797235025,\n",
            "          78.80184331797236,\n",
            "          76.49769585253456,\n",
            "          70.50691244239631,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          76.036866359447,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          80.87557603686636,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.72350230414746,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          79.26267281105991,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.95852534562212,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          68.4331797235023,\n",
            "          76.26728110599078,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          39.63133640552996,\n",
            "          52.764976958525345,\n",
            "          51.61290322580645,\n",
            "          66.12903225806451,\n",
            "          70.04608294930875,\n",
            "          68.66359447004609,\n",
            "          75.11520737327189,\n",
            "          76.036866359447,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.49769585253456,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          58.75576036866359,\n",
            "          74.65437788018433,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          74.65437788018433,\n",
            "          75.57603686635944,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.76036866359447,\n",
            "          71.6589861751152,\n",
            "          73.04147465437788,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          77.41935483870968,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.35023041474655,\n",
            "          73.73271889400922,\n",
            "          72.35023041474655,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          74.88479262672811,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          72.11981566820278,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          67.97235023041475,\n",
            "          76.95852534562212,\n",
            "          72.11981566820278,\n",
            "          76.26728110599078,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.11981566820278,\n",
            "          74.65437788018433,\n",
            "          76.036866359447,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          61.75115207373272,\n",
            "          68.89400921658986,\n",
            "          65.89861751152074,\n",
            "          66.3594470046083,\n",
            "          69.81566820276498,\n",
            "          74.88479262672811,\n",
            "          69.35483870967742,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          76.72811059907833,\n",
            "          72.58064516129032,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          71.42857142857143,\n",
            "          70.96774193548387,\n",
            "          66.58986175115207,\n",
            "          67.74193548387096,\n",
            "          69.12442396313364,\n",
            "          69.35483870967742,\n",
            "          69.5852534562212,\n",
            "          72.11981566820278,\n",
            "          72.11981566820278,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          62.21198156682027,\n",
            "          65.43778801843318,\n",
            "          65.43778801843318,\n",
            "          70.04608294930875,\n",
            "          70.96774193548387,\n",
            "          72.81105990783409,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          67.2811059907834,\n",
            "          67.97235023041475,\n",
            "          69.81566820276498,\n",
            "          74.19354838709677,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          72.11981566820278,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          73.50230414746544,\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.18894009216591,\n",
            "          73.50230414746544,\n",
            "          72.11981566820278,\n",
            "          76.72811059907833,\n",
            "          69.81566820276498,\n",
            "          70.50691244239631,\n",
            "          66.82027649769586,\n",
            "          68.66359447004609,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          67.74193548387096,\n",
            "          62.67281105990783,\n",
            "          72.58064516129032,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 36, using model = LogModel, selection_function = MarginSamplingSelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [124 126] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.59      0.45      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [1 0 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [1 0 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [226 274] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.41935483870968, 80.64516129032258]\n",
            "saved Active-learning-experiment-36.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 37, using model = LogModel, selection_function = MarginSamplingSelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [58 67] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.53      0.46      0.49       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.67       434\n",
            "weighted avg       0.74      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[275  46]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [105 145] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.011 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [154 221] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       321\n",
            "           1       0.64      0.49      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0\n",
            " 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [217 283] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.34562211981567, 79.03225806451613, 79.49308755760369, 80.4147465437788]\n",
            "saved Active-learning-experiment-37.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 38, using model = LogModel, selection_function = MarginSamplingSelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [19 31] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83       321\n",
            "           1       0.52      0.48      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.67       434\n",
            "weighted avg       0.74      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [43 57] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 69.124424 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.74      0.78       321\n",
            "           1       0.43      0.55      0.48       113\n",
            "\n",
            "    accuracy                           0.69       434\n",
            "   macro avg       0.63      0.65      0.63       434\n",
            "weighted avg       0.72      0.69      0.70       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[238  83]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [63 87] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 75.115207 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83       321\n",
            "           1       0.52      0.47      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.66      0.67       434\n",
            "weighted avg       0.74      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 81 119] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.84       321\n",
            "           1       0.53      0.48      0.50       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.75      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [103 147] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.48      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [119 181] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.86       321\n",
            "           1       0.63      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [141 209] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86       321\n",
            "           1       0.60      0.44      0.51       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.67      0.68       434\n",
            "weighted avg       0.76      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0\n",
            " 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [162 238] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.49      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [202 248] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       321\n",
            "           1       0.66      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1\n",
            " 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0\n",
            " 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1\n",
            " 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [228 272] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.65      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [75.11520737327189, 69.12442396313364, 75.11520737327189, 75.34562211981567, 78.11059907834101, 79.03225806451613, 77.88018433179722, 78.3410138248848, 80.18433179723502, 79.95391705069125]\n",
            "saved Active-learning-experiment-38.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 39, using model = LogModel, selection_function = MarginSamplingSelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [ 5 20] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 71.658986 \n",
            "Classification report for classifier LogisticRegression(C=2.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.83      0.81       321\n",
            "           1       0.45      0.40      0.42       113\n",
            "\n",
            "    accuracy                           0.72       434\n",
            "   macro avg       0.62      0.61      0.62       434\n",
            "weighted avg       0.71      0.72      0.71       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[266  55]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [11 39] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.89      0.84       321\n",
            "           1       0.54      0.38      0.45       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.67      0.63      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 0 1 ... 0 0 1]\n",
            "probabilities: (1252, 2) \n",
            " [0 0 1 ... 0 0 1]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [17 58] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.267281 \n",
            "Classification report for classifier LogisticRegression(C=0.6666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.85       321\n",
            "           1       0.56      0.42      0.48       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [24 76] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85       321\n",
            "           1       0.58      0.35      0.44       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.63      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 73  40]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [34 91] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.93      0.86       321\n",
            "           1       0.63      0.36      0.46       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.64      0.66       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 72  41]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [ 45 105] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [ 52 123] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.2857142857142857, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.73      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 59 141] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.88       321\n",
            "           1       0.77      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.80      0.69      0.71       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[307  14]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [ 68 157] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 82.258065 \n",
            "Classification report for classifier LogisticRegression(C=0.2222222222222222, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89       321\n",
            "           1       0.78      0.44      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.81      0.70      0.73       434\n",
            "weighted avg       0.82      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[307  14]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [ 79 171] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.75      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.79      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [ 83 192] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.18181818181818182, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.72      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [ 94 206] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.71      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [102 223] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 81.797235 \n",
            "Classification report for classifier LogisticRegression(C=0.15384615384615385, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89       321\n",
            "           1       0.75      0.45      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0\n",
            " 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0\n",
            " 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [111 239] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.66      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.67      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0\n",
            " 0 1 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [123 252] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 82.027650 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89       321\n",
            "           1       0.76      0.45      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.80      0.70      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0\n",
            " 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [129 271] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.96      0.88       321\n",
            "           1       0.78      0.40      0.53       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.80      0.68      0.71       434\n",
            "weighted avg       0.81      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[308  13]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0\n",
            " 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1\n",
            " 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [134 291] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 82.027650 \n",
            "Classification report for classifier LogisticRegression(C=0.11764705882352941, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89       321\n",
            "           1       0.78      0.43      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.80      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[307  14]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1\n",
            " 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [137 313] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.96      0.88       321\n",
            "           1       0.78      0.37      0.50       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.80      0.67      0.69       434\n",
            "weighted avg       0.80      0.81      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[309  12]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0\n",
            " 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [139 336] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.016 s \n",
            "\n",
            "Accuracy rate for 82.258065 \n",
            "Classification report for classifier LogisticRegression(C=0.10526315789473684, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89       321\n",
            "           1       0.83      0.40      0.54       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.83      0.69      0.71       434\n",
            "weighted avg       0.82      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[312   9]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0\n",
            " 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [144 356] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 82.027650 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89       321\n",
            "           1       0.79      0.42      0.55       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.81      0.69      0.72       434\n",
            "weighted avg       0.82      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[308  13]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "final active learning accuracies [71.6589861751152, 75.57603686635944, 76.26728110599078, 76.49769585253456, 77.88018433179722, 80.4147465437788, 80.87557603686636, 81.5668202764977, 82.25806451612904, 81.33640552995391, 80.64516129032258, 80.4147465437788, 81.79723502304147, 79.26267281105991, 82.02764976958525, 81.33640552995391, 82.02764976958525, 80.87557603686636, 82.25806451612904, 82.02764976958525]\n",
            "saved Active-learning-experiment-39.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 40, using model = LogModel, selection_function = MarginSamplingSelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [8 2] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 40.552995 \n",
            "Classification report for classifier LogisticRegression(C=5.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.34      0.46       321\n",
            "           1       0.24      0.60      0.35       113\n",
            "\n",
            "    accuracy                           0.41       434\n",
            "   macro avg       0.47      0.47      0.40       434\n",
            "weighted avg       0.59      0.41      0.43       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[108 213]\n",
            " [ 45  68]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [1 1 1 ... 1 1 0]\n",
            "probabilities: (1292, 2) \n",
            " [1 1 1 ... 1 1 0]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [13  7] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 53.686636 \n",
            "Classification report for classifier LogisticRegression(C=2.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.52      0.62       321\n",
            "           1       0.30      0.59      0.40       113\n",
            "\n",
            "    accuracy                           0.54       434\n",
            "   macro avg       0.54      0.56      0.51       434\n",
            "weighted avg       0.66      0.54      0.56       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[166 155]\n",
            " [ 46  67]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [1 1 1 ... 1 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [1 1 1 ... 1 0 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [18 12] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 71.198157 \n",
            "Classification report for classifier LogisticRegression(C=1.6666666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.80       321\n",
            "           1       0.45      0.50      0.47       113\n",
            "\n",
            "    accuracy                           0.71       434\n",
            "   macro avg       0.63      0.64      0.64       434\n",
            "weighted avg       0.72      0.71      0.72       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[253  68]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [1 1 0 ... 0 0 1]\n",
            "probabilities: (1272, 2) \n",
            " [1 1 0 ... 0 0 1]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [22 18] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=1.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.85       321\n",
            "           1       0.56      0.47      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 0 ... 0 0 1]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 0 ... 0 0 1]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [24 26] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.023 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [25 35] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.8333333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.52      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [29 41] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.7142857142857143, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.58      0.48      0.52       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [33 47] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.625, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       321\n",
            "           1       0.57      0.41      0.48       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[287  34]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [37 53] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.002 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.5555555555555556, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.86       321\n",
            "           1       0.59      0.48      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [44 56] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.86       321\n",
            "           1       0.64      0.42      0.51       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.67      0.69       434\n",
            "weighted avg       0.77      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [46 64] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.45454545454545453, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [49 71] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.4166666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.69      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [52 78] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.38461538461538464, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.67      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [55 85] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.032 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.35714285714285715, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [59 91] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [ 60 100] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.3125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.66      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [ 64 106] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.034 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.29411764705882354, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.86       321\n",
            "           1       0.63      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [ 69 111] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.2777777777777778, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.45      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [ 70 120] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.2631578947368421, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 77 123] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       321\n",
            "           1       0.63      0.52      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.71      0.72       434\n",
            "weighted avg       0.79      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 54  59]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [ 82 128] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.23809523809523808, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.63      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.69       434\n",
            "weighted avg       0.77      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [ 85 135] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.22727272727272727, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [ 89 141] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.21739130434782608, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [ 95 145] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier LogisticRegression(C=0.20833333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.67      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [ 98 152] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.65      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [ 98 162] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.19230769230769232, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       321\n",
            "           1       0.64      0.45      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [101 169] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier LogisticRegression(C=0.18518518518518517, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.67      0.43      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [102 178] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.17857142857142858, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.45      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [103 187] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.1724137931034483, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [108 192] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [111 199] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.16129032258064516, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1\n",
            " 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0\n",
            " 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0\n",
            " 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [113 207] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.15625, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1\n",
            " 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [118 212] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier LogisticRegression(C=0.15151515151515152, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 0\n",
            " 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1\n",
            " 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [124 216] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier LogisticRegression(C=0.14705882352941177, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.70      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0\n",
            " 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0\n",
            " 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0\n",
            " 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1\n",
            " 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [128 222] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.69      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0\n",
            " 1 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0\n",
            " 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
            " 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1\n",
            " 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [133 227] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.1388888888888889, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1\n",
            " 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0\n",
            " 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0\n",
            " 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0\n",
            " 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [140 230] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.019 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.13513513513513514, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1\n",
            " 1 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1\n",
            " 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1\n",
            " 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
            " 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1\n",
            " 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 1 0 1 0 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [145 235] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 81.797235 \n",
            "Classification report for classifier LogisticRegression(C=0.13157894736842105, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.51      0.59       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.72      0.74       434\n",
            "weighted avg       0.81      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0\n",
            " 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0\n",
            " 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1\n",
            " 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1\n",
            " 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 1 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [148 242] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.1282051282051282, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0\n",
            " 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0\n",
            " 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [157 243] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.67      0.50      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0\n",
            " 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0\n",
            " 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0\n",
            " 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0\n",
            " 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [164 246] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.12195121951219512, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.86       321\n",
            "           1       0.63      0.46      0.53       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.68      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0\n",
            " 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1\n",
            " 0 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [171 249] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier LogisticRegression(C=0.11904761904761904, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.70      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1\n",
            " 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0\n",
            " 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
            " 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0\n",
            " 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1\n",
            " 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [176 254] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.11627906976744186, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0\n",
            " 0 1 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0\n",
            " 1 1 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1\n",
            " 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [181 259] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.11363636363636363, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.49      0.58       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0\n",
            " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1\n",
            " 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
            " 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1\n",
            " 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0\n",
            " 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [190 260] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       321\n",
            "           1       0.54      0.40      0.46       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.67      0.64      0.65       434\n",
            "weighted avg       0.74      0.76      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1\n",
            " 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
            " 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0\n",
            " 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1\n",
            " 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [194 266] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.026 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.10869565217391304, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1\n",
            " 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0\n",
            " 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1\n",
            " 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [197 273] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.10638297872340426, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1\n",
            " 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1\n",
            " 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0\n",
            " 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1\n",
            " 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [201 279] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.10416666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.49      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0\n",
            " 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0\n",
            " 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0\n",
            " 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [203 287] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.10204081632653061, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.68      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1\n",
            " 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0\n",
            " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1\n",
            " 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1\n",
            " 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1\n",
            " 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0\n",
            " 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0\n",
            " 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [206 294] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.027 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.66      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "final active learning accuracies [40.55299539170507, 53.686635944700456, 71.19815668202764, 76.72811059907833, 77.64976958525345, 78.57142857142857, 77.41935483870968, 76.72811059907833, 77.88018433179722, 78.80184331797236, 80.18433179723502, 79.95391705069125, 79.49308755760369, 80.18433179723502, 79.26267281105991, 79.49308755760369, 79.03225806451613, 80.4147465437788, 79.49308755760369, 79.49308755760369, 78.80184331797236, 79.26267281105991, 79.95391705069125, 79.72350230414746, 79.49308755760369, 79.03225806451613, 79.72350230414746, 80.18433179723502, 80.18433179723502, 80.4147465437788, 80.4147465437788, 80.4147465437788, 81.10599078341014, 81.10599078341014, 80.87557603686636, 79.95391705069125, 80.18433179723502, 81.79723502304147, 80.87557603686636, 80.64516129032258, 79.03225806451613, 81.10599078341014, 80.64516129032258, 81.5668202764977, 75.57603686635944, 81.5668202764977, 79.49308755760369, 81.33640552995391, 80.64516129032258, 80.18433179723502]\n",
            "saved Active-learning-experiment-40.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          40.55299539170507,\n",
            "          53.686635944700456,\n",
            "          71.19815668202764,\n",
            "          76.72811059907833,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.4147465437788,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          81.79723502304147,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.03225806451613,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          81.5668202764977,\n",
            "          75.57603686635944,\n",
            "          81.5668202764977,\n",
            "          79.49308755760369,\n",
            "          81.33640552995391,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.34562211981567,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.6589861751152,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          81.5668202764977,\n",
            "          82.25806451612904,\n",
            "          81.33640552995391,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          81.79723502304147,\n",
            "          79.26267281105991,\n",
            "          82.02764976958525,\n",
            "          81.33640552995391,\n",
            "          82.02764976958525,\n",
            "          80.87557603686636,\n",
            "          82.25806451612904,\n",
            "          82.02764976958525\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          69.12442396313364,\n",
            "          75.11520737327189,\n",
            "          75.34562211981567,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          70.50691244239631,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          76.72811059907833,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          75.57603686635944,\n",
            "          76.49769585253456,\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          74.88479262672811,\n",
            "          75.34562211981567,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          75.34562211981567,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          74.65437788018433,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          74.42396313364056,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          74.88479262672811,\n",
            "          74.88479262672811,\n",
            "          75.11520737327189,\n",
            "          75.34562211981567,\n",
            "          76.49769585253456,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          75.80645161290323,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          75.34562211981567,\n",
            "          75.57603686635944,\n",
            "          75.34562211981567\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          76.72811059907833,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          76.49769585253456,\n",
            "          75.57603686635944,\n",
            "          76.49769585253456,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          75.57603686635944,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          66.58986175115207,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          76.26728110599078,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          76.26728110599078,\n",
            "          75.34562211981567,\n",
            "          76.49769585253456\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"RfModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          54.14746543778802,\n",
            "          69.12442396313364,\n",
            "          67.51152073732719,\n",
            "          70.73732718894009,\n",
            "          76.95852534562212,\n",
            "          71.42857142857143,\n",
            "          75.34562211981567,\n",
            "          77.88018433179722,\n",
            "          76.26728110599078,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.19354838709677,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          67.51152073732719,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.03225806451613,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          30.184331797235025,\n",
            "          78.80184331797236,\n",
            "          76.49769585253456,\n",
            "          70.50691244239631,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          76.036866359447,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          80.87557603686636,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.72350230414746,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          79.26267281105991,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.95852534562212,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          68.4331797235023,\n",
            "          76.26728110599078,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          39.63133640552996,\n",
            "          52.764976958525345,\n",
            "          51.61290322580645,\n",
            "          66.12903225806451,\n",
            "          70.04608294930875,\n",
            "          68.66359447004609,\n",
            "          75.11520737327189,\n",
            "          76.036866359447,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.49769585253456,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          58.75576036866359,\n",
            "          74.65437788018433,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          74.65437788018433,\n",
            "          75.57603686635944,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.76036866359447,\n",
            "          71.6589861751152,\n",
            "          73.04147465437788,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          77.41935483870968,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.35023041474655,\n",
            "          73.73271889400922,\n",
            "          72.35023041474655,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          74.88479262672811,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          72.11981566820278,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          67.97235023041475,\n",
            "          76.95852534562212,\n",
            "          72.11981566820278,\n",
            "          76.26728110599078,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.11981566820278,\n",
            "          74.65437788018433,\n",
            "          76.036866359447,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          61.75115207373272,\n",
            "          68.89400921658986,\n",
            "          65.89861751152074,\n",
            "          66.3594470046083,\n",
            "          69.81566820276498,\n",
            "          74.88479262672811,\n",
            "          69.35483870967742,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          76.72811059907833,\n",
            "          72.58064516129032,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          71.42857142857143,\n",
            "          70.96774193548387,\n",
            "          66.58986175115207,\n",
            "          67.74193548387096,\n",
            "          69.12442396313364,\n",
            "          69.35483870967742,\n",
            "          69.5852534562212,\n",
            "          72.11981566820278,\n",
            "          72.11981566820278,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          62.21198156682027,\n",
            "          65.43778801843318,\n",
            "          65.43778801843318,\n",
            "          70.04608294930875,\n",
            "          70.96774193548387,\n",
            "          72.81105990783409,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          67.2811059907834,\n",
            "          67.97235023041475,\n",
            "          69.81566820276498,\n",
            "          74.19354838709677,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          72.11981566820278,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          73.50230414746544,\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.18894009216591,\n",
            "          73.50230414746544,\n",
            "          72.11981566820278,\n",
            "          76.72811059907833,\n",
            "          69.81566820276498,\n",
            "          70.50691244239631,\n",
            "          66.82027649769586,\n",
            "          68.66359447004609,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          67.74193548387096,\n",
            "          62.67281105990783,\n",
            "          72.58064516129032,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 41, using model = LogModel, selection_function = EntropySelection, k = 250, iteration = 0.\n",
            "\n",
            "initial random chosen samples (250,)\n",
            "initial train set: (250, 31) (250,) unique(labels): [113 137] [0 1]\n",
            "val set: (1052, 31) (1052,) (250,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.47      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 0 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 0 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [226 274] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.95852534562212, 80.87557603686636]\n",
            "saved Active-learning-experiment-41.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 42, using model = LogModel, selection_function = EntropySelection, k = 125, iteration = 0.\n",
            "\n",
            "initial random chosen samples (125,)\n",
            "initial train set: (125, 31) (125,) unique(labels): [69 56] [0 1]\n",
            "val set: (1177, 31) (1177,) (125,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [128 122] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85       321\n",
            "           1       0.58      0.58      0.58       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.72      0.72       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 47  66]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [178 197] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       321\n",
            "           1       0.66      0.54      0.59       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.75      0.72      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
            " 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1\n",
            " 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1\n",
            " 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1\n",
            " 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 0 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1\n",
            " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1\n",
            " 0 1 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0\n",
            " 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 1 1 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
            " 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [241 259] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.014 s \n",
            "\n",
            "Accuracy rate for 78.571429 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.50      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.72811059907833, 78.3410138248848, 80.64516129032258, 78.57142857142857]\n",
            "saved Active-learning-experiment-42.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 43, using model = LogModel, selection_function = EntropySelection, k = 50, iteration = 0.\n",
            "\n",
            "initial random chosen samples (50,)\n",
            "initial train set: (50, 31) (50,) unique(labels): [28 22] [0 1]\n",
            "val set: (1252, 31) (1252,) (50,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.58      0.46      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [45 55] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.63      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [61 89] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.83      0.84       321\n",
            "           1       0.55      0.58      0.56       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.71      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[268  53]\n",
            " [ 48  65]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [102  98] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       321\n",
            "           1       0.62      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [126 124] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.021 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       321\n",
            "           1       0.64      0.48      0.55       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [145 155] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.53      0.57       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 53  60]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [167 183] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.032258 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86       321\n",
            "           1       0.63      0.47      0.54       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.73      0.69      0.70       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1\n",
            " 1 0 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1\n",
            " 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
            " 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
            " 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1\n",
            " 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0\n",
            " 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [189 211] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 78.801843 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.61      0.51      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.72      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0\n",
            " 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
            " 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1\n",
            " 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
            " 1 0 1 1 1 0 0 1 0 1 1 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [206 244] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       321\n",
            "           1       0.60      0.51      0.55       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.70      0.70       434\n",
            "weighted avg       0.77      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0\n",
            " 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0\n",
            " 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1\n",
            " 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1\n",
            " 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1\n",
            " 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
            " 1 1 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [232 268] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "final active learning accuracies [77.41935483870968, 79.49308755760369, 76.72811059907833, 79.03225806451613, 79.49308755760369, 78.80184331797236, 79.03225806451613, 78.80184331797236, 78.3410138248848, 79.72350230414746]\n",
            "saved Active-learning-experiment-43.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 44, using model = LogModel, selection_function = EntropySelection, k = 25, iteration = 0.\n",
            "\n",
            "initial random chosen samples (25,)\n",
            "initial train set: (25, 31) (25,) unique(labels): [12 13] [0 1]\n",
            "val set: (1277, 31) (1277,) (25,)\n",
            "\n",
            "Train set: (25, 31) y: (25,)\n",
            "Val   set: (1277, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.023 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=2.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.89      0.85       321\n",
            "           1       0.58      0.43      0.49       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.66      0.67       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1277,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1277, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (25, 31) (25,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [18 32] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.96      0.87       321\n",
            "           1       0.73      0.34      0.46       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.77      0.65      0.67       434\n",
            "weighted avg       0.78      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[307  14]\n",
            " [ 75  38]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 0 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 0 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (75, 31) (75,)\n",
            "updated train set: (75, 31) (75,) unique(labels): [28 47] [0 1]\n",
            "val set: (1227, 31) (1227,)\n",
            "\n",
            "Train set: (75, 31) y: (75,)\n",
            "Val   set: (1227, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.6666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.61      0.48      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.68      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[286  35]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1227,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1227, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (75, 31) (75,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [37 63] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84       321\n",
            "           1       0.56      0.55      0.55       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.70      0.70       434\n",
            "weighted avg       0.77      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[272  49]\n",
            " [ 51  62]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (125, 31) (125,)\n",
            "updated train set: (125, 31) (125,) unique(labels): [45 80] [0 1]\n",
            "val set: (1177, 31) (1177,)\n",
            "\n",
            "Train set: (125, 31) y: (125,)\n",
            "Val   set: (1177, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.4, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       321\n",
            "           1       0.54      0.49      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.67      0.68       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[274  47]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1177,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1177, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (125, 31) (125,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [52 98] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       321\n",
            "           1       0.59      0.54      0.56       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.70      0.71       434\n",
            "weighted avg       0.78      0.78      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 52  61]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (175, 31) (175,)\n",
            "updated train set: (175, 31) (175,) unique(labels): [ 60 115] [0 1]\n",
            "val set: (1127, 31) (1127,)\n",
            "\n",
            "Train set: (175, 31) y: (175,)\n",
            "Val   set: (1127, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.2857142857142857, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.69      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1127,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1127, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (175, 31) (175,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 69 131] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (225, 31) (225,)\n",
            "updated train set: (225, 31) (225,) unique(labels): [ 82 143] [0 1]\n",
            "val set: (1077, 31) (1077,)\n",
            "\n",
            "Train set: (225, 31) y: (225,)\n",
            "Val   set: (1077, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier LogisticRegression(C=0.2222222222222222, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.51      0.57       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.71      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "val predicted: (1077,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1077, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (225, 31) (225,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [ 92 158] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.78      0.79      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[289  32]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (275, 31) (275,)\n",
            "updated train set: (275, 31) (275,) unique(labels): [104 171] [0 1]\n",
            "val set: (1027, 31) (1027,)\n",
            "\n",
            "Train set: (275, 31) y: (275,)\n",
            "Val   set: (1027, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier LogisticRegression(C=0.18181818181818182, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       321\n",
            "           1       0.64      0.50      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.70      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[290  31]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1027,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1027, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (275, 31) (275,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [112 188] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 80.875576 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.72      0.43      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (325, 31) (325,)\n",
            "updated train set: (325, 31) (325,) unique(labels): [120 205] [0 1]\n",
            "val set: (977, 31) (977,)\n",
            "\n",
            "Train set: (325, 31) y: (325,)\n",
            "Val   set: (977, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.15384615384615385, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.65      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (977,) [0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0]\n",
            "probabilities: (977, 2) \n",
            " [0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0\n",
            " 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1\n",
            " 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0\n",
            " 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1\n",
            " 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1\n",
            " 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1\n",
            " 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0\n",
            " 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
            " 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0\n",
            " 0 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0]\n",
            "trainset before (325, 31) (325,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [125 225] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.43      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1\n",
            " 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0\n",
            " 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1\n",
            " 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1\n",
            " 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (375, 31) (375,)\n",
            "updated train set: (375, 31) (375,) unique(labels): [135 240] [0 1]\n",
            "val set: (927, 31) (927,)\n",
            "\n",
            "Train set: (375, 31) y: (375,)\n",
            "Val   set: (927, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.13333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87       321\n",
            "           1       0.69      0.39      0.50       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.75      0.66      0.68       434\n",
            "weighted avg       0.78      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (927,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0]\n",
            "probabilities: (927, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1\n",
            " 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
            " 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1\n",
            " 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0\n",
            " 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 1 1\n",
            " 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0\n",
            " 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0\n",
            " 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0\n",
            " 0 0]\n",
            "trainset before (375, 31) (375,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [143 257] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.68      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.67      0.69       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1\n",
            " 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0\n",
            " 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1\n",
            " 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0\n",
            " 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0\n",
            " 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (425, 31) (425,)\n",
            "updated train set: (425, 31) (425,) unique(labels): [150 275] [0 1]\n",
            "val set: (877, 31) (877,)\n",
            "\n",
            "Train set: (425, 31) y: (425,)\n",
            "Val   set: (877, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.11764705882352941, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.88       321\n",
            "           1       0.70      0.43      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (877,) [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (877, 2) \n",
            " [0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0\n",
            " 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1\n",
            " 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1\n",
            " 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (425, 31) (425,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [170 280] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.65      0.43      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
            " 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1\n",
            " 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0\n",
            " 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 1 1\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (475, 31) (475,)\n",
            "updated train set: (475, 31) (475,) unique(labels): [184 291] [0 1]\n",
            "val set: (827, 31) (827,)\n",
            "\n",
            "Train set: (475, 31) y: (475,)\n",
            "Val   set: (827, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.723502 \n",
            "Classification report for classifier LogisticRegression(C=0.10526315789473684, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.92      0.87       321\n",
            "           1       0.67      0.44      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (827,) [0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (827, 2) \n",
            " [0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 1 0\n",
            " 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 1 1\n",
            " 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 1 0 0 1 1 1 1\n",
            " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1\n",
            " 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0\n",
            " 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1\n",
            " 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 1\n",
            " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0\n",
            " 0 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0\n",
            " 0 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (475, 31) (475,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [200 300] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.67      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.68      0.69       434\n",
            "weighted avg       0.78      0.79      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "final active learning accuracies [76.95852534562212, 79.49308755760369, 78.3410138248848, 76.95852534562212, 75.80645161290323, 78.11059907834101, 77.18894009216591, 77.18894009216591, 79.72350230414746, 79.49308755760369, 79.72350230414746, 80.87557603686636, 79.26267281105991, 79.95391705069125, 79.49308755760369, 79.72350230414746, 80.4147465437788, 79.26267281105991, 79.72350230414746, 79.49308755760369]\n",
            "saved Active-learning-experiment-44.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "Count = 45, using model = LogModel, selection_function = EntropySelection, k = 10, iteration = 0.\n",
            "\n",
            "initial random chosen samples (10,)\n",
            "initial train set: (10, 31) (10,) unique(labels): [4 6] [0 1]\n",
            "val set: (1292, 31) (1292,) (10,)\n",
            "\n",
            "Train set: (10, 31) y: (10,)\n",
            "Val   set: (1292, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 64.746544 \n",
            "Classification report for classifier LogisticRegression(C=5.0, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.71      0.75       321\n",
            "           1       0.36      0.48      0.41       113\n",
            "\n",
            "    accuracy                           0.65       434\n",
            "   macro avg       0.58      0.59      0.58       434\n",
            "weighted avg       0.68      0.65      0.66       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[227  94]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1292,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1292, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (10, 31) (10,)\n",
            "trainset after (20, 31) (20,)\n",
            "updated train set: (20, 31) (20,) unique(labels): [ 7 13] [0 1]\n",
            "val set: (1282, 31) (1282,)\n",
            "\n",
            "Train set: (20, 31) y: (20,)\n",
            "Val   set: (1282, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 72.580645 \n",
            "Classification report for classifier LogisticRegression(C=2.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.81      0.81       321\n",
            "           1       0.47      0.48      0.48       113\n",
            "\n",
            "    accuracy                           0.73       434\n",
            "   macro avg       0.64      0.65      0.65       434\n",
            "weighted avg       0.73      0.73      0.73       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[261  60]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1282,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1282, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (20, 31) (20,)\n",
            "trainset after (30, 31) (30,)\n",
            "updated train set: (30, 31) (30,) unique(labels): [ 9 21] [0 1]\n",
            "val set: (1272, 31) (1272,)\n",
            "\n",
            "Train set: (30, 31) y: (30,)\n",
            "Val   set: (1272, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 62.903226 \n",
            "Classification report for classifier LogisticRegression(C=1.6666666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.65      0.72       321\n",
            "           1       0.36      0.56      0.44       113\n",
            "\n",
            "    accuracy                           0.63       434\n",
            "   macro avg       0.58      0.61      0.58       434\n",
            "weighted avg       0.69      0.63      0.65       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[210 111]\n",
            " [ 50  63]]\n",
            "--------------------------------\n",
            "val predicted: (1272,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1272, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (30, 31) (30,)\n",
            "trainset after (40, 31) (40,)\n",
            "updated train set: (40, 31) (40,) unique(labels): [12 28] [0 1]\n",
            "val set: (1262, 31) (1262,)\n",
            "\n",
            "Train set: (40, 31) y: (40,)\n",
            "Val   set: (1262, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 65.898618 \n",
            "Classification report for classifier LogisticRegression(C=1.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.71      0.76       321\n",
            "           1       0.38      0.50      0.44       113\n",
            "\n",
            "    accuracy                           0.66       434\n",
            "   macro avg       0.59      0.61      0.60       434\n",
            "weighted avg       0.69      0.66      0.67       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[229  92]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1262,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1262, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (40, 31) (40,)\n",
            "trainset after (50, 31) (50,)\n",
            "updated train set: (50, 31) (50,) unique(labels): [16 34] [0 1]\n",
            "val set: (1252, 31) (1252,)\n",
            "\n",
            "Train set: (50, 31) y: (50,)\n",
            "Val   set: (1252, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 67.050691 \n",
            "Classification report for classifier LogisticRegression(class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.74      0.77       321\n",
            "           1       0.39      0.48      0.43       113\n",
            "\n",
            "    accuracy                           0.67       434\n",
            "   macro avg       0.60      0.61      0.60       434\n",
            "weighted avg       0.69      0.67      0.68       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[237  84]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1252,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1252, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (50, 31) (50,)\n",
            "trainset after (60, 31) (60,)\n",
            "updated train set: (60, 31) (60,) unique(labels): [21 39] [0 1]\n",
            "val set: (1242, 31) (1242,)\n",
            "\n",
            "Train set: (60, 31) y: (60,)\n",
            "Val   set: (1242, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.8333333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.85       321\n",
            "           1       0.58      0.40      0.47       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.65      0.66       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[288  33]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1242,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1242, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (60, 31) (60,)\n",
            "trainset after (70, 31) (70,)\n",
            "updated train set: (70, 31) (70,) unique(labels): [24 46] [0 1]\n",
            "val set: (1232, 31) (1232,)\n",
            "\n",
            "Train set: (70, 31) y: (70,)\n",
            "Val   set: (1232, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.728111 \n",
            "Classification report for classifier LogisticRegression(C=0.7142857142857143, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.57      0.45      0.50       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.66      0.68       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[282  39]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (1232,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1232, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (70, 31) (70,)\n",
            "trainset after (80, 31) (80,)\n",
            "updated train set: (80, 31) (80,) unique(labels): [27 53] [0 1]\n",
            "val set: (1222, 31) (1222,)\n",
            "\n",
            "Train set: (80, 31) y: (80,)\n",
            "Val   set: (1222, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.625, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.86       321\n",
            "           1       0.61      0.42      0.49       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.66      0.68       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[291  30]\n",
            " [ 66  47]]\n",
            "--------------------------------\n",
            "val predicted: (1222,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1222, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (80, 31) (80,)\n",
            "trainset after (90, 31) (90,)\n",
            "updated train set: (90, 31) (90,) unique(labels): [31 59] [0 1]\n",
            "val set: (1212, 31) (1212,)\n",
            "\n",
            "Train set: (90, 31) y: (90,)\n",
            "Val   set: (1212, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.5555555555555556, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.59      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.70       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1212,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1212, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (90, 31) (90,)\n",
            "trainset after (100, 31) (100,)\n",
            "updated train set: (100, 31) (100,) unique(labels): [34 66] [0 1]\n",
            "val set: (1202, 31) (1202,)\n",
            "\n",
            "Train set: (100, 31) y: (100,)\n",
            "Val   set: (1202, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.003 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.5, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.92      0.86       321\n",
            "           1       0.61      0.37      0.46       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.64      0.66       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (1202,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1202, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (100, 31) (100,)\n",
            "trainset after (110, 31) (110,)\n",
            "updated train set: (110, 31) (110,) unique(labels): [36 74] [0 1]\n",
            "val set: (1192, 31) (1192,)\n",
            "\n",
            "Train set: (110, 31) y: (110,)\n",
            "Val   set: (1192, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.45454545454545453, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86       321\n",
            "           1       0.60      0.48      0.53       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.68      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[285  36]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1192,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1192, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (110, 31) (110,)\n",
            "trainset after (120, 31) (120,)\n",
            "updated train set: (120, 31) (120,) unique(labels): [41 79] [0 1]\n",
            "val set: (1182, 31) (1182,)\n",
            "\n",
            "Train set: (120, 31) y: (120,)\n",
            "Val   set: (1182, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.4166666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.57      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[279  42]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1182,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1182, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (120, 31) (120,)\n",
            "trainset after (130, 31) (130,)\n",
            "updated train set: (130, 31) (130,) unique(labels): [41 89] [0 1]\n",
            "val set: (1172, 31) (1172,)\n",
            "\n",
            "Train set: (130, 31) y: (130,)\n",
            "Val   set: (1172, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.38461538461538464, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       321\n",
            "           1       0.58      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[280  41]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1172,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1172, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (130, 31) (130,)\n",
            "trainset after (140, 31) (140,)\n",
            "updated train set: (140, 31) (140,) unique(labels): [46 94] [0 1]\n",
            "val set: (1162, 31) (1162,)\n",
            "\n",
            "Train set: (140, 31) y: (140,)\n",
            "Val   set: (1162, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 76.497696 \n",
            "Classification report for classifier LogisticRegression(C=0.35714285714285715, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       321\n",
            "           1       0.56      0.48      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.69      0.67      0.68       434\n",
            "weighted avg       0.76      0.76      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[278  43]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "val predicted: (1162,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1162, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (140, 31) (140,)\n",
            "trainset after (150, 31) (150,)\n",
            "updated train set: (150, 31) (150,) unique(labels): [52 98] [0 1]\n",
            "val set: (1152, 31) (1152,)\n",
            "\n",
            "Train set: (150, 31) y: (150,)\n",
            "Val   set: (1152, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.3333333333333333, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1152,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1152, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (150, 31) (150,)\n",
            "trainset after (160, 31) (160,)\n",
            "updated train set: (160, 31) (160,) unique(labels): [ 54 106] [0 1]\n",
            "val set: (1142, 31) (1142,)\n",
            "\n",
            "Train set: (160, 31) y: (160,)\n",
            "Val   set: (1142, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.3125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.54      0.46      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1142,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1142, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (160, 31) (160,)\n",
            "trainset after (170, 31) (170,)\n",
            "updated train set: (170, 31) (170,) unique(labels): [ 57 113] [0 1]\n",
            "val set: (1132, 31) (1132,)\n",
            "\n",
            "Train set: (170, 31) y: (170,)\n",
            "Val   set: (1132, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 76.958525 \n",
            "Classification report for classifier LogisticRegression(C=0.29411764705882354, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.85       321\n",
            "           1       0.56      0.50      0.53       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.68      0.69       434\n",
            "weighted avg       0.76      0.77      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[277  44]\n",
            " [ 56  57]]\n",
            "--------------------------------\n",
            "val predicted: (1132,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1132, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (170, 31) (170,)\n",
            "trainset after (180, 31) (180,)\n",
            "updated train set: (180, 31) (180,) unique(labels): [ 62 118] [0 1]\n",
            "val set: (1122, 31) (1122,)\n",
            "\n",
            "Train set: (180, 31) y: (180,)\n",
            "Val   set: (1122, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.023 s \n",
            "\n",
            "Accuracy rate for 75.806452 \n",
            "Classification report for classifier LogisticRegression(C=0.2777777777777778, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       321\n",
            "           1       0.54      0.47      0.50       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.66      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[276  45]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "val predicted: (1122,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1122, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (180, 31) (180,)\n",
            "trainset after (190, 31) (190,)\n",
            "updated train set: (190, 31) (190,) unique(labels): [ 65 125] [0 1]\n",
            "val set: (1112, 31) (1112,)\n",
            "\n",
            "Train set: (190, 31) y: (190,)\n",
            "Val   set: (1112, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 75.576037 \n",
            "Classification report for classifier LogisticRegression(C=0.2631578947368421, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.84       321\n",
            "           1       0.53      0.49      0.51       113\n",
            "\n",
            "    accuracy                           0.76       434\n",
            "   macro avg       0.68      0.67      0.67       434\n",
            "weighted avg       0.75      0.76      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[273  48]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "val predicted: (1112,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1112, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (190, 31) (190,)\n",
            "trainset after (200, 31) (200,)\n",
            "updated train set: (200, 31) (200,) unique(labels): [ 73 127] [0 1]\n",
            "val set: (1102, 31) (1102,)\n",
            "\n",
            "Train set: (200, 31) y: (200,)\n",
            "Val   set: (1102, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.25, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85       321\n",
            "           1       0.58      0.50      0.54       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.69      0.69       434\n",
            "weighted avg       0.77      0.78      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[281  40]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "val predicted: (1102,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1102, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (200, 31) (200,)\n",
            "trainset after (210, 31) (210,)\n",
            "updated train set: (210, 31) (210,) unique(labels): [ 79 131] [0 1]\n",
            "val set: (1092, 31) (1092,)\n",
            "\n",
            "Train set: (210, 31) y: (210,)\n",
            "Val   set: (1092, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.004 s \n",
            "\n",
            "Accuracy rate for 77.188940 \n",
            "Classification report for classifier LogisticRegression(C=0.23809523809523808, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85       321\n",
            "           1       0.58      0.46      0.51       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.70      0.67      0.68       434\n",
            "weighted avg       0.76      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[283  38]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (1092,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1092, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (210, 31) (210,)\n",
            "trainset after (220, 31) (220,)\n",
            "updated train set: (220, 31) (220,) unique(labels): [ 82 138] [0 1]\n",
            "val set: (1082, 31) (1082,)\n",
            "\n",
            "Train set: (220, 31) y: (220,)\n",
            "Val   set: (1082, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 75.345622 \n",
            "Classification report for classifier LogisticRegression(C=0.22727272727272727, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       321\n",
            "           1       0.54      0.38      0.45       113\n",
            "\n",
            "    accuracy                           0.75       434\n",
            "   macro avg       0.67      0.63      0.64       434\n",
            "weighted avg       0.73      0.75      0.74       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[284  37]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1082,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1082, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (220, 31) (220,)\n",
            "trainset after (230, 31) (230,)\n",
            "updated train set: (230, 31) (230,) unique(labels): [ 85 145] [0 1]\n",
            "val set: (1072, 31) (1072,)\n",
            "\n",
            "Train set: (230, 31) y: (230,)\n",
            "Val   set: (1072, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 77.880184 \n",
            "Classification report for classifier LogisticRegression(C=0.21739130434782608, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.92      0.86       321\n",
            "           1       0.63      0.37      0.47       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.65      0.66       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 71  42]]\n",
            "--------------------------------\n",
            "val predicted: (1072,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1072, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (230, 31) (230,)\n",
            "trainset after (240, 31) (240,)\n",
            "updated train set: (240, 31) (240,) unique(labels): [ 89 151] [0 1]\n",
            "val set: (1062, 31) (1062,)\n",
            "\n",
            "Train set: (240, 31) y: (240,)\n",
            "Val   set: (1062, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.341014 \n",
            "Classification report for classifier LogisticRegression(C=0.20833333333333334, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.86       321\n",
            "           1       0.64      0.38      0.48       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.73      0.65      0.67       434\n",
            "weighted avg       0.77      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1062,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1062, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (240, 31) (240,)\n",
            "trainset after (250, 31) (250,)\n",
            "updated train set: (250, 31) (250,) unique(labels): [ 96 154] [0 1]\n",
            "val set: (1052, 31) (1052,)\n",
            "\n",
            "Train set: (250, 31) y: (250,)\n",
            "Val   set: (1052, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.043 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.2, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.62      0.41      0.49       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.66      0.68       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1052,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1052, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (250, 31) (250,)\n",
            "trainset after (260, 31) (260,)\n",
            "updated train set: (260, 31) (260,) unique(labels): [ 99 161] [0 1]\n",
            "val set: (1042, 31) (1042,)\n",
            "\n",
            "Train set: (260, 31) y: (260,)\n",
            "Val   set: (1042, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 77.649770 \n",
            "Classification report for classifier LogisticRegression(C=0.19230769230769232, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.40      0.48       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.71      0.65      0.67       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1042,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1042, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (260, 31) (260,)\n",
            "trainset after (270, 31) (270,)\n",
            "updated train set: (270, 31) (270,) unique(labels): [101 169] [0 1]\n",
            "val set: (1032, 31) (1032,)\n",
            "\n",
            "Train set: (270, 31) y: (270,)\n",
            "Val   set: (1032, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.262673 \n",
            "Classification report for classifier LogisticRegression(C=0.18518518518518517, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87       321\n",
            "           1       0.67      0.40      0.50       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.74      0.66      0.68       434\n",
            "weighted avg       0.78      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 68  45]]\n",
            "--------------------------------\n",
            "val predicted: (1032,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1032, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (270, 31) (270,)\n",
            "trainset after (280, 31) (280,)\n",
            "updated train set: (280, 31) (280,) unique(labels): [105 175] [0 1]\n",
            "val set: (1022, 31) (1022,)\n",
            "\n",
            "Train set: (280, 31) y: (280,)\n",
            "Val   set: (1022, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 78.110599 \n",
            "Classification report for classifier LogisticRegression(C=0.17857142857142858, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.62      0.41      0.49       113\n",
            "\n",
            "    accuracy                           0.78       434\n",
            "   macro avg       0.72      0.66      0.68       434\n",
            "weighted avg       0.76      0.78      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 67  46]]\n",
            "--------------------------------\n",
            "val predicted: (1022,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1022, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (280, 31) (280,)\n",
            "trainset after (290, 31) (290,)\n",
            "updated train set: (290, 31) (290,) unique(labels): [107 183] [0 1]\n",
            "val set: (1012, 31) (1012,)\n",
            "\n",
            "Train set: (290, 31) y: (290,)\n",
            "Val   set: (1012, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.005 s \n",
            "\n",
            "Accuracy rate for 77.419355 \n",
            "Classification report for classifier LogisticRegression(C=0.1724137931034483, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86       321\n",
            "           1       0.61      0.38      0.47       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.71      0.65      0.66       434\n",
            "weighted avg       0.75      0.77      0.76       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[293  28]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (1012,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1012, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (290, 31) (290,)\n",
            "trainset after (300, 31) (300,)\n",
            "updated train set: (300, 31) (300,) unique(labels): [114 186] [0 1]\n",
            "val set: (1002, 31) (1002,)\n",
            "\n",
            "Train set: (300, 31) y: (300,)\n",
            "Val   set: (1002, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 79.493088 \n",
            "Classification report for classifier LogisticRegression(C=0.16666666666666666, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.94      0.87       321\n",
            "           1       0.69      0.39      0.50       113\n",
            "\n",
            "    accuracy                           0.79       434\n",
            "   macro avg       0.75      0.66      0.68       434\n",
            "weighted avg       0.78      0.79      0.77       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 69  44]]\n",
            "--------------------------------\n",
            "val predicted: (1002,) [0 1 1 ... 0 0 0]\n",
            "probabilities: (1002, 2) \n",
            " [0 1 1 ... 0 0 0]\n",
            "trainset before (300, 31) (300,)\n",
            "trainset after (310, 31) (310,)\n",
            "updated train set: (310, 31) (310,) unique(labels): [118 192] [0 1]\n",
            "val set: (992, 31) (992,)\n",
            "\n",
            "Train set: (310, 31) y: (310,)\n",
            "Val   set: (992, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.645161 \n",
            "Classification report for classifier LogisticRegression(C=0.16129032258064516, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.44      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (992,) [0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0]\n",
            "probabilities: (992, 2) \n",
            " [0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1\n",
            " 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
            " 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0\n",
            " 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
            " 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
            " 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0\n",
            " 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 0 0]\n",
            "trainset before (310, 31) (310,)\n",
            "trainset after (320, 31) (320,)\n",
            "updated train set: (320, 31) (320,) unique(labels): [122 198] [0 1]\n",
            "val set: (982, 31) (982,)\n",
            "\n",
            "Train set: (320, 31) y: (320,)\n",
            "Val   set: (982, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.006 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.15625, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.95      0.88       321\n",
            "           1       0.73      0.38      0.50       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.77      0.67      0.69       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 70  43]]\n",
            "--------------------------------\n",
            "val predicted: (982,) [0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0]\n",
            "probabilities: (982, 2) \n",
            " [0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 0 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
            " 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1\n",
            " 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0\n",
            " 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0\n",
            " 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1\n",
            " 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1\n",
            " 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0]\n",
            "trainset before (320, 31) (320,)\n",
            "trainset after (330, 31) (330,)\n",
            "updated train set: (330, 31) (330,) unique(labels): [126 204] [0 1]\n",
            "val set: (972, 31) (972,)\n",
            "\n",
            "Train set: (330, 31) y: (330,)\n",
            "Val   set: (972, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 80.184332 \n",
            "Classification report for classifier LogisticRegression(C=0.15151515151515152, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.70      0.42      0.53       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (972,) [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0]\n",
            "probabilities: (972, 2) \n",
            " [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0\n",
            " 0 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1\n",
            " 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0\n",
            " 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 1 0 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1\n",
            " 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0\n",
            " 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0\n",
            " 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0\n",
            " 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1\n",
            " 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1\n",
            " 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0\n",
            " 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0]\n",
            "trainset before (330, 31) (330,)\n",
            "trainset after (340, 31) (340,)\n",
            "updated train set: (340, 31) (340,) unique(labels): [129 211] [0 1]\n",
            "val set: (962, 31) (962,)\n",
            "\n",
            "Train set: (340, 31) y: (340,)\n",
            "Val   set: (962, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.010 s \n",
            "\n",
            "Accuracy rate for 80.414747 \n",
            "Classification report for classifier LogisticRegression(C=0.14705882352941177, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.88       321\n",
            "           1       0.70      0.43      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.68      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (962,) [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0]\n",
            "probabilities: (962, 2) \n",
            " [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 1 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1\n",
            " 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1\n",
            " 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0\n",
            " 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1\n",
            " 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0]\n",
            "trainset before (340, 31) (340,)\n",
            "trainset after (350, 31) (350,)\n",
            "updated train set: (350, 31) (350,) unique(labels): [132 218] [0 1]\n",
            "val set: (952, 31) (952,)\n",
            "\n",
            "Train set: (350, 31) y: (350,)\n",
            "Val   set: (952, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 79.953917 \n",
            "Classification report for classifier LogisticRegression(C=0.14285714285714285, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       321\n",
            "           1       0.69      0.42      0.52       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.68      0.70       434\n",
            "weighted avg       0.79      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (952,) [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (952, 2) \n",
            " [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 1\n",
            " 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1\n",
            " 1 1 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0\n",
            " 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0\n",
            " 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1\n",
            " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (350, 31) (350,)\n",
            "trainset after (360, 31) (360,)\n",
            "updated train set: (360, 31) (360,) unique(labels): [137 223] [0 1]\n",
            "val set: (942, 31) (942,)\n",
            "\n",
            "Train set: (360, 31) y: (360,)\n",
            "Val   set: (942, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 82.488479 \n",
            "Classification report for classifier LogisticRegression(C=0.1388888888888889, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89       321\n",
            "           1       0.79      0.44      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.81      0.70      0.73       434\n",
            "weighted avg       0.82      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[308  13]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (942,) [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (942, 2) \n",
            " [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0\n",
            " 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0\n",
            " 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0\n",
            " 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0\n",
            " 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
            " 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
            " 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (360, 31) (360,)\n",
            "trainset after (370, 31) (370,)\n",
            "updated train set: (370, 31) (370,) unique(labels): [140 230] [0 1]\n",
            "val set: (932, 31) (932,)\n",
            "\n",
            "Train set: (370, 31) y: (370,)\n",
            "Val   set: (932, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 82.258065 \n",
            "Classification report for classifier LogisticRegression(C=0.13513513513513514, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89       321\n",
            "           1       0.76      0.46      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.80      0.71      0.73       434\n",
            "weighted avg       0.82      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (932,) [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0]\n",
            "probabilities: (932, 2) \n",
            " [0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0\n",
            " 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0\n",
            " 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1\n",
            " 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0\n",
            " 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0\n",
            " 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1\n",
            " 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0\n",
            " 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1\n",
            " 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1\n",
            " 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1\n",
            " 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
            " 1 0 1 0 0 0 0]\n",
            "trainset before (370, 31) (370,)\n",
            "trainset after (380, 31) (380,)\n",
            "updated train set: (380, 31) (380,) unique(labels): [145 235] [0 1]\n",
            "val set: (922, 31) (922,)\n",
            "\n",
            "Train set: (380, 31) y: (380,)\n",
            "Val   set: (922, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 82.027650 \n",
            "Classification report for classifier LogisticRegression(C=0.13157894736842105, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89       321\n",
            "           1       0.76      0.45      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.80      0.70      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (922,) [0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (922, 2) \n",
            " [0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1\n",
            " 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0\n",
            " 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0\n",
            " 0 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1\n",
            " 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0\n",
            " 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1\n",
            " 1 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0\n",
            " 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0\n",
            " 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (380, 31) (380,)\n",
            "trainset after (390, 31) (390,)\n",
            "updated train set: (390, 31) (390,) unique(labels): [147 243] [0 1]\n",
            "val set: (912, 31) (912,)\n",
            "\n",
            "Train set: (390, 31) y: (390,)\n",
            "Val   set: (912, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.1282051282051282, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88       321\n",
            "           1       0.76      0.42      0.55       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.69      0.71       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[306  15]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "val predicted: (912,) [0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (912, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0\n",
            " 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1\n",
            " 0 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0\n",
            " 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1\n",
            " 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0\n",
            " 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0\n",
            " 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1\n",
            " 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0\n",
            " 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
            " 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (390, 31) (390,)\n",
            "trainset after (400, 31) (400,)\n",
            "updated train set: (400, 31) (400,) unique(labels): [149 251] [0 1]\n",
            "val set: (902, 31) (902,)\n",
            "\n",
            "Train set: (400, 31) y: (400,)\n",
            "Val   set: (902, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.125, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88       321\n",
            "           1       0.75      0.44      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.69      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (902,) [0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (902, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
            " 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1\n",
            " 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1\n",
            " 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (400, 31) (400,)\n",
            "trainset after (410, 31) (410,)\n",
            "updated train set: (410, 31) (410,) unique(labels): [151 259] [0 1]\n",
            "val set: (892, 31) (892,)\n",
            "\n",
            "Train set: (410, 31) y: (410,)\n",
            "Val   set: (892, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.797235 \n",
            "Classification report for classifier LogisticRegression(C=0.12195121951219512, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89       321\n",
            "           1       0.77      0.43      0.55       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.80      0.69      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[306  15]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (892,) [0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0]\n",
            "probabilities: (892, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0\n",
            " 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0\n",
            " 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 0 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0\n",
            " 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0\n",
            " 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0\n",
            " 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1\n",
            " 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1\n",
            " 0 1 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0\n",
            " 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0\n",
            " 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0\n",
            " 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1\n",
            " 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
            " 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0\n",
            " 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1\n",
            " 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 0 0 0]\n",
            "trainset before (410, 31) (410,)\n",
            "trainset after (420, 31) (420,)\n",
            "updated train set: (420, 31) (420,) unique(labels): [157 263] [0 1]\n",
            "val set: (882, 31) (882,)\n",
            "\n",
            "Train set: (420, 31) y: (420,)\n",
            "Val   set: (882, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.008 s \n",
            "\n",
            "Accuracy rate for 81.105991 \n",
            "Classification report for classifier LogisticRegression(C=0.11904761904761904, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.72      0.44      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (882,) [0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (882, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0\n",
            " 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n",
            " 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
            " 1 1 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 1\n",
            " 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
            " 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0\n",
            " 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
            " 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1\n",
            " 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0\n",
            " 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1\n",
            " 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
            " 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1\n",
            " 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1\n",
            " 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1\n",
            " 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0\n",
            " 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1\n",
            " 0 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0\n",
            " 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (420, 31) (420,)\n",
            "trainset after (430, 31) (430,)\n",
            "updated train set: (430, 31) (430,) unique(labels): [161 269] [0 1]\n",
            "val set: (872, 31) (872,)\n",
            "\n",
            "Train set: (430, 31) y: (430,)\n",
            "Val   set: (872, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.11627906976744186, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88       321\n",
            "           1       0.75      0.43      0.55       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.69      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (872,) [0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (872, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0\n",
            " 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0\n",
            " 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
            " 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0\n",
            " 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
            " 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1\n",
            " 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (430, 31) (430,)\n",
            "trainset after (440, 31) (440,)\n",
            "updated train set: (440, 31) (440,) unique(labels): [168 272] [0 1]\n",
            "val set: (862, 31) (862,)\n",
            "\n",
            "Train set: (440, 31) y: (440,)\n",
            "Val   set: (862, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.012 s \n",
            "\n",
            "Accuracy rate for 82.027650 \n",
            "Classification report for classifier LogisticRegression(C=0.11363636363636363, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.96      0.89       321\n",
            "           1       0.78      0.43      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.80      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[307  14]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "val predicted: (862,) [0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (862, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 1\n",
            " 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1\n",
            " 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0\n",
            " 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0\n",
            " 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1\n",
            " 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1\n",
            " 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1\n",
            " 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (440, 31) (440,)\n",
            "trainset after (450, 31) (450,)\n",
            "updated train set: (450, 31) (450,) unique(labels): [170 280] [0 1]\n",
            "val set: (852, 31) (852,)\n",
            "\n",
            "Train set: (450, 31) y: (450,)\n",
            "Val   set: (852, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 82.027650 \n",
            "Classification report for classifier LogisticRegression(C=0.1111111111111111, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89       321\n",
            "           1       0.77      0.44      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.80      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[306  15]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "val predicted: (852,) [0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "probabilities: (852, 2) \n",
            " [0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1\n",
            " 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1\n",
            " 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0\n",
            " 0 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0\n",
            " 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1\n",
            " 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1\n",
            " 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
            " 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0\n",
            " 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1\n",
            " 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0\n",
            " 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1\n",
            " 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0\n",
            " 0]\n",
            "trainset before (450, 31) (450,)\n",
            "trainset after (460, 31) (460,)\n",
            "updated train set: (460, 31) (460,) unique(labels): [171 289] [0 1]\n",
            "val set: (842, 31) (842,)\n",
            "\n",
            "Train set: (460, 31) y: (460,)\n",
            "Val   set: (842, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 82.027650 \n",
            "Classification report for classifier LogisticRegression(C=0.10869565217391304, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89       321\n",
            "           1       0.75      0.46      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.70      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[304  17]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "val predicted: (842,) [0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (842, 2) \n",
            " [0 1 1 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
            " 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
            " 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
            " 1 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0\n",
            " 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
            " 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1\n",
            " 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1\n",
            " 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1\n",
            " 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1\n",
            " 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1\n",
            " 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
            " 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1\n",
            " 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0\n",
            " 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
            " 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (460, 31) (460,)\n",
            "trainset after (470, 31) (470,)\n",
            "updated train set: (470, 31) (470,) unique(labels): [179 291] [0 1]\n",
            "val set: (832, 31) (832,)\n",
            "\n",
            "Train set: (470, 31) y: (470,)\n",
            "Val   set: (832, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.10638297872340426, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.74      0.45      0.56       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (832,) [0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (832, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
            " 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1\n",
            " 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1\n",
            " 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1\n",
            " 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
            " 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
            " 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1\n",
            " 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0\n",
            " 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1\n",
            " 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (470, 31) (470,)\n",
            "trainset after (480, 31) (480,)\n",
            "updated train set: (480, 31) (480,) unique(labels): [181 299] [0 1]\n",
            "val set: (822, 31) (822,)\n",
            "\n",
            "Train set: (480, 31) y: (480,)\n",
            "Val   set: (822, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.009 s \n",
            "\n",
            "Accuracy rate for 82.027650 \n",
            "Classification report for classifier LogisticRegression(C=0.10416666666666667, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89       321\n",
            "           1       0.76      0.45      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.80      0.70      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (822,) [0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0]\n",
            "probabilities: (822, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0\n",
            " 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1\n",
            " 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1\n",
            " 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1\n",
            " 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1\n",
            " 1 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0\n",
            " 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0\n",
            " 0 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1\n",
            " 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0\n",
            " 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0\n",
            " 0 1 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
            " 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1\n",
            " 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0\n",
            " 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1\n",
            " 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0\n",
            " 0 1 0 1 0 0 0 0]\n",
            "trainset before (480, 31) (480,)\n",
            "trainset after (490, 31) (490,)\n",
            "updated train set: (490, 31) (490,) unique(labels): [187 303] [0 1]\n",
            "val set: (812, 31) (812,)\n",
            "\n",
            "Train set: (490, 31) y: (490,)\n",
            "Val   set: (812, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.336406 \n",
            "Classification report for classifier LogisticRegression(C=0.10204081632653061, class_weight='balanced',\n",
            "                   multi_class='multinomial', penalty='l1', solver='saga',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.45      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "val predicted: (812,) [0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "probabilities: (812, 2) \n",
            " [0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0\n",
            " 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1\n",
            " 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 1\n",
            " 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0\n",
            " 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0\n",
            " 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1\n",
            " 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0\n",
            " 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1\n",
            " 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0\n",
            " 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0\n",
            " 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0\n",
            " 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1\n",
            " 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0\n",
            " 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0]\n",
            "trainset before (490, 31) (490,)\n",
            "trainset after (500, 31) (500,)\n",
            "updated train set: (500, 31) (500,) unique(labels): [197 303] [0 1]\n",
            "val set: (802, 31) (802,)\n",
            "\n",
            "Train set: (500, 31) y: (500,)\n",
            "Val   set: (802, 31)\n",
            "Test  set: (434, 31)\n",
            "training multinomial logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Example run in 0.007 s \n",
            "\n",
            "Accuracy rate for 81.566820 \n",
            "Classification report for classifier LogisticRegression(C=0.1, class_weight='balanced', multi_class='multinomial',\n",
            "                   penalty='l1', solver='saga', tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.88       321\n",
            "           1       0.75      0.43      0.55       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.69      0.72       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[305  16]\n",
            " [ 64  49]]\n",
            "--------------------------------\n",
            "final active learning accuracies [64.74654377880185, 72.58064516129032, 62.903225806451616, 65.89861751152074, 67.05069124423963, 76.72811059907833, 76.72811059907833, 77.88018433179722, 77.88018433179722, 77.41935483870968, 78.11059907834101, 77.18894009216591, 77.41935483870968, 76.49769585253456, 77.64976958525345, 75.80645161290323, 76.95852534562212, 75.80645161290323, 75.57603686635944, 77.64976958525345, 77.18894009216591, 75.34562211981567, 77.88018433179722, 78.3410138248848, 78.11059907834101, 77.64976958525345, 79.26267281105991, 78.11059907834101, 77.41935483870968, 79.49308755760369, 80.64516129032258, 80.18433179723502, 80.18433179723502, 80.4147465437788, 79.95391705069125, 82.48847926267281, 82.25806451612904, 82.02764976958525, 81.5668202764977, 81.5668202764977, 81.79723502304147, 81.10599078341014, 81.5668202764977, 82.02764976958525, 82.02764976958525, 82.02764976958525, 81.5668202764977, 82.02764976958525, 81.33640552995391, 81.5668202764977]\n",
            "saved Active-learning-experiment-45.pkl /Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset ['Active-learning-experiment-13.pkl', 'Decision_tree.ipynb', 'Active-learning-experiment-12.pkl', 'Logit_default_f10.pdf', 'Logistic.ipynb', 'Active-learning-experiment-38.pkl', 'Active-learning-experiment-10.pkl', 'SVC.ipynb', 'RF_f5e50_featureimp.pdf', 'Active-learning-experiment-11.pkl', 'Active-learning-experiment-39.pkl', 'Active-learning-experiment-15.pkl', 'Active-learning-experiment-29.pkl', 'Active-learning-experiment-28.pkl', 'Active-learning-experiment-14.pkl', 'Active-learning-experiment-16.pkl', 'Active-learning-experiment-17.pkl', 'Logistic_Scikit.ipynb', 'Active-learning-experiment-8.pkl', 'Active-learning-experiment-9.pkl', 'Best classifier_RF_f5e50.pdf', 'KNN.ipynb', 'GBDC.ipynb', 'README.md', 'Active-learning-experiment-45.pkl', 'all_training.csv', 'Active-learning-experiment-2.pkl', 'Active-learning-experiment-3.pkl', 'Active-learning-experiment-44.pkl', 'Active-learning-experiment-1.pkl', 'Log_ROC.png', 'Active-learning-experiment-43.pkl', 'Active-learning-experiment-4.pkl', 'Active-learning-experiment-5.pkl', 'Active-learning-experiment-42.pkl', 'Logit_default_f7(p_removal).pdf', 'Active-learning-experiment-40.pkl', 'Active_learning.ipynb', 'Active-learning-experiment-7.pkl', 'Active-learning-experiment-6.pkl', 'Active-learning-experiment-41.pkl', 'Random_forest.ipynb', 'Active-learning-experiment-32.pkl', 'Active-learning-experiment-26.pkl', 'Model_select.ipynb', 'Active-learning-experiment-27.pkl', 'Active-learning-experiment-33.pkl', 'Active-learning-experiment-19.pkl', 'Active-learning-experiment-25.pkl', 'Active-learning-experiment-31.pkl', '.git', '.vscode', 'Active-learning-experiment-30.pkl', 'Active-learning-experiment-24.pkl', 'Active-learning-experiment-18.pkl', 'RF_f5e50_modelselect.pdf', 'Active-learning-experiment-20.pkl', 'Active-learning-experiment-34.pkl', 'Active-learning-experiment-35.pkl', 'Active-learning-experiment-21.pkl', 'Active-learning-experiment-37.pkl', 'Active-learning-experiment-23.pkl', 'Logit_default_f8(std_removal).pdf', 'Active-learning-experiment-22.pkl', 'Active-learning-experiment-36.pkl']\n",
            "{\n",
            "  \"LogModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          64.74654377880185,\n",
            "          72.58064516129032,\n",
            "          62.903225806451616,\n",
            "          65.89861751152074,\n",
            "          67.05069124423963,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          77.41935483870968,\n",
            "          78.11059907834101,\n",
            "          77.18894009216591,\n",
            "          77.41935483870968,\n",
            "          76.49769585253456,\n",
            "          77.64976958525345,\n",
            "          75.80645161290323,\n",
            "          76.95852534562212,\n",
            "          75.80645161290323,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.34562211981567,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          82.48847926267281,\n",
            "          82.25806451612904,\n",
            "          82.02764976958525,\n",
            "          81.5668202764977,\n",
            "          81.5668202764977,\n",
            "          81.79723502304147,\n",
            "          81.10599078341014,\n",
            "          81.5668202764977,\n",
            "          82.02764976958525,\n",
            "          82.02764976958525,\n",
            "          82.02764976958525,\n",
            "          81.5668202764977,\n",
            "          82.02764976958525,\n",
            "          81.33640552995391,\n",
            "          81.5668202764977\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          80.64516129032258,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          76.95852534562212,\n",
            "          75.80645161290323,\n",
            "          78.11059907834101,\n",
            "          77.18894009216591,\n",
            "          77.18894009216591,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.87557603686636,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          79.49308755760369,\n",
            "          76.72811059907833,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          40.55299539170507,\n",
            "          53.686635944700456,\n",
            "          71.19815668202764,\n",
            "          76.72811059907833,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          80.4147465437788,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014,\n",
            "          81.10599078341014,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          81.79723502304147,\n",
            "          80.87557603686636,\n",
            "          80.64516129032258,\n",
            "          79.03225806451613,\n",
            "          81.10599078341014,\n",
            "          80.64516129032258,\n",
            "          81.5668202764977,\n",
            "          75.57603686635944,\n",
            "          81.5668202764977,\n",
            "          79.49308755760369,\n",
            "          81.33640552995391,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.34562211981567,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.6589861751152,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078,\n",
            "          76.49769585253456,\n",
            "          77.88018433179722,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636,\n",
            "          81.5668202764977,\n",
            "          82.25806451612904,\n",
            "          81.33640552995391,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          81.79723502304147,\n",
            "          79.26267281105991,\n",
            "          82.02764976958525,\n",
            "          81.33640552995391,\n",
            "          82.02764976958525,\n",
            "          80.87557603686636,\n",
            "          82.25806451612904,\n",
            "          82.02764976958525\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          80.64516129032258\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          69.12442396313364,\n",
            "          75.11520737327189,\n",
            "          75.34562211981567,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          70.50691244239631,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          76.72811059907833,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          76.95852534562212,\n",
            "          75.57603686635944,\n",
            "          76.49769585253456,\n",
            "          76.036866359447,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          74.88479262672811,\n",
            "          75.34562211981567,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          75.34562211981567,\n",
            "          74.88479262672811,\n",
            "          74.65437788018433,\n",
            "          74.65437788018433,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          75.57603686635944,\n",
            "          74.42396313364056,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          75.11520737327189,\n",
            "          74.88479262672811,\n",
            "          74.88479262672811,\n",
            "          75.11520737327189,\n",
            "          75.34562211981567,\n",
            "          76.49769585253456,\n",
            "          75.11520737327189,\n",
            "          74.65437788018433,\n",
            "          75.80645161290323,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          75.34562211981567,\n",
            "          75.57603686635944,\n",
            "          75.34562211981567\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          76.72811059907833,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          76.49769585253456,\n",
            "          75.57603686635944,\n",
            "          76.49769585253456,\n",
            "          74.88479262672811,\n",
            "          76.036866359447,\n",
            "          76.49769585253456,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          76.036866359447,\n",
            "          76.26728110599078,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          75.57603686635944,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          66.58986175115207,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          76.26728110599078,\n",
            "          75.57603686635944,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          76.26728110599078,\n",
            "          75.34562211981567,\n",
            "          76.49769585253456\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"RfModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          54.14746543778802,\n",
            "          69.12442396313364,\n",
            "          67.51152073732719,\n",
            "          70.73732718894009,\n",
            "          76.95852534562212,\n",
            "          71.42857142857143,\n",
            "          75.34562211981567,\n",
            "          77.88018433179722,\n",
            "          76.26728110599078,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          78.80184331797236,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          80.64516129032258,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          80.18433179723502,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.87557603686636\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.19354838709677,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          77.41935483870968,\n",
            "          67.51152073732719,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          77.88018433179722,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          80.18433179723502,\n",
            "          80.4147465437788,\n",
            "          79.03225806451613,\n",
            "          80.64516129032258,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.95852534562212,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          30.184331797235025,\n",
            "          78.80184331797236,\n",
            "          76.49769585253456,\n",
            "          70.50691244239631,\n",
            "          77.41935483870968,\n",
            "          76.72811059907833,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          80.18433179723502,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          80.87557603686636,\n",
            "          79.95391705069125,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          76.036866359447,\n",
            "          77.64976958525345,\n",
            "          77.41935483870968,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          80.87557603686636,\n",
            "          79.26267281105991,\n",
            "          80.4147465437788,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.72350230414746,\n",
            "          78.80184331797236,\n",
            "          79.26267281105991,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          80.18433179723502,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          72.81105990783409,\n",
            "          73.27188940092167,\n",
            "          75.80645161290323,\n",
            "          76.26728110599078,\n",
            "          79.26267281105991,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.95852534562212,\n",
            "          79.26267281105991,\n",
            "          78.3410138248848,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          80.18433179723502,\n",
            "          80.64516129032258,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          81.10599078341014\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          78.11059907834101,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          68.4331797235023,\n",
            "          76.26728110599078,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          79.72350230414746,\n",
            "          79.03225806451613,\n",
            "          80.18433179723502\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          52.07373271889401,\n",
            "          39.63133640552996,\n",
            "          52.764976958525345,\n",
            "          51.61290322580645,\n",
            "          66.12903225806451,\n",
            "          70.04608294930875,\n",
            "          68.66359447004609,\n",
            "          75.11520737327189,\n",
            "          76.036866359447,\n",
            "          77.18894009216591,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          77.18894009216591,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          80.4147465437788,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.49769585253456,\n",
            "          78.3410138248848,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          77.18894009216591,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          58.75576036866359,\n",
            "          74.65437788018433,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          74.65437788018433,\n",
            "          75.57603686635944,\n",
            "          78.3410138248848,\n",
            "          77.64976958525345,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          75.11520737327189,\n",
            "          76.49769585253456,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          55.76036866359447,\n",
            "          71.6589861751152,\n",
            "          73.04147465437788,\n",
            "          74.65437788018433,\n",
            "          77.18894009216591,\n",
            "          77.88018433179722,\n",
            "          76.49769585253456,\n",
            "          77.41935483870968,\n",
            "          79.26267281105991,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  },\n",
            "  \"SvmModel\": {\n",
            "    \"EntropySelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          72.35023041474655,\n",
            "          73.73271889400922,\n",
            "          72.35023041474655,\n",
            "          72.58064516129032,\n",
            "          73.04147465437788,\n",
            "          74.88479262672811,\n",
            "          75.80645161290323,\n",
            "          75.34562211981567,\n",
            "          76.72811059907833,\n",
            "          76.72811059907833,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          79.72350230414746,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.64976958525345,\n",
            "          78.3410138248848,\n",
            "          78.57142857142857,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          78.57142857142857,\n",
            "          78.57142857142857,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613,\n",
            "          78.80184331797236,\n",
            "          78.80184331797236,\n",
            "          79.03225806451613,\n",
            "          79.26267281105991,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          74.42396313364056,\n",
            "          72.11981566820278,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          75.11520737327189,\n",
            "          67.97235023041475,\n",
            "          76.95852534562212,\n",
            "          72.11981566820278,\n",
            "          76.26728110599078,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991,\n",
            "          79.26267281105991,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          77.88018433179722,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          72.11981566820278,\n",
            "          74.65437788018433,\n",
            "          76.036866359447,\n",
            "          77.41935483870968,\n",
            "          77.88018433179722,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.95391705069125,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"MarginSamplingSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          61.75115207373272,\n",
            "          68.89400921658986,\n",
            "          65.89861751152074,\n",
            "          66.3594470046083,\n",
            "          69.81566820276498,\n",
            "          74.88479262672811,\n",
            "          69.35483870967742,\n",
            "          71.42857142857143,\n",
            "          71.42857142857143,\n",
            "          76.72811059907833,\n",
            "          72.58064516129032,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.95391705069125,\n",
            "          79.95391705069125,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.72350230414746,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746,\n",
            "          79.95391705069125,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          80.4147465437788,\n",
            "          79.95391705069125,\n",
            "          80.18433179723502,\n",
            "          79.26267281105991,\n",
            "          79.49308755760369,\n",
            "          79.72350230414746\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          79.03225806451613\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          71.19815668202764,\n",
            "          71.19815668202764,\n",
            "          71.42857142857143,\n",
            "          70.96774193548387,\n",
            "          66.58986175115207,\n",
            "          67.74193548387096,\n",
            "          69.12442396313364,\n",
            "          69.35483870967742,\n",
            "          69.5852534562212,\n",
            "          72.11981566820278,\n",
            "          72.11981566820278,\n",
            "          78.11059907834101,\n",
            "          78.80184331797236,\n",
            "          79.49308755760369,\n",
            "          79.49308755760369,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          76.036866359447,\n",
            "          78.80184331797236\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          62.21198156682027,\n",
            "          65.43778801843318,\n",
            "          65.43778801843318,\n",
            "          70.04608294930875,\n",
            "          70.96774193548387,\n",
            "          72.81105990783409,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          79.26267281105991\n",
            "        ]\n",
            "      ]\n",
            "    },\n",
            "    \"RandomSelection\": {\n",
            "      \"10\": [\n",
            "        [\n",
            "          67.2811059907834,\n",
            "          67.97235023041475,\n",
            "          69.81566820276498,\n",
            "          74.19354838709677,\n",
            "          70.73732718894009,\n",
            "          70.73732718894009,\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          72.11981566820278,\n",
            "          75.11520737327189,\n",
            "          75.57603686635944,\n",
            "          73.50230414746544,\n",
            "          75.80645161290323,\n",
            "          77.88018433179722,\n",
            "          75.80645161290323,\n",
            "          77.64976958525345,\n",
            "          77.18894009216591,\n",
            "          75.57603686635944,\n",
            "          77.64976958525345,\n",
            "          75.57603686635944,\n",
            "          78.57142857142857,\n",
            "          77.64976958525345,\n",
            "          78.57142857142857,\n",
            "          78.80184331797236,\n",
            "          78.57142857142857,\n",
            "          79.26267281105991,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          79.03225806451613,\n",
            "          79.03225806451613,\n",
            "          78.57142857142857,\n",
            "          79.49308755760369,\n",
            "          78.80184331797236,\n",
            "          77.88018433179722,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.41935483870968,\n",
            "          77.64976958525345,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          77.88018433179722,\n",
            "          78.11059907834101,\n",
            "          78.11059907834101,\n",
            "          78.57142857142857,\n",
            "          78.11059907834101,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ],\n",
            "      \"125\": [\n",
            "        [\n",
            "          73.963133640553,\n",
            "          77.64976958525345,\n",
            "          79.26267281105991,\n",
            "          78.11059907834101\n",
            "        ]\n",
            "      ],\n",
            "      \"25\": [\n",
            "        [\n",
            "          73.73271889400922,\n",
            "          77.18894009216591,\n",
            "          73.50230414746544,\n",
            "          72.11981566820278,\n",
            "          76.72811059907833,\n",
            "          69.81566820276498,\n",
            "          70.50691244239631,\n",
            "          66.82027649769586,\n",
            "          68.66359447004609,\n",
            "          76.49769585253456,\n",
            "          76.72811059907833,\n",
            "          76.26728110599078,\n",
            "          76.036866359447,\n",
            "          76.95852534562212,\n",
            "          77.64976958525345,\n",
            "          77.88018433179722,\n",
            "          78.3410138248848,\n",
            "          78.11059907834101,\n",
            "          79.03225806451613,\n",
            "          79.49308755760369\n",
            "        ]\n",
            "      ],\n",
            "      \"250\": [\n",
            "        [\n",
            "          75.57603686635944,\n",
            "          77.64976958525345\n",
            "        ]\n",
            "      ],\n",
            "      \"50\": [\n",
            "        [\n",
            "          67.74193548387096,\n",
            "          62.67281105990783,\n",
            "          72.58064516129032,\n",
            "          76.26728110599078,\n",
            "          76.95852534562212,\n",
            "          79.03225806451613,\n",
            "          77.64976958525345,\n",
            "          76.95852534562212,\n",
            "          76.95852534562212,\n",
            "          77.41935483870968\n",
            "        ]\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "---------------------------- FINISHED ---------------------------\n",
            "\n",
            "{'SvmModel': {'RandomSelection': {'250': [[75.57603686635944, 77.64976958525345]], '125': [[73.963133640553, 77.64976958525345, 79.26267281105991, 78.11059907834101]], '50': [[67.74193548387096, 62.67281105990783, 72.58064516129032, 76.26728110599078, 76.95852534562212, 79.03225806451613, 77.64976958525345, 76.95852534562212, 76.95852534562212, 77.41935483870968]], '25': [[73.73271889400922, 77.18894009216591, 73.50230414746544, 72.11981566820278, 76.72811059907833, 69.81566820276498, 70.50691244239631, 66.82027649769586, 68.66359447004609, 76.49769585253456, 76.72811059907833, 76.26728110599078, 76.036866359447, 76.95852534562212, 77.64976958525345, 77.88018433179722, 78.3410138248848, 78.11059907834101, 79.03225806451613, 79.49308755760369]], '10': [[67.2811059907834, 67.97235023041475, 69.81566820276498, 74.19354838709677, 70.73732718894009, 70.73732718894009, 73.963133640553, 77.64976958525345, 75.57603686635944, 72.11981566820278, 75.11520737327189, 75.57603686635944, 73.50230414746544, 75.80645161290323, 77.88018433179722, 75.80645161290323, 77.64976958525345, 77.18894009216591, 75.57603686635944, 77.64976958525345, 75.57603686635944, 78.57142857142857, 77.64976958525345, 78.57142857142857, 78.80184331797236, 78.57142857142857, 79.26267281105991, 78.80184331797236, 77.88018433179722, 79.03225806451613, 79.03225806451613, 78.57142857142857, 79.49308755760369, 78.80184331797236, 77.88018433179722, 77.64976958525345, 78.11059907834101, 77.41935483870968, 77.41935483870968, 77.41935483870968, 77.64976958525345, 78.11059907834101, 78.11059907834101, 78.11059907834101, 77.88018433179722, 78.11059907834101, 78.11059907834101, 78.57142857142857, 78.11059907834101, 77.41935483870968]]}, 'MarginSamplingSelection': {'250': [[76.036866359447, 78.80184331797236]], '125': [[75.80645161290323, 77.88018433179722, 78.3410138248848, 79.03225806451613]], '50': [[62.21198156682027, 65.43778801843318, 65.43778801843318, 70.04608294930875, 70.96774193548387, 72.81105990783409, 77.64976958525345, 78.57142857142857, 79.49308755760369, 79.26267281105991]], '25': [[71.19815668202764, 71.19815668202764, 71.42857142857143, 70.96774193548387, 66.58986175115207, 67.74193548387096, 69.12442396313364, 69.35483870967742, 69.5852534562212, 72.11981566820278, 72.11981566820278, 78.11059907834101, 78.80184331797236, 79.49308755760369, 79.49308755760369, 78.3410138248848, 78.11059907834101, 77.88018433179722, 77.88018433179722, 78.11059907834101]], '10': [[61.75115207373272, 68.89400921658986, 65.89861751152074, 66.3594470046083, 69.81566820276498, 74.88479262672811, 69.35483870967742, 71.42857142857143, 71.42857142857143, 76.72811059907833, 72.58064516129032, 79.72350230414746, 79.72350230414746, 78.11059907834101, 79.03225806451613, 78.3410138248848, 78.3410138248848, 78.11059907834101, 78.3410138248848, 78.3410138248848, 78.3410138248848, 79.03225806451613, 79.49308755760369, 79.72350230414746, 80.4147465437788, 79.95391705069125, 79.95391705069125, 80.18433179723502, 79.95391705069125, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.49308755760369, 79.49308755760369, 79.49308755760369, 79.49308755760369, 79.72350230414746, 79.95391705069125, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.4147465437788, 79.95391705069125, 80.18433179723502, 79.26267281105991, 79.49308755760369, 79.72350230414746]]}, 'EntropySelection': {'250': [[77.88018433179722, 79.72350230414746]], '125': [[74.42396313364056, 72.11981566820278, 79.03225806451613, 78.57142857142857]], '50': [[72.11981566820278, 74.65437788018433, 76.036866359447, 77.41935483870968, 77.88018433179722, 78.57142857142857, 79.49308755760369, 79.49308755760369, 79.95391705069125, 79.49308755760369]], '25': [[75.11520737327189, 67.97235023041475, 76.95852534562212, 72.11981566820278, 76.26728110599078, 79.03225806451613, 78.3410138248848, 79.03225806451613, 78.3410138248848, 78.11059907834101, 79.49308755760369, 79.26267281105991, 79.26267281105991, 79.95391705069125, 79.49308755760369, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.49308755760369]], '10': [[72.35023041474655, 73.73271889400922, 72.35023041474655, 72.58064516129032, 73.04147465437788, 74.88479262672811, 75.80645161290323, 75.34562211981567, 76.72811059907833, 76.72811059907833, 78.11059907834101, 78.57142857142857, 77.64976958525345, 79.72350230414746, 78.57142857142857, 78.11059907834101, 77.64976958525345, 78.3410138248848, 78.57142857142857, 79.95391705069125, 79.72350230414746, 80.4147465437788, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.72350230414746, 79.49308755760369, 79.72350230414746, 79.49308755760369, 79.49308755760369, 79.26267281105991, 79.49308755760369, 78.57142857142857, 78.57142857142857, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.26267281105991, 79.26267281105991, 78.80184331797236, 79.03225806451613, 79.03225806451613, 79.49308755760369, 79.26267281105991, 79.03225806451613, 78.80184331797236, 78.80184331797236, 79.03225806451613, 79.26267281105991, 79.03225806451613]]}}, 'RfModel': {'RandomSelection': {'250': [[73.73271889400922, 77.41935483870968]], '125': [[77.18894009216591, 79.49308755760369, 79.26267281105991, 79.26267281105991]], '50': [[55.76036866359447, 71.6589861751152, 73.04147465437788, 74.65437788018433, 77.18894009216591, 77.88018433179722, 76.49769585253456, 77.41935483870968, 79.26267281105991, 78.57142857142857]], '25': [[58.75576036866359, 74.65437788018433, 75.11520737327189, 76.49769585253456, 74.65437788018433, 75.57603686635944, 78.3410138248848, 77.64976958525345, 76.26728110599078, 76.95852534562212, 75.11520737327189, 76.49769585253456, 78.57142857142857, 78.80184331797236, 77.88018433179722, 76.49769585253456, 78.11059907834101, 78.57142857142857, 77.88018433179722, 77.88018433179722]], '10': [[52.07373271889401, 39.63133640552996, 52.764976958525345, 51.61290322580645, 66.12903225806451, 70.04608294930875, 68.66359447004609, 75.11520737327189, 76.036866359447, 77.18894009216591, 78.3410138248848, 77.64976958525345, 78.3410138248848, 76.49769585253456, 76.72811059907833, 76.72811059907833, 77.18894009216591, 78.11059907834101, 79.49308755760369, 79.49308755760369, 79.03225806451613, 79.49308755760369, 79.49308755760369, 78.3410138248848, 80.4147465437788, 78.80184331797236, 77.88018433179722, 77.64976958525345, 77.18894009216591, 77.64976958525345, 76.95852534562212, 76.49769585253456, 78.3410138248848, 79.49308755760369, 78.57142857142857, 79.26267281105991, 78.57142857142857, 79.03225806451613, 78.11059907834101, 78.3410138248848, 79.26267281105991, 79.03225806451613, 79.03225806451613, 78.57142857142857, 78.80184331797236, 79.49308755760369, 78.11059907834101, 78.57142857142857, 78.80184331797236, 79.49308755760369]]}, 'MarginSamplingSelection': {'250': [[78.11059907834101, 78.57142857142857]], '125': [[78.11059907834101, 78.11059907834101, 79.49308755760369, 79.72350230414746]], '50': [[68.4331797235023, 76.26728110599078, 80.4147465437788, 78.80184331797236, 79.49308755760369, 79.95391705069125, 80.4147465437788, 79.72350230414746, 79.03225806451613, 80.18433179723502]], '25': [[72.81105990783409, 73.27188940092167, 75.80645161290323, 76.26728110599078, 79.26267281105991, 77.64976958525345, 78.11059907834101, 77.88018433179722, 77.18894009216591, 77.88018433179722, 76.95852534562212, 79.26267281105991, 78.3410138248848, 79.72350230414746, 80.4147465437788, 80.18433179723502, 80.64516129032258, 80.4147465437788, 80.4147465437788, 81.10599078341014]], '10': [[30.184331797235025, 78.80184331797236, 76.49769585253456, 70.50691244239631, 77.41935483870968, 76.72811059907833, 78.3410138248848, 79.49308755760369, 78.80184331797236, 79.95391705069125, 79.49308755760369, 80.18433179723502, 78.57142857142857, 79.03225806451613, 80.87557603686636, 79.95391705069125, 79.26267281105991, 78.3410138248848, 79.03225806451613, 78.11059907834101, 79.03225806451613, 78.57142857142857, 77.64976958525345, 76.036866359447, 77.64976958525345, 77.41935483870968, 78.57142857142857, 79.03225806451613, 78.11059907834101, 80.87557603686636, 79.26267281105991, 80.4147465437788, 79.26267281105991, 79.72350230414746, 79.03225806451613, 79.49308755760369, 79.26267281105991, 78.11059907834101, 78.80184331797236, 77.88018433179722, 79.72350230414746, 78.80184331797236, 79.26267281105991, 79.72350230414746, 79.95391705069125, 79.95391705069125, 80.18433179723502, 80.18433179723502, 79.72350230414746, 79.95391705069125]]}, 'EntropySelection': {'250': [[76.95852534562212, 79.49308755760369]], '125': [[74.19354838709677, 79.03225806451613, 79.26267281105991, 79.72350230414746]], '50': [[76.036866359447, 76.95852534562212, 77.41935483870968, 79.95391705069125, 79.49308755760369, 79.72350230414746, 79.49308755760369, 80.64516129032258, 80.4147465437788, 81.10599078341014]], '25': [[77.41935483870968, 67.51152073732719, 78.11059907834101, 79.49308755760369, 78.80184331797236, 79.95391705069125, 80.18433179723502, 79.26267281105991, 79.49308755760369, 77.88018433179722, 79.49308755760369, 79.72350230414746, 79.49308755760369, 80.4147465437788, 79.72350230414746, 80.18433179723502, 80.4147465437788, 79.03225806451613, 80.64516129032258, 79.72350230414746]], '10': [[54.14746543778802, 69.12442396313364, 67.51152073732719, 70.73732718894009, 76.95852534562212, 71.42857142857143, 75.34562211981567, 77.88018433179722, 76.26728110599078, 78.57142857142857, 78.57142857142857, 77.18894009216591, 77.88018433179722, 78.80184331797236, 78.11059907834101, 78.11059907834101, 78.80184331797236, 78.80184331797236, 80.18433179723502, 79.49308755760369, 79.03225806451613, 78.57142857142857, 77.64976958525345, 79.72350230414746, 79.49308755760369, 80.64516129032258, 79.95391705069125, 80.18433179723502, 79.72350230414746, 80.64516129032258, 80.18433179723502, 79.72350230414746, 79.72350230414746, 79.95391705069125, 79.26267281105991, 80.18433179723502, 79.49308755760369, 79.49308755760369, 79.95391705069125, 79.26267281105991, 78.80184331797236, 79.26267281105991, 80.18433179723502, 79.49308755760369, 79.49308755760369, 79.72350230414746, 79.72350230414746, 79.72350230414746, 80.4147465437788, 80.87557603686636]]}}, 'LogModel': {'RandomSelection': {'250': [[77.88018433179722, 79.95391705069125]], '125': [[76.72811059907833, 78.3410138248848, 78.11059907834101, 78.80184331797236]], '50': [[66.58986175115207, 77.18894009216591, 78.3410138248848, 76.26728110599078, 75.57603686635944, 76.26728110599078, 76.95852534562212, 76.26728110599078, 75.34562211981567, 76.49769585253456]], '25': [[75.57603686635944, 76.72811059907833, 78.57142857142857, 77.88018433179722, 76.49769585253456, 76.49769585253456, 75.57603686635944, 76.49769585253456, 74.88479262672811, 76.036866359447, 76.49769585253456, 75.11520737327189, 75.57603686635944, 76.036866359447, 76.26728110599078, 76.72811059907833, 76.26728110599078, 75.57603686635944, 77.18894009216591, 75.57603686635944]], '10': [[70.50691244239631, 75.11520737327189, 75.57603686635944, 77.18894009216591, 78.11059907834101, 76.72811059907833, 77.41935483870968, 76.72811059907833, 76.036866359447, 76.49769585253456, 76.95852534562212, 75.57603686635944, 76.49769585253456, 76.036866359447, 75.11520737327189, 75.11520737327189, 74.88479262672811, 75.34562211981567, 75.11520737327189, 75.11520737327189, 75.11520737327189, 74.65437788018433, 75.34562211981567, 74.88479262672811, 74.65437788018433, 74.65437788018433, 75.11520737327189, 75.11520737327189, 75.11520737327189, 75.11520737327189, 75.11520737327189, 75.57603686635944, 75.57603686635944, 74.42396313364056, 75.11520737327189, 75.11520737327189, 75.11520737327189, 74.88479262672811, 74.88479262672811, 75.11520737327189, 75.34562211981567, 76.49769585253456, 75.11520737327189, 74.65437788018433, 75.80645161290323, 75.80645161290323, 75.34562211981567, 75.34562211981567, 75.57603686635944, 75.34562211981567]]}, 'MarginSamplingSelection': {'250': [[77.41935483870968, 80.64516129032258]], '125': [[75.34562211981567, 79.03225806451613, 79.49308755760369, 80.4147465437788]], '50': [[75.11520737327189, 69.12442396313364, 75.11520737327189, 75.34562211981567, 78.11059907834101, 79.03225806451613, 77.88018433179722, 78.3410138248848, 80.18433179723502, 79.95391705069125]], '25': [[71.6589861751152, 75.57603686635944, 76.26728110599078, 76.49769585253456, 77.88018433179722, 80.4147465437788, 80.87557603686636, 81.5668202764977, 82.25806451612904, 81.33640552995391, 80.64516129032258, 80.4147465437788, 81.79723502304147, 79.26267281105991, 82.02764976958525, 81.33640552995391, 82.02764976958525, 80.87557603686636, 82.25806451612904, 82.02764976958525]], '10': [[40.55299539170507, 53.686635944700456, 71.19815668202764, 76.72811059907833, 77.64976958525345, 78.57142857142857, 77.41935483870968, 76.72811059907833, 77.88018433179722, 78.80184331797236, 80.18433179723502, 79.95391705069125, 79.49308755760369, 80.18433179723502, 79.26267281105991, 79.49308755760369, 79.03225806451613, 80.4147465437788, 79.49308755760369, 79.49308755760369, 78.80184331797236, 79.26267281105991, 79.95391705069125, 79.72350230414746, 79.49308755760369, 79.03225806451613, 79.72350230414746, 80.18433179723502, 80.18433179723502, 80.4147465437788, 80.4147465437788, 80.4147465437788, 81.10599078341014, 81.10599078341014, 80.87557603686636, 79.95391705069125, 80.18433179723502, 81.79723502304147, 80.87557603686636, 80.64516129032258, 79.03225806451613, 81.10599078341014, 80.64516129032258, 81.5668202764977, 75.57603686635944, 81.5668202764977, 79.49308755760369, 81.33640552995391, 80.64516129032258, 80.18433179723502]]}, 'EntropySelection': {'250': [[76.95852534562212, 80.87557603686636]], '125': [[76.72811059907833, 78.3410138248848, 80.64516129032258, 78.57142857142857]], '50': [[77.41935483870968, 79.49308755760369, 76.72811059907833, 79.03225806451613, 79.49308755760369, 78.80184331797236, 79.03225806451613, 78.80184331797236, 78.3410138248848, 79.72350230414746]], '25': [[76.95852534562212, 79.49308755760369, 78.3410138248848, 76.95852534562212, 75.80645161290323, 78.11059907834101, 77.18894009216591, 77.18894009216591, 79.72350230414746, 79.49308755760369, 79.72350230414746, 80.87557603686636, 79.26267281105991, 79.95391705069125, 79.49308755760369, 79.72350230414746, 80.4147465437788, 79.26267281105991, 79.72350230414746, 79.49308755760369]], '10': [[64.74654377880185, 72.58064516129032, 62.903225806451616, 65.89861751152074, 67.05069124423963, 76.72811059907833, 76.72811059907833, 77.88018433179722, 77.88018433179722, 77.41935483870968, 78.11059907834101, 77.18894009216591, 77.41935483870968, 76.49769585253456, 77.64976958525345, 75.80645161290323, 76.95852534562212, 75.80645161290323, 75.57603686635944, 77.64976958525345, 77.18894009216591, 75.34562211981567, 77.88018433179722, 78.3410138248848, 78.11059907834101, 77.64976958525345, 79.26267281105991, 78.11059907834101, 77.41935483870968, 79.49308755760369, 80.64516129032258, 80.18433179723502, 80.18433179723502, 80.4147465437788, 79.95391705069125, 82.48847926267281, 82.25806451612904, 82.02764976958525, 81.5668202764977, 81.5668202764977, 81.79723502304147, 81.10599078341014, 81.5668202764977, 82.02764976958525, 82.02764976958525, 82.02764976958525, 81.5668202764977, 82.02764976958525, 81.33640552995391, 81.5668202764977]]}}}\n",
            "{'LogModel': {'EntropySelection': {'10': [[64.74654377880185, 72.58064516129032, 62.903225806451616, 65.89861751152074, 67.05069124423963, 76.72811059907833, 76.72811059907833, 77.88018433179722, 77.88018433179722, 77.41935483870968, 78.11059907834101, 77.18894009216591, 77.41935483870968, 76.49769585253456, 77.64976958525345, 75.80645161290323, 76.95852534562212, 75.80645161290323, 75.57603686635944, 77.64976958525345, 77.18894009216591, 75.34562211981567, 77.88018433179722, 78.3410138248848, 78.11059907834101, 77.64976958525345, 79.26267281105991, 78.11059907834101, 77.41935483870968, 79.49308755760369, 80.64516129032258, 80.18433179723502, 80.18433179723502, 80.4147465437788, 79.95391705069125, 82.48847926267281, 82.25806451612904, 82.02764976958525, 81.5668202764977, 81.5668202764977, 81.79723502304147, 81.10599078341014, 81.5668202764977, 82.02764976958525, 82.02764976958525, 82.02764976958525, 81.5668202764977, 82.02764976958525, 81.33640552995391, 81.5668202764977]], '125': [[76.72811059907833, 78.3410138248848, 80.64516129032258, 78.57142857142857]], '25': [[76.95852534562212, 79.49308755760369, 78.3410138248848, 76.95852534562212, 75.80645161290323, 78.11059907834101, 77.18894009216591, 77.18894009216591, 79.72350230414746, 79.49308755760369, 79.72350230414746, 80.87557603686636, 79.26267281105991, 79.95391705069125, 79.49308755760369, 79.72350230414746, 80.4147465437788, 79.26267281105991, 79.72350230414746, 79.49308755760369]], '250': [[76.95852534562212, 80.87557603686636]], '50': [[77.41935483870968, 79.49308755760369, 76.72811059907833, 79.03225806451613, 79.49308755760369, 78.80184331797236, 79.03225806451613, 78.80184331797236, 78.3410138248848, 79.72350230414746]]}, 'MarginSamplingSelection': {'10': [[40.55299539170507, 53.686635944700456, 71.19815668202764, 76.72811059907833, 77.64976958525345, 78.57142857142857, 77.41935483870968, 76.72811059907833, 77.88018433179722, 78.80184331797236, 80.18433179723502, 79.95391705069125, 79.49308755760369, 80.18433179723502, 79.26267281105991, 79.49308755760369, 79.03225806451613, 80.4147465437788, 79.49308755760369, 79.49308755760369, 78.80184331797236, 79.26267281105991, 79.95391705069125, 79.72350230414746, 79.49308755760369, 79.03225806451613, 79.72350230414746, 80.18433179723502, 80.18433179723502, 80.4147465437788, 80.4147465437788, 80.4147465437788, 81.10599078341014, 81.10599078341014, 80.87557603686636, 79.95391705069125, 80.18433179723502, 81.79723502304147, 80.87557603686636, 80.64516129032258, 79.03225806451613, 81.10599078341014, 80.64516129032258, 81.5668202764977, 75.57603686635944, 81.5668202764977, 79.49308755760369, 81.33640552995391, 80.64516129032258, 80.18433179723502]], '125': [[75.34562211981567, 79.03225806451613, 79.49308755760369, 80.4147465437788]], '25': [[71.6589861751152, 75.57603686635944, 76.26728110599078, 76.49769585253456, 77.88018433179722, 80.4147465437788, 80.87557603686636, 81.5668202764977, 82.25806451612904, 81.33640552995391, 80.64516129032258, 80.4147465437788, 81.79723502304147, 79.26267281105991, 82.02764976958525, 81.33640552995391, 82.02764976958525, 80.87557603686636, 82.25806451612904, 82.02764976958525]], '250': [[77.41935483870968, 80.64516129032258]], '50': [[75.11520737327189, 69.12442396313364, 75.11520737327189, 75.34562211981567, 78.11059907834101, 79.03225806451613, 77.88018433179722, 78.3410138248848, 80.18433179723502, 79.95391705069125]]}, 'RandomSelection': {'10': [[70.50691244239631, 75.11520737327189, 75.57603686635944, 77.18894009216591, 78.11059907834101, 76.72811059907833, 77.41935483870968, 76.72811059907833, 76.036866359447, 76.49769585253456, 76.95852534562212, 75.57603686635944, 76.49769585253456, 76.036866359447, 75.11520737327189, 75.11520737327189, 74.88479262672811, 75.34562211981567, 75.11520737327189, 75.11520737327189, 75.11520737327189, 74.65437788018433, 75.34562211981567, 74.88479262672811, 74.65437788018433, 74.65437788018433, 75.11520737327189, 75.11520737327189, 75.11520737327189, 75.11520737327189, 75.11520737327189, 75.57603686635944, 75.57603686635944, 74.42396313364056, 75.11520737327189, 75.11520737327189, 75.11520737327189, 74.88479262672811, 74.88479262672811, 75.11520737327189, 75.34562211981567, 76.49769585253456, 75.11520737327189, 74.65437788018433, 75.80645161290323, 75.80645161290323, 75.34562211981567, 75.34562211981567, 75.57603686635944, 75.34562211981567]], '125': [[76.72811059907833, 78.3410138248848, 78.11059907834101, 78.80184331797236]], '25': [[75.57603686635944, 76.72811059907833, 78.57142857142857, 77.88018433179722, 76.49769585253456, 76.49769585253456, 75.57603686635944, 76.49769585253456, 74.88479262672811, 76.036866359447, 76.49769585253456, 75.11520737327189, 75.57603686635944, 76.036866359447, 76.26728110599078, 76.72811059907833, 76.26728110599078, 75.57603686635944, 77.18894009216591, 75.57603686635944]], '250': [[77.88018433179722, 79.95391705069125]], '50': [[66.58986175115207, 77.18894009216591, 78.3410138248848, 76.26728110599078, 75.57603686635944, 76.26728110599078, 76.95852534562212, 76.26728110599078, 75.34562211981567, 76.49769585253456]]}}, 'RfModel': {'EntropySelection': {'10': [[54.14746543778802, 69.12442396313364, 67.51152073732719, 70.73732718894009, 76.95852534562212, 71.42857142857143, 75.34562211981567, 77.88018433179722, 76.26728110599078, 78.57142857142857, 78.57142857142857, 77.18894009216591, 77.88018433179722, 78.80184331797236, 78.11059907834101, 78.11059907834101, 78.80184331797236, 78.80184331797236, 80.18433179723502, 79.49308755760369, 79.03225806451613, 78.57142857142857, 77.64976958525345, 79.72350230414746, 79.49308755760369, 80.64516129032258, 79.95391705069125, 80.18433179723502, 79.72350230414746, 80.64516129032258, 80.18433179723502, 79.72350230414746, 79.72350230414746, 79.95391705069125, 79.26267281105991, 80.18433179723502, 79.49308755760369, 79.49308755760369, 79.95391705069125, 79.26267281105991, 78.80184331797236, 79.26267281105991, 80.18433179723502, 79.49308755760369, 79.49308755760369, 79.72350230414746, 79.72350230414746, 79.72350230414746, 80.4147465437788, 80.87557603686636]], '125': [[74.19354838709677, 79.03225806451613, 79.26267281105991, 79.72350230414746]], '25': [[77.41935483870968, 67.51152073732719, 78.11059907834101, 79.49308755760369, 78.80184331797236, 79.95391705069125, 80.18433179723502, 79.26267281105991, 79.49308755760369, 77.88018433179722, 79.49308755760369, 79.72350230414746, 79.49308755760369, 80.4147465437788, 79.72350230414746, 80.18433179723502, 80.4147465437788, 79.03225806451613, 80.64516129032258, 79.72350230414746]], '250': [[76.95852534562212, 79.49308755760369]], '50': [[76.036866359447, 76.95852534562212, 77.41935483870968, 79.95391705069125, 79.49308755760369, 79.72350230414746, 79.49308755760369, 80.64516129032258, 80.4147465437788, 81.10599078341014]]}, 'MarginSamplingSelection': {'10': [[30.184331797235025, 78.80184331797236, 76.49769585253456, 70.50691244239631, 77.41935483870968, 76.72811059907833, 78.3410138248848, 79.49308755760369, 78.80184331797236, 79.95391705069125, 79.49308755760369, 80.18433179723502, 78.57142857142857, 79.03225806451613, 80.87557603686636, 79.95391705069125, 79.26267281105991, 78.3410138248848, 79.03225806451613, 78.11059907834101, 79.03225806451613, 78.57142857142857, 77.64976958525345, 76.036866359447, 77.64976958525345, 77.41935483870968, 78.57142857142857, 79.03225806451613, 78.11059907834101, 80.87557603686636, 79.26267281105991, 80.4147465437788, 79.26267281105991, 79.72350230414746, 79.03225806451613, 79.49308755760369, 79.26267281105991, 78.11059907834101, 78.80184331797236, 77.88018433179722, 79.72350230414746, 78.80184331797236, 79.26267281105991, 79.72350230414746, 79.95391705069125, 79.95391705069125, 80.18433179723502, 80.18433179723502, 79.72350230414746, 79.95391705069125]], '125': [[78.11059907834101, 78.11059907834101, 79.49308755760369, 79.72350230414746]], '25': [[72.81105990783409, 73.27188940092167, 75.80645161290323, 76.26728110599078, 79.26267281105991, 77.64976958525345, 78.11059907834101, 77.88018433179722, 77.18894009216591, 77.88018433179722, 76.95852534562212, 79.26267281105991, 78.3410138248848, 79.72350230414746, 80.4147465437788, 80.18433179723502, 80.64516129032258, 80.4147465437788, 80.4147465437788, 81.10599078341014]], '250': [[78.11059907834101, 78.57142857142857]], '50': [[68.4331797235023, 76.26728110599078, 80.4147465437788, 78.80184331797236, 79.49308755760369, 79.95391705069125, 80.4147465437788, 79.72350230414746, 79.03225806451613, 80.18433179723502]]}, 'RandomSelection': {'10': [[52.07373271889401, 39.63133640552996, 52.764976958525345, 51.61290322580645, 66.12903225806451, 70.04608294930875, 68.66359447004609, 75.11520737327189, 76.036866359447, 77.18894009216591, 78.3410138248848, 77.64976958525345, 78.3410138248848, 76.49769585253456, 76.72811059907833, 76.72811059907833, 77.18894009216591, 78.11059907834101, 79.49308755760369, 79.49308755760369, 79.03225806451613, 79.49308755760369, 79.49308755760369, 78.3410138248848, 80.4147465437788, 78.80184331797236, 77.88018433179722, 77.64976958525345, 77.18894009216591, 77.64976958525345, 76.95852534562212, 76.49769585253456, 78.3410138248848, 79.49308755760369, 78.57142857142857, 79.26267281105991, 78.57142857142857, 79.03225806451613, 78.11059907834101, 78.3410138248848, 79.26267281105991, 79.03225806451613, 79.03225806451613, 78.57142857142857, 78.80184331797236, 79.49308755760369, 78.11059907834101, 78.57142857142857, 78.80184331797236, 79.49308755760369]], '125': [[77.18894009216591, 79.49308755760369, 79.26267281105991, 79.26267281105991]], '25': [[58.75576036866359, 74.65437788018433, 75.11520737327189, 76.49769585253456, 74.65437788018433, 75.57603686635944, 78.3410138248848, 77.64976958525345, 76.26728110599078, 76.95852534562212, 75.11520737327189, 76.49769585253456, 78.57142857142857, 78.80184331797236, 77.88018433179722, 76.49769585253456, 78.11059907834101, 78.57142857142857, 77.88018433179722, 77.88018433179722]], '250': [[73.73271889400922, 77.41935483870968]], '50': [[55.76036866359447, 71.6589861751152, 73.04147465437788, 74.65437788018433, 77.18894009216591, 77.88018433179722, 76.49769585253456, 77.41935483870968, 79.26267281105991, 78.57142857142857]]}}, 'SvmModel': {'EntropySelection': {'10': [[72.35023041474655, 73.73271889400922, 72.35023041474655, 72.58064516129032, 73.04147465437788, 74.88479262672811, 75.80645161290323, 75.34562211981567, 76.72811059907833, 76.72811059907833, 78.11059907834101, 78.57142857142857, 77.64976958525345, 79.72350230414746, 78.57142857142857, 78.11059907834101, 77.64976958525345, 78.3410138248848, 78.57142857142857, 79.95391705069125, 79.72350230414746, 80.4147465437788, 79.95391705069125, 79.72350230414746, 79.95391705069125, 79.72350230414746, 79.49308755760369, 79.72350230414746, 79.49308755760369, 79.49308755760369, 79.26267281105991, 79.49308755760369, 78.57142857142857, 78.57142857142857, 79.03225806451613, 79.03225806451613, 79.03225806451613, 79.26267281105991, 79.26267281105991, 78.80184331797236, 79.03225806451613, 79.03225806451613, 79.49308755760369, 79.26267281105991, 79.03225806451613, 78.80184331797236, 78.80184331797236, 79.03225806451613, 79.26267281105991, 79.03225806451613]], '125': [[74.42396313364056, 72.11981566820278, 79.03225806451613, 78.57142857142857]], '25': [[75.11520737327189, 67.97235023041475, 76.95852534562212, 72.11981566820278, 76.26728110599078, 79.03225806451613, 78.3410138248848, 79.03225806451613, 78.3410138248848, 78.11059907834101, 79.49308755760369, 79.26267281105991, 79.26267281105991, 79.95391705069125, 79.49308755760369, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.49308755760369]], '250': [[77.88018433179722, 79.72350230414746]], '50': [[72.11981566820278, 74.65437788018433, 76.036866359447, 77.41935483870968, 77.88018433179722, 78.57142857142857, 79.49308755760369, 79.49308755760369, 79.95391705069125, 79.49308755760369]]}, 'MarginSamplingSelection': {'10': [[61.75115207373272, 68.89400921658986, 65.89861751152074, 66.3594470046083, 69.81566820276498, 74.88479262672811, 69.35483870967742, 71.42857142857143, 71.42857142857143, 76.72811059907833, 72.58064516129032, 79.72350230414746, 79.72350230414746, 78.11059907834101, 79.03225806451613, 78.3410138248848, 78.3410138248848, 78.11059907834101, 78.3410138248848, 78.3410138248848, 78.3410138248848, 79.03225806451613, 79.49308755760369, 79.72350230414746, 80.4147465437788, 79.95391705069125, 79.95391705069125, 80.18433179723502, 79.95391705069125, 79.95391705069125, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.72350230414746, 79.49308755760369, 79.49308755760369, 79.49308755760369, 79.49308755760369, 79.72350230414746, 79.95391705069125, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.4147465437788, 79.95391705069125, 80.18433179723502, 79.26267281105991, 79.49308755760369, 79.72350230414746]], '125': [[75.80645161290323, 77.88018433179722, 78.3410138248848, 79.03225806451613]], '25': [[71.19815668202764, 71.19815668202764, 71.42857142857143, 70.96774193548387, 66.58986175115207, 67.74193548387096, 69.12442396313364, 69.35483870967742, 69.5852534562212, 72.11981566820278, 72.11981566820278, 78.11059907834101, 78.80184331797236, 79.49308755760369, 79.49308755760369, 78.3410138248848, 78.11059907834101, 77.88018433179722, 77.88018433179722, 78.11059907834101]], '250': [[76.036866359447, 78.80184331797236]], '50': [[62.21198156682027, 65.43778801843318, 65.43778801843318, 70.04608294930875, 70.96774193548387, 72.81105990783409, 77.64976958525345, 78.57142857142857, 79.49308755760369, 79.26267281105991]]}, 'RandomSelection': {'10': [[67.2811059907834, 67.97235023041475, 69.81566820276498, 74.19354838709677, 70.73732718894009, 70.73732718894009, 73.963133640553, 77.64976958525345, 75.57603686635944, 72.11981566820278, 75.11520737327189, 75.57603686635944, 73.50230414746544, 75.80645161290323, 77.88018433179722, 75.80645161290323, 77.64976958525345, 77.18894009216591, 75.57603686635944, 77.64976958525345, 75.57603686635944, 78.57142857142857, 77.64976958525345, 78.57142857142857, 78.80184331797236, 78.57142857142857, 79.26267281105991, 78.80184331797236, 77.88018433179722, 79.03225806451613, 79.03225806451613, 78.57142857142857, 79.49308755760369, 78.80184331797236, 77.88018433179722, 77.64976958525345, 78.11059907834101, 77.41935483870968, 77.41935483870968, 77.41935483870968, 77.64976958525345, 78.11059907834101, 78.11059907834101, 78.11059907834101, 77.88018433179722, 78.11059907834101, 78.11059907834101, 78.57142857142857, 78.11059907834101, 77.41935483870968]], '125': [[73.963133640553, 77.64976958525345, 79.26267281105991, 78.11059907834101]], '25': [[73.73271889400922, 77.18894009216591, 73.50230414746544, 72.11981566820278, 76.72811059907833, 69.81566820276498, 70.50691244239631, 66.82027649769586, 68.66359447004609, 76.49769585253456, 76.72811059907833, 76.26728110599078, 76.036866359447, 76.95852534562212, 77.64976958525345, 77.88018433179722, 78.3410138248848, 78.11059907834101, 79.03225806451613, 79.49308755760369]], '250': [[75.57603686635944, 77.64976958525345]], '50': [[67.74193548387096, 62.67281105990783, 72.58064516129032, 76.26728110599078, 76.95852534562212, 79.03225806451613, 77.64976958525345, 76.95852534562212, 76.95852534562212, 77.41935483870968]]}}}\n"
          ]
        }
      ],
      "source": [
        "(X, y) = download()\n",
        "(X_train_full, y_train_full, X_test, y_test) = split(trainset_size)\n",
        "print ('train:', X_train_full.shape, y_train_full.shape)\n",
        "print ('test :', X_test.shape, y_test.shape)\n",
        "classes = len(np.unique(y))\n",
        "print ('unique classes', classes)\n",
        "\n",
        "def pickle_save(fname, data):\n",
        "  filehandler = open(fname,\"wb\")\n",
        "  pickle.dump(data,filehandler)\n",
        "  filehandler.close() \n",
        "  print('saved', fname, os.getcwd(), os.listdir())\n",
        "\n",
        "def pickle_load(fname):\n",
        "  print(os.getcwd(), os.listdir())\n",
        "  file = open(fname,'rb')\n",
        "  data = pickle.load(file)\n",
        "  file.close()\n",
        "  print(data)\n",
        "  return data\n",
        "  \n",
        "def experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
        "    algos_temp = []\n",
        "    print ('stopping at:', max_queried)\n",
        "    count = 0\n",
        "    for model_object in models:\n",
        "      if model_object.__name__ not in d:\n",
        "          d[model_object.__name__] = {}\n",
        "      \n",
        "      for selection_function in selection_functions:\n",
        "        if selection_function.__name__ not in d[model_object.__name__]:\n",
        "            d[model_object.__name__][selection_function.__name__] = {}\n",
        "        \n",
        "        for k in Ks:\n",
        "            d[model_object.__name__][selection_function.__name__][str(k)] = []           \n",
        "            \n",
        "            for i in range(0, repeats):\n",
        "                count+=1\n",
        "                if count >= contfrom:\n",
        "                    print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
        "                    alg = TheAlgorithm(k, \n",
        "                                       model_object, \n",
        "                                       selection_function\n",
        "                                       )\n",
        "                    alg.run(X_train_full, y_train_full, X_test, y_test)\n",
        "                    d[model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
        "                    fname = 'Active-learning-experiment-' + str(count) + '.pkl'\n",
        "                    pickle_save(fname, d)\n",
        "                    if count % 5 == 0:\n",
        "                        print(json.dumps(d, indent=2, sort_keys=True))\n",
        "                    print ()\n",
        "                    print ('---------------------------- FINISHED ---------------------------')\n",
        "                    print ()\n",
        "    return d\n",
        "\n",
        "\n",
        "max_queried = 500 \n",
        "\n",
        "repeats = 1\n",
        "\n",
        "models = [SvmModel, RfModel, LogModel] \n",
        "\n",
        "selection_functions = [RandomSelection, MarginSamplingSelection, EntropySelection] \n",
        "\n",
        "Ks = [250,125,50,25,10] \n",
        "\n",
        "d = {}\n",
        "stopped_at = -1 \n",
        "\n",
        "# print('directory dump including pickle files:', os.getcwd(), np.sort(os.listdir()))  \n",
        "# d = pickle_load('Active-learning-experiment-' + str(stopped_at) + '.pkl')  \n",
        "# print(json.dumps(d, indent=2, sort_keys=True))\n",
        "\n",
        "d = experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
        "print (d)\n",
        "results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So which is the better model? under the stopping condition and hyper parameters - random forest is the winner!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"261.049219pt\" version=\"1.1\" viewBox=\"0 0 384.83125 261.049219\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-18T18:55:13.861981</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 261.049219 \nL 384.83125 261.049219 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \nL 368.0875 10.999219 \nL 33.2875 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m396fa297b8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 70.4875 228.439219 \nL 70.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 107.6875 228.439219 \nL 107.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 144.8875 228.439219 \nL 144.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 182.0875 228.439219 \nL 182.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 219.2875 228.439219 \nL 219.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 256.4875 228.439219 \nL 256.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 293.6875 228.439219 \nL 293.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 330.8875 228.439219 \nL 330.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#m396fa297b8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"me3d539fe5b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#me3d539fe5b\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 232.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 192.199219 \nL 368.0875 192.199219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#me3d539fe5b\" y=\"192.199219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 195.998438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 155.959219 \nL 368.0875 155.959219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#me3d539fe5b\" y=\"155.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 159.758438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 119.719219 \nL 368.0875 119.719219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#me3d539fe5b\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 123.518438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 83.479219 \nL 368.0875 83.479219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#me3d539fe5b\" y=\"83.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 87.278438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 47.239219 \nL 368.0875 47.239219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#me3d539fe5b\" y=\"47.239219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 51.038438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#me3d539fe5b\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M -1 50.863219 \nL 368.0875 50.863219 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 182.0875 106.191845 \nL 368.0875 92.831477 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 89.0875 93.6665 \nL 182.0875 85.316269 \nL 275.0875 86.151292 \nL 368.0875 86.151292 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 171.323643 \nL 70.4875 113.707053 \nL 107.6875 108.696915 \nL 144.8875 102.851753 \nL 182.0875 93.6665 \nL 219.2875 91.161431 \nL 256.4875 96.171569 \nL 293.6875 92.831477 \nL 330.8875 86.151292 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 14.6875 160.468343 \nL 33.2875 102.851753 \nL 51.8875 101.181707 \nL 70.4875 96.171569 \nL 89.0875 102.851753 \nL 107.6875 99.511661 \nL 126.2875 89.491385 \nL 144.8875 91.996454 \nL 163.4875 97.006592 \nL 182.0875 94.501523 \nL 200.6875 101.181707 \nL 219.2875 96.171569 \nL 237.8875 88.656362 \nL 256.4875 87.821339 \nL 275.0875 91.161431 \nL 293.6875 96.171569 \nL 312.2875 90.326408 \nL 330.8875 88.656362 \nL 349.4875 91.161431 \nL 368.0875 91.161431 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 3.5275 184.684011 \nL 10.9675 229.775256 \nL 18.4075 182.178942 \nL 25.8475 186.354057 \nL 33.2875 133.747606 \nL 40.7275 119.552214 \nL 48.1675 124.562352 \nL 55.6075 101.181707 \nL 63.0475 97.841615 \nL 70.4875 93.6665 \nL 77.9275 89.491385 \nL 85.3675 91.996454 \nL 92.8075 89.491385 \nL 100.2475 96.171569 \nL 107.6875 95.336546 \nL 115.1275 95.336546 \nL 122.5675 93.6665 \nL 130.0075 90.326408 \nL 137.4475 85.316269 \nL 144.8875 85.316269 \nL 152.3275 86.986316 \nL 159.7675 85.316269 \nL 167.2075 85.316269 \nL 174.6475 89.491385 \nL 182.0875 81.976177 \nL 189.5275 87.821339 \nL 196.9675 91.161431 \nL 204.4075 91.996454 \nL 211.8475 93.6665 \nL 219.2875 91.996454 \nL 226.7275 94.501523 \nL 234.1675 96.171569 \nL 241.6075 89.491385 \nL 249.0475 85.316269 \nL 256.4875 88.656362 \nL 263.9275 86.151292 \nL 271.3675 88.656362 \nL 278.8075 86.986316 \nL 286.2475 90.326408 \nL 293.6875 89.491385 \nL 301.1275 86.151292 \nL 308.5675 86.986316 \nL 316.0075 86.986316 \nL 323.4475 88.656362 \nL 330.8875 87.821339 \nL 338.3275 85.316269 \nL 345.7675 90.326408 \nL 353.2075 88.656362 \nL 360.6475 87.821339 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 182.0875 90.326408 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 89.0875 90.326408 \nL 182.0875 90.326408 \nL 275.0875 85.316269 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 125.397375 \nL 70.4875 97.006592 \nL 107.6875 81.976177 \nL 144.8875 87.821339 \nL 182.0875 85.316269 \nL 219.2875 83.646223 \nL 256.4875 81.976177 \nL 293.6875 84.481246 \nL 330.8875 86.986316 \nL 368.0875 82.8112 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 14.6875 109.531938 \nL 33.2875 107.861892 \nL 51.8875 98.676638 \nL 70.4875 97.006592 \nL 89.0875 86.151292 \nL 107.6875 91.996454 \nL 126.2875 90.326408 \nL 144.8875 91.161431 \nL 163.4875 93.6665 \nL 182.0875 91.161431 \nL 200.6875 94.501523 \nL 219.2875 86.151292 \nL 237.8875 89.491385 \nL 256.4875 84.481246 \nL 275.0875 81.976177 \nL 293.6875 82.8112 \nL 312.2875 81.141154 \nL 330.8875 81.976177 \nL 349.4875 81.976177 \nL 368.0875 79.471108 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 3.610349 262.049219 \nL 10.9675 87.821339 \nL 18.4075 96.171569 \nL 25.8475 117.882168 \nL 33.2875 92.831477 \nL 40.7275 95.336546 \nL 48.1675 89.491385 \nL 55.6075 85.316269 \nL 63.0475 87.821339 \nL 70.4875 83.646223 \nL 77.9275 85.316269 \nL 85.3675 82.8112 \nL 92.8075 88.656362 \nL 100.2475 86.986316 \nL 107.6875 80.306131 \nL 115.1275 83.646223 \nL 122.5675 86.151292 \nL 130.0075 89.491385 \nL 137.4475 86.986316 \nL 144.8875 90.326408 \nL 152.3275 86.986316 \nL 159.7675 88.656362 \nL 167.2075 91.996454 \nL 174.6475 97.841615 \nL 182.0875 91.996454 \nL 189.5275 92.831477 \nL 196.9675 88.656362 \nL 204.4075 86.986316 \nL 211.8475 90.326408 \nL 219.2875 80.306131 \nL 226.7275 86.151292 \nL 234.1675 81.976177 \nL 241.6075 86.151292 \nL 249.0475 84.481246 \nL 256.4875 86.986316 \nL 263.9275 85.316269 \nL 271.3675 86.151292 \nL 278.8075 90.326408 \nL 286.2475 87.821339 \nL 293.6875 91.161431 \nL 301.1275 84.481246 \nL 308.5675 87.821339 \nL 316.0075 86.151292 \nL 323.4475 84.481246 \nL 330.8875 83.646223 \nL 338.3275 83.646223 \nL 345.7675 82.8112 \nL 353.2075 82.8112 \nL 360.6475 84.481246 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 182.0875 94.501523 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 89.0875 104.521799 \nL 182.0875 86.986316 \nL 275.0875 86.151292 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 33.2875 97.841615 \nL 70.4875 94.501523 \nL 107.6875 92.831477 \nL 144.8875 83.646223 \nL 182.0875 85.316269 \nL 219.2875 84.481246 \nL 256.4875 85.316269 \nL 293.6875 81.141154 \nL 330.8875 81.976177 \nL 368.0875 79.471108 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 14.6875 92.831477 \nL 33.2875 128.737468 \nL 51.8875 90.326408 \nL 70.4875 85.316269 \nL 89.0875 87.821339 \nL 107.6875 83.646223 \nL 126.2875 82.8112 \nL 144.8875 86.151292 \nL 163.4875 85.316269 \nL 182.0875 91.161431 \nL 200.6875 85.316269 \nL 219.2875 84.481246 \nL 237.8875 85.316269 \nL 256.4875 81.976177 \nL 275.0875 84.481246 \nL 293.6875 82.8112 \nL 312.2875 81.976177 \nL 330.8875 86.986316 \nL 349.4875 81.141154 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path clip-path=\"url(#p4656a40664)\" d=\"M 3.5275 177.168804 \nL 10.9675 122.892306 \nL 18.4075 128.737468 \nL 25.8475 117.047145 \nL 33.2875 94.501523 \nL 40.7275 114.542076 \nL 48.1675 100.346684 \nL 55.6075 91.161431 \nL 63.0475 97.006592 \nL 70.4875 88.656362 \nL 77.9275 88.656362 \nL 85.3675 93.6665 \nL 92.8075 91.161431 \nL 100.2475 87.821339 \nL 107.6875 90.326408 \nL 115.1275 90.326408 \nL 122.5675 87.821339 \nL 130.0075 87.821339 \nL 137.4475 82.8112 \nL 144.8875 85.316269 \nL 152.3275 86.986316 \nL 159.7675 88.656362 \nL 167.2075 91.996454 \nL 174.6475 84.481246 \nL 182.0875 85.316269 \nL 189.5275 81.141154 \nL 196.9675 83.646223 \nL 204.4075 82.8112 \nL 211.8475 84.481246 \nL 219.2875 81.141154 \nL 226.7275 82.8112 \nL 234.1675 84.481246 \nL 241.6075 84.481246 \nL 249.0475 83.646223 \nL 256.4875 86.151292 \nL 263.9275 82.8112 \nL 271.3675 85.316269 \nL 278.8075 85.316269 \nL 286.2475 83.646223 \nL 293.6875 86.151292 \nL 301.1275 87.821339 \nL 308.5675 86.151292 \nL 316.0075 82.8112 \nL 323.4475 85.316269 \nL 330.8875 85.316269 \nL 338.3275 84.481246 \nL 345.7675 84.481246 \nL 353.2075 84.481246 \nL 360.6475 81.976177 \nL 368.0875 80.306131 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 135.239062 253.849219 \nL 361.0875 253.849219 \nQ 363.0875 253.849219 363.0875 251.849219 \nL 363.0875 17.999219 \nQ 363.0875 15.999219 361.0875 15.999219 \nL 135.239062 15.999219 \nQ 133.239062 15.999219 133.239062 17.999219 \nL 133.239062 251.849219 \nQ 133.239062 253.849219 135.239062 253.849219 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 137.239062 24.097656 \nL 157.239062 24.097656 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(165.239062 27.597656)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_53\">\n     <path d=\"M 137.239062 38.775781 \nL 157.239062 38.775781 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_54\"/>\n    <g id=\"text_19\">\n     <!-- RfModel-RandomSelection-250 -->\n     <g transform=\"translate(165.239062 42.275781)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1479.146484\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_55\">\n     <path d=\"M 137.239062 53.453906 \nL 157.239062 53.453906 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_56\"/>\n    <g id=\"text_20\">\n     <!-- RfModel-RandomSelection-125 -->\n     <g transform=\"translate(165.239062 56.953906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1479.146484\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_57\">\n     <path d=\"M 137.239062 68.132031 \nL 157.239062 68.132031 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_58\"/>\n    <g id=\"text_21\">\n     <!-- RfModel-RandomSelection-50 -->\n     <g transform=\"translate(165.239062 71.632031)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_59\">\n     <path d=\"M 137.239062 82.810156 \nL 157.239062 82.810156 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_60\"/>\n    <g id=\"text_22\">\n     <!-- RfModel-RandomSelection-25 -->\n     <g transform=\"translate(165.239062 86.310156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_61\">\n     <path d=\"M 137.239062 97.488281 \nL 157.239062 97.488281 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_62\"/>\n    <g id=\"text_23\">\n     <!-- RfModel-RandomSelection-10 -->\n     <g transform=\"translate(165.239062 100.988281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_63\">\n     <path d=\"M 137.239062 112.166406 \nL 157.239062 112.166406 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_64\"/>\n    <g id=\"text_24\">\n     <!-- RfModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(165.239062 115.666406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_65\">\n     <path d=\"M 137.239062 126.844531 \nL 157.239062 126.844531 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_66\"/>\n    <g id=\"text_25\">\n     <!-- RfModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(165.239062 130.344531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_67\">\n     <path d=\"M 137.239062 141.522656 \nL 157.239062 141.522656 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_68\"/>\n    <g id=\"text_26\">\n     <!-- RfModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(165.239062 145.022656)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_69\">\n     <path d=\"M 137.239062 156.200781 \nL 157.239062 156.200781 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_70\"/>\n    <g id=\"text_27\">\n     <!-- RfModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(165.239062 159.700781)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_71\">\n     <path d=\"M 137.239062 170.878906 \nL 157.239062 170.878906 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_72\"/>\n    <g id=\"text_28\">\n     <!-- RfModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(165.239062 174.378906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_73\">\n     <path d=\"M 137.239062 185.557031 \nL 157.239062 185.557031 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_74\"/>\n    <g id=\"text_29\">\n     <!-- RfModel-EntropySelection-250 -->\n     <g transform=\"translate(165.239062 189.057031)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1453.658203\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_75\">\n     <path d=\"M 137.239062 200.235156 \nL 157.239062 200.235156 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_76\"/>\n    <g id=\"text_30\">\n     <!-- RfModel-EntropySelection-125 -->\n     <g transform=\"translate(165.239062 203.735156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1453.658203\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_77\">\n     <path d=\"M 137.239062 214.913281 \nL 157.239062 214.913281 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_78\"/>\n    <g id=\"text_31\">\n     <!-- RfModel-EntropySelection-50 -->\n     <g transform=\"translate(165.239062 218.413281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_79\">\n     <path d=\"M 137.239062 229.591406 \nL 157.239062 229.591406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_80\"/>\n    <g id=\"text_32\">\n     <!-- RfModel-EntropySelection-25 -->\n     <g transform=\"translate(165.239062 233.091406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_81\">\n     <path d=\"M 137.239062 244.269531 \nL 157.239062 244.269531 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_82\"/>\n    <g id=\"text_33\">\n     <!-- RfModel-EntropySelection-10 -->\n     <g transform=\"translate(165.239062 247.769531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4656a40664\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"261.049219pt\" version=\"1.1\" viewBox=\"0 0 384.83125 261.049219\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-18T18:55:14.474991</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 261.049219 \nL 384.83125 261.049219 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \nL 368.0875 10.999219 \nL 33.2875 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me743d71774\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 70.4875 228.439219 \nL 70.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 107.6875 228.439219 \nL 107.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 144.8875 228.439219 \nL 144.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 182.0875 228.439219 \nL 182.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 219.2875 228.439219 \nL 219.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 256.4875 228.439219 \nL 256.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 293.6875 228.439219 \nL 293.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 330.8875 228.439219 \nL 330.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#me743d71774\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m21ea5e235b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m21ea5e235b\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 232.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 192.199219 \nL 368.0875 192.199219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m21ea5e235b\" y=\"192.199219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 195.998438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 155.959219 \nL 368.0875 155.959219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m21ea5e235b\" y=\"155.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 159.758438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 119.719219 \nL 368.0875 119.719219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m21ea5e235b\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 123.518438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 83.479219 \nL 368.0875 83.479219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m21ea5e235b\" y=\"83.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 87.278438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 47.239219 \nL 368.0875 47.239219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m21ea5e235b\" y=\"47.239219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 51.038438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m21ea5e235b\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M -1 58.111219 \nL 368.0875 58.111219 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 182.0875 99.511661 \nL 368.0875 91.996454 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 89.0875 105.356822 \nL 182.0875 91.996454 \nL 275.0875 86.151292 \nL 368.0875 90.326408 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 127.902445 \nL 70.4875 146.272951 \nL 107.6875 110.366961 \nL 144.8875 97.006592 \nL 182.0875 94.501523 \nL 219.2875 86.986316 \nL 256.4875 91.996454 \nL 293.6875 94.501523 \nL 330.8875 94.501523 \nL 368.0875 92.831477 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 14.6875 106.191845 \nL 33.2875 93.6665 \nL 51.8875 107.026869 \nL 70.4875 112.037007 \nL 89.0875 95.336546 \nL 107.6875 120.387237 \nL 126.2875 117.882168 \nL 144.8875 131.242537 \nL 163.4875 124.562352 \nL 182.0875 96.171569 \nL 200.6875 95.336546 \nL 219.2875 97.006592 \nL 237.8875 97.841615 \nL 256.4875 94.501523 \nL 275.0875 91.996454 \nL 293.6875 91.161431 \nL 312.2875 89.491385 \nL 330.8875 90.326408 \nL 349.4875 86.986316 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 3.5275 129.572491 \nL 10.9675 127.067422 \nL 18.4075 120.387237 \nL 25.8475 104.521799 \nL 33.2875 117.047145 \nL 40.7275 117.047145 \nL 48.1675 105.356822 \nL 55.6075 91.996454 \nL 63.0475 99.511661 \nL 70.4875 112.037007 \nL 77.9275 101.181707 \nL 85.3675 99.511661 \nL 92.8075 107.026869 \nL 100.2475 98.676638 \nL 107.6875 91.161431 \nL 115.1275 98.676638 \nL 122.5675 91.996454 \nL 130.0075 93.6665 \nL 137.4475 99.511661 \nL 144.8875 91.996454 \nL 152.3275 99.511661 \nL 159.7675 88.656362 \nL 167.2075 91.996454 \nL 174.6475 88.656362 \nL 182.0875 87.821339 \nL 189.5275 88.656362 \nL 196.9675 86.151292 \nL 204.4075 87.821339 \nL 211.8475 91.161431 \nL 219.2875 86.986316 \nL 226.7275 86.986316 \nL 234.1675 88.656362 \nL 241.6075 85.316269 \nL 249.0475 87.821339 \nL 256.4875 91.161431 \nL 263.9275 91.996454 \nL 271.3675 90.326408 \nL 278.8075 92.831477 \nL 286.2475 92.831477 \nL 293.6875 92.831477 \nL 301.1275 91.996454 \nL 308.5675 90.326408 \nL 316.0075 90.326408 \nL 323.4475 90.326408 \nL 330.8875 91.161431 \nL 338.3275 90.326408 \nL 345.7675 90.326408 \nL 353.2075 88.656362 \nL 360.6475 90.326408 \nL 368.0875 92.831477 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 182.0875 97.841615 \nL 368.0875 87.821339 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 89.0875 98.676638 \nL 182.0875 91.161431 \nL 275.0875 89.491385 \nL 368.0875 86.986316 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 147.942998 \nL 70.4875 136.252675 \nL 107.6875 136.252675 \nL 144.8875 119.552214 \nL 182.0875 116.212122 \nL 219.2875 109.531938 \nL 256.4875 91.996454 \nL 293.6875 88.656362 \nL 330.8875 85.316269 \nL 368.0875 86.151292 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 14.6875 115.377099 \nL 33.2875 115.377099 \nL 51.8875 114.542076 \nL 70.4875 116.212122 \nL 89.0875 132.07756 \nL 107.6875 127.902445 \nL 126.2875 122.892306 \nL 144.8875 122.057283 \nL 163.4875 121.22226 \nL 182.0875 112.037007 \nL 200.6875 112.037007 \nL 219.2875 90.326408 \nL 237.8875 87.821339 \nL 256.4875 85.316269 \nL 275.0875 85.316269 \nL 293.6875 89.491385 \nL 312.2875 90.326408 \nL 330.8875 91.161431 \nL 349.4875 91.161431 \nL 368.0875 90.326408 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 3.5275 149.613044 \nL 10.9675 123.727329 \nL 18.4075 134.582629 \nL 25.8475 132.912583 \nL 33.2875 120.387237 \nL 40.7275 102.01673 \nL 48.1675 122.057283 \nL 55.6075 114.542076 \nL 63.0475 114.542076 \nL 70.4875 95.336546 \nL 77.9275 110.366961 \nL 85.3675 84.481246 \nL 92.8075 84.481246 \nL 100.2475 90.326408 \nL 107.6875 86.986316 \nL 115.1275 89.491385 \nL 122.5675 89.491385 \nL 130.0075 90.326408 \nL 137.4475 89.491385 \nL 144.8875 89.491385 \nL 152.3275 89.491385 \nL 159.7675 86.986316 \nL 167.2075 85.316269 \nL 174.6475 84.481246 \nL 182.0875 81.976177 \nL 189.5275 83.646223 \nL 196.9675 83.646223 \nL 204.4075 82.8112 \nL 211.8475 83.646223 \nL 219.2875 83.646223 \nL 226.7275 84.481246 \nL 234.1675 84.481246 \nL 241.6075 84.481246 \nL 249.0475 84.481246 \nL 256.4875 85.316269 \nL 263.9275 85.316269 \nL 271.3675 85.316269 \nL 278.8075 85.316269 \nL 286.2475 84.481246 \nL 293.6875 83.646223 \nL 301.1275 81.976177 \nL 308.5675 81.976177 \nL 316.0075 81.976177 \nL 323.4475 81.976177 \nL 330.8875 81.976177 \nL 338.3275 83.646223 \nL 345.7675 82.8112 \nL 353.2075 86.151292 \nL 360.6475 85.316269 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 182.0875 91.161431 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 89.0875 103.686776 \nL 182.0875 112.037007 \nL 275.0875 86.986316 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 33.2875 112.037007 \nL 70.4875 102.851753 \nL 107.6875 97.841615 \nL 144.8875 92.831477 \nL 182.0875 91.161431 \nL 219.2875 88.656362 \nL 256.4875 85.316269 \nL 293.6875 85.316269 \nL 330.8875 83.646223 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 14.6875 101.181707 \nL 33.2875 127.067422 \nL 51.8875 94.501523 \nL 70.4875 112.037007 \nL 89.0875 97.006592 \nL 107.6875 86.986316 \nL 126.2875 89.491385 \nL 144.8875 86.986316 \nL 163.4875 89.491385 \nL 182.0875 90.326408 \nL 200.6875 85.316269 \nL 219.2875 86.151292 \nL 237.8875 86.151292 \nL 256.4875 83.646223 \nL 275.0875 85.316269 \nL 293.6875 84.481246 \nL 312.2875 84.481246 \nL 330.8875 84.481246 \nL 349.4875 84.481246 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path clip-path=\"url(#p4b2e0e5506)\" d=\"M 3.5275 111.201984 \nL 10.9675 106.191845 \nL 18.4075 111.201984 \nL 25.8475 110.366961 \nL 33.2875 108.696915 \nL 40.7275 102.01673 \nL 48.1675 98.676638 \nL 55.6075 100.346684 \nL 63.0475 95.336546 \nL 70.4875 95.336546 \nL 77.9275 90.326408 \nL 85.3675 88.656362 \nL 92.8075 91.996454 \nL 100.2475 84.481246 \nL 107.6875 88.656362 \nL 115.1275 90.326408 \nL 122.5675 91.996454 \nL 130.0075 89.491385 \nL 137.4475 88.656362 \nL 144.8875 83.646223 \nL 152.3275 84.481246 \nL 159.7675 81.976177 \nL 167.2075 83.646223 \nL 174.6475 84.481246 \nL 182.0875 83.646223 \nL 189.5275 84.481246 \nL 196.9675 85.316269 \nL 204.4075 84.481246 \nL 211.8475 85.316269 \nL 219.2875 85.316269 \nL 226.7275 86.151292 \nL 234.1675 85.316269 \nL 241.6075 88.656362 \nL 249.0475 88.656362 \nL 256.4875 86.986316 \nL 263.9275 86.986316 \nL 271.3675 86.986316 \nL 278.8075 86.151292 \nL 286.2475 86.151292 \nL 293.6875 87.821339 \nL 301.1275 86.986316 \nL 308.5675 86.986316 \nL 316.0075 85.316269 \nL 323.4475 86.151292 \nL 330.8875 86.986316 \nL 338.3275 87.821339 \nL 345.7675 87.821339 \nL 353.2075 86.986316 \nL 360.6475 86.151292 \nL 368.0875 86.986316 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 123.7 253.849219 \nL 361.0875 253.849219 \nQ 363.0875 253.849219 363.0875 251.849219 \nL 363.0875 17.999219 \nQ 363.0875 15.999219 361.0875 15.999219 \nL 123.7 15.999219 \nQ 121.7 15.999219 121.7 17.999219 \nL 121.7 251.849219 \nQ 121.7 253.849219 123.7 253.849219 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 125.7 24.097656 \nL 145.7 24.097656 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(153.7 27.597656)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_53\">\n     <path d=\"M 125.7 38.775781 \nL 145.7 38.775781 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_54\"/>\n    <g id=\"text_19\">\n     <!-- SvmModel-RandomSelection-250 -->\n     <g transform=\"translate(153.7 42.275781)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1594.527344\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_55\">\n     <path d=\"M 125.7 53.453906 \nL 145.7 53.453906 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_56\"/>\n    <g id=\"text_20\">\n     <!-- SvmModel-RandomSelection-125 -->\n     <g transform=\"translate(153.7 56.953906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1594.527344\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_57\">\n     <path d=\"M 125.7 68.132031 \nL 145.7 68.132031 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_58\"/>\n    <g id=\"text_21\">\n     <!-- SvmModel-RandomSelection-50 -->\n     <g transform=\"translate(153.7 71.632031)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_59\">\n     <path d=\"M 125.7 82.810156 \nL 145.7 82.810156 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_60\"/>\n    <g id=\"text_22\">\n     <!-- SvmModel-RandomSelection-25 -->\n     <g transform=\"translate(153.7 86.310156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_61\">\n     <path d=\"M 125.7 97.488281 \nL 145.7 97.488281 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_62\"/>\n    <g id=\"text_23\">\n     <!-- SvmModel-RandomSelection-10 -->\n     <g transform=\"translate(153.7 100.988281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"623.628906\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"684.908203\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"748.287109\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"811.763672\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"872.945312\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"970.357422\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1033.833984\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1095.357422\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1123.140625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1184.664062\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1239.644531\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1278.853516\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1306.636719\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1367.818359\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1431.197266\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1467.28125\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1530.904297\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_63\">\n     <path d=\"M 125.7 112.166406 \nL 145.7 112.166406 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_64\"/>\n    <g id=\"text_24\">\n     <!-- SvmModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(153.7 115.666406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1990.193359\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_65\">\n     <path d=\"M 125.7 126.844531 \nL 145.7 126.844531 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_66\"/>\n    <g id=\"text_25\">\n     <!-- SvmModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(153.7 130.344531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1990.193359\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_67\">\n     <path d=\"M 125.7 141.522656 \nL 145.7 141.522656 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_68\"/>\n    <g id=\"text_26\">\n     <!-- SvmModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(153.7 145.022656)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_69\">\n     <path d=\"M 125.7 156.200781 \nL 145.7 156.200781 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_70\"/>\n    <g id=\"text_27\">\n     <!-- SvmModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(153.7 159.700781)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_71\">\n     <path d=\"M 125.7 170.878906 \nL 145.7 170.878906 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_72\"/>\n    <g id=\"text_28\">\n     <!-- SvmModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(153.7 174.378906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"642.675781\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"703.955078\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"743.318359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"806.794922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"834.578125\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"897.957031\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"961.433594\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"1022.712891\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1120.125\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1183.601562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1211.384766\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.167969\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1302.546875\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1366.023438\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1429.5\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1491.023438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1518.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1580.330078\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1635.310547\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1674.519531\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1702.302734\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1763.484375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1826.863281\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1862.947266\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1926.570312\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_73\">\n     <path d=\"M 125.7 185.557031 \nL 145.7 185.557031 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_74\"/>\n    <g id=\"text_29\">\n     <!-- SvmModel-EntropySelection-250 -->\n     <g transform=\"translate(153.7 189.057031)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1569.039062\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_75\">\n     <path d=\"M 125.7 200.235156 \nL 145.7 200.235156 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_76\"/>\n    <g id=\"text_30\">\n     <!-- SvmModel-EntropySelection-125 -->\n     <g transform=\"translate(153.7 203.735156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1569.039062\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_77\">\n     <path d=\"M 125.7 214.913281 \nL 145.7 214.913281 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_78\"/>\n    <g id=\"text_31\">\n     <!-- SvmModel-EntropySelection-50 -->\n     <g transform=\"translate(153.7 218.413281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_79\">\n     <path d=\"M 125.7 229.591406 \nL 145.7 229.591406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_80\"/>\n    <g id=\"text_32\">\n     <!-- SvmModel-EntropySelection-25 -->\n     <g transform=\"translate(153.7 233.091406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_81\">\n     <path d=\"M 125.7 244.269531 \nL 145.7 244.269531 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_82\"/>\n    <g id=\"text_33\">\n     <!-- SvmModel-EntropySelection-10 -->\n     <g transform=\"translate(153.7 247.769531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"122.65625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"220.068359\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"306.347656\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"367.529297\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"431.005859\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"492.529297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"520.3125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"556.396484\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"619.580078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"682.958984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"722.167969\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"761.03125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"822.212891\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"885.689453\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"944.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1008.345703\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.869141\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1097.652344\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1159.175781\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1214.15625\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1253.365234\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1281.148438\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1342.330078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1405.708984\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1441.792969\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1505.416016\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4b2e0e5506\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"261.049219pt\" version=\"1.1\" viewBox=\"0 0 384.83125 261.049219\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-18T18:55:15.063344</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 261.049219 \nL 384.83125 261.049219 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \nL 368.0875 10.999219 \nL 33.2875 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mb122c21256\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 70.4875 228.439219 \nL 70.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 107.6875 228.439219 \nL 107.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 144.8875 228.439219 \nL 144.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 182.0875 228.439219 \nL 182.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 219.2875 228.439219 \nL 219.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 256.4875 228.439219 \nL 256.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 293.6875 228.439219 \nL 293.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 330.8875 228.439219 \nL 330.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#mb122c21256\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mb5e342fdf8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb5e342fdf8\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 232.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 192.199219 \nL 368.0875 192.199219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb5e342fdf8\" y=\"192.199219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 195.998438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 155.959219 \nL 368.0875 155.959219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb5e342fdf8\" y=\"155.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 159.758438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 119.719219 \nL 368.0875 119.719219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb5e342fdf8\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 123.518438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 83.479219 \nL 368.0875 83.479219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb5e342fdf8\" y=\"83.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 87.278438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 47.239219 \nL 368.0875 47.239219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb5e342fdf8\" y=\"47.239219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 51.038438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mb5e342fdf8\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M -1 58.111219 \nL 368.0875 58.111219 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 182.0875 91.161431 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 89.0875 95.336546 \nL 182.0875 89.491385 \nL 275.0875 90.326408 \nL 368.0875 87.821339 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 132.07756 \nL 70.4875 93.6665 \nL 107.6875 89.491385 \nL 144.8875 97.006592 \nL 182.0875 99.511661 \nL 219.2875 97.006592 \nL 256.4875 94.501523 \nL 293.6875 97.006592 \nL 330.8875 100.346684 \nL 368.0875 96.171569 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 14.6875 99.511661 \nL 33.2875 95.336546 \nL 51.8875 88.656362 \nL 70.4875 91.161431 \nL 89.0875 96.171569 \nL 107.6875 96.171569 \nL 126.2875 99.511661 \nL 144.8875 96.171569 \nL 163.4875 102.01673 \nL 182.0875 97.841615 \nL 200.6875 96.171569 \nL 219.2875 101.181707 \nL 237.8875 99.511661 \nL 256.4875 97.841615 \nL 275.0875 97.006592 \nL 293.6875 95.336546 \nL 312.2875 97.006592 \nL 330.8875 99.511661 \nL 349.4875 93.6665 \nL 368.0875 99.511661 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 3.5275 117.882168 \nL 10.9675 101.181707 \nL 18.4075 99.511661 \nL 25.8475 93.6665 \nL 33.2875 90.326408 \nL 40.7275 95.336546 \nL 48.1675 92.831477 \nL 55.6075 95.336546 \nL 63.0475 97.841615 \nL 70.4875 96.171569 \nL 77.9275 94.501523 \nL 85.3675 99.511661 \nL 92.8075 96.171569 \nL 100.2475 97.841615 \nL 107.6875 101.181707 \nL 115.1275 101.181707 \nL 122.5675 102.01673 \nL 130.0075 100.346684 \nL 137.4475 101.181707 \nL 144.8875 101.181707 \nL 152.3275 101.181707 \nL 159.7675 102.851753 \nL 167.2075 100.346684 \nL 174.6475 102.01673 \nL 182.0875 102.851753 \nL 189.5275 102.851753 \nL 196.9675 101.181707 \nL 204.4075 101.181707 \nL 211.8475 101.181707 \nL 219.2875 101.181707 \nL 226.7275 101.181707 \nL 234.1675 99.511661 \nL 241.6075 99.511661 \nL 249.0475 103.686776 \nL 256.4875 101.181707 \nL 263.9275 101.181707 \nL 271.3675 101.181707 \nL 278.8075 102.01673 \nL 286.2475 102.01673 \nL 293.6875 101.181707 \nL 301.1275 100.346684 \nL 308.5675 96.171569 \nL 316.0075 101.181707 \nL 323.4475 102.851753 \nL 330.8875 98.676638 \nL 338.3275 98.676638 \nL 345.7675 100.346684 \nL 353.2075 100.346684 \nL 360.6475 99.511661 \nL 368.0875 100.346684 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 182.0875 92.831477 \nL 368.0875 81.141154 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 89.0875 100.346684 \nL 182.0875 86.986316 \nL 275.0875 85.316269 \nL 368.0875 81.976177 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 101.181707 \nL 70.4875 122.892306 \nL 107.6875 101.181707 \nL 144.8875 100.346684 \nL 182.0875 90.326408 \nL 219.2875 86.986316 \nL 256.4875 91.161431 \nL 293.6875 89.491385 \nL 330.8875 82.8112 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 14.6875 113.707053 \nL 33.2875 99.511661 \nL 51.8875 97.006592 \nL 70.4875 96.171569 \nL 89.0875 91.161431 \nL 107.6875 81.976177 \nL 126.2875 80.306131 \nL 144.8875 77.801062 \nL 163.4875 75.295993 \nL 182.0875 78.636085 \nL 200.6875 81.141154 \nL 219.2875 81.976177 \nL 237.8875 76.966039 \nL 256.4875 86.151292 \nL 275.0875 76.131016 \nL 293.6875 78.636085 \nL 312.2875 76.131016 \nL 330.8875 80.306131 \nL 349.4875 75.295993 \nL 368.0875 76.131016 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 3.5275 226.435163 \nL 10.9675 178.83885 \nL 18.4075 115.377099 \nL 25.8475 95.336546 \nL 33.2875 91.996454 \nL 40.7275 88.656362 \nL 48.1675 92.831477 \nL 55.6075 95.336546 \nL 63.0475 91.161431 \nL 70.4875 87.821339 \nL 77.9275 82.8112 \nL 85.3675 83.646223 \nL 92.8075 85.316269 \nL 100.2475 82.8112 \nL 107.6875 86.151292 \nL 115.1275 85.316269 \nL 122.5675 86.986316 \nL 130.0075 81.976177 \nL 137.4475 85.316269 \nL 144.8875 85.316269 \nL 152.3275 87.821339 \nL 159.7675 86.151292 \nL 167.2075 83.646223 \nL 174.6475 84.481246 \nL 182.0875 85.316269 \nL 189.5275 86.986316 \nL 196.9675 84.481246 \nL 204.4075 82.8112 \nL 211.8475 82.8112 \nL 219.2875 81.976177 \nL 226.7275 81.976177 \nL 234.1675 81.976177 \nL 241.6075 79.471108 \nL 249.0475 79.471108 \nL 256.4875 80.306131 \nL 263.9275 83.646223 \nL 271.3675 82.8112 \nL 278.8075 76.966039 \nL 286.2475 80.306131 \nL 293.6875 81.141154 \nL 301.1275 86.986316 \nL 308.5675 79.471108 \nL 316.0075 81.141154 \nL 323.4475 77.801062 \nL 330.8875 99.511661 \nL 338.3275 77.801062 \nL 345.7675 85.316269 \nL 353.2075 78.636085 \nL 360.6475 81.141154 \nL 368.0875 82.8112 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 182.0875 94.501523 \nL 368.0875 80.306131 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 89.0875 95.336546 \nL 182.0875 89.491385 \nL 275.0875 81.141154 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 33.2875 92.831477 \nL 70.4875 85.316269 \nL 107.6875 95.336546 \nL 144.8875 86.986316 \nL 182.0875 85.316269 \nL 219.2875 87.821339 \nL 256.4875 86.986316 \nL 293.6875 87.821339 \nL 330.8875 89.491385 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 14.6875 94.501523 \nL 33.2875 85.316269 \nL 51.8875 89.491385 \nL 70.4875 94.501523 \nL 89.0875 98.676638 \nL 107.6875 90.326408 \nL 126.2875 93.6665 \nL 144.8875 93.6665 \nL 163.4875 84.481246 \nL 182.0875 85.316269 \nL 200.6875 84.481246 \nL 219.2875 80.306131 \nL 237.8875 86.151292 \nL 256.4875 83.646223 \nL 275.0875 85.316269 \nL 293.6875 84.481246 \nL 312.2875 81.976177 \nL 330.8875 86.151292 \nL 349.4875 84.481246 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path clip-path=\"url(#p489552b484)\" d=\"M 3.5275 138.757744 \nL 10.9675 110.366961 \nL 18.4075 145.437928 \nL 25.8475 134.582629 \nL 33.2875 130.407514 \nL 40.7275 95.336546 \nL 48.1675 95.336546 \nL 55.6075 91.161431 \nL 63.0475 91.161431 \nL 70.4875 92.831477 \nL 77.9275 90.326408 \nL 85.3675 93.6665 \nL 92.8075 92.831477 \nL 100.2475 96.171569 \nL 107.6875 91.996454 \nL 115.1275 98.676638 \nL 122.5675 94.501523 \nL 130.0075 98.676638 \nL 137.4475 99.511661 \nL 144.8875 91.996454 \nL 152.3275 93.6665 \nL 159.7675 100.346684 \nL 167.2075 91.161431 \nL 174.6475 89.491385 \nL 182.0875 90.326408 \nL 189.5275 91.996454 \nL 196.9675 86.151292 \nL 204.4075 90.326408 \nL 211.8475 92.831477 \nL 219.2875 85.316269 \nL 226.7275 81.141154 \nL 234.1675 82.8112 \nL 241.6075 82.8112 \nL 249.0475 81.976177 \nL 256.4875 83.646223 \nL 263.9275 74.46097 \nL 271.3675 75.295993 \nL 278.8075 76.131016 \nL 286.2475 77.801062 \nL 293.6875 77.801062 \nL 301.1275 76.966039 \nL 308.5675 79.471108 \nL 316.0075 77.801062 \nL 323.4475 76.131016 \nL 330.8875 76.131016 \nL 338.3275 76.131016 \nL 345.7675 77.801062 \nL 353.2075 76.131016 \nL 360.6475 78.636085 \nL 368.0875 77.801062 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 127.84375 253.849219 \nL 361.0875 253.849219 \nQ 363.0875 253.849219 363.0875 251.849219 \nL 363.0875 17.999219 \nQ 363.0875 15.999219 361.0875 15.999219 \nL 127.84375 15.999219 \nQ 125.84375 15.999219 125.84375 17.999219 \nL 125.84375 251.849219 \nQ 125.84375 253.849219 127.84375 253.849219 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 129.84375 24.097656 \nL 149.84375 24.097656 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(157.84375 27.597656)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_53\">\n     <path d=\"M 129.84375 38.775781 \nL 149.84375 38.775781 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_54\"/>\n    <g id=\"text_19\">\n     <!-- LogModel-RandomSelection-250 -->\n     <g transform=\"translate(157.84375 42.275781)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1553.080078\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_55\">\n     <path d=\"M 129.84375 53.453906 \nL 149.84375 53.453906 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_56\"/>\n    <g id=\"text_20\">\n     <!-- LogModel-RandomSelection-125 -->\n     <g transform=\"translate(157.84375 56.953906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1553.080078\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_57\">\n     <path d=\"M 129.84375 68.132031 \nL 149.84375 68.132031 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_58\"/>\n    <g id=\"text_21\">\n     <!-- LogModel-RandomSelection-50 -->\n     <g transform=\"translate(157.84375 71.632031)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_59\">\n     <path d=\"M 129.84375 82.810156 \nL 149.84375 82.810156 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_60\"/>\n    <g id=\"text_22\">\n     <!-- LogModel-RandomSelection-25 -->\n     <g transform=\"translate(157.84375 86.310156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_61\">\n     <path d=\"M 129.84375 97.488281 \nL 149.84375 97.488281 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_62\"/>\n    <g id=\"text_23\">\n     <!-- LogModel-RandomSelection-10 -->\n     <g transform=\"translate(157.84375 100.988281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"582.181641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"643.460938\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"706.839844\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"770.316406\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"831.498047\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"928.910156\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"992.386719\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1053.910156\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1081.693359\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1143.216797\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1198.197266\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1237.40625\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1265.189453\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1326.371094\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1389.75\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1425.833984\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1489.457031\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_63\">\n     <path d=\"M 129.84375 112.166406 \nL 149.84375 112.166406 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_64\"/>\n    <g id=\"text_24\">\n     <!-- LogModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(157.84375 115.666406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1948.746094\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_65\">\n     <path d=\"M 129.84375 126.844531 \nL 149.84375 126.844531 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_66\"/>\n    <g id=\"text_25\">\n     <!-- LogModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(157.84375 130.344531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1948.746094\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_67\">\n     <path d=\"M 129.84375 141.522656 \nL 149.84375 141.522656 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_68\"/>\n    <g id=\"text_26\">\n     <!-- LogModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(157.84375 145.022656)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_69\">\n     <path d=\"M 129.84375 156.200781 \nL 149.84375 156.200781 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_70\"/>\n    <g id=\"text_27\">\n     <!-- LogModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(157.84375 159.700781)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_71\">\n     <path d=\"M 129.84375 170.878906 \nL 149.84375 170.878906 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_72\"/>\n    <g id=\"text_28\">\n     <!-- LogModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(157.84375 174.378906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"601.228516\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"662.507812\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"701.871094\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"765.347656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"793.130859\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"856.509766\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"919.986328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"981.265625\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1078.677734\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1142.154297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1169.9375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1197.720703\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1261.099609\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1324.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1388.052734\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1449.576172\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1477.359375\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1538.882812\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1593.863281\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1633.072266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1660.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1722.037109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1785.416016\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1821.5\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1885.123047\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_73\">\n     <path d=\"M 129.84375 185.557031 \nL 149.84375 185.557031 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_74\"/>\n    <g id=\"text_29\">\n     <!-- LogModel-EntropySelection-250 -->\n     <g transform=\"translate(157.84375 189.057031)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1527.591797\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_75\">\n     <path d=\"M 129.84375 200.235156 \nL 149.84375 200.235156 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_76\"/>\n    <g id=\"text_30\">\n     <!-- LogModel-EntropySelection-125 -->\n     <g transform=\"translate(157.84375 203.735156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1527.591797\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_77\">\n     <path d=\"M 129.84375 214.913281 \nL 149.84375 214.913281 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_78\"/>\n    <g id=\"text_31\">\n     <!-- LogModel-EntropySelection-50 -->\n     <g transform=\"translate(157.84375 218.413281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_79\">\n     <path d=\"M 129.84375 229.591406 \nL 149.84375 229.591406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_80\"/>\n    <g id=\"text_32\">\n     <!-- LogModel-EntropySelection-25 -->\n     <g transform=\"translate(157.84375 233.091406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_81\">\n     <path d=\"M 129.84375 244.269531 \nL 149.84375 244.269531 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_82\"/>\n    <g id=\"text_33\">\n     <!-- LogModel-EntropySelection-10 -->\n     <g transform=\"translate(157.84375 247.769531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"178.621094\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"264.900391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"326.082031\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"389.558594\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"451.082031\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"478.865234\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"514.949219\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"578.132812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"641.511719\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"680.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"719.583984\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"780.765625\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"844.242188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"903.421875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"966.898438\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1028.421875\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1056.205078\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1117.728516\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1172.708984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1211.917969\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1239.701172\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1300.882812\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1364.261719\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1400.345703\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1463.96875\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p489552b484\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def performance_plot(fully_supervised_accuracy, dic, models, selection_functions, Ks, repeats):  \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot([0,500],[fully_supervised_accuracy, fully_supervised_accuracy],label = 'algorithm-upper-bound')\n",
        "    for model_object in models:\n",
        "      for selection_function in selection_functions:\n",
        "        for idx, k in enumerate(Ks):\n",
        "            x = np.arange(float(Ks[idx]), 500 + float(Ks[idx]), float(Ks[idx]))            \n",
        "            Sum = np.array(dic[model_object][selection_function][k][0])\n",
        "            for i in range(1, repeats):\n",
        "                Sum = Sum + np.array(dic[model_object][selection_function][k][i])\n",
        "            mean = Sum / repeats\n",
        "            ax.plot(x, mean ,label = model_object + '-' + selection_function + '-' + str(k))\n",
        "    ax.legend()\n",
        "    ax.set_xlim([50,500])\n",
        "    ax.set_ylim([40,100])\n",
        "    ax.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "models_str = ['SvmModel', 'RfModel', 'LogModel']\n",
        "selection_functions_str = ['RandomSelection', 'MarginSamplingSelection', 'EntropySelection']\n",
        "Ks_str = ['250','125','50','25','10'] \n",
        "repeats = 1\n",
        "random_forest_upper_bound = 89.\n",
        "svm_upper_bound = 87.\n",
        "log_upper_bound = 87.\n",
        "total_experiments = len(models_str) * len(selection_functions_str) * len(Ks_str) * repeats\n",
        "\n",
        "print('So which is the better model? under the stopping condition and hyper parameters - random forest is the winner!')\n",
        "performance_plot(random_forest_upper_bound, d, ['RfModel'] , selection_functions_str    , Ks_str, 1)\n",
        "performance_plot(svm_upper_bound, d, ['SvmModel'] , selection_functions_str    , Ks_str, 1)\n",
        "performance_plot(log_upper_bound, d, ['LogModel'] , selection_functions_str    , Ks_str, 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So which is the best sample selection function? margin sampling is the winner!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"261.049219pt\" version=\"1.1\" viewBox=\"0 0 384.83125 261.049219\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-18T18:55:16.021040</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 261.049219 \nL 384.83125 261.049219 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \nL 368.0875 10.999219 \nL 33.2875 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc5ac01ce01\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 70.4875 228.439219 \nL 70.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 107.6875 228.439219 \nL 107.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 144.8875 228.439219 \nL 144.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 182.0875 228.439219 \nL 182.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 219.2875 228.439219 \nL 219.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 256.4875 228.439219 \nL 256.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 293.6875 228.439219 \nL 293.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 330.8875 228.439219 \nL 330.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#mc5ac01ce01\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m082d819b83\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m082d819b83\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 232.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 192.199219 \nL 368.0875 192.199219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m082d819b83\" y=\"192.199219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 195.998438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 155.959219 \nL 368.0875 155.959219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m082d819b83\" y=\"155.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 159.758438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 119.719219 \nL 368.0875 119.719219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m082d819b83\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 123.518438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 83.479219 \nL 368.0875 83.479219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m082d819b83\" y=\"83.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 87.278438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 47.239219 \nL 368.0875 47.239219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m082d819b83\" y=\"47.239219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 51.038438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m082d819b83\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M -1 50.863219 \nL 368.0875 50.863219 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 182.0875 106.191845 \nL 368.0875 92.831477 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 89.0875 93.6665 \nL 182.0875 85.316269 \nL 275.0875 86.151292 \nL 368.0875 86.151292 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 171.323643 \nL 70.4875 113.707053 \nL 107.6875 108.696915 \nL 144.8875 102.851753 \nL 182.0875 93.6665 \nL 219.2875 91.161431 \nL 256.4875 96.171569 \nL 293.6875 92.831477 \nL 330.8875 86.151292 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 14.6875 160.468343 \nL 33.2875 102.851753 \nL 51.8875 101.181707 \nL 70.4875 96.171569 \nL 89.0875 102.851753 \nL 107.6875 99.511661 \nL 126.2875 89.491385 \nL 144.8875 91.996454 \nL 163.4875 97.006592 \nL 182.0875 94.501523 \nL 200.6875 101.181707 \nL 219.2875 96.171569 \nL 237.8875 88.656362 \nL 256.4875 87.821339 \nL 275.0875 91.161431 \nL 293.6875 96.171569 \nL 312.2875 90.326408 \nL 330.8875 88.656362 \nL 349.4875 91.161431 \nL 368.0875 91.161431 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 3.5275 184.684011 \nL 10.9675 229.775256 \nL 18.4075 182.178942 \nL 25.8475 186.354057 \nL 33.2875 133.747606 \nL 40.7275 119.552214 \nL 48.1675 124.562352 \nL 55.6075 101.181707 \nL 63.0475 97.841615 \nL 70.4875 93.6665 \nL 77.9275 89.491385 \nL 85.3675 91.996454 \nL 92.8075 89.491385 \nL 100.2475 96.171569 \nL 107.6875 95.336546 \nL 115.1275 95.336546 \nL 122.5675 93.6665 \nL 130.0075 90.326408 \nL 137.4475 85.316269 \nL 144.8875 85.316269 \nL 152.3275 86.986316 \nL 159.7675 85.316269 \nL 167.2075 85.316269 \nL 174.6475 89.491385 \nL 182.0875 81.976177 \nL 189.5275 87.821339 \nL 196.9675 91.161431 \nL 204.4075 91.996454 \nL 211.8475 93.6665 \nL 219.2875 91.996454 \nL 226.7275 94.501523 \nL 234.1675 96.171569 \nL 241.6075 89.491385 \nL 249.0475 85.316269 \nL 256.4875 88.656362 \nL 263.9275 86.151292 \nL 271.3675 88.656362 \nL 278.8075 86.986316 \nL 286.2475 90.326408 \nL 293.6875 89.491385 \nL 301.1275 86.151292 \nL 308.5675 86.986316 \nL 316.0075 86.986316 \nL 323.4475 88.656362 \nL 330.8875 87.821339 \nL 338.3275 85.316269 \nL 345.7675 90.326408 \nL 353.2075 88.656362 \nL 360.6475 87.821339 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 182.0875 90.326408 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 89.0875 90.326408 \nL 182.0875 90.326408 \nL 275.0875 85.316269 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 125.397375 \nL 70.4875 97.006592 \nL 107.6875 81.976177 \nL 144.8875 87.821339 \nL 182.0875 85.316269 \nL 219.2875 83.646223 \nL 256.4875 81.976177 \nL 293.6875 84.481246 \nL 330.8875 86.986316 \nL 368.0875 82.8112 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 14.6875 109.531938 \nL 33.2875 107.861892 \nL 51.8875 98.676638 \nL 70.4875 97.006592 \nL 89.0875 86.151292 \nL 107.6875 91.996454 \nL 126.2875 90.326408 \nL 144.8875 91.161431 \nL 163.4875 93.6665 \nL 182.0875 91.161431 \nL 200.6875 94.501523 \nL 219.2875 86.151292 \nL 237.8875 89.491385 \nL 256.4875 84.481246 \nL 275.0875 81.976177 \nL 293.6875 82.8112 \nL 312.2875 81.141154 \nL 330.8875 81.976177 \nL 349.4875 81.976177 \nL 368.0875 79.471108 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 3.610349 262.049219 \nL 10.9675 87.821339 \nL 18.4075 96.171569 \nL 25.8475 117.882168 \nL 33.2875 92.831477 \nL 40.7275 95.336546 \nL 48.1675 89.491385 \nL 55.6075 85.316269 \nL 63.0475 87.821339 \nL 70.4875 83.646223 \nL 77.9275 85.316269 \nL 85.3675 82.8112 \nL 92.8075 88.656362 \nL 100.2475 86.986316 \nL 107.6875 80.306131 \nL 115.1275 83.646223 \nL 122.5675 86.151292 \nL 130.0075 89.491385 \nL 137.4475 86.986316 \nL 144.8875 90.326408 \nL 152.3275 86.986316 \nL 159.7675 88.656362 \nL 167.2075 91.996454 \nL 174.6475 97.841615 \nL 182.0875 91.996454 \nL 189.5275 92.831477 \nL 196.9675 88.656362 \nL 204.4075 86.986316 \nL 211.8475 90.326408 \nL 219.2875 80.306131 \nL 226.7275 86.151292 \nL 234.1675 81.976177 \nL 241.6075 86.151292 \nL 249.0475 84.481246 \nL 256.4875 86.986316 \nL 263.9275 85.316269 \nL 271.3675 86.151292 \nL 278.8075 90.326408 \nL 286.2475 87.821339 \nL 293.6875 91.161431 \nL 301.1275 84.481246 \nL 308.5675 87.821339 \nL 316.0075 86.151292 \nL 323.4475 84.481246 \nL 330.8875 83.646223 \nL 338.3275 83.646223 \nL 345.7675 82.8112 \nL 353.2075 82.8112 \nL 360.6475 84.481246 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_46\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 182.0875 94.501523 \nL 368.0875 85.316269 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_47\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 89.0875 104.521799 \nL 182.0875 86.986316 \nL 275.0875 86.151292 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_48\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 33.2875 97.841615 \nL 70.4875 94.501523 \nL 107.6875 92.831477 \nL 144.8875 83.646223 \nL 182.0875 85.316269 \nL 219.2875 84.481246 \nL 256.4875 85.316269 \nL 293.6875 81.141154 \nL 330.8875 81.976177 \nL 368.0875 79.471108 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_49\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 14.6875 92.831477 \nL 33.2875 128.737468 \nL 51.8875 90.326408 \nL 70.4875 85.316269 \nL 89.0875 87.821339 \nL 107.6875 83.646223 \nL 126.2875 82.8112 \nL 144.8875 86.151292 \nL 163.4875 85.316269 \nL 182.0875 91.161431 \nL 200.6875 85.316269 \nL 219.2875 84.481246 \nL 237.8875 85.316269 \nL 256.4875 81.976177 \nL 275.0875 84.481246 \nL 293.6875 82.8112 \nL 312.2875 81.976177 \nL 330.8875 86.986316 \nL 349.4875 81.141154 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_50\">\n    <path clip-path=\"url(#p9505bd74ef)\" d=\"M 3.5275 177.168804 \nL 10.9675 122.892306 \nL 18.4075 128.737468 \nL 25.8475 117.047145 \nL 33.2875 94.501523 \nL 40.7275 114.542076 \nL 48.1675 100.346684 \nL 55.6075 91.161431 \nL 63.0475 97.006592 \nL 70.4875 88.656362 \nL 77.9275 88.656362 \nL 85.3675 93.6665 \nL 92.8075 91.161431 \nL 100.2475 87.821339 \nL 107.6875 90.326408 \nL 115.1275 90.326408 \nL 122.5675 87.821339 \nL 130.0075 87.821339 \nL 137.4475 82.8112 \nL 144.8875 85.316269 \nL 152.3275 86.986316 \nL 159.7675 88.656362 \nL 167.2075 91.996454 \nL 174.6475 84.481246 \nL 182.0875 85.316269 \nL 189.5275 81.141154 \nL 196.9675 83.646223 \nL 204.4075 82.8112 \nL 211.8475 84.481246 \nL 219.2875 81.141154 \nL 226.7275 82.8112 \nL 234.1675 84.481246 \nL 241.6075 84.481246 \nL 249.0475 83.646223 \nL 256.4875 86.151292 \nL 263.9275 82.8112 \nL 271.3675 85.316269 \nL 278.8075 85.316269 \nL 286.2475 83.646223 \nL 293.6875 86.151292 \nL 301.1275 87.821339 \nL 308.5675 86.151292 \nL 316.0075 82.8112 \nL 323.4475 85.316269 \nL 330.8875 85.316269 \nL 338.3275 84.481246 \nL 345.7675 84.481246 \nL 353.2075 84.481246 \nL 360.6475 81.976177 \nL 368.0875 80.306131 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 135.239062 253.849219 \nL 361.0875 253.849219 \nQ 363.0875 253.849219 363.0875 251.849219 \nL 363.0875 17.999219 \nQ 363.0875 15.999219 361.0875 15.999219 \nL 135.239062 15.999219 \nQ 133.239062 15.999219 133.239062 17.999219 \nL 133.239062 251.849219 \nQ 133.239062 253.849219 135.239062 253.849219 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 137.239062 24.097656 \nL 157.239062 24.097656 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(165.239062 27.597656)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_53\">\n     <path d=\"M 137.239062 38.775781 \nL 157.239062 38.775781 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_54\"/>\n    <g id=\"text_19\">\n     <!-- RfModel-RandomSelection-250 -->\n     <g transform=\"translate(165.239062 42.275781)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1479.146484\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_55\">\n     <path d=\"M 137.239062 53.453906 \nL 157.239062 53.453906 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_56\"/>\n    <g id=\"text_20\">\n     <!-- RfModel-RandomSelection-125 -->\n     <g transform=\"translate(165.239062 56.953906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1479.146484\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_57\">\n     <path d=\"M 137.239062 68.132031 \nL 157.239062 68.132031 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_58\"/>\n    <g id=\"text_21\">\n     <!-- RfModel-RandomSelection-50 -->\n     <g transform=\"translate(165.239062 71.632031)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_59\">\n     <path d=\"M 137.239062 82.810156 \nL 157.239062 82.810156 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_60\"/>\n    <g id=\"text_22\">\n     <!-- RfModel-RandomSelection-25 -->\n     <g transform=\"translate(165.239062 86.310156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_61\">\n     <path d=\"M 137.239062 97.488281 \nL 157.239062 97.488281 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_62\"/>\n    <g id=\"text_23\">\n     <!-- RfModel-RandomSelection-10 -->\n     <g transform=\"translate(165.239062 100.988281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"508.248047\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"569.527344\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"632.90625\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"696.382812\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"757.564453\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"854.976562\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"918.453125\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"979.976562\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1007.759766\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1069.283203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1124.263672\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1163.472656\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1191.255859\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1252.4375\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1315.816406\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1351.900391\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1415.523438\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_63\">\n     <path d=\"M 137.239062 112.166406 \nL 157.239062 112.166406 \n\" style=\"fill:none;stroke:#e377c2;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_64\"/>\n    <g id=\"text_24\">\n     <!-- RfModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(165.239062 115.666406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_65\">\n     <path d=\"M 137.239062 126.844531 \nL 157.239062 126.844531 \n\" style=\"fill:none;stroke:#7f7f7f;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_66\"/>\n    <g id=\"text_25\">\n     <!-- RfModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(165.239062 130.344531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_67\">\n     <path d=\"M 137.239062 141.522656 \nL 157.239062 141.522656 \n\" style=\"fill:none;stroke:#bcbd22;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_68\"/>\n    <g id=\"text_26\">\n     <!-- RfModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(165.239062 145.022656)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_69\">\n     <path d=\"M 137.239062 156.200781 \nL 157.239062 156.200781 \n\" style=\"fill:none;stroke:#17becf;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_70\"/>\n    <g id=\"text_27\">\n     <!-- RfModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(165.239062 159.700781)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_71\">\n     <path d=\"M 137.239062 170.878906 \nL 157.239062 170.878906 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_72\"/>\n    <g id=\"text_28\">\n     <!-- RfModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(165.239062 174.378906)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_73\">\n     <path d=\"M 137.239062 185.557031 \nL 157.239062 185.557031 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_74\"/>\n    <g id=\"text_29\">\n     <!-- RfModel-EntropySelection-250 -->\n     <g transform=\"translate(165.239062 189.057031)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1453.658203\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_75\">\n     <path d=\"M 137.239062 200.235156 \nL 157.239062 200.235156 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_76\"/>\n    <g id=\"text_30\">\n     <!-- RfModel-EntropySelection-125 -->\n     <g transform=\"translate(165.239062 203.735156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1453.658203\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_77\">\n     <path d=\"M 137.239062 214.913281 \nL 157.239062 214.913281 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_78\"/>\n    <g id=\"text_31\">\n     <!-- RfModel-EntropySelection-50 -->\n     <g transform=\"translate(165.239062 218.413281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_79\">\n     <path d=\"M 137.239062 229.591406 \nL 157.239062 229.591406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_80\"/>\n    <g id=\"text_32\">\n     <!-- RfModel-EntropySelection-25 -->\n     <g transform=\"translate(165.239062 233.091406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_81\">\n     <path d=\"M 137.239062 244.269531 \nL 157.239062 244.269531 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_82\"/>\n    <g id=\"text_33\">\n     <!-- RfModel-EntropySelection-10 -->\n     <g transform=\"translate(165.239062 247.769531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"504.199219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"567.578125\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"606.787109\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"645.650391\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"706.832031\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"770.308594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"892.964844\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"954.488281\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"982.271484\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1043.794922\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1098.775391\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1137.984375\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1165.767578\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1226.949219\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1290.328125\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1326.412109\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1390.035156\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9505bd74ef\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nSo which is the best k? k=10 is the winner\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"252.317344pt\" version=\"1.1\" viewBox=\"0 0 384.83125 252.317344\" width=\"384.83125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-18T18:55:16.576738</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 252.317344 \nL 384.83125 252.317344 \nL 384.83125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \nL 368.0875 10.999219 \nL 33.2875 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m283ab6a0ba\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(26.925 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 70.4875 228.439219 \nL 70.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"70.4875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(60.94375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 107.6875 228.439219 \nL 107.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.6875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(98.14375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 144.8875 228.439219 \nL 144.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.8875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(135.34375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 182.0875 228.439219 \nL 182.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.0875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 250 -->\n      <g transform=\"translate(172.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 219.2875 228.439219 \nL 219.2875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.2875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 300 -->\n      <g transform=\"translate(209.74375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 256.4875 228.439219 \nL 256.4875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.4875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 350 -->\n      <g transform=\"translate(246.94375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 293.6875 228.439219 \nL 293.6875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.6875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 400 -->\n      <g transform=\"translate(284.14375 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 330.8875 228.439219 \nL 330.8875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 450 -->\n      <g transform=\"translate(321.34375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#m283ab6a0ba\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 500 -->\n      <g transform=\"translate(358.54375 243.037656)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mbc53bb1b7d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbc53bb1b7d\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 232.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 33.2875 192.199219 \nL 368.0875 192.199219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbc53bb1b7d\" y=\"192.199219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 195.998438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 33.2875 155.959219 \nL 368.0875 155.959219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbc53bb1b7d\" y=\"155.959219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 159.758438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 33.2875 119.719219 \nL 368.0875 119.719219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbc53bb1b7d\" y=\"119.719219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 70 -->\n      <g transform=\"translate(13.5625 123.518438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 33.2875 83.479219 \nL 368.0875 83.479219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbc53bb1b7d\" y=\"83.479219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 87.278438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 33.2875 47.239219 \nL 368.0875 47.239219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbc53bb1b7d\" y=\"47.239219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 90 -->\n      <g transform=\"translate(13.5625 51.038438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-39\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <path clip-path=\"url(#pba475609da)\" d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbc53bb1b7d\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#pba475609da)\" d=\"M -1 50.863219 \nL 368.0875 50.863219 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#pba475609da)\" d=\"M 182.0875 90.326408 \nL 368.0875 88.656362 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#pba475609da)\" d=\"M 89.0875 90.326408 \nL 182.0875 90.326408 \nL 275.0875 85.316269 \nL 368.0875 84.481246 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#pba475609da)\" d=\"M 33.2875 125.397375 \nL 70.4875 97.006592 \nL 107.6875 81.976177 \nL 144.8875 87.821339 \nL 182.0875 85.316269 \nL 219.2875 83.646223 \nL 256.4875 81.976177 \nL 293.6875 84.481246 \nL 330.8875 86.986316 \nL 368.0875 82.8112 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#pba475609da)\" d=\"M 14.6875 109.531938 \nL 33.2875 107.861892 \nL 51.8875 98.676638 \nL 70.4875 97.006592 \nL 89.0875 86.151292 \nL 107.6875 91.996454 \nL 126.2875 90.326408 \nL 144.8875 91.161431 \nL 163.4875 93.6665 \nL 182.0875 91.161431 \nL 200.6875 94.501523 \nL 219.2875 86.151292 \nL 237.8875 89.491385 \nL 256.4875 84.481246 \nL 275.0875 81.976177 \nL 293.6875 82.8112 \nL 312.2875 81.141154 \nL 330.8875 81.976177 \nL 349.4875 81.976177 \nL 368.0875 79.471108 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#pba475609da)\" d=\"M 3.979071 253.317344 \nL 10.9675 87.821339 \nL 18.4075 96.171569 \nL 25.8475 117.882168 \nL 33.2875 92.831477 \nL 40.7275 95.336546 \nL 48.1675 89.491385 \nL 55.6075 85.316269 \nL 63.0475 87.821339 \nL 70.4875 83.646223 \nL 77.9275 85.316269 \nL 85.3675 82.8112 \nL 92.8075 88.656362 \nL 100.2475 86.986316 \nL 107.6875 80.306131 \nL 115.1275 83.646223 \nL 122.5675 86.151292 \nL 130.0075 89.491385 \nL 137.4475 86.986316 \nL 144.8875 90.326408 \nL 152.3275 86.986316 \nL 159.7675 88.656362 \nL 167.2075 91.996454 \nL 174.6475 97.841615 \nL 182.0875 91.996454 \nL 189.5275 92.831477 \nL 196.9675 88.656362 \nL 204.4075 86.986316 \nL 211.8475 90.326408 \nL 219.2875 80.306131 \nL 226.7275 86.151292 \nL 234.1675 81.976177 \nL 241.6075 86.151292 \nL 249.0475 84.481246 \nL 256.4875 86.986316 \nL 263.9275 85.316269 \nL 271.3675 86.151292 \nL 278.8075 90.326408 \nL 286.2475 87.821339 \nL 293.6875 91.161431 \nL 301.1275 84.481246 \nL 308.5675 87.821339 \nL 316.0075 86.151292 \nL 323.4475 84.481246 \nL 330.8875 83.646223 \nL 338.3275 83.646223 \nL 345.7675 82.8112 \nL 353.2075 82.8112 \nL 360.6475 84.481246 \nL 368.0875 83.646223 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 228.439219 \nL 33.2875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 228.439219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 228.439219 \nL 368.0875 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.999219 \nL 368.0875 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 40.2875 223.439219 \nL 266.135938 223.439219 \nQ 268.135938 223.439219 268.135938 221.439219 \nL 268.135938 134.370469 \nQ 268.135938 132.370469 266.135938 132.370469 \nL 40.2875 132.370469 \nQ 38.2875 132.370469 38.2875 134.370469 \nL 38.2875 221.439219 \nQ 38.2875 223.439219 40.2875 223.439219 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_41\">\n     <path d=\"M 42.2875 140.468906 \nL 62.2875 140.468906 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_42\"/>\n    <g id=\"text_18\">\n     <!-- algorithm-upper-bound -->\n     <g transform=\"translate(70.2875 143.968906)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 313 2009 \nL 1997 2009 \nL 1997 1497 \nL 313 1497 \nL 313 2009 \nz\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"213.720703\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"254.833984\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"282.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"321.826172\" xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"385.205078\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"482.617188\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"518.701172\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"582.080078\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"645.556641\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"709.033203\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"770.556641\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"805.294922\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"841.378906\" xlink:href=\"#DejaVuSans-62\"/>\n      <use x=\"904.855469\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"966.037109\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"1029.416016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1092.794922\" xlink:href=\"#DejaVuSans-64\"/>\n     </g>\n    </g>\n    <g id=\"line2d_43\">\n     <path d=\"M 42.2875 155.147031 \nL 62.2875 155.147031 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_44\"/>\n    <g id=\"text_19\">\n     <!-- RfModel-MarginSamplingSelection-250 -->\n     <g transform=\"translate(70.2875 158.647031)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_45\">\n     <path d=\"M 42.2875 169.825156 \nL 62.2875 169.825156 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_46\"/>\n    <g id=\"text_20\">\n     <!-- RfModel-MarginSamplingSelection-125 -->\n     <g transform=\"translate(70.2875 173.325156)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1874.8125\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_47\">\n     <path d=\"M 42.2875 184.503281 \nL 62.2875 184.503281 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_48\"/>\n    <g id=\"text_21\">\n     <!-- RfModel-MarginSamplingSelection-50 -->\n     <g transform=\"translate(70.2875 188.003281)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-35\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"line2d_49\">\n     <path d=\"M 42.2875 199.181406 \nL 62.2875 199.181406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_50\"/>\n    <g id=\"text_22\">\n     <!-- RfModel-MarginSamplingSelection-25 -->\n     <g transform=\"translate(70.2875 202.681406)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 42.2875 213.859531 \nL 62.2875 213.859531 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_52\"/>\n    <g id=\"text_23\">\n     <!-- RfModel-MarginSamplingSelection-10 -->\n     <g transform=\"translate(70.2875 217.359531)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-66\"/>\n      <use x=\"104.6875\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"190.966797\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"252.148438\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"315.625\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"377.148438\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"404.931641\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"441.015625\" xlink:href=\"#DejaVuSans-4d\"/>\n      <use x=\"527.294922\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"588.574219\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"627.9375\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"691.414062\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"719.197266\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"782.576172\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"846.052734\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"907.332031\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"1004.744141\" xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"1068.220703\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1096.003906\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1123.787109\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1187.166016\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"1250.642578\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1314.119141\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1375.642578\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"1403.425781\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"1464.949219\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"1519.929688\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"1559.138672\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"1586.921875\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"1648.103516\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"1711.482422\" xlink:href=\"#DejaVuSans-2d\"/>\n      <use x=\"1747.566406\" xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"1811.189453\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pba475609da\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print('So which is the best sample selection function? margin sampling is the winner!')\n",
        "performance_plot(random_forest_upper_bound, d, ['RfModel'], selection_functions_str    , Ks_str, 1)\n",
        "print()\n",
        "print('So which is the best k? k=10 is the winner')\n",
        "performance_plot(random_forest_upper_bound, d, ['RfModel'] , ['MarginSamplingSelection'], Ks_str, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}