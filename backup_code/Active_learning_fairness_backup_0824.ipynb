{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "print(__doc__)\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import statsmodels.api as sm\n",
        "\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "def retrieve_data_recid():\n",
        "    \n",
        "    attributes = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree']\n",
        "    bias = 'race'\n",
        "    target = 'two_year_recid'\n",
        "\n",
        "    # np.random.seed(42)\n",
        "\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/RecidivismData_Normalized.csv\", sep=',')\n",
        "    data_col = data.columns\n",
        "    df = data[(data[bias]==2)|(data[bias]==3)].copy().values\n",
        "\n",
        "    kf = KFold(n_splits=4) #differ from original method\n",
        "    for train_index, test_index in kf.split(df):\n",
        "        train, test = df[train_index], df[test_index]\n",
        "        # print(\"Size of X_train_full, X_test:\", train.shape, test.shape)\n",
        "\n",
        "    df_train = pd.DataFrame(data=train, columns=data_col)\n",
        "    df_test = pd.DataFrame(data=test, columns=data_col)\n",
        "\n",
        "    labeled = df_train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=5)) # ten sample in total labeled initially\n",
        "    # labeled = df_train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=5, random_state=42)) # ten sample in total labeled initially\n",
        "    df_X_labeled = labeled[attributes]\n",
        "    df_y_labeled = labeled[target]\n",
        "    X_labeled = df_X_labeled.values\n",
        "    y_labeled = df_y_labeled.values.astype('int64')\n",
        "    b_labeled = labeled[bias].values-2\n",
        "    (row_size, col_size) = X_labeled.shape\n",
        "\n",
        "    unlabeled = df_train.drop(df_X_labeled.index)\n",
        "    df_X_unlabeled = unlabeled[attributes]\n",
        "    df_y_unlabeled = unlabeled[target]\n",
        "    X_unlabeled = df_X_unlabeled.values\n",
        "    y_unlabeled = df_y_unlabeled.values.astype('int64')\n",
        "    b_unlabeled = unlabeled[bias].values-2\n",
        "\n",
        "    X_test = df_test[attributes].values\n",
        "    y_test = df_test[target].values\n",
        "    y_test=y_test.astype('int')\n",
        "    b_test = df_test[bias].values-2\n",
        "\n",
        "    X_fair_est = X_unlabeled\n",
        "    y_fair_est = y_unlabeled\n",
        "    b_fair_est = b_unlabeled\n",
        "    \n",
        "    return (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    def fit_predict(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        self.classifier = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "        self.classifier.fit(X_labeled, y_labeled)\n",
        "        # self.y_test_predicted = self.classifier.predict(X_test)\n",
        "        # self.y_unlabeled_predicted = self.classifier.predict(X_unlabeled)\n",
        "        self.y_test_score = self.classifier.score(X_test, y_test)\n",
        "        return (X_labeled, X_test, self.y_test_score)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def random_selection(probas_val, step):\n",
        "    # random_state = check_random_state(63)\n",
        "    selection = np.random.choice(probas_val.shape[0], step, replace=False)\n",
        "    return selection\n",
        "\n",
        "def entropy_selection(probas_val, step):\n",
        "    e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "    selection = (np.argsort(e)[::-1])[:step]\n",
        "    return selection"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    def train(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        (X_labeled, X_test, self.y_test_score) = \\\n",
        "            self.model_object.fit_predict(X_labeled, y_labeled, X_test, y_test)\n",
        "        return (X_labeled, X_test)\n",
        "\n",
        "    def get_test_accuracy(self, i):\n",
        "        classif_rate = self.y_test_score * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        # print(\"Accuracy rate is %f \" % (classif_rate))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "def normalizer(e_loss, f_loss):\n",
        "    e_loss = np.reshape(e_loss, (1,len(e_loss)))\n",
        "    f_loss = np.reshape(f_loss, (1,len(f_loss)))\n",
        "    e_scaled = preprocessing.normalize(e_loss)\n",
        "    # e_scaled=((e_loss-e_loss.min())/(e_loss.max()-e_loss.min()))\n",
        "    f_scaled = preprocessing.normalize(f_loss)\n",
        "    # f_scaled=((f_loss-f_loss.min())/(f_loss.max()-f_loss.min()))\n",
        "    return (e_scaled, f_scaled)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "def log_loss(probas_val):\n",
        "    \n",
        "    eps = np.finfo(probas_val.dtype).eps\n",
        "    probas_val = np.clip(probas_val, eps, 1 - eps)\n",
        "    e_loss = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "\n",
        "    return e_loss"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# separation \\ Equal opportunity - Hardt, Price, Srebro (2016)\n",
        "\n",
        "def mut_inf(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "    f_loss=mutual_info_score(b_fair_est, y_fair_pred)\n",
        "    f_loss=abs(f_loss)\n",
        "    # print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def stats_parity(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0=X_fair_est[(b_fair_est==0)].shape[0]\n",
        "    b1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1=X_fair_est[(b_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0p1/b0)-(b1p1/b1)\n",
        "    # print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def eqops(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0y1p1/b0y1)-(b1y1p1/b1y1)\n",
        "    # print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def eqods(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y0p1=X_fair_est[(b_fair_est==0)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0y0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)].shape[0]\n",
        "    b1y0p1=X_fair_est[(b_fair_est==1)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b1y0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)].shape[0]\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    fpr_loss=abs((b0y0p1/b0y0)-(b1y0p1/b1y0))\n",
        "    tpr_loss=abs((b0y1p1/b0y1)-(b1y1p1/b1y1))\n",
        "\n",
        "\n",
        "    f_loss = (fpr_loss+tpr_loss)/2 # temporary solution\n",
        "    \n",
        "    return f_loss \n",
        "\n",
        "def disp_mist(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y0p1=X_fair_est[(b_fair_est==0)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0y0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)].shape[0]\n",
        "    b1y0p1=X_fair_est[(b_fair_est==1)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b1y0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)].shape[0]\n",
        "\n",
        "    b0y1p0=X_fair_est[(b_fair_est==0)&(y_fair_est==1)&(y_fair_pred==0)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p0=X_fair_est[(b_fair_est==1)&(y_fair_est==1)&(y_fair_pred==0)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    fpr_loss=abs((b0y0p1/b0y0)-(b1y0p1/b1y0))\n",
        "    fnr_loss=abs((b0y1p0/b0y1)-(b1y1p0/b1y1))\n",
        "\n",
        "\n",
        "    f_loss = (fpr_loss+fnr_loss)/2 # temporary solution\n",
        "    \n",
        "    return f_loss \n",
        "\n",
        "def disp_impt(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0=X_fair_est[(b_fair_est==0)].shape[0]\n",
        "    b1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1=X_fair_est[(b_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0p1/b0)/(b1p1/b1)\n",
        "    # print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\n",
        "    f_loss = f_loss-1\n",
        "    return f_loss\n",
        "    "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# selecting fairness criteria\n",
        "\n",
        "def fair_measure(X_fair_est, y_fair_est, b_fair_est, classifier=None, criteria=0):\n",
        "\n",
        "    if criteria == 'mutual_information':\n",
        "        return mut_inf(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'equal_opportunity':\n",
        "        return eqops(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'statistical_parity':\n",
        "        return stats_parity(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'disparate_mistreatment':\n",
        "        return disp_mist(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'equalized_odds':\n",
        "        return eqods(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == \"disparate_impact\":\n",
        "        return disp_impt(X_fair_est, y_fair_est, b_fair_est, classifier)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "def unfairness_reduction_sampling(query_size, X_unlabeled, X_labeled, y_labeled, classifier, X_fair_est, y_fair_est, b_fair_est, probas_val, step, criteria):\n",
        "    # further to be defined, now assume only fairness loss\n",
        "    # query size used here\n",
        "    div = 0\n",
        "\n",
        "    unlabeled_size = len(X_unlabeled)\n",
        "    f_loss = np.zeros(unlabeled_size)\n",
        "    for i in range(unlabeled_size):\n",
        "        f_loss_temp = []\n",
        "        for j in range(2):\n",
        "            X_labeled_temp = np.append(X_labeled, [X_unlabeled[i]], axis = 0)\n",
        "            y_labeled_temp = np.append(y_labeled, [j], axis = 0)\n",
        "            classifier_temp = LogisticRegression(solver='liblinear').fit(X_labeled_temp, y_labeled_temp)\n",
        "            f_loss_temp = np.append(f_loss_temp, fair_measure(X_fair_est, y_fair_est, b_fair_est, classifier=classifier_temp, criteria=criteria))\n",
        "            f_loss_temp[np.isnan(f_loss_temp)] = 0\n",
        "            \n",
        "        proba_0 = classifier.predict_proba(X_unlabeled)[i][0]\n",
        "        proba_1 = 1 - proba_0\n",
        "        f_loss[i] = (f_loss_temp).dot([proba_0, proba_1])\n",
        "\n",
        "    e_loss = log_loss(probas_val)\n",
        "\n",
        "    e_scaled, f_scaled = normalizer(e_loss, f_loss)\n",
        "    f_scaled[np.isnan(f_scaled)] = 0\n",
        "\n",
        "    loss = div*(e_loss)+(1-div)*f_loss\n",
        "    \n",
        "    selection = np.argsort(loss)[::-1][:step]\n",
        "\n",
        "    return selection"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def pre_filter(X_data, b_data, y_data, budget): \n",
        "\n",
        "    temp_columns = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree','race','two_year_recid']\n",
        "    # np.random.seed(84)\n",
        "\n",
        "    # print(\"Debug data:\", X_data.shape, b_data.shape, y_data.shape)\n",
        "    temp_data = np.c_[X_data, b_data, y_data]\n",
        "    temp_df = pd.DataFrame(data=temp_data, columns=temp_columns)\n",
        "    candidates_data = temp_df.groupby('race', group_keys=False).apply(lambda x: x.sample(n=math.ceil(budget/2)))\n",
        "    # candidates_data = temp_df.groupby('race', group_keys=False).apply(lambda x: x.sample(n=math.ceil(budget/2), random_state = 84))\n",
        "    # print(\"Debug ceil:\", math.ceil(budget/2))\n",
        "    # print(\"Debug ceil:\", math.ceil(budget/2))\n",
        "    # print(\"Debug cand:\", candidates)\n",
        "    candidates_index = candidates_data.index.values\n",
        "    # print(\"Debug index:\", candidates_index.shape, candidates_index)\n",
        "\n",
        "    return candidates_index"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "def metrics(X_test, y_test, b_test, classifier):\n",
        "    y_test_pred = classifier.predict(X_test)\n",
        "    # print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, classification_report(y_test, y_test_pred)))\n",
        "    # print(\"Confusion matrix:\\n%s\" % confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
        "    tpr = tp/(tp+fn)\n",
        "    tnr = tn/(tn+fp) \n",
        "    fpr = fp/(fp+tn)\n",
        "    fnr = fn/(tp+fn)\n",
        "    fdr = fp/(tp+fp)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "\n",
        "    # model_displays = {}\n",
        "    # for i in np.unique(y_test):\n",
        "    #     model_displays[i] = plot_roc_curve(\n",
        "    #         classifier, X_test[b_test==i], y_test[b_test==i], ax=ax, name=i)\n",
        "    # ax.set_title('ROC curve')\n",
        "    # plt.show() "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "def scatters(X_unlabeled, b_unlabeled, probas_val, uncertain_samples):\n",
        "    y_plot = probas_val\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_axes([0,0,1,1])\n",
        "    ax.scatter(X_unlabeled[:,1], y_plot[:,1], c=b_unlabeled, cmap = 'Pastel1')\n",
        "    ax.scatter(X_unlabeled[:,1][uncertain_samples], y_plot[:,1][uncertain_samples], c=b_unlabeled[uncertain_samples], edgecolors='black', s=100, cmap = 'Pastel1')\n",
        "    ax.set_xlabel('Age')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Population plot')\n",
        "    y_plot = np.delete(y_plot, uncertain_samples, axis=0)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "def performance_progression(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, pplot_file_name,run):\n",
        "    fig, (ax1, ax2) = plt.subplots(2,figsize=(10,10))\n",
        "    fig.suptitle('Fairness and accuracy metrics')\n",
        "    ax1.plot(x_axis, ur_fairness, color='r', label='unfair-active')\n",
        "    ax1.plot(x_axis, rs_fairness, color='g', label='random-active')\n",
        "    ax1.plot(x_axis, nonal_fairness, color='b', label='non-active')\n",
        "    ax1.legend()\n",
        "    ax2.plot(x_axis, ur_accuracies, color='r', label='unfair-active')\n",
        "    ax2.plot(x_axis, rs_accuracies, color='g', label='random-active')\n",
        "    ax2.plot(x_axis, nonal_accuracies, color='b', label='non-active')\n",
        "    ax2.legend()\n",
        "    ax1.set_xlabel('Sample size')\n",
        "    ax1.set_ylabel('Unfairness')\n",
        "    ax2.set_ylabel('Accuracies')\n",
        "    plt.savefig(pplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "def trade_off_plot(ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, tplot_file_name, smooth_tplot_file_name,run):\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    ur_index = np.argsort(ur_accuracies)\n",
        "    rs_index = np.argsort(rs_accuracies)\n",
        "    nonal_index = np.argsort(nonal_accuracies) \n",
        "\n",
        "    plt.plot(np.array(ur_accuracies)[ur_index], np.array(ur_fairness)[ur_index], color='r', label='unfair-active')\n",
        "    plt.plot(np.array(rs_accuracies)[rs_index], np.array(rs_fairness)[rs_index], color='g', label='random-active')\n",
        "    plt.plot(np.array(nonal_accuracies)[nonal_index], np.array(nonal_fairness)[nonal_index], color='b', label='non-active')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Accuracies')\n",
        "    plt.ylabel('Unfairness')\n",
        "    plt.savefig(tplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "    ur_smooth = sm.nonparametric.lowess(np.array(ur_fairness)[ur_index], np.array(ur_accuracies)[ur_index], frac = 0.5)\n",
        "    rs_smooth = sm.nonparametric.lowess(np.array(rs_fairness)[rs_index], np.array(rs_accuracies)[rs_index], frac = 0.5)\n",
        "    nonal_smooth = sm.nonparametric.lowess(np.array(nonal_fairness)[nonal_index], np.array(nonal_accuracies)[nonal_index], frac = 0.5)\n",
        "\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    plt.plot(ur_smooth[:, 0], ur_smooth[:, 1], color='r', label='unfair-active')\n",
        "    plt.plot(rs_smooth[:, 0], rs_smooth[:, 1], color='g', label='random-active')\n",
        "    plt.plot(nonal_smooth[:, 0], nonal_smooth[:, 1], color='b', label='non-active')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Approximate accuracies')\n",
        "    plt.ylabel('Approximate unfairness')\n",
        "    plt.savefig(smooth_tplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "class active_learning(object):\n",
        "\n",
        "    def __init__(self, step, budget, model_object, criteria):\n",
        "        self.step = step\n",
        "        self.budget = budget\n",
        "        self.model_object = model_object\n",
        "        # self.sample_selection_function = selection_function\n",
        "        self.criteria = criteria\n",
        "        \n",
        "    def run(self, X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, sub_option):\n",
        "  \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_labeled, X_test) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(active_iteration)\n",
        "\n",
        "        self.query_size = len(X_labeled)\n",
        "        fairness = []\n",
        "        fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "        metrics(X_test, y_test, b_test, self.clf_model.model_object.classifier)\n",
        "\n",
        "        while self.query_size <= self.budget-self.step:\n",
        "\n",
        "            active_iteration += 1\n",
        "            self.query_size += self.step\n",
        "\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_unlabeled)\n",
        "            \n",
        "            # print(\"Debug predicted:\", y_unlabeled_predicted.shape)\n",
        "            # print(\"Debug probas_val:\", probas_val.shape)\n",
        "\n",
        "            if sub_option == \"Pre_filter\":\n",
        "                # post_entropy_index = entropy_selection(probas_val, self.budget*2)\n",
        "                candidates_index = pre_filter(X_unlabeled, b_unlabeled, y_unlabeled, self.budget)\n",
        "                uncertain_samples = unfairness_reduction_sampling(self.query_size, X_unlabeled[candidates_index], X_labeled, y_labeled, self.clf_model.model_object.classifier, X_fair_est, y_fair_est, b_fair_est, probas_val[candidates_index], self.step, self.criteria)\n",
        "\n",
        "            elif sub_option == \"No_filter\": \n",
        "                uncertain_samples = unfairness_reduction_sampling(self.query_size, X_unlabeled, X_labeled, y_labeled, self.clf_model.model_object.classifier, X_fair_est, y_fair_est, b_fair_est, probas_val, self.step, self.criteria)\n",
        "\n",
        "            elif sub_option == \"Filter_only\":\n",
        "                uncertain_samples = random_selection(probas_val, self.step)\n",
        "            # print(\"Debug shape of X_unlabeled and loss:\", selection)\n",
        "\n",
        "            # scatters(X_unlabeled, b_unlabeled, probas_val, uncertain_samples)\n",
        "\n",
        "            X_labeled = np.concatenate((X_labeled, X_unlabeled[uncertain_samples]))\n",
        "            y_labeled = np.concatenate((y_labeled, y_unlabeled[uncertain_samples]))\n",
        "            X_unlabeled = np.delete(X_unlabeled, uncertain_samples, axis=0)\n",
        "            y_unlabeled = np.delete(y_unlabeled, uncertain_samples, axis=0)\n",
        "            b_unlabeled = np.delete(b_unlabeled, uncertain_samples, axis=0)\n",
        "            \n",
        "            (X_labeled, X_test) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "            fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "            self.clf_model.get_test_accuracy(active_iteration)\n",
        "\n",
        "            metrics(X_test, y_test, b_test, self.clf_model.model_object.classifier)\n",
        "\n",
        "        return self.clf_model.accuracies, fairness"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "def non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled,y_labeled, X_test, y_test, b_test, budget, step, unfairness_criteria): \n",
        "\n",
        "    initial_X_train = X_labeled\n",
        "    initial_y_train = y_labeled\n",
        "    nonal_X_train = X_unlabeled\n",
        "    nonal_y_train = y_unlabeled\n",
        "    nonal_X_test = X_test\n",
        "    nonal_y_test = y_test\n",
        "    nonal_b_test = b_test\n",
        "    nonal_fairness = []\n",
        "\n",
        "    nonal_accuracies=[]\n",
        "\n",
        "    classifier_nonal = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "\n",
        "    classifier_nonal.fit(initial_X_train, initial_y_train)\n",
        "    initial_y_pred = classifier_nonal.predict(nonal_X_test)\n",
        "    nonal_fairness = np.append(nonal_fairness, fair_measure(nonal_X_test, nonal_y_test, nonal_b_test, classifier=classifier_nonal, criteria=unfairness_criteria))\n",
        "    nonal_accuracies.append(accuracy_score(nonal_y_test, initial_y_pred)*100)\n",
        "\n",
        "    for i in np.arange(init_index+step,budget+1,step):\n",
        "        classifier_nonal.fit(nonal_X_train[:i], nonal_y_train[:i])\n",
        "        nonal_y_pred = classifier_nonal.predict(nonal_X_test)\n",
        "        nonal_fairness = np.append(nonal_fairness, fair_measure(nonal_X_test, nonal_y_test, nonal_b_test, classifier=classifier_nonal, criteria=unfairness_criteria))\n",
        "        nonal_accuracies.append(accuracy_score(nonal_y_test, nonal_y_pred)*100)\n",
        "        metrics(X_test, y_test, b_test, classifier_nonal)\n",
        "\n",
        "    return nonal_accuracies, nonal_fairness"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "def experiment(model,budget,step,criteria,sub_option=None):\n",
        "    \n",
        "    (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est) = retrieve_data_recid()\n",
        "    init_index = len(X_labeled)\n",
        "        \n",
        "    act_alg = active_learning(step, budget, model, criteria)\n",
        "\n",
        "    ur_accuracies, ur_fairness = act_alg.run(X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, sub_option)\n",
        "\n",
        "    rs_accuracies, rs_fairness = act_alg.run(X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, \"Filter_only\")\n",
        "\n",
        "    nonal_accuracies, nonal_fairness = non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled, y_labeled, X_test, y_test, b_test, budget, step, criteria)\n",
        "\n",
        "    x_axis = np.arange(init_index,budget+1,step)\n",
        "\n",
        "    return init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "def results(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, run):\n",
        "\n",
        "    data = (np.array([x_axis, ur_accuracies, ur_fairness, rs_accuracies, rs_fairness, nonal_accuracies, nonal_fairness])).T\n",
        "\n",
        "    log_file_name = \"result_log/{run}.Result.csv\"\n",
        "    pplot_file_name = \"result_log/{:03}.Perf_Plot.png\"\n",
        "    tplot_file_name = \"result_log/{:03}.Trdoff_Plot.png\"\n",
        "    smooth_tplot_file_name = \"result_log/{:03}.Smooth.Trdoff_Plot.png\"\n",
        "    np.savetxt(log_file_name.format(run = run), data, delimiter=',', header=\"x_axis, ur_accuracies, ur_fairness, rs_accuracies, rs_fairness, nonal_accuracies, nonal_fairness\", fmt='%d,%f,%f,%f,%f,%f,%f')\n",
        "\n",
        "    performance_progression(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, pplot_file_name, run)\n",
        "    trade_off_plot(ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, tplot_file_name, smooth_tplot_file_name, run)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "for i in np.arange(30):\n",
        "    init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies = experiment(LogModel,500,50,\"mutual_information\",\"Pre_filter\")\n",
        "    results(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, run = i)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Iteration: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_51661/2815730800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minit_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mur_fairness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_fairness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonal_fairness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mur_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonal_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mutual_information\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"No_filter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mur_fairness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_fairness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonal_fairness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mur_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonal_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_51661/2077332540.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(model, budget, step, criteria, sub_option)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mact_alg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mur_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mur_fairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_option\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mrs_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_fairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Filter_only\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_51661/3183879732.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, sub_option)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msub_option\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"No_filter\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0muncertain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munfairness_reduction_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msub_option\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Filter_only\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_51661/2993685636.py\u001b[0m in \u001b[0;36munfairness_reduction_sampling\u001b[0;34m(query_size, X_unlabeled, X_labeled, y_labeled, classifier, X_fair_est, y_fair_est, b_fair_est, probas_val, step, criteria)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0my_labeled_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mclassifier_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mf_loss_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_loss_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfair_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mf_loss_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_loss_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_51661/1687591239.py\u001b[0m in \u001b[0;36mfair_measure\u001b[0;34m(X_fair_est, y_fair_est, b_fair_est, classifier, criteria)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mutual_information'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmut_inf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'equal_opportunity'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meqops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_51661/565103596.py\u001b[0m in \u001b[0;36mmut_inf\u001b[0;34m(X_fair_est, y_fair_est, b_fair_est, classifier)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_fair_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mf_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmutual_info_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mf_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Repo/ML-for-COVID-19-dataset/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Repo/ML-for-COVID-19-dataset/lib/python3.8/site-packages/sklearn/metrics/cluster/_supervised.py\u001b[0m in \u001b[0;36mmutual_info_score\u001b[0;34m(labels_true, labels_pred, contingency)\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontingency\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mlabels_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_clusterings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         \u001b[0mcontingency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontingency_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         contingency = check_array(contingency,\n",
            "\u001b[0;32m~/Documents/Repo/ML-for-COVID-19-dataset/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Repo/ML-for-COVID-19-dataset/lib/python3.8/site-packages/sklearn/metrics/cluster/_supervised.py\u001b[0m in \u001b[0;36mcontingency_matrix\u001b[0;34m(labels_true, labels_pred, eps, sparse, dtype)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot set 'eps' when sparse=True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m~/Documents/Repo/ML-for-COVID-19-dataset/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Repo/ML-for-COVID-19-dataset/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptional_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mergesort'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'quicksort'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d",
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "name": "Logistic.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a2ef89d34cbfddaf50816f8d91581a3ca0913b9280767ed31a38c2db7dcc022c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.2 64-bit ('ML-for-COVID-19-dataset': venv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}