{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from scipy.special import expit\n",
        "from scipy import stats\n",
        "from pylab import rcParams\n",
        "import mplcursors\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import neighbors\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "\n",
        "# pd.options.display.max_rows = 20\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "max_queried = 500\n",
        "trainset_size = 1302"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_prep():\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/ML-for-COVID-19-dataset/main/all_training.csv\", sep=',')\n",
        "    # Column selection\n",
        "    df = data.iloc[:,np.r_[3:34]].copy()\n",
        "    # define row and column index\n",
        "    col = df.columns\n",
        "    row = [i for i in range(df.shape[0])]\n",
        "    # define imputer\n",
        "    imputer = IterativeImputer(estimator=linear_model.BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
        "    # fit on the dataset\n",
        "    imputer.fit(df)\n",
        "    # transform the dataset\n",
        "    df_imputed = imputer.transform(df)\n",
        "    # convert back to pandas dataframe and rename back to df_normalized\n",
        "    df = pd.DataFrame(data=df_imputed, index=row, columns=col)\n",
        "    X = df\n",
        "    y = data.target    \n",
        "    # Recursive feature elimination\n",
        "    rdmreg = RandomForestClassifier(n_estimators=100)\n",
        "    # Define the method\n",
        "    rfe = RFE(estimator=rdmreg, n_features_to_select=10)\n",
        "    # Fit the model\n",
        "    rfe = rfe.fit(X, y.values.ravel())\n",
        "    print(rfe.support_)\n",
        "    # Drop columns that failed RFE test\n",
        "    col = df.columns[rfe.support_]\n",
        "    X = X[col]\n",
        "    X = X.to_numpy()\n",
        "    print ('df:', X.shape, y.shape)\n",
        "    return (X, y)\n",
        "\n",
        "\n",
        "# def split(train_size):\n",
        "#     X_train_full = X[:train_size]\n",
        "#     y_train_full = y[:train_size]\n",
        "#     X_test = X[train_size:]\n",
        "#     y_test = y[train_size:]   \n",
        "#     return (X_train_full, y_train_full, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SvmModel(BaseModel):\n",
        "\n",
        "    model_type = 'Support Vector Machine with linear Kernel'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training svm...')\n",
        "        self.classifier = SVC(\n",
        "            C=1, \n",
        "            kernel='linear', \n",
        "            probability=True,\n",
        "            class_weight=c_weight\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    model_type = 'Logistic Regression' \n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training logistic regression...')\n",
        "        # train_samples = X_train.shape[0]\n",
        "        self.classifier = LogisticRegression(\n",
        "            # C=50. / train_samples,\n",
        "            penalty='l1',\n",
        "            solver='liblinear',\n",
        "            tol=0.1,\n",
        "            class_weight=c_weight\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted,\n",
        "                self.test_y_predicted)\n",
        "\n",
        "class RfModel(BaseModel):\n",
        "\n",
        "    model_type = 'Random Forest'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training random forest...')\n",
        "        self.classifier = RandomForestClassifier(\n",
        "            n_estimators=500, \n",
        "            class_weight=c_weight, \n",
        "            n_jobs=-1\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
        "\n",
        "class GDBCModel(BaseModel):\n",
        "\n",
        "    model_type = 'Gradient Boost classifier'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training GDBC...')\n",
        "        self.classifier = GradientBoostingClassifier(\n",
        "            n_estimators=1200,\n",
        "            max_depth=3,\n",
        "            subsample=0.5,\n",
        "            learning_rate=0.01,\n",
        "            min_samples_leaf=1,\n",
        "            random_state=3\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)\n",
        "\n",
        "class KnnModel(BaseModel):\n",
        "\n",
        "    model_type = 'Nearest Neighbour classifier'\n",
        "    def fit_predict(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('training KNN...')\n",
        "        self.classifier = neighbors.KNeighborsClassifier(\n",
        "            n_neighbors = 10,\n",
        "            n_jobs = -1\n",
        "            )\n",
        "        self.classifier.fit(X_train, y_train)\n",
        "        self.test_y_predicted = self.classifier.predict(X_test)\n",
        "        self.val_y_predicted = self.classifier.predict(X_val)\n",
        "        return (X_train, X_val, X_test, self.val_y_predicted, self.test_y_predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    # we train normally and use the probabilities to select the most uncertain samples\n",
        "    def train(self, X_train, y_train, X_val, X_test, c_weight):\n",
        "        print ('Train set:', X_train.shape)\n",
        "        print ('Validation set:', X_val.shape)\n",
        "        print ('Test set:', X_test.shape)\n",
        "        t0 = time.time()\n",
        "        (X_train, X_val, X_test, self.val_y_predicted,\n",
        "         self.test_y_predicted) = \\\n",
        "            self.model_object.fit_predict(X_train, y_train, X_val, X_test, c_weight)\n",
        "        self.run_time = time.time() - t0\n",
        "        return (X_train, X_val, X_test)\n",
        "\n",
        "    # we want accuracy only for the test set\n",
        "    def get_test_accuracy(self, i, y_test):\n",
        "        classif_rate = np.mean(self.test_y_predicted.ravel() == y_test.ravel()) * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        print('--------------------------------')\n",
        "        print('y-test set:',y_test.shape)\n",
        "        print('Training run in %.3f s' % self.run_time,'\\n')\n",
        "        print(\"Accuracy rate is %f \" % (classif_rate))    \n",
        "        print(\"Classification report for %s:\\n%s\\n\" % (self.model_object.classifier, metrics.classification_report(y_test, self.test_y_predicted)))\n",
        "        print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, self.test_y_predicted))\n",
        "        print('--------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QueryFunction(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # def stream_select(self):\n",
        "    #     pass\n",
        "\n",
        "    def pool_select(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomSelection(QueryFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        random_state = check_random_state(0)\n",
        "        # probas_val.shape[0] is the size of validation set\n",
        "        selection = np.random.choice(probas_val.shape[0], batch_size, replace=False)\n",
        "        print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',batch_size)\n",
        "        return selection\n",
        "\n",
        "\n",
        "class EntropySelection(QueryFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "        selection = (np.argsort(e)[::-1])[:batch_size]\n",
        "        return selection\n",
        "\n",
        "class MinStdSelection(QueryFunction):\n",
        "\n",
        "    # select the samples where the std is smallest. There is uncertainty regarding the relevant class\n",
        "    # and then train on these \"hard\" to classify samples.\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        std = np.std(probas_val * 100, axis=1) \n",
        "        selection = std.argsort()[:batch_size]\n",
        "        selection = selection.astype('int64')\n",
        "        print('std',std.shape,std)\n",
        "        print('selection',selection, selection.shape, std[selection])\n",
        "        return selection\n",
        "\n",
        "class LeastConfidenceSelection(QueryFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        sort_prob = -np.sort(-probas_val, axis=1)\n",
        "        values = sort_prob[:, 0]\n",
        "        selection = np.argsort(values)[:batch_size]\n",
        "        return selection\n",
        "      \n",
        "      \n",
        "class MarginSamplingSelection(QueryFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        sort_prob = -np.sort(-probas_val, axis=1)\n",
        "        values = sort_prob[:, 0] - sort_prob[:, 1]\n",
        "        selection = np.argsort(values)[:batch_size]\n",
        "        return selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Normalize(object):\n",
        "    \n",
        "    def normalize(self, X_train, X_val, X_test):\n",
        "        self.scaler = RobustScaler()\n",
        "        X_train = self.scaler.fit_transform(X_train)\n",
        "        X_val   = self.scaler.transform(X_val)\n",
        "        X_test  = self.scaler.transform(X_test)\n",
        "        return (X_train, X_val, X_test) \n",
        "    \n",
        "    def inverse(self, X_train, X_val, X_test):\n",
        "        X_train = self.scaler.inverse_transform(X_train)\n",
        "        X_val   = self.scaler.inverse_transform(X_val)\n",
        "        X_test  = self.scaler.inverse_transform(X_test)\n",
        "        return (X_train, X_val, X_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_random_samples(initial_samples, X_train_full,\n",
        "                         y_train_full):\n",
        "\n",
        "    random_state = check_random_state(0)\n",
        "\n",
        "    permutation = np.random.choice(len(X_train_full),initial_samples,replace=False)\n",
        "    \n",
        "    # print ()\n",
        "    # print(type(permutation))\n",
        "    # print ('initial random chosen samples', permutation)\n",
        "\n",
        "    X_train = X_train_full[permutation]\n",
        "    y_train = y_train_full[permutation]\n",
        "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "\n",
        "    # bin_count = np.bincount(y_train.astype('int64'))\n",
        "    # unique = np.unique(y_train.astype('int64'))\n",
        "    # print (\n",
        "    #     'initial train set:',\n",
        "    #     X_train.shape,\n",
        "    #     y_train.shape,\n",
        "    #     'unique(labels):',\n",
        "    #     bin_count,\n",
        "    #     unique,\n",
        "    #     )\n",
        "    return (permutation, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_loss(self, probs):\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for i in range(len(probs)):\n",
        "            for prob in probs[i]:\n",
        "                loss -= (prob*np.log(prob))\n",
        "\n",
        "        return loss/(len(probs)*1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TheAlgorithm(object):\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    def __init__(self, step, model_object, selection_function):\n",
        "        self.step = step\n",
        "        self.model_object = model_object\n",
        "        self.sample_selection_function = selection_function\n",
        "        \n",
        "# To-do: Move initiation selections as arguments\n",
        "    def run(self, X_train_full, y_train_full, X_test, y_test, initial_queried, max_queried):\n",
        "\n",
        "        # initialize process by applying base learner to labeled training data set to obtain Classifier\n",
        "        (permutation, X_train, y_train) = \\\n",
        "            get_random_samples(initial_queried, X_train_full, y_train_full)\n",
        "        self.queried = initial_queried\n",
        "        # self.samplecount = [self.initiation_selections]\n",
        "\n",
        "        # assign the val set the rest of the 'unlabelled' training data\n",
        "        X_val = np.array([])\n",
        "        y_val = np.array([])\n",
        "        X_val = np.copy(X_train_full)\n",
        "        X_val = np.delete(X_val, permutation, axis=0)\n",
        "        y_val = np.copy(y_train_full)\n",
        "        y_val = np.delete(y_val, permutation, axis=0)\n",
        "        # print ('Val set:', X_val.shape, y_val.shape, permutation.shape)\n",
        "        # print ()\n",
        "\n",
        "        # normalize data\n",
        "        normalizer = Normalize()\n",
        "        X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)\n",
        "           \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(1, y_test)\n",
        "\n",
        "        # queried_num = [self.queried]\n",
        "\n",
        "        while self.queried <= max_queried-self.step:\n",
        "\n",
        "            active_iteration += 1\n",
        "            self.queried += self.step\n",
        "            # queried_num.append(self.queried)\n",
        "            # get validation probabilities\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_val)\n",
        "            # print('Classifier class:', self.clf_model.model_object.classifier.classes_)\n",
        "            # print('Probas_val:', probas_val)\n",
        "            # pred_val = \\\n",
        "            #     self.clf_model.val_y_predicted\n",
        "            # model_val = \\\n",
        "            #     self.clf_model\n",
        "            # print ('val predicted:',\n",
        "            #         self.clf_model.val_y_predicted.shape,\n",
        "            #         self.clf_model.val_y_predicted)\n",
        "            # display probability of binary value predictions of the validation set\n",
        "            # print ('probas_val value', probas_val)\n",
        "            # display which binary value has the highest probabilities of the validation set\n",
        "            # print ('probabilities:', probas_val.shape, '\\n',\n",
        "            #        np.argmax(probas_val, axis=1))\n",
        "\n",
        "            # select samples using a selection function\n",
        "            uncertain_samples = self.sample_selection_function.pool_select(probas_val, self.step)\n",
        "\n",
        "            # normalization needs to be inversed and recalculated based on the new train and test set.\n",
        "            X_train, X_val, X_test = normalizer.inverse(X_train, X_val, X_test)   \n",
        "\n",
        "            # get the uncertain samples from the validation set\n",
        "            # print ('trainset before adding uncertain samples', X_train.shape, y_train.shape)\n",
        "            X_train = np.concatenate((X_train, X_val[uncertain_samples]))\n",
        "            y_train = np.concatenate((y_train, y_val[uncertain_samples]))\n",
        "            # print ('trainset after adding uncertain samples', X_train.shape, y_train.shape)\n",
        "            # self.samplecount.append(X_train.shape[0])\n",
        "\n",
        "            \n",
        "\n",
        "            # bin_count = np.bincount(y_train.astype('int64'))\n",
        "            # unique = np.unique(y_train.astype('int64'))\n",
        "            # print (\n",
        "            #     'updated train set:',\n",
        "            #     X_train.shape,\n",
        "            #     y_train.shape,\n",
        "            #     'unique(labels):',\n",
        "            #     bin_count,\n",
        "            #     unique,\n",
        "            #     )\n",
        "\n",
        "            X_val = np.delete(X_val, uncertain_samples, axis=0)\n",
        "            y_val = np.delete(y_val, uncertain_samples, axis=0)\n",
        "            # print ('val set:', X_val.shape, y_val.shape)\n",
        "            # print ()\n",
        "\n",
        "            # normalize again after creating the 'new' train/test sets\n",
        "            normalizer = Normalize()\n",
        "            X_train, X_val, X_test = normalizer.normalize(X_train, X_val, X_test)               \n",
        "\n",
        "            # self.queried += self.step\n",
        "\n",
        "            (X_train, X_val, X_test) = self.clf_model.train(X_train, y_train, X_val, X_test, 'balanced')\n",
        "            self.clf_model.get_test_accuracy(active_iteration, y_test)\n",
        "\n",
        "        # print('Queried numbers', queried_num)\n",
        "        return self.clf_model.accuracies\n",
        "        # print ('final active learning accuracies',\n",
        "        #        self.clf_model.accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pool_experiment(model,sampling_method,max_queried,initial_queried,step):\n",
        "    (X, y) = data_prep()\n",
        "    # (X_train_full, X_test,y_train_full,  y_test) = train_test_split(X, y, test_size=0.25)\n",
        "    # print(type(X_train_full))\n",
        "\n",
        "    kf = KFold(n_splits=4)\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train_full, X_test = X[train_index], X[test_index]\n",
        "        y_train_full, y_test = y[train_index], y[test_index]\n",
        "        \n",
        "    act_alg = TheAlgorithm(step, model , sampling_method)\n",
        "    accuracies = act_alg.run(X_train_full,y_train_full,X_test,y_test,initial_queried,max_queried)\n",
        "\n",
        "    (permutation, X_train_selected, y_train_selected) = get_random_samples(initial_queried, X_train_full, y_train_full)\n",
        "    original_accuracies=[]\n",
        "    classifier_original = LogisticRegression(\n",
        "            # C=50. / X_train_full.shape[0],\n",
        "            penalty='l1',\n",
        "            solver='liblinear',\n",
        "            tol=0.1,\n",
        "            class_weight='balanced')\n",
        "    x_axis = []\n",
        "    for i in range(initial_queried-1,max_queried,step):\n",
        "        classifier_original.fit(X_train_selected[:i+1], y_train_selected[:i+1])\n",
        "        y_pred_original = classifier_original.predict(X_test)\n",
        "        original_accuracies.append(accuracy_score(y_test, y_pred_original)*100)\n",
        "        x_axis.append(i+1)\n",
        "    print(\"accuracies\",accuracies)\n",
        "    print(\"nonactive_accuracies\",original_accuracies)\n",
        "    print(\"x-axis:\",x_axis)\n",
        "    print(\"x-axis length:\",len(x_axis))\n",
        "    # x_axis = np.linspace(initial_queried,max_queried,num=(max_queried - initial_queried)//step +1,endpoint=True)\n",
        "    plt.plot(x_axis, accuracies, 'r',label='active') \n",
        "    plt.plot(x_axis, original_accuracies, 'blue',label='non-active') \n",
        "    plt.legend()\n",
        "    plt.xlabel('Sample Size')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# print ('train:', X_train_full.shape, y_train_full.shape)\n",
        "# print ('test :', X_test.shape, y_test.shape)\n",
        "# classes = len(np.unique(y))\n",
        "# print ('unique classes', classes)\n",
        "\n",
        "# def pickle_save(fname, data):\n",
        "#   filehandler = open(fname,\"wb\")\n",
        "#   pickle.dump(data,filehandler)\n",
        "#   filehandler.close()\n",
        "#   print('saved', fname, os.getcwd(), os.listdir())\n",
        "\n",
        "# def pickle_load(fname):\n",
        "#   print(os.getcwd(), os.listdir())\n",
        "#   file = open(fname,'rb')\n",
        "#   data = pickle.load(file)\n",
        "#   file.close()\n",
        "#   print(data)\n",
        "#   return data\n",
        "  \n",
        "def batch_experiment(d, models, selection_functions, Ks, repeats, contfrom):\n",
        "\n",
        "    (X, y) = data_prep()\n",
        "    # (X_train_full, X_test,y_train_full,  y_test) = train_test_split(X, y, test_size=0.25)\n",
        "    # print(type(X_train_full))\n",
        "\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=4)\n",
        "\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train_full, X_test = X[train_index], X[test_index]\n",
        "        y_train_full, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    algos_temp = []\n",
        "    print ('stopping at:', max_queried)\n",
        "    count = 0\n",
        "    for model_object in models:\n",
        "      if model_object.__name__ not in d:\n",
        "          d[model_object.__name__] = {}\n",
        "      \n",
        "      for selection_function in selection_functions:\n",
        "        if selection_function.__name__ not in d[model_object.__name__]:\n",
        "            d[model_object.__name__][selection_function.__name__] = {}\n",
        "        \n",
        "        for k in Ks:\n",
        "            d[model_object.__name__][selection_function.__name__][str(k)] = []           \n",
        "            \n",
        "            for i in range(0, repeats):\n",
        "                count+=1\n",
        "                if count >= contfrom:\n",
        "                    # print ('Count = %s, using model = %s, selection_function = %s, k = %s, iteration = %s.' % (count, model_object.__name__, selection_function.__name__, k, i))\n",
        "                    alg = TheAlgorithm(k, \n",
        "                                       model_object, \n",
        "                                       selection_function\n",
        "                                       )\n",
        "                    alg.run(X_train_full, y_train_full, X_test, y_test)\n",
        "                    d[model_object.__name__][selection_function.__name__][str(k)].append(alg.clf_model.accuracies)\n",
        "                    # fname = '/Users/wenxuanhuang/Documents/Repo/ML-for-COVID-19-dataset/Results/Active-learning-experiment-' + str(count) + '.pkl'\n",
        "                    # pickle_save(fname, d)\n",
        "                    # if count % 5 == 0:\n",
        "                    #     print(json.dumps(d, indent=2, sort_keys=True))\n",
        "                    # print ()\n",
        "                    # print ('---------------------------- FINISHED ---------------------------')\n",
        "                    # print ()\n",
        "    return d\n",
        "\n",
        "\n",
        "# max_queried = 500 \n",
        "# repeats = 1\n",
        "# models = [LogModel] \n",
        "# # models = [SvmModel, RfModel, LogModel, GDBCModel, KnnModel] \n",
        "# selection_functions = [LeastConfidenceSelection] \n",
        "# # selection_functions = [RandomSelection, MarginSamplingSelection, EntropySelection, MinStdSelection, LeastConfidenceSelection] \n",
        "# Ks = [250,125,50,25,10] \n",
        "# d = {}\n",
        "# stopped_at = -1 \n",
        "# # print('directory dump including pickle files:', os.getcwd(), np.sort(os.listdir()))  \n",
        "# # d = pickle_load('Active-learning-experiment-' + str(stopped_at) + '.pkl')  \n",
        "# # print(json.dumps(d, indent=2, sort_keys=True))\n",
        "# d = batch_experiment(d, models, selection_functions, Ks, repeats, stopped_at+1)\n",
        "# # print (d)\n",
        "# # results = json.loads(json.dumps(d, indent=2, sort_keys=True))\n",
        "# # print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %matplotlib widget\n",
        "# # %matplotlib inline\n",
        "# def performance_plot(fully_supervised_accuracy, dic, models, selection_functions, Ks, repeats):  \n",
        "#     fig, ax = plt.subplots()\n",
        "#     ax.plot([0,500],[fully_supervised_accuracy, fully_supervised_accuracy],label = 'upper-bound')\n",
        "#     for model_object in models:\n",
        "#       for selection_function in selection_functions:\n",
        "#         for idx, k in enumerate(Ks):\n",
        "#             x = np.arange(float(Ks[idx]), 500 + float(Ks[idx]), float(Ks[idx]))            \n",
        "#             Sum = np.array(dic[model_object][selection_function][k][0])\n",
        "#             for i in range(1, repeats):\n",
        "#                 Sum = Sum + np.array(dic[model_object][selection_function][k][i])\n",
        "#             mean = Sum / repeats\n",
        "#             ax.plot(x, mean, label = model_object[0:3] + '-' + selection_function[0:3] + '-' + str(k))\n",
        "#     ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "#     ax.set_xlim([50,500])\n",
        "#     ax.set_ylim([20,83])\n",
        "#     ax.grid(True)\n",
        "#     mplcursors.cursor()\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # models_str = ['SvmModel', 'RfModel', 'LogModel','GDBCModel','KnnModel']\n",
        "# models_str = ['LogModel']\n",
        "# # selection_functions_str = ['RandomSelection', 'MarginSamplingSelection', 'EntropySelection', 'MinStdSelection','LeastConfidenceSelection']\n",
        "# selection_functions_str = ['LeastConfidenceSelection']\n",
        "# Ks_str = ['250','125','50','25','10'] \n",
        "# repeats = 10\n",
        "# # random_forest_upper_bound = 89.\n",
        "# # svm_upper_bound = 87.\n",
        "# log_upper_bound = 87.\n",
        "# # gdbc_upper_bound = 86.\n",
        "# # knn_upper_bound = 86.\n",
        "# total_experiments = len(models_str) * len(selection_functions_str) * len(Ks_str) * repeats\n",
        "\n",
        "# print('So which is the better model? under the stopping condition and hyper parameters')\n",
        "# # performance_plot(random_forest_upper_bound, d, ['RfModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# # performance_plot(svm_upper_bound, d, ['SvmModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# performance_plot(log_upper_bound, d, ['LogModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# # performance_plot(gdbc_upper_bound, d, ['GDBCModel'] , selection_functions_str    , Ks_str, 1)\n",
        "# # performance_plot(log_upper_bound, d, ['KnnModel'] , selection_functions_str    , Ks_str, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ True False False False False False  True False  True  True False False\n",
            " False  True  True False False False False False False False False False\n",
            "  True  True  True False False  True False]\n",
            "df: (1736, 10) (1736,)\n",
            "Train set: (26, 10)\n",
            "Validation set: (1276, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 76.728111 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85       321\n",
            "           1       0.59      0.36      0.45       113\n",
            "\n",
            "    accuracy                           0.77       434\n",
            "   macro avg       0.69      0.64      0.65       434\n",
            "weighted avg       0.75      0.77      0.75       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[292  29]\n",
            " [ 72  41]]\n",
            "--------------------------------\n",
            "Train set: (38, 10)\n",
            "Validation set: (1264, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.94      0.88       321\n",
            "           1       0.73      0.42      0.54       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.68      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[303  18]\n",
            " [ 65  48]]\n",
            "--------------------------------\n",
            "Train set: (50, 10)\n",
            "Validation set: (1252, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 82.488479 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89       321\n",
            "           1       0.75      0.50      0.60       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.79      0.72      0.74       434\n",
            "weighted avg       0.82      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "Train set: (62, 10)\n",
            "Validation set: (1240, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "Train set: (74, 10)\n",
            "Validation set: (1228, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.87       321\n",
            "           1       0.68      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "Train set: (86, 10)\n",
            "Validation set: (1216, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.87       321\n",
            "           1       0.67      0.49      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "Train set: (98, 10)\n",
            "Validation set: (1204, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "Train set: (110, 10)\n",
            "Validation set: (1192, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.184332 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "Train set: (122, 10)\n",
            "Validation set: (1180, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.70      0.72       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[295  26]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "Train set: (134, 10)\n",
            "Validation set: (1168, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.66      0.47      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "Train set: (146, 10)\n",
            "Validation set: (1156, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.69      0.50      0.58       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 57  56]]\n",
            "--------------------------------\n",
            "Train set: (158, 10)\n",
            "Validation set: (1144, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.88       321\n",
            "           1       0.68      0.48      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "Train set: (170, 10)\n",
            "Validation set: (1132, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 79.723502 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.66      0.46      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.74      0.69      0.71       434\n",
            "weighted avg       0.78      0.80      0.78       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[294  27]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "Train set: (182, 10)\n",
            "Validation set: (1120, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.72      0.44      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.69      0.71       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[302  19]\n",
            " [ 63  50]]\n",
            "--------------------------------\n",
            "Train set: (194, 10)\n",
            "Validation set: (1108, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 79.953917 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       321\n",
            "           1       0.67      0.45      0.54       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.75      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "Train set: (206, 10)\n",
            "Validation set: (1096, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.002 s \n",
            "\n",
            "Accuracy rate is 80.414747 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.45      0.55       113\n",
            "\n",
            "    accuracy                           0.80       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.80      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 62  51]]\n",
            "--------------------------------\n",
            "Train set: (218, 10)\n",
            "Validation set: (1084, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "Train set: (230, 10)\n",
            "Validation set: (1072, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 80.645161 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.46      0.55       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.69      0.71       434\n",
            "weighted avg       0.79      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "Train set: (242, 10)\n",
            "Validation set: (1060, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "Train set: (254, 10)\n",
            "Validation set: (1048, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "Train set: (266, 10)\n",
            "Validation set: (1036, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.73      0.47      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.70      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "Train set: (278, 10)\n",
            "Validation set: (1024, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "Train set: (290, 10)\n",
            "Validation set: (1012, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.79       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "Train set: (302, 10)\n",
            "Validation set: (1000, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.72      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "Train set: (314, 10)\n",
            "Validation set: (988, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "Train set: (326, 10)\n",
            "Validation set: (976, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88       321\n",
            "           1       0.72      0.46      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.78      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[301  20]\n",
            " [ 61  52]]\n",
            "--------------------------------\n",
            "Train set: (338, 10)\n",
            "Validation set: (964, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.005 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       321\n",
            "           1       0.70      0.51      0.59       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.77      0.72      0.74       434\n",
            "weighted avg       0.81      0.82      0.81       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[296  25]\n",
            " [ 55  58]]\n",
            "--------------------------------\n",
            "Train set: (350, 10)\n",
            "Validation set: (952, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.005 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.71      0.47      0.56       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "Train set: (362, 10)\n",
            "Validation set: (940, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.72      0.47      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 60  53]]\n",
            "--------------------------------\n",
            "Train set: (374, 10)\n",
            "Validation set: (928, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.005 s \n",
            "\n",
            "Accuracy rate is 81.566820 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.72      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.82       434\n",
            "   macro avg       0.78      0.71      0.73       434\n",
            "weighted avg       0.81      0.82      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[300  21]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "Train set: (386, 10)\n",
            "Validation set: (916, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 80.875576 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.69      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.76      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "Train set: (398, 10)\n",
            "Validation set: (904, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.336406 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.71      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[299  22]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "Train set: (410, 10)\n",
            "Validation set: (892, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.003 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       321\n",
            "           1       0.70      0.48      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.70      0.72       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[298  23]\n",
            " [ 59  54]]\n",
            "--------------------------------\n",
            "Train set: (422, 10)\n",
            "Validation set: (880, 10)\n",
            "Test set: (434, 10)\n",
            "training logistic regression...\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "--------------------------------\n",
            "y-test set: (434,)\n",
            "Training run in 0.004 s \n",
            "\n",
            "Accuracy rate is 81.105991 \n",
            "Classification report for LogisticRegression(class_weight='balanced', penalty='l1', solver='liblinear',\n",
            "                   tol=0.1):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88       321\n",
            "           1       0.70      0.49      0.57       113\n",
            "\n",
            "    accuracy                           0.81       434\n",
            "   macro avg       0.77      0.71      0.73       434\n",
            "weighted avg       0.80      0.81      0.80       434\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            "[[297  24]\n",
            " [ 58  55]]\n",
            "--------------------------------\n",
            "accuracies [76.72811059907833, 80.87557603686636, 82.48847926267281, 80.87557603686636, 80.4147465437788, 80.4147465437788, 80.4147465437788, 80.18433179723502, 80.4147465437788, 79.95391705069125, 81.10599078341014, 80.64516129032258, 79.72350230414746, 81.10599078341014, 79.95391705069125, 80.4147465437788, 80.87557603686636, 80.64516129032258, 80.87557603686636, 81.10599078341014, 81.5668202764977, 81.10599078341014, 80.87557603686636, 81.5668202764977, 81.10599078341014, 81.33640552995391, 81.5668202764977, 81.10599078341014, 81.33640552995391, 81.5668202764977, 80.87557603686636, 81.33640552995391, 81.10599078341014, 81.10599078341014]\n",
            "nonactive_accuracies [64.51612903225806, 62.21198156682027, 62.21198156682027, 63.36405529953917, 62.21198156682027, 62.903225806451616, 63.36405529953917, 62.67281105990783, 62.67281105990783, 61.9815668202765, 63.36405529953917, 62.44239631336406, 62.67281105990783, 62.44239631336406, 62.67281105990783, 63.133640552995395, 62.21198156682027, 62.67281105990783, 63.36405529953917, 63.824884792626726, 62.903225806451616, 63.36405529953917, 65.89861751152074, 62.44239631336406, 62.21198156682027, 66.3594470046083, 63.824884792626726, 63.36405529953917, 62.21198156682027, 62.21198156682027, 62.44239631336406, 61.9815668202765, 65.2073732718894, 62.21198156682027]\n",
            "x-axis: [26, 38, 50, 62, 74, 86, 98, 110, 122, 134, 146, 158, 170, 182, 194, 206, 218, 230, 242, 254, 266, 278, 290, 302, 314, 326, 338, 350, 362, 374, 386, 398, 410, 422]\n",
            "x-axis length: 34\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "pool_experiment(LogModel,LeastConfidenceSelection,433,26,12)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d",
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "name": "Logistic.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}