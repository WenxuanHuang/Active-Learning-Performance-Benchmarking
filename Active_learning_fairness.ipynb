{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ]
        }
      ],
      "source": [
        "print(__doc__)\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# from ace import model\n",
        "# from ace import ace\n",
        "import ace.model\n",
        "import ace.ace\n",
        "\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_data_recid():\n",
        "\n",
        "    \"\"\" This function is used for retrieving dataset COMPAS and split data entries into labeled (training, testing) and unlabeled data (validation) \n",
        "    Prediction task is to determine whether a person will recidive after first prosecution\"\"\"\n",
        "\n",
        "    \"\"\" Binary classification\n",
        "    =================  ======================\n",
        "    samples total      5875\n",
        "    Dimensionality     9(Features)+1(Bias)\n",
        "    Features           real\n",
        "    Classes            2\n",
        "    =================  ======================\n",
        "\n",
        "    Source\n",
        "    ----------\n",
        "    How We Analyzed the COMPAS Recidivism Algorithm, by Jeff Larson, Surya Mattu, Lauren Kirchner and Julia Angwin, May 23, 2016\n",
        "    https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm\n",
        "    https://github.com/propublica/compas-analysis\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    none\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_labeled: training data, ndarray, shape (10, 9)\n",
        "    y_labeled: training target, ndarray, shape (10, ) \n",
        "    b_labeled: bias attribute of training set, ndarray, shape (10, )\n",
        "\n",
        "    X_unlabeled: sample pool data, ndarray, shape (4397, 9)\n",
        "    y_unlabeled: sample pool target, ndarray, shape (4397, )\n",
        "    b_unlabeled: bias attribute of sample pool set, ndarray, shape (4397, )\n",
        "\n",
        "    X_test: testing data, ndarray, shape (1468, 9)\n",
        "    y_test: testing target, ndarray, shape (1468, )\n",
        "    b_test: bias attribute of testing set, ndarray, shape (1468, )\n",
        "    \"\"\"\n",
        "    \n",
        "    # mc_attributes = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree','race']\n",
        "    attributes = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree']\n",
        "    bias = 'race'\n",
        "    target = 'two_year_recid'\n",
        "\n",
        "    # np.random.seed(42)\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/RecidivismData_Normalized.csv\", sep=',')\n",
        "    data_col = data.columns\n",
        "    df = data[(data[bias]==2)|(data[bias]==3)].copy().values\n",
        "    # print(df.shape)\n",
        "\n",
        "    b_Xb_data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/b_Xb_Mc.csv\", sep=',').values\n",
        "    X_Xb_data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/X_Xb_Mc.csv\", sep=',').values\n",
        "    y_Xy_data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/y_Xy_Mc.csv\", sep=',').values\n",
        "    X_Xy_data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/X_Xy_Mc.csv\", sep=',').values\n",
        "\n",
        "    print(b_Xb_data.shape, X_Xb_data.shape, y_Xy_data.shape, X_Xy_data.shape)\n",
        "\n",
        "    kf = KFold(n_splits=4)\n",
        "    for train_index, test_index in kf.split(df):\n",
        "        train, test = df[train_index], df[test_index]\n",
        "        # print(\"Size of X_train_full, X_test:\", train.shape, test.shape)\n",
        "\n",
        "    df_train = pd.DataFrame(data=train, columns=data_col)\n",
        "    df_test = pd.DataFrame(data=test, columns=data_col)\n",
        "\n",
        "    labeled = df_train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=5)) # ten sample in total labeled initially\n",
        "    # labeled = df_train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=5, random_state=42)) # with a random state for stable output\n",
        "    df_X_labeled = labeled[attributes]\n",
        "    df_y_labeled = labeled[target]\n",
        "    X_labeled = df_X_labeled.values\n",
        "    y_labeled = df_y_labeled.values.astype('int64')\n",
        "    b_labeled = labeled[bias].values-2 #degrade bias into binary options\n",
        "    (row_size, col_size) = X_labeled.shape \n",
        "    # print(X_labeled.shape)\n",
        "\n",
        "    unlabeled = df_train.drop(df_X_labeled.index)\n",
        "    df_X_unlabeled = unlabeled[attributes]\n",
        "    df_y_unlabeled = unlabeled[target]\n",
        "    X_unlabeled = df_X_unlabeled.values\n",
        "    y_unlabeled = df_y_unlabeled.values.astype('int64')\n",
        "    b_unlabeled = unlabeled[bias].values-2\n",
        "    # print(X_unlabeled.shape)\n",
        "\n",
        "    X_test = df_test[attributes].values\n",
        "    y_test = df_test[target].values\n",
        "    y_test=y_test.astype('int')\n",
        "    b_test = df_test[bias].values-2\n",
        "    # print(X_test.shape)\n",
        "    \n",
        "    return (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    def fit_predict(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        self.classifier = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "        self.classifier.fit(X_labeled, y_labeled)\n",
        "        features_weight= self.classifier.coef_.T\n",
        "        # self.y_test_predicted = self.classifier.predict(X_test)\n",
        "        # self.y_unlabeled_predicted = self.classifier.predict(X_unlabeled)\n",
        "        self.y_test_score = self.classifier.score(X_test, y_test)\n",
        "        return (X_labeled, X_test, self.y_test_score, features_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    def train(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        (X_labeled, X_test, self.y_test_score, features_weight) = \\\n",
        "            self.model_object.fit_predict(X_labeled, y_labeled, X_test, y_test)\n",
        "        return (X_labeled, X_test, features_weight)\n",
        "\n",
        "    def get_test_accuracy(self, i):\n",
        "        classif_rate = self.y_test_score * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        # print(\"Accuracy rate is %f \" % (classif_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_selection(probas_val, step):\n",
        "    # random_state = check_random_state(63)\n",
        "    selection = np.random.choice(probas_val.shape[0], step, replace=False)\n",
        "    return selection\n",
        "\n",
        "# def entropy_selection(probas_val, step):\n",
        "#     e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "#     selection = (np.argsort(e)[::-1])[:step]\n",
        "#     return selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalizer(e_loss, f_loss):\n",
        "    e_loss = np.reshape(e_loss, (1,len(e_loss)))\n",
        "    f_loss = np.reshape(f_loss, (1,len(f_loss)))\n",
        "    e_scaled = preprocessing.normalize(e_loss)\n",
        "    # e_scaled=((e_loss-e_loss.min())/(e_loss.max()-e_loss.min()))\n",
        "    f_scaled = preprocessing.normalize(f_loss)\n",
        "    # f_scaled=((f_loss-f_loss.min())/(f_loss.max()-f_loss.min()))\n",
        "    e_scaled = e_scaled.flatten()\n",
        "    f_scaled = f_scaled.flatten()\n",
        "    return (e_scaled, f_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_loss(probas_val):\n",
        "    \n",
        "    eps = np.finfo(probas_val.dtype).eps\n",
        "    probas_val = np.clip(probas_val, eps, 1 - eps)\n",
        "    e_loss = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "\n",
        "    return e_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" This function is used for calculating fairness loss, aka the unfairness of a logistic regression model. The fairness measurement is used to provide loss values that helps determine the fluctuation of unfairness change with each iteration that adds samples to the labeled dataset. There are many different methods for unfairness measurement, however, five different meausures are coded and only one of them will be used for unfairness measurement in the correlation-based sampling. \"\"\"\n",
        "\n",
        "\"\"\" Formulas\n",
        "\n",
        "Mutual Information: MI(B,Y') = \\sum_{i = 1}^{|B|}\\sum_{j = 1}^{|Y'|}\\frac{|B_{i} \\cap Y'_{j}|}{N} log\\frac{N|B_{i} \\cap Y'_{j}|}{|B_{i}||Y'_{j}|}\n",
        "Statistical Parity: SP(B,Y') = |P(Y' = 1|B = a) - P(Y' = 1|B = b)| \\quad \\forall a,b \\in B\n",
        "Equal Opportunity: EOP(B,Y') = |P(Y' = 1|B = a, Y = 1) - P(Y' = 1|B = b, Y = 1)| \\quad \\forall a,b \\in B\n",
        "Equalized Odds: EOD(B,Y') = |P(Y' = 1|B = a, Y = y) - P(Y' = 1|B = b, Y = y)| \\quad \\forall a,b \\in B, \\quad \\forall 0,1 \\in y\n",
        "Conditional Use Accuracy Equality: CUAE(B,Y') = |P(Y' = y|B = a, Y = y) - P(Y' = y|B = b, Y = y)| \\quad \\forall a,b \\in B, \\quad \\forall 0,1 \\in y\n",
        "Disparate Impact: DI(B,Y') = |P(Y' = 1|B = a) / P(Y' = 1|B = b)| \\quad \\forall a,b \\in B\n",
        "\n",
        "Source\n",
        "----------\n",
        "\n",
        "separation \\ Equal opportunity - Hardt, Price, Srebro (2016)\n",
        "separation \\ Equalized adds - Hardt, Price, Srebro (2016)\n",
        "independence \\ Mutual information - Cover, T.M.; Thomas, J.A. (1991). Elements of Information Theory (Wiley ed.)\n",
        "sufficiency \\ Conditional Use Accuracy Equality - Richard Berk et al., (2017) Fairness in Criminal Justice Risk Assessments: The State of the Art\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "X_fair_est: testing data, ndarray\n",
        "y_fair_est: testing target, ndarray\n",
        "b_fair_est: bias attribute of testing set, ndarray\n",
        "\n",
        "Returns\n",
        "-------\n",
        "f_loss: fairness loss, int\n",
        "\"\"\"\n",
        "\n",
        "def mut_inf(X_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    f_loss=mutual_info_score(b_fair_est, y_fair_pred)\n",
        "    f_loss=abs(f_loss)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def stats_parity(X_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0=X_fair_est[(b_fair_est==0)].shape[0]\n",
        "    b1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1=X_fair_est[(b_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0p1/b0)-(b1p1/b1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def eqops(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0y1p1/b0y1)-(b1y1p1/b1y1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def eqods(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y0p1=X_fair_est[(b_fair_est==0)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0y0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)].shape[0]\n",
        "    b1y0p1=X_fair_est[(b_fair_est==1)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b1y0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)].shape[0]\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    fpr_loss=abs((b0y0p1/b0y0)-(b1y0p1/b1y0))\n",
        "    tpr_loss=abs((b0y1p1/b0y1)-(b1y1p1/b1y1))\n",
        "\n",
        "\n",
        "    f_loss = (fpr_loss+tpr_loss)/2 # justification of half in terms of fpr_loss UNION tpr_loss\n",
        "    \n",
        "    return f_loss \n",
        "\n",
        "def cuae(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y0p0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)&(y_fair_pred==0)].shape[0]\n",
        "    b0y0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)].shape[0]\n",
        "    b1y0p0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)&(y_fair_pred==0)].shape[0]\n",
        "    b1y0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)].shape[0]\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    tnr_loss=abs((b0y0p0/b0y0)-(b1y0p0/b1y0))\n",
        "    tpr_loss=abs((b0y1p1/b0y1)-(b1y1p1/b1y1))\n",
        "\n",
        "    f_loss = (tnr_loss+tpr_loss)/2\n",
        "    \n",
        "    return f_loss \n",
        "\n",
        "def disp_impt(X_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0=X_fair_est[(b_fair_est==0)].shape[0]\n",
        "    b1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1=X_fair_est[(b_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0p1/b0)/(b1p1/b1)\n",
        "    f_loss = f_loss-1\n",
        "    return f_loss\n",
        "# selecting fairness criteria\n",
        "def fair_measure(X_fair_est, y_fair_est, b_fair_est, classifier=None, criteria=0):\n",
        "    if criteria == 'mutual_information':\n",
        "        return mut_inf(X_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'equal_opportunity':\n",
        "        return eqops(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'statistical_parity':\n",
        "        return stats_parity(X_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'equalized_odds':\n",
        "        return eqods(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == \"disparate_impact\":\n",
        "        return disp_impt(X_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == \"conditional_use_accuracy_equality\":\n",
        "        return cuae(X_fair_est, y_fair_est, b_fair_est, classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fair_loss_corr(X_data, prob_expected, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, Xy_std):\n",
        "\n",
        "    # print(\"Correlation between X and S:\", Xb_corr)\n",
        "    # print(\"Correlation between y' and S:\", yhatb_corr)\n",
        "    # print(\"d:prob_expected:\", prob_expected.shape, prob_expected) output: shape(2,1), probabilities of y' =0 and 1\n",
        "\n",
        "    fair_loss = 0\n",
        "    fair_improvement = 0\n",
        "    row_expected = row_size + 1 \n",
        "    for i in range(len(prob_expected)): \n",
        "        y_expected = (y_labeled_sum + i)/row_expected\n",
        "        Xy_expected = np.add(Xy_labeled_sum , X_data*i)/row_expected\n",
        "        X_expected = np.add(X_labeled_sum , X_data)/row_expected\n",
        "        Xy_corr_expected = (np.subtract(Xy_expected, (X_expected * y_expected)))/Xy_std\n",
        "        # print(\"Correlation between X and y:\", Xy_corr)\n",
        "        # print(\"Expected correlation between X and y:\", Xy_corr_expected)\n",
        "        fair_improvement = np.dot((abs(features_weight*Xb_corr)).transpose(),(abs(Xy_corr)-abs(Xy_corr_expected)).reshape(Xy_corr_expected.shape[0],1))[0,0] # if expected corr(xy) is reduced, the feature weight is higher\n",
        "        fair_loss += prob_expected[i]*fair_improvement # fairness improvement for a point, both y=1 and y=0 are added as one\n",
        "        # print(\"Shape of fair_improvement and temp[i]\",fair_improvement, prob_expected[i])\n",
        "    return fair_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unfairness_correlation_sampling(X_unlabeled, classifier, probas_val, step, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, div, Xy_std):\n",
        "    # further to be defined, now assume only fairness loss\n",
        "    # query size used here\n",
        "\n",
        "    unlabeled_size = len(X_unlabeled)\n",
        "    f_loss = np.zeros(unlabeled_size)\n",
        "    \n",
        "    for j in range(unlabeled_size): \n",
        "            proba_temp = classifier.predict_proba(X_unlabeled[j].reshape(1, -1)).reshape(-1,1)\n",
        "            # first reshape integrate as single entry\n",
        "            # second reshape transpose as vertical 2D array\n",
        "            f_loss[j] = fair_loss_corr(X_unlabeled[j],proba_temp, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, Xy_std)\n",
        "\n",
        "    e_loss = log_loss(probas_val)\n",
        "\n",
        "#     print(\"Debug f_loss before:\", f_loss.shape, f_loss)\n",
        "\n",
        "    e_scaled, f_scaled = normalizer(e_loss, f_loss)\n",
        "    f_scaled[np.isnan(f_scaled)] = 0\n",
        "\n",
        "#     print(\"Debug f_loss after:\", f_scaled.shape, f_scaled)\n",
        "\n",
        "    loss = div*(e_scaled)+(1-div)*f_scaled\n",
        "    selection = np.argsort(loss)[::-1][:step]\n",
        "    \n",
        "\n",
        "    return selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pre_filter(X_data, b_data, y_data, budget): \n",
        "\n",
        "    temp_columns = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree','race','two_year_recid']\n",
        "    # np.random.seed(84)\n",
        "\n",
        "    # print(\"Debug data:\", X_data.shape, b_data.shape, y_data.shape)\n",
        "    temp_data = np.c_[X_data, b_data, y_data]\n",
        "    temp_df = pd.DataFrame(data=temp_data, columns=temp_columns)\n",
        "    candidates_data = temp_df.groupby('race', group_keys=False).apply(lambda x: x.sample(n=math.ceil(budget/2)))\n",
        "    # candidates_data = temp_df.groupby('race', group_keys=False).apply(lambda x: x.sample(n=math.ceil(budget/2), random_state = 84))\n",
        "    # print(\"Debug ceil:\", math.ceil(budget/2))\n",
        "    # print(\"Debug ceil:\", math.ceil(budget/2))\n",
        "    # print(\"Debug cand:\", candidates)\n",
        "    candidates_index = candidates_data.index.values\n",
        "    # print(\"Debug index:\", candidates_index.shape, candidates_index)\n",
        "\n",
        "    return candidates_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metrics(X_test, y_test, b_test, classifier):\n",
        "    y_test_pred = classifier.predict(X_test)\n",
        "    # print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, classification_report(y_test, y_test_pred)))\n",
        "    # print(\"Confusion matrix:\\n%s\" % confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "    # tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
        "    # tpr = tp/(tp+fn)\n",
        "    # tnr = tn/(tn+fp) \n",
        "    # fpr = fp/(fp+tn)\n",
        "    # fnr = fn/(tp+fn)\n",
        "    # fdr = fp/(tp+fp)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "\n",
        "    # model_displays = {}\n",
        "    # for i in np.unique(y_test):\n",
        "    #     model_displays[i] = plot_roc_curve(\n",
        "    #         classifier, X_test[b_test==i], y_test[b_test==i], ax=ax, name=i)\n",
        "    # ax.set_title('ROC curve')\n",
        "    # plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scatters(X_unlabeled, b_unlabeled, probas_val, uncertain_samples):\n",
        "    y_plot = probas_val\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_axes([0,0,1,1])\n",
        "    ax.scatter(X_unlabeled[:,1], y_plot[:,1], c=b_unlabeled, cmap = 'Pastel1')\n",
        "    ax.scatter(X_unlabeled[:,1][uncertain_samples], y_plot[:,1][uncertain_samples], c=b_unlabeled[uncertain_samples], edgecolors='black', s=100, cmap = 'Pastel1')\n",
        "    ax.set_xlabel('Age')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Population plot')\n",
        "    y_plot = np.delete(y_plot, uncertain_samples, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def performance_progression(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, pplot_file_name,run):\n",
        "    fig, (ax1, ax2) = plt.subplots(2,figsize=(10,10))\n",
        "    fig.suptitle('Fairness and accuracy metrics')\n",
        "    ax1.plot(x_axis, ur_fairness, color='r', label='unfair-active')\n",
        "    ax1.plot(x_axis, rs_fairness, color='g', label='random-active')\n",
        "    ax1.plot(x_axis, nonal_fairness, color='b', label='non-active')\n",
        "    ax1.legend()\n",
        "    ax2.plot(x_axis, ur_accuracies, color='r', label='unfair-active')\n",
        "    ax2.plot(x_axis, rs_accuracies, color='g', label='random-active')\n",
        "    ax2.plot(x_axis, nonal_accuracies, color='b', label='non-active')\n",
        "    ax2.legend()\n",
        "    ax1.set_xlabel('Sample size')\n",
        "    ax1.set_ylabel('Unfairness')\n",
        "    ax2.set_ylabel('Accuracies')\n",
        "    plt.savefig(pplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trade_off_plot(ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, tplot_file_name, smooth_tplot_file_name,run):\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    ur_index = np.argsort(ur_accuracies)\n",
        "    rs_index = np.argsort(rs_accuracies)\n",
        "    nonal_index = np.argsort(nonal_accuracies) \n",
        "\n",
        "    plt.plot(np.array(ur_accuracies)[ur_index], np.array(ur_fairness)[ur_index], color='r', label='unfair-active')\n",
        "    plt.plot(np.array(rs_accuracies)[rs_index], np.array(rs_fairness)[rs_index], color='g', label='random-active')\n",
        "    plt.plot(np.array(nonal_accuracies)[nonal_index], np.array(nonal_fairness)[nonal_index], color='b', label='non-active')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Accuracies')\n",
        "    plt.ylabel('Unfairness')\n",
        "    plt.savefig(tplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "    ur_smooth = sm.nonparametric.lowess(np.array(ur_fairness)[ur_index], np.array(ur_accuracies)[ur_index], frac = 0.5)\n",
        "    rs_smooth = sm.nonparametric.lowess(np.array(rs_fairness)[rs_index], np.array(rs_accuracies)[rs_index], frac = 0.5)\n",
        "    nonal_smooth = sm.nonparametric.lowess(np.array(nonal_fairness)[nonal_index], np.array(nonal_accuracies)[nonal_index], frac = 0.5)\n",
        "\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    plt.plot(ur_smooth[:, 0], ur_smooth[:, 1], color='r', label='unfair-active')\n",
        "    plt.plot(rs_smooth[:, 0], rs_smooth[:, 1], color='g', label='random-active')\n",
        "    plt.plot(nonal_smooth[:, 0], nonal_smooth[:, 1], color='b', label='non-active')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Approximate accuracies')\n",
        "    plt.ylabel('Approximate unfairness')\n",
        "    plt.savefig(smooth_tplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def maximal_correlation_transform(ace_model, x, y):\n",
        "    \n",
        "    ace_model.build_model_from_xy(x, y)\n",
        "    X_transformed = ace_model.ace.x_transforms\n",
        "    y_transformed = ace_model.ace.y_transform\n",
        "    \n",
        "    return X_transformed, y_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Correlation-based sample selection:  \"\"\"\n",
        "\n",
        "\"\"\" Formulas\n",
        "\n",
        "Mutual Information: MI(B,Y') = \\sum_{i = 1}^{|B|}\\sum_{j = 1}^{|Y'|}\\frac{|B_{i} \\cap Y'_{j}|}{N} log\\frac{N|B_{i} \\cap Y'_{j}|}{|B_{i}||Y'_{j}|}\n",
        "Statistical Parity: SP(B,Y') = |P(Y' = 1|B = a) - P(Y' = 1|B = b)| \\quad \\forall a,b \\in B\n",
        "Equal Opportunity: EOP(B,Y') = |P(Y' = 1|B = a, Y = 1) - P(Y' = 1|B = b, Y = 1)| \\quad \\forall a,b \\in B\n",
        "Equalized Odds: EOD(B,Y') = |P(Y' = 1|B = a, Y = y) - P(Y' = 1|B = b, Y = y)| \\quad \\forall a,b \\in B, \\quad \\forall 0,1 \\in y\n",
        "Conditional Use Accuracy Equality: CUAE(B,Y') = |P(Y' = y|B = a, Y = y) - P(Y' = y|B = b, Y = y)| \\quad \\forall a,b \\in B, \\quad \\forall 0,1 \\in y\n",
        "Disparate Impact: DI(B,Y') = |P(Y' = 1|B = a) / P(Y' = 1|B = b)| \\quad \\forall a,b \\in B\n",
        "\n",
        "Source\n",
        "----------\n",
        "\n",
        "separation \\ Equal opportunity - Hardt, Price, Srebro (2016)\n",
        "separation \\ Equalized adds - Hardt, Price, Srebro (2016)\n",
        "independence \\ Mutual information - Cover, T.M.; Thomas, J.A. (1991). Elements of Information Theory (Wiley ed.)\n",
        "sufficiency \\ Conditional Use Accuracy Equality - Richard Berk et al., (2017) Fairness in Criminal Justice Risk Assessments: The State of the Art\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "X_fair_est: testing data, ndarray\n",
        "y_fair_est: testing target, ndarray\n",
        "b_fair_est: bias attribute of testing set, ndarray\n",
        "\n",
        "Returns\n",
        "-------\n",
        "f_loss: fairness loss, int\n",
        "\"\"\"\n",
        "\n",
        "class active_learning(object):\n",
        "\n",
        "    def __init__(self, step, budget, model_object, criteria):\n",
        "        self.step = step\n",
        "        self.budget = budget\n",
        "        self.model_object = model_object\n",
        "        # self.sample_selection_function = selection_function\n",
        "        self.criteria = criteria\n",
        "        \n",
        "    def run(self, X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc, X_labeled_Xy_Mc, y_labeled_Xy_Mc, X_unlabeled_Xy_Mc, y_unlabeled_Xy_Mc, sub_option, af_div):\n",
        "  \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_labeled, X_test, features_weight) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "\n",
        "        Xb_corr = np.corrcoef(np.concatenate((X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc.reshape(b_unlabeled_Xb_Mc.shape[0],1)),1).T.astype(float))[0:col_size,-1].reshape(col_size,1) # the correlation of features and sensitive attribute (do not iteratively change)\n",
        "\n",
        "        \"\"\"Xb correlation: should it be the correlation of X and b for labeled or unlabeled data?\"\"\"\n",
        "\n",
        "        Xy_labeled_sum = np.zeros(col_size) # 1D array\n",
        "        X_labeled_sum = np.zeros(col_size) # 1D array\n",
        "        y_labeled_sum = np.sum(y_labeled_Xy_Mc) #integer, sum of all labeled y\n",
        "\n",
        "        for i in range(row_size):\n",
        "            X_labeled_sum = np.add(X_labeled_sum, X_labeled_Xy_Mc[i])\n",
        "            Xy_labeled_sum = np.add(Xy_labeled_sum, X_labeled_Xy_Mc[i]*y_labeled_Xy_Mc[i])\n",
        "\n",
        "        y_mean = 1./row_size*y_labeled_sum\n",
        "        Xy_corr = (1./row_size*np.subtract(Xy_labeled_sum,X_labeled_sum*y_mean))/(np.std(X_labeled_Xy_Mc)*np.std(y_labeled_Xy_Mc)) # (sum of x for y=1 - sum of x with ratio of y=1)/row size\n",
        "        yhatb_corr = np.dot(Xb_corr.transpose(),features_weight)[0,0] # Correlation between prediction of y and biased feature\n",
        "\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(active_iteration)\n",
        "        self.query_size = len(X_labeled)\n",
        "        fairness = []\n",
        "        fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "        metrics(X_test, y_test, b_test, self.clf_model.model_object.classifier)\n",
        "\n",
        "        while self.query_size <= self.budget-self.step:\n",
        "\n",
        "            active_iteration += 1\n",
        "            self.query_size += self.step\n",
        "\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_unlabeled)\n",
        "\n",
        "            Xy_std = np.std(X_labeled)*np.std(y_labeled)\n",
        "\n",
        "            # yb_corr=np.dot(Xb_corr.transpose(),features_weight)[0,0]\n",
        "            \n",
        "            # print(\"Debug predicted:\", y_unlabeled_predicted.shape)\n",
        "            # print(\"Debug probas_val:\", probas_val.shape)\n",
        "\n",
        "            # if sub_option == \"Legacy_filter\":\n",
        "            #     # post_entropy_index = entropy_selection(probas_val, self.budget*2)\n",
        "            #     candidates_index = pre_filter(X_unlabeled, b_unlabeled, y_unlabeled, self.budget)\n",
        "            #     uncertain_samples = unfairness_reduction_sampling(self.query_size, X_unlabeled[candidates_index], X_labeled, y_labeled, self.clf_model.model_object.classifier, probas_val[candidates_index], self.step, self.criteria)\n",
        "\n",
        "            if sub_option == \"Pre_filter\":\n",
        "                candidates_index = pre_filter(X_unlabeled, b_unlabeled, y_unlabeled, self.budget)\n",
        "                uncertain_samples = unfairness_correlation_sampling(X_unlabeled[candidates_index], self.clf_model.model_object.classifier, probas_val[candidates_index], self.step, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, af_div, Xy_std)\n",
        "\n",
        "            elif sub_option == \"No_filter\": \n",
        "                uncertain_samples = unfairness_correlation_sampling(X_unlabeled, self.clf_model.model_object.classifier, probas_val, self.step, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, af_div, Xy_std)\n",
        "\n",
        "            elif sub_option == \"Filter_only\":\n",
        "                uncertain_samples = random_selection(probas_val, self.step)\n",
        "            # print(\"Debug shape of X_unlabeled and loss:\", selection)\n",
        "\n",
        "            # scatters(X_unlabeled, b_unlabeled, probas_val, uncertain_samples)\n",
        "\n",
        "            X_labeled = np.concatenate((X_labeled, X_unlabeled[uncertain_samples]))\n",
        "            y_labeled = np.concatenate((y_labeled, y_unlabeled[uncertain_samples]))\n",
        "            X_labeled_Xy_Mc = np.concatenate((X_labeled_Xy_Mc, X_unlabeled_Xy_Mc[uncertain_samples])) # needs verification\n",
        "            X_labeled_Xy_Mc = np.concatenate((y_labeled_Xy_Mc, y_unlabeled_Xy_Mc[uncertain_samples])) # needs verification\n",
        "\n",
        "            (X_labeled, X_test, features_weight) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "\n",
        "            row_size += self.step\n",
        "\n",
        "            for i in uncertain_samples:\n",
        "\n",
        "                X_labeled_sum = np.add(X_labeled_sum,X_unlabeled_Xy_Mc[i])\n",
        "                # print(\"Debug before update:\", Xy_labeled_sum)\n",
        "                y_labeled_sum += y_unlabeled_Xy_Mc[i]; \n",
        "                Xy_labeled_sum = np.add(Xy_labeled_sum, (X_unlabeled_Xy_Mc[i]*y_unlabeled_Xy_Mc[i]))\n",
        "\n",
        "            y_expected = 1./row_size*y_labeled_sum\n",
        "            Xy_corr = (1./row_size*np.subtract(Xy_labeled_sum,X_labeled_sum*y_expected))/(np.std(X_labeled_Xy_Mc)*np.std(X_labeled_Xy_Mc))\n",
        "            yhatb_corr = np.dot(Xb_corr.transpose(),features_weight)[0,0] # update based on features weight change\n",
        "            # print(\"Debug Xb corr:\", Xb_corr)\n",
        "            # print(\"Debug yhatb corr:\", yhatb_corr)\n",
        "\n",
        "            X_unlabeled = np.delete(X_unlabeled, uncertain_samples, axis=0)\n",
        "            y_unlabeled = np.delete(y_unlabeled, uncertain_samples, axis=0)\n",
        "            b_unlabeled = np.delete(b_unlabeled, uncertain_samples, axis=0)\n",
        "\n",
        "            X_unlabeled_Xb_Mc = np.delete(X_unlabeled_Xb_Mc, uncertain_samples, axis=0)\n",
        "            b_unlabeled_Xb_Mc = np.delete(b_unlabeled_Xb_Mc, uncertain_samples, axis=0)\n",
        "            X_unlabeled_Xy_Mc = np.delete(X_unlabeled_Xy_Mc, uncertain_samples, axis=0)\n",
        "            y_unlabeled_Xy_Mc = np.delete(y_unlabeled_Xy_Mc, uncertain_samples, axis=0)\n",
        "            X_labeled_Xy_Mc = np.delete(X_labeled_Xy_Mc, uncertain_samples, axis=0)\n",
        "            y_labeled_Xy_Mc = np.delete(y_labeled_Xy_Mc, uncertain_samples, axis=0)\n",
        "\n",
        "            fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "            self.clf_model.get_test_accuracy(active_iteration)\n",
        "\n",
        "            # metrics(X_test, y_test, b_test, self.clf_model.model_object.classifier)\n",
        "\n",
        "        return self.clf_model.accuracies, fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled,y_labeled, X_test, y_test, b_test, budget, step, unfairness_criteria): \n",
        "\n",
        "    initial_X_train = X_labeled\n",
        "    initial_y_train = y_labeled\n",
        "    nonal_X_train = X_unlabeled\n",
        "    nonal_y_train = y_unlabeled\n",
        "    nonal_X_test = X_test\n",
        "    nonal_y_test = y_test\n",
        "    nonal_b_test = b_test\n",
        "    nonal_fairness = []\n",
        "\n",
        "    nonal_accuracies=[]\n",
        "\n",
        "    classifier_nonal = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "\n",
        "    classifier_nonal.fit(initial_X_train, initial_y_train)\n",
        "    initial_y_pred = classifier_nonal.predict(nonal_X_test)\n",
        "    nonal_fairness = np.append(nonal_fairness, fair_measure(nonal_X_test, nonal_y_test, nonal_b_test, classifier=classifier_nonal, criteria=unfairness_criteria))\n",
        "    nonal_accuracies.append(accuracy_score(nonal_y_test, initial_y_pred)*100)\n",
        "\n",
        "    for i in np.arange(init_index+step,budget+1,step):\n",
        "        classifier_nonal.fit(nonal_X_train[:i], nonal_y_train[:i])\n",
        "        nonal_y_pred = classifier_nonal.predict(nonal_X_test)\n",
        "        nonal_fairness = np.append(nonal_fairness, fair_measure(nonal_X_test, nonal_y_test, nonal_b_test, classifier=classifier_nonal, criteria=unfairness_criteria))\n",
        "        nonal_accuracies.append(accuracy_score(nonal_y_test, nonal_y_pred)*100)\n",
        "        metrics(X_test, y_test, b_test, classifier_nonal)\n",
        "\n",
        "    return nonal_accuracies, nonal_fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def results(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, run):\n",
        "\n",
        "    data = (np.array([x_axis, ur_accuracies, ur_fairness, rs_accuracies, rs_fairness, nonal_accuracies, nonal_fairness])).T\n",
        "\n",
        "    log_file_name = \"result_log/{run}.Result.csv\"\n",
        "    pplot_file_name = \"result_log/{:03}.Perf_Plot.png\"\n",
        "    tplot_file_name = \"result_log/{:03}.Trdoff_Plot.png\"\n",
        "    smooth_tplot_file_name = \"result_log/{:03}.Smooth.Trdoff_Plot.png\"\n",
        "    np.savetxt(log_file_name.format(run = run), data, delimiter=',', header=\"x_axis, ur_accuracies, ur_fairness, rs_accuracies, rs_fairness, nonal_accuracies, nonal_fairness\", fmt='%d,%f,%f,%f,%f,%f,%f')\n",
        "\n",
        "    performance_progression(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, pplot_file_name, run)\n",
        "    trade_off_plot(ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, tplot_file_name, smooth_tplot_file_name, run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def experiment(model,budget,step,criteria,sub_option=None):\n",
        "    \n",
        "    (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test) = retrieve_data_recid()\n",
        "    init_index = len(X_labeled)\n",
        "\n",
        "    act_alg = active_learning(step, budget, model, criteria)\n",
        "\n",
        "    # Xb_ace = ace.model.Model()\n",
        "    # Xb_ace.build_model_from_xy(X_unlabeled.T.tolist(), b_unlabeled.tolist())\n",
        "    # X_unlabeled_Xb_Mc = Xb_ace.ace.x_transforms\n",
        "    # b_unlabeled_Xb_Mc = Xb_ace.ace.y_transform\n",
        "\n",
        "    Xy_ace = ace.model.Model()\n",
        "    # Xy_ace.build_model_from_xy(X_unlabeled.T.tolist(), y_unlabeled.tolist())\n",
        "    # X_unlabeled_Xy_Mc = Xy_ace.ace.x_transforms\n",
        "    # y_unlabeled_Xy_Mc = Xy_ace.ace.y_transform\n",
        "\n",
        "    # X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc = maximal_correlation_transform(Xb_ace, X_unlabeled.T.tolist(), b_unlabeled.tolist())\n",
        "    # X_unlabeled_Xy_Mc, y_unlabeled_Xy_Mc = maximal_correlation_transform(Xy_ace, X_unlabeled.T.tolist(), y_unlabeled.tolist())\n",
        "    X_labeled_Xy_Mc, y_labeled_Xy_Mc = maximal_correlation_transform(Xy_ace, X_labeled.T.tolist(), y_labeled.tolist())\n",
        "\n",
        "    for i in np.arange(10):\n",
        "\n",
        "        af_div = i/10\n",
        "        print(\"Ratio of e_loss/f_loss is\",af_div,\"/\",1-af_div)\n",
        "\n",
        "        ur_accuracies, ur_fairness = act_alg.run(X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc, X_labeled_Xy_Mc, y_labeled_Xy_Mc, X_unlabeled_Xy_Mc, y_unlabeled_Xy_Mc, sub_option, af_div)\n",
        "        rs_accuracies, rs_fairness = act_alg.run(X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc, X_labeled_Xy_Mc, y_labeled_Xy_Mc, X_unlabeled_Xy_Mc, y_unlabeled_Xy_Mc, \"Filter_only\", af_div)\n",
        "        nonal_accuracies, nonal_fairness = non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled, y_labeled, X_test, y_test, b_test, budget, step, criteria)\n",
        "        x_axis = np.arange(init_index,budget+1,step)\n",
        "        results(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, run = i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5875, 2) (5875, 10) (5875, 2) (5875, 10)\n",
            "* Starting outer iteration 000. Current err =  1.00000E+00\n",
            "  Starting inner iteration 000. Current err =  1.00000E+00\n",
            "  Starting inner iteration 001. Current err =  5.69745E-02\n",
            "  Starting inner iteration 002. Current err =  4.94582E-02\n",
            "  Starting inner iteration 003. Current err =  4.85170E-02\n",
            "  Starting inner iteration 004. Current err =  3.78844E-02\n",
            "  Starting inner iteration 005. Current err =  3.56773E-02\n",
            "  Starting inner iteration 006. Current err =  3.40760E-02\n",
            "  Starting inner iteration 007. Current err =  3.25316E-02\n",
            "  Starting inner iteration 008. Current err =  3.11473E-02\n",
            "  Starting inner iteration 009. Current err =  3.06265E-02\n",
            "  Starting inner iteration 010. Current err =  2.97747E-02\n",
            "  Starting inner iteration 011. Current err =  2.92014E-02\n",
            "  Starting inner iteration 012. Current err =  2.87593E-02\n",
            "  Starting inner iteration 013. Current err =  2.84077E-02\n",
            "  Starting inner iteration 014. Current err =  2.81255E-02\n",
            "  Starting inner iteration 015. Current err =  2.78957E-02\n",
            "  Starting inner iteration 016. Current err =  2.77048E-02\n",
            "  Starting inner iteration 017. Current err =  2.75425E-02\n",
            "  Starting inner iteration 018. Current err =  2.74014E-02\n",
            "  Starting inner iteration 019. Current err =  2.72764E-02"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wenxuanhuang/Documents/Repo/Active-Learning-Performance-Benchmarking/thesis-project/lib/python3.8/site-packages/ace/smoother.py:309: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  (xi - self._mean_x_in_window) ** 2 /\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Starting inner iteration 020. Current err =  2.71643E-02\n",
            "  Starting inner iteration 021. Current err =  2.70631E-02\n",
            "  Starting inner iteration 022. Current err =  2.69716E-02\n",
            "  Starting inner iteration 023. Current err =  2.54657E-02\n",
            "  Starting inner iteration 024. Current err =  2.50554E-02\n",
            "  Starting inner iteration 025. Current err =  2.37821E-02\n",
            "  Starting inner iteration 026. Current err =  2.34389E-02\n",
            "  Starting inner iteration 027. Current err =  2.33044E-02\n",
            "Ratio of e_loss/f_loss is 0.0 / 1.0\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_unlabeled_Xb_Mc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_28683/527169389.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mutual_information\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"No_filter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_28683/1907173034.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(model, budget, step, criteria, sub_option)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ratio of e_loss/f_loss is\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maf_div\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0maf_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mur_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mur_fairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled_Xb_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled_Xb_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_option\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maf_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mrs_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_fairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled_Xb_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled_Xb_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Filter_only\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maf_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnonal_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonal_fairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_active_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_unlabeled_Xb_Mc' is not defined"
          ]
        }
      ],
      "source": [
        "experiment(LogModel,200,10,\"mutual_information\",\"No_filter\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d",
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "name": "Logistic.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b6d1a97b03e52be1c553c50910cdb19760ee3e4d178c0a737039468e0b0f9e5f"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit ('thesis-project': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
