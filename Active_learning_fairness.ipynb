{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(__doc__)\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import statsmodels.api as sm\n",
        "\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def retrieve_data_recid():\n",
        "    \n",
        "    attributes = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree']\n",
        "    bias = 'race'\n",
        "    target = 'two_year_recid'\n",
        "\n",
        "    # np.random.seed(42)\n",
        "\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/RecidivismData_Normalized.csv\", sep=',')\n",
        "    data_col = data.columns\n",
        "    df = data[(data[bias]==2)|(data[bias]==3)].copy().values\n",
        "\n",
        "    kf = KFold(n_splits=4) #differ from original method\n",
        "    for train_index, test_index in kf.split(df):\n",
        "        train, test = df[train_index], df[test_index]\n",
        "        # print(\"Size of X_train_full, X_test:\", train.shape, test.shape)\n",
        "\n",
        "    df_train = pd.DataFrame(data=train, columns=data_col)\n",
        "    df_test = pd.DataFrame(data=test, columns=data_col)\n",
        "\n",
        "    labeled = df_train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=5)) # ten sample in total labeled initially\n",
        "    # labeled = df_train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=5, random_state=42)) # ten sample in total labeled initially\n",
        "    df_X_labeled = labeled[attributes]\n",
        "    df_y_labeled = labeled[target]\n",
        "    X_labeled = df_X_labeled.values\n",
        "    y_labeled = df_y_labeled.values.astype('int64')\n",
        "    b_labeled = labeled[bias].values-2\n",
        "    (row_size, col_size) = X_labeled.shape\n",
        "\n",
        "    unlabeled = df_train.drop(df_X_labeled.index)\n",
        "    df_X_unlabeled = unlabeled[attributes]\n",
        "    df_y_unlabeled = unlabeled[target]\n",
        "    X_unlabeled = df_X_unlabeled.values\n",
        "    y_unlabeled = df_y_unlabeled.values.astype('int64')\n",
        "    b_unlabeled = unlabeled[bias].values-2\n",
        "\n",
        "    X_test = df_test[attributes].values\n",
        "    y_test = df_test[target].values\n",
        "    y_test=y_test.astype('int')\n",
        "    b_test = df_test[bias].values-2\n",
        "\n",
        "    X_fair_est = X_unlabeled\n",
        "    y_fair_est = y_unlabeled\n",
        "    b_fair_est = b_unlabeled\n",
        "    \n",
        "    return (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    def fit_predict(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        self.classifier = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "        self.classifier.fit(X_labeled, y_labeled)\n",
        "        features_weight= self.classifier.coef_.T\n",
        "        # self.y_test_predicted = self.classifier.predict(X_test)\n",
        "        # self.y_unlabeled_predicted = self.classifier.predict(X_unlabeled)\n",
        "        self.y_test_score = self.classifier.score(X_test, y_test)\n",
        "        return (X_labeled, X_test, self.y_test_score, features_weight)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    def train(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        (X_labeled, X_test, self.y_test_score, features_weight) = \\\n",
        "            self.model_object.fit_predict(X_labeled, y_labeled, X_test, y_test)\n",
        "        return (X_labeled, X_test, features_weight)\n",
        "\n",
        "    def get_test_accuracy(self, i):\n",
        "        classif_rate = self.y_test_score * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        # print(\"Accuracy rate is %f \" % (classif_rate))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def random_selection(probas_val, step):\n",
        "    # random_state = check_random_state(63)\n",
        "    selection = np.random.choice(probas_val.shape[0], step, replace=False)\n",
        "    return selection\n",
        "\n",
        "# def entropy_selection(probas_val, step):\n",
        "#     e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "#     selection = (np.argsort(e)[::-1])[:step]\n",
        "#     return selection"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def normalizer(e_loss, f_loss):\n",
        "    e_loss = np.reshape(e_loss, (1,len(e_loss)))\n",
        "    f_loss = np.reshape(f_loss, (1,len(f_loss)))\n",
        "    e_scaled = preprocessing.normalize(e_loss)\n",
        "    # e_scaled=((e_loss-e_loss.min())/(e_loss.max()-e_loss.min()))\n",
        "    f_scaled = preprocessing.normalize(f_loss)\n",
        "    # f_scaled=((f_loss-f_loss.min())/(f_loss.max()-f_loss.min()))\n",
        "    return (e_scaled, f_scaled)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def log_loss(probas_val):\n",
        "    \n",
        "    eps = np.finfo(probas_val.dtype).eps\n",
        "    probas_val = np.clip(probas_val, eps, 1 - eps)\n",
        "    e_loss = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "\n",
        "    return e_loss"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def fair_loss_covariance(X_data, temp, row_size, y_labeled_sum, yhatb_covariance, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_covariance, Xy_covariance):\n",
        "\n",
        "    fair_loss = 0\n",
        "    fair_improvement = 0\n",
        "    row_expected = row_size + 1\n",
        "    for i in range(len(temp)): \n",
        "        y_expected = (y_labeled_sum + i)/row_expected\n",
        "        Xy_expected = np.add(Xy_labeled_sum , X_data*i)/row_expected\n",
        "        X_expected = np.add(X_labeled_sum , X_data)/row_expected\n",
        "        Xy_covariance_expected = np.subtract(Xy_expected, (X_expected* y_expected))\n",
        "        fair_improvement = np.dot((abs(features_weight*Xb_covariance)).transpose(),(abs(Xy_covariance)-abs(Xy_covariance_expected)).reshape(Xy_covariance_expected.shape[0],1))[0,0]\n",
        "        fair_loss += temp[i]*fair_improvement\n",
        "    return fair_loss"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# separation \\ Equal opportunity - Hardt, Price, Srebro (2016)\n",
        "\n",
        "def mut_inf(X_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "    f_loss=mutual_info_score(b_fair_est, y_fair_pred)\n",
        "    f_loss=abs(f_loss)\n",
        "    # print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def stats_parity(X_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0=X_fair_est[(b_fair_est==0)].shape[0]\n",
        "    b1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1=X_fair_est[(b_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0p1/b0)-(b1p1/b1)\n",
        "    # print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def eqops(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0y1p1/b0y1)-(b1y1p1/b1y1)\n",
        "    # print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def eqods(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y0p1=X_fair_est[(b_fair_est==0)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0y0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)].shape[0]\n",
        "    b1y0p1=X_fair_est[(b_fair_est==1)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b1y0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)].shape[0]\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    fpr_loss=abs((b0y0p1/b0y0)-(b1y0p1/b1y0))\n",
        "    tpr_loss=abs((b0y1p1/b0y1)-(b1y1p1/b1y1))\n",
        "\n",
        "\n",
        "    f_loss = (fpr_loss+tpr_loss)/2 # temporary solution\n",
        "    \n",
        "    return f_loss \n",
        "\n",
        "def disp_impt(X_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0=X_fair_est[(b_fair_est==0)].shape[0]\n",
        "    b1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1=X_fair_est[(b_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0p1/b0)/(b1p1/b1)\n",
        "    # print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\n",
        "    f_loss = f_loss-1\n",
        "    return f_loss\n",
        "# selecting fairness criteria\n",
        "def fair_measure(X_fair_est, y_fair_est, b_fair_est, classifier=None, criteria=0):\n",
        "    if criteria == 'mutual_information':\n",
        "        return mut_inf(X_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'equal_opportunity':\n",
        "        return eqops(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'statistical_parity':\n",
        "        return stats_parity(X_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'equalized_odds':\n",
        "        return eqods(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == \"disparate_impact\":\n",
        "        return disp_impt(X_fair_est, b_fair_est, classifier)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def unfairness_covariance_sampling(X_unlabeled, classifier, probas_val, step, row_size, y_labeled_sum, yhatb_covariance, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_covariance, Xy_covariance):\n",
        "    # further to be defined, now assume only fairness loss\n",
        "    # query size used here\n",
        "    div = 0\n",
        "\n",
        "    unlabeled_size = len(X_unlabeled)\n",
        "    f_loss = np.zeros(unlabeled_size)\n",
        "    \n",
        "    for j in range(unlabeled_size): \n",
        "            proba_temp = classifier.predict_proba(X_unlabeled[j].reshape(1, -1)).reshape(-1,1)\n",
        "            # first reshape integrate as single entry\n",
        "            # second reshape transpose as vertical 2D array\n",
        "            f_loss[j] = fair_loss_covariance(X_unlabeled[j],proba_temp, row_size, y_labeled_sum, yhatb_covariance, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_covariance, Xy_covariance)\n",
        "\n",
        "    e_loss = log_loss(probas_val)\n",
        "\n",
        "    e_scaled, f_scaled = normalizer(e_loss, f_loss)\n",
        "    f_scaled[np.isnan(f_scaled)] = 0\n",
        "\n",
        "    loss = div*(e_loss)+(1-div)*f_loss\n",
        "    \n",
        "    selection = np.argsort(loss)[::-1][:step]\n",
        "\n",
        "    return selection"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def pre_filter(X_data, b_data, y_data, budget): \n",
        "\n",
        "    temp_columns = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree','race','two_year_recid']\n",
        "    # np.random.seed(84)\n",
        "\n",
        "    # print(\"Debug data:\", X_data.shape, b_data.shape, y_data.shape)\n",
        "    temp_data = np.c_[X_data, b_data, y_data]\n",
        "    temp_df = pd.DataFrame(data=temp_data, columns=temp_columns)\n",
        "    candidates_data = temp_df.groupby('race', group_keys=False).apply(lambda x: x.sample(n=math.ceil(budget/2)))\n",
        "    # candidates_data = temp_df.groupby('race', group_keys=False).apply(lambda x: x.sample(n=math.ceil(budget/2), random_state = 84))\n",
        "    # print(\"Debug ceil:\", math.ceil(budget/2))\n",
        "    # print(\"Debug ceil:\", math.ceil(budget/2))\n",
        "    # print(\"Debug cand:\", candidates)\n",
        "    candidates_index = candidates_data.index.values\n",
        "    # print(\"Debug index:\", candidates_index.shape, candidates_index)\n",
        "\n",
        "    return candidates_index"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def metrics(X_test, y_test, b_test, classifier):\n",
        "    y_test_pred = classifier.predict(X_test)\n",
        "    # print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, classification_report(y_test, y_test_pred)))\n",
        "    # print(\"Confusion matrix:\\n%s\" % confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "    # tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
        "    # tpr = tp/(tp+fn)\n",
        "    # tnr = tn/(tn+fp) \n",
        "    # fpr = fp/(fp+tn)\n",
        "    # fnr = fn/(tp+fn)\n",
        "    # fdr = fp/(tp+fp)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "\n",
        "    # model_displays = {}\n",
        "    # for i in np.unique(y_test):\n",
        "    #     model_displays[i] = plot_roc_curve(\n",
        "    #         classifier, X_test[b_test==i], y_test[b_test==i], ax=ax, name=i)\n",
        "    # ax.set_title('ROC curve')\n",
        "    # plt.show() "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def scatters(X_unlabeled, b_unlabeled, probas_val, uncertain_samples):\n",
        "    y_plot = probas_val\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_axes([0,0,1,1])\n",
        "    ax.scatter(X_unlabeled[:,1], y_plot[:,1], c=b_unlabeled, cmap = 'Pastel1')\n",
        "    ax.scatter(X_unlabeled[:,1][uncertain_samples], y_plot[:,1][uncertain_samples], c=b_unlabeled[uncertain_samples], edgecolors='black', s=100, cmap = 'Pastel1')\n",
        "    ax.set_xlabel('Age')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Population plot')\n",
        "    y_plot = np.delete(y_plot, uncertain_samples, axis=0)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def performance_progression(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, pplot_file_name,run):\n",
        "    fig, (ax1, ax2) = plt.subplots(2,figsize=(10,10))\n",
        "    fig.suptitle('Fairness and accuracy metrics')\n",
        "    ax1.plot(x_axis, ur_fairness, color='r', label='unfair-active')\n",
        "    ax1.plot(x_axis, rs_fairness, color='g', label='random-active')\n",
        "    ax1.plot(x_axis, nonal_fairness, color='b', label='non-active')\n",
        "    ax1.legend()\n",
        "    ax2.plot(x_axis, ur_accuracies, color='r', label='unfair-active')\n",
        "    ax2.plot(x_axis, rs_accuracies, color='g', label='random-active')\n",
        "    ax2.plot(x_axis, nonal_accuracies, color='b', label='non-active')\n",
        "    ax2.legend()\n",
        "    ax1.set_xlabel('Sample size')\n",
        "    ax1.set_ylabel('Unfairness')\n",
        "    ax2.set_ylabel('Accuracies')\n",
        "    plt.savefig(pplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def trade_off_plot(ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, tplot_file_name, smooth_tplot_file_name,run):\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    ur_index = np.argsort(ur_accuracies)\n",
        "    rs_index = np.argsort(rs_accuracies)\n",
        "    nonal_index = np.argsort(nonal_accuracies) \n",
        "\n",
        "    plt.plot(np.array(ur_accuracies)[ur_index], np.array(ur_fairness)[ur_index], color='r', label='unfair-active')\n",
        "    plt.plot(np.array(rs_accuracies)[rs_index], np.array(rs_fairness)[rs_index], color='g', label='random-active')\n",
        "    plt.plot(np.array(nonal_accuracies)[nonal_index], np.array(nonal_fairness)[nonal_index], color='b', label='non-active')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Accuracies')\n",
        "    plt.ylabel('Unfairness')\n",
        "    plt.savefig(tplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "    ur_smooth = sm.nonparametric.lowess(np.array(ur_fairness)[ur_index], np.array(ur_accuracies)[ur_index], frac = 0.5)\n",
        "    rs_smooth = sm.nonparametric.lowess(np.array(rs_fairness)[rs_index], np.array(rs_accuracies)[rs_index], frac = 0.5)\n",
        "    nonal_smooth = sm.nonparametric.lowess(np.array(nonal_fairness)[nonal_index], np.array(nonal_accuracies)[nonal_index], frac = 0.5)\n",
        "\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    plt.plot(ur_smooth[:, 0], ur_smooth[:, 1], color='r', label='unfair-active')\n",
        "    plt.plot(rs_smooth[:, 0], rs_smooth[:, 1], color='g', label='random-active')\n",
        "    plt.plot(nonal_smooth[:, 0], nonal_smooth[:, 1], color='b', label='non-active')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Approximate accuracies')\n",
        "    plt.ylabel('Approximate unfairness')\n",
        "    plt.savefig(smooth_tplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class active_learning(object):\n",
        "\n",
        "    def __init__(self, step, budget, model_object, criteria):\n",
        "        self.step = step\n",
        "        self.budget = budget\n",
        "        self.model_object = model_object\n",
        "        # self.sample_selection_function = selection_function\n",
        "        self.criteria = criteria\n",
        "        \n",
        "    def run(self, X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, sub_option):\n",
        "  \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_labeled, X_test, features_weight) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "\n",
        "        Xb_covariance = np.cov(np.concatenate((X_unlabeled, b_unlabeled.reshape(b_unlabeled.shape[0],1)),1).T.astype(float))[0:col_size,-1].reshape(col_size,1) # the ovariance of features and sensitive attribute\n",
        "\n",
        "        Xy_labeled_sum = np.zeros(col_size)\n",
        "        X_labeled_sum = np.zeros(col_size)\n",
        "        y_labeled_sum = np.sum(y_labeled)\n",
        "\n",
        "        for i in range(row_size):\n",
        "            X_labeled_sum = np.add(X_labeled_sum, X_labeled[i])\n",
        "            Xy_labeled_sum = np.add(Xy_labeled_sum, X_labeled[i]*y_labeled[i])\n",
        "\n",
        "        y_ratio = 1./row_size*y_labeled_sum \n",
        "        Xy_covariance = 1./row_size*np.subtract(Xy_labeled_sum,X_labeled_sum*y_ratio)\n",
        "        yhatb_covariance = np.dot(Xb_covariance.transpose(),features_weight)[0,0]\n",
        "\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(active_iteration)\n",
        "        self.query_size = len(X_labeled)\n",
        "        fairness = []\n",
        "        fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "        metrics(X_test, y_test, b_test, self.clf_model.model_object.classifier)\n",
        "\n",
        "\n",
        "        while self.query_size <= self.budget-self.step:\n",
        "\n",
        "            active_iteration += 1\n",
        "            self.query_size += self.step\n",
        "\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_unlabeled)\n",
        "\n",
        "            yb_covariance=np.dot(Xb_covariance.transpose(),features_weight)[0,0]\n",
        "            \n",
        "            # print(\"Debug predicted:\", y_unlabeled_predicted.shape)\n",
        "            # print(\"Debug probas_val:\", probas_val.shape)\n",
        "\n",
        "            # if sub_option == \"Legacy_filter\":\n",
        "            #     # post_entropy_index = entropy_selection(probas_val, self.budget*2)\n",
        "            #     candidates_index = pre_filter(X_unlabeled, b_unlabeled, y_unlabeled, self.budget)\n",
        "            #     uncertain_samples = unfairness_reduction_sampling(self.query_size, X_unlabeled[candidates_index], X_labeled, y_labeled, self.clf_model.model_object.classifier, X_fair_est, y_fair_est, b_fair_est, probas_val[candidates_index], self.step, self.criteria)\n",
        "\n",
        "            if sub_option == \"Pre_filter\":\n",
        "                candidates_index = pre_filter(X_unlabeled, b_unlabeled, y_unlabeled, self.budget)\n",
        "                uncertain_samples = unfairness_covariance_sampling(X_unlabeled[candidates_index], self.clf_model.model_object.classifier, probas_val[candidates_index], self.step, row_size, y_labeled_sum, yhatb_covariance, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_covariance, Xy_covariance)\n",
        "\n",
        "            elif sub_option == \"No_filter\": \n",
        "                uncertain_samples = unfairness_covariance_sampling(X_unlabeled, self.clf_model.model_object.classifier, probas_val, self.step, row_size, y_labeled_sum, yhatb_covariance, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_covariance, Xy_covariance)\n",
        "\n",
        "            elif sub_option == \"Filter_only\":\n",
        "                uncertain_samples = random_selection(probas_val, self.step)\n",
        "            # print(\"Debug shape of X_unlabeled and loss:\", selection)\n",
        "\n",
        "            # scatters(X_unlabeled, b_unlabeled, probas_val, uncertain_samples)\n",
        "\n",
        "            X_labeled = np.concatenate((X_labeled, X_unlabeled[uncertain_samples]))\n",
        "            y_labeled = np.concatenate((y_labeled, y_unlabeled[uncertain_samples]))\n",
        "\n",
        "            (X_labeled, X_test, features_weight) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "\n",
        "            row_size += self.step\n",
        "\n",
        "            for i in uncertain_samples:\n",
        "\n",
        "                X_labeled_sum = np.add(X_labeled_sum,X_unlabeled[i])\n",
        "                # print(\"Debug before update:\", Xy_labeled_sum)\n",
        "                y_labeled_sum += y_unlabeled[i]; \n",
        "                Xy_labeled_sum = np.add(Xy_labeled_sum, (X_unlabeled[i]*y_unlabeled[i]))\n",
        "\n",
        "            y_expected = 1./row_size*y_labeled_sum\n",
        "            Xy_covariance = 1./row_size*np.subtract(Xy_labeled_sum,X_labeled_sum*y_expected)\n",
        "            yhatb_covariance = np.dot(Xb_covariance.transpose(),features_weight)[0,0]\n",
        "\n",
        "            X_unlabeled = np.delete(X_unlabeled, uncertain_samples, axis=0)\n",
        "            y_unlabeled = np.delete(y_unlabeled, uncertain_samples, axis=0)\n",
        "            b_unlabeled = np.delete(b_unlabeled, uncertain_samples, axis=0)\n",
        "\n",
        "            fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "            self.clf_model.get_test_accuracy(active_iteration)\n",
        "\n",
        "            metrics(X_test, y_test, b_test, self.clf_model.model_object.classifier)\n",
        "\n",
        "        return self.clf_model.accuracies, fairness"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled,y_labeled, X_test, y_test, b_test, budget, step, unfairness_criteria): \n",
        "\n",
        "    initial_X_train = X_labeled\n",
        "    initial_y_train = y_labeled\n",
        "    nonal_X_train = X_unlabeled\n",
        "    nonal_y_train = y_unlabeled\n",
        "    nonal_X_test = X_test\n",
        "    nonal_y_test = y_test\n",
        "    nonal_b_test = b_test\n",
        "    nonal_fairness = []\n",
        "\n",
        "    nonal_accuracies=[]\n",
        "\n",
        "    classifier_nonal = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "\n",
        "    classifier_nonal.fit(initial_X_train, initial_y_train)\n",
        "    initial_y_pred = classifier_nonal.predict(nonal_X_test)\n",
        "    nonal_fairness = np.append(nonal_fairness, fair_measure(nonal_X_test, nonal_y_test, nonal_b_test, classifier=classifier_nonal, criteria=unfairness_criteria))\n",
        "    nonal_accuracies.append(accuracy_score(nonal_y_test, initial_y_pred)*100)\n",
        "\n",
        "    for i in np.arange(init_index+step,budget+1,step):\n",
        "        classifier_nonal.fit(nonal_X_train[:i], nonal_y_train[:i])\n",
        "        nonal_y_pred = classifier_nonal.predict(nonal_X_test)\n",
        "        nonal_fairness = np.append(nonal_fairness, fair_measure(nonal_X_test, nonal_y_test, nonal_b_test, classifier=classifier_nonal, criteria=unfairness_criteria))\n",
        "        nonal_accuracies.append(accuracy_score(nonal_y_test, nonal_y_pred)*100)\n",
        "        metrics(X_test, y_test, b_test, classifier_nonal)\n",
        "\n",
        "    return nonal_accuracies, nonal_fairness"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def experiment(model,budget,step,criteria,sub_option=None):\n",
        "    \n",
        "    (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est) = retrieve_data_recid()\n",
        "    init_index = len(X_labeled)\n",
        "        \n",
        "    act_alg = active_learning(step, budget, model, criteria)\n",
        "\n",
        "    ur_accuracies, ur_fairness = act_alg.run(X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, sub_option)\n",
        "\n",
        "    rs_accuracies, rs_fairness = act_alg.run(X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, \"Filter_only\")\n",
        "\n",
        "    nonal_accuracies, nonal_fairness = non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled, y_labeled, X_test, y_test, b_test, budget, step, criteria)\n",
        "\n",
        "    x_axis = np.arange(init_index,budget+1,step)\n",
        "\n",
        "    return init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def results(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, run):\n",
        "\n",
        "    data = (np.array([x_axis, ur_accuracies, ur_fairness, rs_accuracies, rs_fairness, nonal_accuracies, nonal_fairness])).T\n",
        "\n",
        "    log_file_name = \"result_log/{run}.Result.csv\"\n",
        "    pplot_file_name = \"result_log/{:03}.Perf_Plot.png\"\n",
        "    tplot_file_name = \"result_log/{:03}.Trdoff_Plot.png\"\n",
        "    smooth_tplot_file_name = \"result_log/{:03}.Smooth.Trdoff_Plot.png\"\n",
        "    np.savetxt(log_file_name.format(run = run), data, delimiter=',', header=\"x_axis, ur_accuracies, ur_fairness, rs_accuracies, rs_fairness, nonal_accuracies, nonal_fairness\", fmt='%d,%f,%f,%f,%f,%f,%f')\n",
        "\n",
        "    performance_progression(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, pplot_file_name, run)\n",
        "    trade_off_plot(ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, tplot_file_name, smooth_tplot_file_name, run)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in np.arange(30):\n",
        "    init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies = experiment(LogModel,200,10,\"equalized_odds\",\"No_filter\")\n",
        "    results(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, run = i)"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d",
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "name": "Logistic.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a2ef89d34cbfddaf50816f8d91581a3ca0913b9280767ed31a38c2db7dcc022c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.2 64-bit ('ML-for-COVID-19-dataset': venv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}