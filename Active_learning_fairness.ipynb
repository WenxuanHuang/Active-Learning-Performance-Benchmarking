{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "4763f7b3-6c5b-45cb-f411-fd7c6ea8b38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Formulas\n",
            "\n",
            "Mutual Information: MI(B,Y') = \\sum_{i = 1}^{|B|}\\sum_{j = 1}^{|Y'|}\frac{|B_{i} \\cap Y'_{j}|}{N} log\frac{N|B_{i} \\cap Y'_{j}|}{|B_{i}||Y'_{j}|}\n",
            "Statistical Parity: SP(B,Y') = |P(Y' = 1|B = a) - P(Y' = 1|B = b)| \\quad \forall a,b \\in B\n",
            "Equal Opportunity: EOP(B,Y') = |P(Y' = 1|B = a, Y = 1) - P(Y' = 1|B = b, Y = 1)| \\quad \forall a,b \\in B\n",
            "Equalized Odds: EOD(B,Y') = |P(Y' = 1|B = a, Y = y) - P(Y' = 1|B = b, Y = y)| \\quad \forall a,b \\in B, \\quad \forall 0,1 \\in y\n",
            "Conditional Use Accuracy Equality: CUAE(B,Y') = |P(Y' = y|B = a, Y = y) - P(Y' = y|B = b, Y = y)| \\quad \forall a,b \\in B, \\quad \forall 0,1 \\in y\n",
            "Disparate Impact: DI(B,Y') = |P(Y' = 1|B = a) / P(Y' = 1|B = b)| \\quad \forall a,b \\in B\n",
            "\n",
            "Source\n",
            "----------\n",
            "\n",
            "separation \\ Equal opportunity - Hardt, Price, Srebro (2016)\n",
            "separation \\ Equalized adds - Hardt, Price, Srebro (2016)\n",
            "independence \\ Mutual information - Cover, T.M.; Thomas, J.A. (1991). Elements of Information Theory (Wiley ed.)\n",
            "sufficiency \\ Conditional Use Accuracy Equality - Richard Berk et al., (2017) Fairness in Criminal Justice Risk Assessments: The State of the Art\n",
            "\n",
            "Parameters\n",
            "----------\n",
            "X_fair_est: testing data, ndarray\n",
            "y_fair_est: testing target, ndarray\n",
            "b_fair_est: bias attribute of testing set, ndarray\n",
            "\n",
            "Returns\n",
            "-------\n",
            "f_loss: fairness loss, int\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(__doc__)\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# from ace import model\n",
        "# from ace import ace\n",
        "import ace.model\n",
        "import ace.ace\n",
        "\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_data_recid():\n",
        "\n",
        "    \"\"\" This function is used for retrieving dataset COMPAS and split data entries into labeled (training, testing) and unlabeled data (validation) \n",
        "    Prediction task is to determine whether a person will recidive after first prosecution\"\"\"\n",
        "\n",
        "    \"\"\" Binary classification\n",
        "    =================  ======================\n",
        "    samples total      5875\n",
        "    Dimensionality     9(Features)+1(Bias)\n",
        "    Features           real\n",
        "    Classes            2\n",
        "    =================  ======================\n",
        "\n",
        "    Source\n",
        "    ----------\n",
        "    How We Analyzed the COMPAS Recidivism Algorithm, by Jeff Larson, Surya Mattu, Lauren Kirchner and Julia Angwin, May 23, 2016\n",
        "    https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm\n",
        "    https://github.com/propublica/compas-analysis\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    none\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_labeled: training data, ndarray, shape (10, 9)\n",
        "    y_labeled: training target, ndarray, shape (10, ) \n",
        "    b_labeled: bias attribute of training set, ndarray, shape (10, )\n",
        "\n",
        "    X_unlabeled: sample pool data, ndarray, shape (4397, 9)\n",
        "    y_unlabeled: sample pool target, ndarray, shape (4397, )\n",
        "    b_unlabeled: bias attribute of sample pool set, ndarray, shape (4397, )\n",
        "\n",
        "    X_test: testing data, ndarray, shape (1468, 9)\n",
        "    y_test: testing target, ndarray, shape (1468, )\n",
        "    b_test: bias attribute of testing set, ndarray, shape (1468, )\n",
        "    \"\"\"\n",
        "    \n",
        "    # mc_attributes = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree','race']\n",
        "    attributes = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree']\n",
        "    bias = 'race'\n",
        "    target = 'two_year_recid'\n",
        "\n",
        "    # np.random.seed(42)\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/RecidivismData_Normalized.csv\", sep=',')\n",
        "    data_col = data.columns\n",
        "    df = data[(data[bias]==2)|(data[bias]==3)].copy().values\n",
        "    # print(df.shape)\n",
        "\n",
        "    b_Xb_data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/b_Xb_Mc.csv\", sep=',').values\n",
        "    X_Xb_data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/X_Xb_Mc.csv\", sep=',').values\n",
        "    y_Xy_data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/y_Xy_Mc.csv\", sep=',').values\n",
        "    X_Xy_data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/X_Xy_Mc.csv\", sep=',').values\n",
        "\n",
        "    print(b_Xb_data.shape, X_Xb_data.shape, y_Xy_data.shape, X_Xy_data.shape)\n",
        "\n",
        "    kf = KFold(n_splits=4)\n",
        "    for train_index, test_index in kf.split(df):\n",
        "        train, test = df[train_index], df[test_index]\n",
        "        # print(\"Size of X_train_full, X_test:\", train.shape, test.shape)\n",
        "\n",
        "    df_train = pd.DataFrame(data=train, columns=data_col)\n",
        "    df_test = pd.DataFrame(data=test, columns=data_col)\n",
        "\n",
        "    labeled = df_train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=5)) # ten sample in total labeled initially\n",
        "    # labeled = df_train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=5, random_state=42)) # with a random state for stable output\n",
        "    df_X_labeled = labeled[attributes]\n",
        "    df_y_labeled = labeled[target]\n",
        "    X_labeled = df_X_labeled.values\n",
        "    y_labeled = df_y_labeled.values.astype('int64')\n",
        "    b_labeled = labeled[bias].values-2 #degrade bias into binary options\n",
        "    (row_size, col_size) = X_labeled.shape \n",
        "    # print(X_labeled.shape)\n",
        "\n",
        "    unlabeled = df_train.drop(df_X_labeled.index)\n",
        "    df_X_unlabeled = unlabeled[attributes]\n",
        "    df_y_unlabeled = unlabeled[target]\n",
        "    X_unlabeled = df_X_unlabeled.values\n",
        "    y_unlabeled = df_y_unlabeled.values.astype('int64')\n",
        "    b_unlabeled = unlabeled[bias].values-2\n",
        "    # print(X_unlabeled.shape)\n",
        "\n",
        "    X_test = df_test[attributes].values\n",
        "    y_test = df_test[target].values\n",
        "    y_test=y_test.astype('int')\n",
        "    b_test = df_test[bias].values-2\n",
        "    # print(X_test.shape)\n",
        "    \n",
        "    return (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    def fit_predict(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        self.classifier = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "        self.classifier.fit(X_labeled, y_labeled)\n",
        "        features_weight= self.classifier.coef_.T\n",
        "        # self.y_test_predicted = self.classifier.predict(X_test)\n",
        "        # self.y_unlabeled_predicted = self.classifier.predict(X_unlabeled)\n",
        "        self.y_test_score = self.classifier.score(X_test, y_test)\n",
        "        return (X_labeled, X_test, self.y_test_score, features_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    def train(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        (X_labeled, X_test, self.y_test_score, features_weight) = \\\n",
        "            self.model_object.fit_predict(X_labeled, y_labeled, X_test, y_test)\n",
        "        return (X_labeled, X_test, features_weight)\n",
        "\n",
        "    def get_test_accuracy(self, i):\n",
        "        classif_rate = self.y_test_score * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        # print(\"Accuracy rate is %f \" % (classif_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_selection(probas_val, step):\n",
        "    # random_state = check_random_state(63)\n",
        "    selection = np.random.choice(probas_val.shape[0], step, replace=False)\n",
        "    return selection\n",
        "\n",
        "# def entropy_selection(probas_val, step):\n",
        "#     e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "#     selection = (np.argsort(e)[::-1])[:step]\n",
        "#     return selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalizer(e_loss, f_loss):\n",
        "    e_loss = np.reshape(e_loss, (1,len(e_loss)))\n",
        "    f_loss = np.reshape(f_loss, (1,len(f_loss)))\n",
        "    e_scaled = preprocessing.normalize(e_loss)\n",
        "    # e_scaled=((e_loss-e_loss.min())/(e_loss.max()-e_loss.min()))\n",
        "    f_scaled = preprocessing.normalize(f_loss)\n",
        "    # f_scaled=((f_loss-f_loss.min())/(f_loss.max()-f_loss.min()))\n",
        "    e_scaled = e_scaled.flatten()\n",
        "    f_scaled = f_scaled.flatten()\n",
        "    return (e_scaled, f_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_loss(probas_val):\n",
        "    \n",
        "    eps = np.finfo(probas_val.dtype).eps\n",
        "    probas_val = np.clip(probas_val, eps, 1 - eps)\n",
        "    e_loss = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "\n",
        "    return e_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" This function is used for calculating fairness loss, aka the unfairness of a logistic regression model. The fairness measurement is used to provide loss values that helps determine the fluctuation of unfairness change with each iteration that adds samples to the labeled dataset. There are many different methods for unfairness measurement, however, five different meausures are coded and only one of them will be used for unfairness measurement in the correlation-based sampling. \"\"\"\n",
        "\n",
        "\"\"\" Formulas\n",
        "\n",
        "Mutual Information: MI(B,Y') = \\sum_{i = 1}^{|B|}\\sum_{j = 1}^{|Y'|}\\frac{|B_{i} \\cap Y'_{j}|}{N} log\\frac{N|B_{i} \\cap Y'_{j}|}{|B_{i}||Y'_{j}|}\n",
        "Statistical Parity: SP(B,Y') = |P(Y' = 1|B = a) - P(Y' = 1|B = b)| \\quad \\forall a,b \\in B\n",
        "Equal Opportunity: EOP(B,Y') = |P(Y' = 1|B = a, Y = 1) - P(Y' = 1|B = b, Y = 1)| \\quad \\forall a,b \\in B\n",
        "Equalized Odds: EOD(B,Y') = |P(Y' = 1|B = a, Y = y) - P(Y' = 1|B = b, Y = y)| \\quad \\forall a,b \\in B, \\quad \\forall 0,1 \\in y\n",
        "Conditional Use Accuracy Equality: CUAE(B,Y') = |P(Y' = y|B = a, Y = y) - P(Y' = y|B = b, Y = y)| \\quad \\forall a,b \\in B, \\quad \\forall 0,1 \\in y\n",
        "Disparate Impact: DI(B,Y') = |P(Y' = 1|B = a) / P(Y' = 1|B = b)| \\quad \\forall a,b \\in B\n",
        "\n",
        "Source\n",
        "----------\n",
        "\n",
        "separation \\ Equal opportunity - Hardt, Price, Srebro (2016)\n",
        "separation \\ Equalized adds - Hardt, Price, Srebro (2016)\n",
        "independence \\ Mutual information - Cover, T.M.; Thomas, J.A. (1991). Elements of Information Theory (Wiley ed.)\n",
        "sufficiency \\ Conditional Use Accuracy Equality - Richard Berk et al., (2017) Fairness in Criminal Justice Risk Assessments: The State of the Art\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "X_fair_est: testing data, ndarray\n",
        "y_fair_est: testing target, ndarray\n",
        "b_fair_est: bias attribute of testing set, ndarray\n",
        "\n",
        "Returns\n",
        "-------\n",
        "f_loss: fairness loss, int\n",
        "\"\"\"\n",
        "\n",
        "def mut_inf(X_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    f_loss=mutual_info_score(b_fair_est, y_fair_pred)\n",
        "    f_loss=abs(f_loss)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def stats_parity(X_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0=X_fair_est[(b_fair_est==0)].shape[0]\n",
        "    b1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1=X_fair_est[(b_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0p1/b0)-(b1p1/b1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def eqops(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0y1p1/b0y1)-(b1y1p1/b1y1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def eqods(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y0p1=X_fair_est[(b_fair_est==0)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0y0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)].shape[0]\n",
        "    b1y0p1=X_fair_est[(b_fair_est==1)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b1y0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)].shape[0]\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    fpr_loss=abs((b0y0p1/b0y0)-(b1y0p1/b1y0))\n",
        "    tpr_loss=abs((b0y1p1/b0y1)-(b1y1p1/b1y1))\n",
        "\n",
        "\n",
        "    f_loss = (fpr_loss+tpr_loss)/2 # justification of half in terms of fpr_loss UNION tpr_loss\n",
        "    \n",
        "    return f_loss \n",
        "\n",
        "def cuae(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y0p0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)&(y_fair_pred==0)].shape[0]\n",
        "    b0y0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)].shape[0]\n",
        "    b1y0p0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)&(y_fair_pred==0)].shape[0]\n",
        "    b1y0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)].shape[0]\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    tnr_loss=abs((b0y0p0/b0y0)-(b1y0p0/b1y0))\n",
        "    tpr_loss=abs((b0y1p1/b0y1)-(b1y1p1/b1y1))\n",
        "\n",
        "    f_loss = (tnr_loss+tpr_loss)/2\n",
        "    \n",
        "    return f_loss \n",
        "\n",
        "def disp_impt(X_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0=X_fair_est[(b_fair_est==0)].shape[0]\n",
        "    b1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1=X_fair_est[(b_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=(b0p1/b0)/(b1p1/b1)\n",
        "    f_loss = f_loss-1\n",
        "    return f_loss\n",
        "# selecting fairness criteria\n",
        "def fair_measure(X_fair_est, y_fair_est, b_fair_est, classifier=None, criteria=0):\n",
        "    if criteria == 'mutual_information':\n",
        "        return mut_inf(X_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'equal_opportunity':\n",
        "        return eqops(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'statistical_parity':\n",
        "        return stats_parity(X_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'equalized_odds':\n",
        "        return eqods(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == \"disparate_impact\":\n",
        "        return disp_impt(X_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == \"conditional_use_accuracy_equality\":\n",
        "        return cuae(X_fair_est, y_fair_est, b_fair_est, classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fair_loss_corr(X_data, prob_expected, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, Xy_std):\n",
        "\n",
        "    # print(\"Correlation between X and S:\", Xb_corr)\n",
        "    # print(\"Correlation between y' and S:\", yhatb_corr)\n",
        "    # print(\"d:prob_expected:\", prob_expected.shape, prob_expected) output: shape(2,1), probabilities of y' =0 and 1\n",
        "\n",
        "    fair_loss = 0\n",
        "    fair_improvement = 0\n",
        "    row_expected = row_size + 1 \n",
        "    for i in range(len(prob_expected)): \n",
        "        y_expected = (y_labeled_sum + i)/row_expected\n",
        "        Xy_expected = np.add(Xy_labeled_sum , X_data*i)/row_expected\n",
        "        X_expected = np.add(X_labeled_sum , X_data)/row_expected\n",
        "        Xy_corr_expected = (np.subtract(Xy_expected, (X_expected * y_expected)))/Xy_std\n",
        "        # print(\"Correlation between X and y:\", Xy_corr)\n",
        "        # print(\"Expected correlation between X and y:\", Xy_corr_expected)\n",
        "        fair_improvement = np.dot((abs(features_weight*Xb_corr)).transpose(),(abs(Xy_corr)-abs(Xy_corr_expected)).reshape(Xy_corr_expected.shape[0],1))[0,0] # if expected corr(xy) is reduced, the feature weight is higher\n",
        "        fair_loss += prob_expected[i]*fair_improvement # fairness improvement for a point, both y=1 and y=0 are added as one\n",
        "        # print(\"Shape of fair_improvement and temp[i]\",fair_improvement, prob_expected[i])\n",
        "    return fair_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unfairness_correlation_sampling(X_unlabeled, classifier, probas_val, step, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, div, Xy_std):\n",
        "    # further to be defined, now assume only fairness loss\n",
        "    # query size used here\n",
        "\n",
        "    unlabeled_size = len(X_unlabeled)\n",
        "    f_loss = np.zeros(unlabeled_size)\n",
        "    \n",
        "    for j in range(unlabeled_size): \n",
        "            proba_temp = classifier.predict_proba(X_unlabeled[j].reshape(1, -1)).reshape(-1,1)\n",
        "            # first reshape integrate as single entry\n",
        "            # second reshape transpose as vertical 2D array\n",
        "            f_loss[j] = fair_loss_corr(X_unlabeled[j],proba_temp, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, Xy_std)\n",
        "\n",
        "    e_loss = log_loss(probas_val)\n",
        "\n",
        "#     print(\"Debug f_loss before:\", f_loss.shape, f_loss)\n",
        "\n",
        "    e_scaled, f_scaled = normalizer(e_loss, f_loss)\n",
        "    f_scaled[np.isnan(f_scaled)] = 0\n",
        "\n",
        "#     print(\"Debug f_loss after:\", f_scaled.shape, f_scaled)\n",
        "\n",
        "    loss = div*(e_scaled)+(1-div)*f_scaled\n",
        "    selection = np.argsort(loss)[::-1][:step]\n",
        "    \n",
        "\n",
        "    return selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pre_filter(X_data, b_data, y_data, budget): \n",
        "\n",
        "    temp_columns = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree','race','two_year_recid']\n",
        "    # np.random.seed(84)\n",
        "\n",
        "    # print(\"Debug data:\", X_data.shape, b_data.shape, y_data.shape)\n",
        "    temp_data = np.c_[X_data, b_data, y_data]\n",
        "    temp_df = pd.DataFrame(data=temp_data, columns=temp_columns)\n",
        "    candidates_data = temp_df.groupby('race', group_keys=False).apply(lambda x: x.sample(n=math.ceil(budget/2)))\n",
        "    # candidates_data = temp_df.groupby('race', group_keys=False).apply(lambda x: x.sample(n=math.ceil(budget/2), random_state = 84))\n",
        "    # print(\"Debug ceil:\", math.ceil(budget/2))\n",
        "    # print(\"Debug ceil:\", math.ceil(budget/2))\n",
        "    # print(\"Debug cand:\", candidates)\n",
        "    candidates_index = candidates_data.index.values\n",
        "    # print(\"Debug index:\", candidates_index.shape, candidates_index)\n",
        "\n",
        "    return candidates_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metrics(X_test, y_test, b_test, classifier):\n",
        "    y_test_pred = classifier.predict(X_test)\n",
        "    # print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, classification_report(y_test, y_test_pred)))\n",
        "    # print(\"Confusion matrix:\\n%s\" % confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "    # tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
        "    # tpr = tp/(tp+fn)\n",
        "    # tnr = tn/(tn+fp) \n",
        "    # fpr = fp/(fp+tn)\n",
        "    # fnr = fn/(tp+fn)\n",
        "    # fdr = fp/(tp+fp)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "\n",
        "    # model_displays = {}\n",
        "    # for i in np.unique(y_test):\n",
        "    #     model_displays[i] = plot_roc_curve(\n",
        "    #         classifier, X_test[b_test==i], y_test[b_test==i], ax=ax, name=i)\n",
        "    # ax.set_title('ROC curve')\n",
        "    # plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scatters(X_unlabeled, b_unlabeled, probas_val, uncertain_samples):\n",
        "    y_plot = probas_val\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_axes([0,0,1,1])\n",
        "    ax.scatter(X_unlabeled[:,1], y_plot[:,1], c=b_unlabeled, cmap = 'Pastel1')\n",
        "    ax.scatter(X_unlabeled[:,1][uncertain_samples], y_plot[:,1][uncertain_samples], c=b_unlabeled[uncertain_samples], edgecolors='black', s=100, cmap = 'Pastel1')\n",
        "    ax.set_xlabel('Age')\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Population plot')\n",
        "    y_plot = np.delete(y_plot, uncertain_samples, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def performance_progression(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, pplot_file_name,run):\n",
        "    fig, (ax1, ax2) = plt.subplots(2,figsize=(10,10))\n",
        "    fig.suptitle('Fairness and accuracy metrics')\n",
        "    ax1.plot(x_axis, ur_fairness, color='r', label='unfair-active')\n",
        "    ax1.plot(x_axis, rs_fairness, color='g', label='random-active')\n",
        "    ax1.plot(x_axis, nonal_fairness, color='b', label='non-active')\n",
        "    ax1.legend()\n",
        "    ax2.plot(x_axis, ur_accuracies, color='r', label='unfair-active')\n",
        "    ax2.plot(x_axis, rs_accuracies, color='g', label='random-active')\n",
        "    ax2.plot(x_axis, nonal_accuracies, color='b', label='non-active')\n",
        "    ax2.legend()\n",
        "    ax1.set_xlabel('Sample size')\n",
        "    ax1.set_ylabel('Unfairness')\n",
        "    ax2.set_ylabel('Accuracies')\n",
        "    plt.savefig(pplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trade_off_plot(ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, tplot_file_name, smooth_tplot_file_name,run):\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    ur_index = np.argsort(ur_accuracies)\n",
        "    rs_index = np.argsort(rs_accuracies)\n",
        "    nonal_index = np.argsort(nonal_accuracies) \n",
        "\n",
        "    plt.plot(np.array(ur_accuracies)[ur_index], np.array(ur_fairness)[ur_index], color='r', label='unfair-active')\n",
        "    plt.plot(np.array(rs_accuracies)[rs_index], np.array(rs_fairness)[rs_index], color='g', label='random-active')\n",
        "    plt.plot(np.array(nonal_accuracies)[nonal_index], np.array(nonal_fairness)[nonal_index], color='b', label='non-active')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Accuracies')\n",
        "    plt.ylabel('Unfairness')\n",
        "    plt.savefig(tplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()\n",
        "\n",
        "    ur_smooth = sm.nonparametric.lowess(np.array(ur_fairness)[ur_index], np.array(ur_accuracies)[ur_index], frac = 0.5)\n",
        "    rs_smooth = sm.nonparametric.lowess(np.array(rs_fairness)[rs_index], np.array(rs_accuracies)[rs_index], frac = 0.5)\n",
        "    nonal_smooth = sm.nonparametric.lowess(np.array(nonal_fairness)[nonal_index], np.array(nonal_accuracies)[nonal_index], frac = 0.5)\n",
        "\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    plt.plot(ur_smooth[:, 0], ur_smooth[:, 1], color='r', label='unfair-active')\n",
        "    plt.plot(rs_smooth[:, 0], rs_smooth[:, 1], color='g', label='random-active')\n",
        "    plt.plot(nonal_smooth[:, 0], nonal_smooth[:, 1], color='b', label='non-active')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Approximate accuracies')\n",
        "    plt.ylabel('Approximate unfairness')\n",
        "    plt.savefig(smooth_tplot_file_name.format(run), bbox_inches='tight', dpi=200)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def maximal_correlation_transform(ace_model, x, y):\n",
        "    \n",
        "    ace_model.build_model_from_xy(x, y)\n",
        "    X_transformed = ace_model.ace.x_transforms\n",
        "    y_transformed = ace_model.ace.y_transform\n",
        "    \n",
        "    return X_transformed, y_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Correlation-based sample selection:  \"\"\"\n",
        "\n",
        "\"\"\" Formulas\n",
        "\n",
        "Mutual Information: MI(B,Y') = \\sum_{i = 1}^{|B|}\\sum_{j = 1}^{|Y'|}\\frac{|B_{i} \\cap Y'_{j}|}{N} log\\frac{N|B_{i} \\cap Y'_{j}|}{|B_{i}||Y'_{j}|}\n",
        "Statistical Parity: SP(B,Y') = |P(Y' = 1|B = a) - P(Y' = 1|B = b)| \\quad \\forall a,b \\in B\n",
        "Equal Opportunity: EOP(B,Y') = |P(Y' = 1|B = a, Y = 1) - P(Y' = 1|B = b, Y = 1)| \\quad \\forall a,b \\in B\n",
        "Equalized Odds: EOD(B,Y') = |P(Y' = 1|B = a, Y = y) - P(Y' = 1|B = b, Y = y)| \\quad \\forall a,b \\in B, \\quad \\forall 0,1 \\in y\n",
        "Conditional Use Accuracy Equality: CUAE(B,Y') = |P(Y' = y|B = a, Y = y) - P(Y' = y|B = b, Y = y)| \\quad \\forall a,b \\in B, \\quad \\forall 0,1 \\in y\n",
        "Disparate Impact: DI(B,Y') = |P(Y' = 1|B = a) / P(Y' = 1|B = b)| \\quad \\forall a,b \\in B\n",
        "\n",
        "Source\n",
        "----------\n",
        "\n",
        "separation \\ Equal opportunity - Hardt, Price, Srebro (2016)\n",
        "separation \\ Equalized adds - Hardt, Price, Srebro (2016)\n",
        "independence \\ Mutual information - Cover, T.M.; Thomas, J.A. (1991). Elements of Information Theory (Wiley ed.)\n",
        "sufficiency \\ Conditional Use Accuracy Equality - Richard Berk et al., (2017) Fairness in Criminal Justice Risk Assessments: The State of the Art\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "X_fair_est: testing data, ndarray\n",
        "y_fair_est: testing target, ndarray\n",
        "b_fair_est: bias attribute of testing set, ndarray\n",
        "\n",
        "Returns\n",
        "-------\n",
        "f_loss: fairness loss, int\n",
        "\"\"\"\n",
        "\n",
        "class active_learning(object):\n",
        "\n",
        "    def __init__(self, step, budget, model_object, criteria):\n",
        "        self.step = step\n",
        "        self.budget = budget\n",
        "        self.model_object = model_object\n",
        "        # self.sample_selection_function = selection_function\n",
        "        self.criteria = criteria\n",
        "        \n",
        "    def run(self, X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc, X_labeled_Xy_Mc, y_labeled_Xy_Mc, X_unlabeled_Xy_Mc, y_unlabeled_Xy_Mc, sub_option, af_div):\n",
        "  \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_labeled, X_test, features_weight) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "\n",
        "        Xb_corr = np.corrcoef(np.concatenate((X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc.reshape(b_unlabeled_Xb_Mc.shape[0],1)),1).T.astype(float))[0:col_size,-1].reshape(col_size,1) # the correlation of features and sensitive attribute (do not iteratively change)\n",
        "\n",
        "        \"\"\"Xb correlation: should it be the correlation of X and b for labeled or unlabeled data?\"\"\"\n",
        "\n",
        "        Xy_labeled_sum = np.zeros(col_size) # 1D array\n",
        "        X_labeled_sum = np.zeros(col_size) # 1D array\n",
        "        y_labeled_sum = np.sum(y_labeled_Xy_Mc) #integer, sum of all labeled y\n",
        "\n",
        "        for i in range(row_size):\n",
        "            X_labeled_sum = np.add(X_labeled_sum, X_labeled_Xy_Mc[i])\n",
        "            Xy_labeled_sum = np.add(Xy_labeled_sum, X_labeled_Xy_Mc[i]*y_labeled_Xy_Mc[i])\n",
        "\n",
        "        y_mean = 1./row_size*y_labeled_sum\n",
        "        Xy_corr = (1./row_size*np.subtract(Xy_labeled_sum,X_labeled_sum*y_mean))/(np.std(X_labeled_Xy_Mc)*np.std(y_labeled_Xy_Mc)) # (sum of x for y=1 - sum of x with ratio of y=1)/row size\n",
        "        yhatb_corr = np.dot(Xb_corr.transpose(),features_weight)[0,0] # Correlation between prediction of y and biased feature\n",
        "\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(active_iteration)\n",
        "        self.query_size = len(X_labeled)\n",
        "        fairness = []\n",
        "        fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "        metrics(X_test, y_test, b_test, self.clf_model.model_object.classifier)\n",
        "\n",
        "        while self.query_size <= self.budget-self.step:\n",
        "\n",
        "            active_iteration += 1\n",
        "            self.query_size += self.step\n",
        "\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_unlabeled)\n",
        "\n",
        "            Xy_std = np.std(X_labeled)*np.std(y_labeled)\n",
        "\n",
        "            # yb_corr=np.dot(Xb_corr.transpose(),features_weight)[0,0]\n",
        "            \n",
        "            # print(\"Debug predicted:\", y_unlabeled_predicted.shape)\n",
        "            # print(\"Debug probas_val:\", probas_val.shape)\n",
        "\n",
        "            # if sub_option == \"Legacy_filter\":\n",
        "            #     # post_entropy_index = entropy_selection(probas_val, self.budget*2)\n",
        "            #     candidates_index = pre_filter(X_unlabeled, b_unlabeled, y_unlabeled, self.budget)\n",
        "            #     uncertain_samples = unfairness_reduction_sampling(self.query_size, X_unlabeled[candidates_index], X_labeled, y_labeled, self.clf_model.model_object.classifier, probas_val[candidates_index], self.step, self.criteria)\n",
        "\n",
        "            if sub_option == \"Pre_filter\":\n",
        "                candidates_index = pre_filter(X_unlabeled, b_unlabeled, y_unlabeled, self.budget)\n",
        "                uncertain_samples = unfairness_correlation_sampling(X_unlabeled[candidates_index], self.clf_model.model_object.classifier, probas_val[candidates_index], self.step, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, af_div, Xy_std)\n",
        "\n",
        "            elif sub_option == \"No_filter\": \n",
        "                uncertain_samples = unfairness_correlation_sampling(X_unlabeled, self.clf_model.model_object.classifier, probas_val, self.step, row_size, y_labeled_sum, yhatb_corr, Xy_labeled_sum, X_labeled_sum, features_weight, Xb_corr, Xy_corr, af_div, Xy_std)\n",
        "\n",
        "            elif sub_option == \"Filter_only\":\n",
        "                uncertain_samples = random_selection(probas_val, self.step)\n",
        "            # print(\"Debug shape of X_unlabeled and loss:\", selection)\n",
        "\n",
        "            # scatters(X_unlabeled, b_unlabeled, probas_val, uncertain_samples)\n",
        "\n",
        "            X_labeled = np.concatenate((X_labeled, X_unlabeled[uncertain_samples]))\n",
        "            y_labeled = np.concatenate((y_labeled, y_unlabeled[uncertain_samples]))\n",
        "            X_labeled_Xy_Mc = np.concatenate((X_labeled_Xy_Mc, X_unlabeled_Xy_Mc[uncertain_samples])) # needs verification\n",
        "            X_labeled_Xy_Mc = np.concatenate((y_labeled_Xy_Mc, y_unlabeled_Xy_Mc[uncertain_samples])) # needs verification\n",
        "\n",
        "            (X_labeled, X_test, features_weight) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "\n",
        "            row_size += self.step\n",
        "\n",
        "            for i in uncertain_samples:\n",
        "\n",
        "                X_labeled_sum = np.add(X_labeled_sum,X_unlabeled_Xy_Mc[i])\n",
        "                # print(\"Debug before update:\", Xy_labeled_sum)\n",
        "                y_labeled_sum += y_unlabeled_Xy_Mc[i]; \n",
        "                Xy_labeled_sum = np.add(Xy_labeled_sum, (X_unlabeled_Xy_Mc[i]*y_unlabeled_Xy_Mc[i]))\n",
        "\n",
        "            y_expected = 1./row_size*y_labeled_sum\n",
        "            Xy_corr = (1./row_size*np.subtract(Xy_labeled_sum,X_labeled_sum*y_expected))/(np.std(X_labeled_Xy_Mc)*np.std(X_labeled_Xy_Mc))\n",
        "            yhatb_corr = np.dot(Xb_corr.transpose(),features_weight)[0,0] # update based on features weight change\n",
        "            # print(\"Debug Xb corr:\", Xb_corr)\n",
        "            # print(\"Debug yhatb corr:\", yhatb_corr)\n",
        "\n",
        "            X_unlabeled = np.delete(X_unlabeled, uncertain_samples, axis=0)\n",
        "            y_unlabeled = np.delete(y_unlabeled, uncertain_samples, axis=0)\n",
        "            b_unlabeled = np.delete(b_unlabeled, uncertain_samples, axis=0)\n",
        "\n",
        "            X_unlabeled_Xb_Mc = np.delete(X_unlabeled_Xb_Mc, uncertain_samples, axis=0)\n",
        "            b_unlabeled_Xb_Mc = np.delete(b_unlabeled_Xb_Mc, uncertain_samples, axis=0)\n",
        "            X_unlabeled_Xy_Mc = np.delete(X_unlabeled_Xy_Mc, uncertain_samples, axis=0)\n",
        "            y_unlabeled_Xy_Mc = np.delete(y_unlabeled_Xy_Mc, uncertain_samples, axis=0)\n",
        "            X_labeled_Xy_Mc = np.delete(X_labeled_Xy_Mc, uncertain_samples, axis=0)\n",
        "            y_labeled_Xy_Mc = np.delete(y_labeled_Xy_Mc, uncertain_samples, axis=0)\n",
        "\n",
        "            fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "            self.clf_model.get_test_accuracy(active_iteration)\n",
        "\n",
        "            # metrics(X_test, y_test, b_test, self.clf_model.model_object.classifier)\n",
        "\n",
        "        return self.clf_model.accuracies, fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled,y_labeled, X_test, y_test, b_test, budget, step, unfairness_criteria): \n",
        "\n",
        "    initial_X_train = X_labeled\n",
        "    initial_y_train = y_labeled\n",
        "    nonal_X_train = X_unlabeled\n",
        "    nonal_y_train = y_unlabeled\n",
        "    nonal_X_test = X_test\n",
        "    nonal_y_test = y_test\n",
        "    nonal_b_test = b_test\n",
        "    nonal_fairness = []\n",
        "\n",
        "    nonal_accuracies=[]\n",
        "\n",
        "    classifier_nonal = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "\n",
        "    classifier_nonal.fit(initial_X_train, initial_y_train)\n",
        "    initial_y_pred = classifier_nonal.predict(nonal_X_test)\n",
        "    nonal_fairness = np.append(nonal_fairness, fair_measure(nonal_X_test, nonal_y_test, nonal_b_test, classifier=classifier_nonal, criteria=unfairness_criteria))\n",
        "    nonal_accuracies.append(accuracy_score(nonal_y_test, initial_y_pred)*100)\n",
        "\n",
        "    for i in np.arange(init_index+step,budget+1,step):\n",
        "        classifier_nonal.fit(nonal_X_train[:i], nonal_y_train[:i])\n",
        "        nonal_y_pred = classifier_nonal.predict(nonal_X_test)\n",
        "        nonal_fairness = np.append(nonal_fairness, fair_measure(nonal_X_test, nonal_y_test, nonal_b_test, classifier=classifier_nonal, criteria=unfairness_criteria))\n",
        "        nonal_accuracies.append(accuracy_score(nonal_y_test, nonal_y_pred)*100)\n",
        "        metrics(X_test, y_test, b_test, classifier_nonal)\n",
        "\n",
        "    return nonal_accuracies, nonal_fairness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def results(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, run):\n",
        "\n",
        "    data = (np.array([x_axis, ur_accuracies, ur_fairness, rs_accuracies, rs_fairness, nonal_accuracies, nonal_fairness])).T\n",
        "\n",
        "    log_file_name = \"result_log/{run}.Result.csv\"\n",
        "    pplot_file_name = \"result_log/{:03}.Perf_Plot.png\"\n",
        "    tplot_file_name = \"result_log/{:03}.Trdoff_Plot.png\"\n",
        "    smooth_tplot_file_name = \"result_log/{:03}.Smooth.Trdoff_Plot.png\"\n",
        "    np.savetxt(log_file_name.format(run = run), data, delimiter=',', header=\"x_axis, ur_accuracies, ur_fairness, rs_accuracies, rs_fairness, nonal_accuracies, nonal_fairness\", fmt='%d,%f,%f,%f,%f,%f,%f')\n",
        "\n",
        "    performance_progression(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, pplot_file_name, run)\n",
        "    trade_off_plot(ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, tplot_file_name, smooth_tplot_file_name, run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def experiment(model,budget,step,criteria,sub_option=None):\n",
        "    \n",
        "    (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test) = retrieve_data_recid()\n",
        "    init_index = len(X_labeled)\n",
        "\n",
        "    act_alg = active_learning(step, budget, model, criteria)\n",
        "\n",
        "    # Xb_ace = ace.model.Model()\n",
        "    # Xb_ace.build_model_from_xy(X_unlabeled.T.tolist(), b_unlabeled.tolist())\n",
        "    # X_unlabeled_Xb_Mc = Xb_ace.ace.x_transforms\n",
        "    # b_unlabeled_Xb_Mc = Xb_ace.ace.y_transform\n",
        "\n",
        "    Xy_ace = ace.model.Model()\n",
        "    # Xy_ace.build_model_from_xy(X_unlabeled.T.tolist(), y_unlabeled.tolist())\n",
        "    # X_unlabeled_Xy_Mc = Xy_ace.ace.x_transforms\n",
        "    # y_unlabeled_Xy_Mc = Xy_ace.ace.y_transform\n",
        "\n",
        "    # X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc = maximal_correlation_transform(Xb_ace, X_unlabeled.T.tolist(), b_unlabeled.tolist())\n",
        "    # X_unlabeled_Xy_Mc, y_unlabeled_Xy_Mc = maximal_correlation_transform(Xy_ace, X_unlabeled.T.tolist(), y_unlabeled.tolist())\n",
        "    X_labeled_Xy_Mc, y_labeled_Xy_Mc = maximal_correlation_transform(Xy_ace, X_labeled.T.tolist(), y_labeled.tolist())\n",
        "\n",
        "    for i in np.arange(10):\n",
        "\n",
        "        af_div = i/10\n",
        "        print(\"Ratio of e_loss/f_loss is\",af_div,\"/\",1-af_div)\n",
        "\n",
        "        ur_accuracies, ur_fairness = act_alg.run(X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc, X_labeled_Xy_Mc, y_labeled_Xy_Mc, X_unlabeled_Xy_Mc, y_unlabeled_Xy_Mc, sub_option, af_div)\n",
        "        rs_accuracies, rs_fairness = act_alg.run(X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_unlabeled_Xb_Mc, b_unlabeled_Xb_Mc, X_labeled_Xy_Mc, y_labeled_Xy_Mc, X_unlabeled_Xy_Mc, y_unlabeled_Xy_Mc, \"Filter_only\", af_div)\n",
        "        nonal_accuracies, nonal_fairness = non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled, y_labeled, X_test, y_test, b_test, budget, step, criteria)\n",
        "        x_axis = np.arange(init_index,budget+1,step)\n",
        "        results(init_index, x_axis, budget, step, ur_fairness, rs_fairness, nonal_fairness, ur_accuracies, rs_accuracies, nonal_accuracies, run = i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5875, 2) (5875, 10) (5875, 2) (5875, 10)\n",
            "* Starting outer iteration 000. Current err =  1.00000E+00\n",
            "  Starting inner iteration 000. Current err =  1.00000E+00\n",
            "  Starting inner iteration 001. Current err =  1.28084E-01\n",
            "  Starting inner iteration 002. Current err =  8.85996E-02\n",
            "  Starting inner iteration 003. Current err =  7.75080E-02\n",
            "  Starting inner iteration 004. Current err =  6.99508E-02\n",
            "  Starting inner iteration 005. Current err =  6.40759E-02\n",
            "  Starting inner iteration 006. Current err =  5.94251E-02\n",
            "  Starting inner iteration 007. Current err =  5.57186E-02\n",
            "  Starting inner iteration 008. Current err =  5.27506E-02\n",
            "  Starting inner iteration 009. Current err =  5.03647E-02\n",
            "  Starting inner iteration 010. Current err =  4.84405E-02\n",
            "  Starting inner iteration 011. Current err =  4.68845E-02\n",
            "  Starting inner iteration 012. Current err =  4.56232E-02\n",
            "  Starting inner iteration 013. Current err =  4.45990E-02\n",
            "  Starting inner iteration 014. Current err =  4.37658E-02\n",
            "  Starting inner iteration 015. Current err =  4.30871E-02\n",
            "  Starting inner iteration 016. Current err =  4.25336E-02\n",
            "  Starting inner iteration 017. Current err =  4.20816E-02\n",
            "  Starting inner iteration 018. Current err =  4.17121E-02\n",
            "  Starting inner iteration 019. Current err =  4.14099E-02\n",
            "  Starting inner iteration 020. Current err =  4.11623E-02\n",
            "  Starting inner iteration 021. Current err =  4.09594E-02\n",
            "  Starting inner iteration 022. Current err =  4.07929E-02\n",
            "  Starting inner iteration 023. Current err =  4.06562E-02\n",
            "  Starting inner iteration 024. Current err =  4.05437E-02\n",
            "  Starting inner iteration 025. Current err =  4.04512E-02\n",
            "  Starting inner iteration 026. Current err =  4.03749E-02\n",
            "  Starting inner iteration 027. Current err =  4.03120E-02\n",
            "  Starting inner iteration 028. Current err =  4.02601E-02\n",
            "  Starting inner iteration 029. Current err =  4.02171E-02\n",
            "  Starting inner iteration 030. Current err =  4.01815E-02\n",
            "  Starting inner iteration 031. Current err =  4.01520E-02\n",
            "  Starting inner iteration 032. Current err =  4.01275E-02\n",
            "  Starting inner iteration 033. Current err =  4.01071E-02\n",
            "  Starting inner iteration 034. Current err =  4.00901E-02\n",
            "  Starting inner iteration 035. Current err =  4.00759E-02\n",
            "  Starting inner iteration 036. Current err =  4.00641E-02\n",
            "  Starting inner iteration 037. Current err =  4.00542E-02\n",
            "  Starting inner iteration 038. Current err =  4.00458E-02\n",
            "  Starting inner iteration 039. Current err =  4.00389E-02\n",
            "  Starting inner iteration 040. Current err =  4.00330E-02\n",
            "  Starting inner iteration 041. Current err =  4.00281E-02\n",
            "  Starting inner iteration 042. Current err =  4.00239E-02\n",
            "  Starting inner iteration 043. Current err =  4.00204E-02\n",
            "  Starting inner iteration 044. Current err =  4.00174E-02\n",
            "  Starting inner iteration 045. Current err =  4.00149E-02\n",
            "  Starting inner iteration 046. Current err =  4.00127E-02\n",
            "  Starting inner iteration 047. Current err =  4.00109E-02\n",
            "  Starting inner iteration 048. Current err =  4.00094E-02\n",
            "  Starting inner iteration 049. Current err =  4.00080E-02\n",
            "  Starting inner iteration 050. Current err =  4.00069E-02\n",
            "  Starting inner iteration 051. Current err =  4.00059E-02\n",
            "  Starting inner iteration 052. Current err =  4.00051E-02\n",
            "  Starting inner iteration 053. Current err =  4.00044E-02\n",
            "  Starting inner iteration 054. Current err =  4.00038E-02\n",
            "  Starting inner iteration 055. Current err =  4.00033E-02\n",
            "  Starting inner iteration 056. Current err =  4.00029E-02\n",
            "  Starting inner iteration 057. Current err =  4.00025E-02\n",
            "  Starting inner iteration 058. Current err =  4.00021E-02\n",
            "  Starting inner iteration 059. Current err =  4.00019E-02\n",
            "  Starting inner iteration 060. Current err =  4.00016E-02\n",
            "  Starting inner iteration 061. Current err =  4.00014E-02\n",
            "  Starting inner iteration 062. Current err =  4.00012E-02\n",
            "  Starting inner iteration 063. Current err =  4.00011E-02\n",
            "  Starting inner iteration 064. Current err =  4.00009E-02\n",
            "  Starting inner iteration 065. Current err =  4.00008E-02\n",
            "  Starting inner iteration 066. Current err =  4.00007E-02\n",
            "  Starting inner iteration 067. Current err =  4.00006E-02\n",
            "  Starting inner iteration 068. Current err =  4.00005E-02\n",
            "  Starting inner iteration 069. Current err =  4.00005E-02\n",
            "  Starting inner iteration 070. Current err =  4.00004E-02\n",
            "  Starting inner iteration 071. Current err =  4.00004E-02\n",
            "  Starting inner iteration 072. Current err =  4.00003E-02\n",
            "  Starting inner iteration 073. Current err =  4.00003E-02\n",
            "  Starting inner iteration 074. Current err =  4.00003E-02\n",
            "  Starting inner iteration 075. Current err =  4.00002E-02\n",
            "  Starting inner iteration 076. Current err =  4.00002E-02\n",
            "  Starting inner iteration 077. Current err =  4.00002E-02\n",
            "  Starting inner iteration 078. Current err =  4.00002E-02\n",
            "  Starting inner iteration 079. Current err =  4.00001E-02\n",
            "  Starting inner iteration 080. Current err =  4.00001E-02\n",
            "  Starting inner iteration 081. Current err =  4.00001E-02\n",
            "  Starting inner iteration 082. Current err =  4.00001E-02\n",
            "  Starting inner iteration 083. Current err =  4.00001E-02\n",
            "  Starting inner iteration 084. Current err =  4.00001E-02\n",
            "  Starting inner iteration 085. Current err =  4.00001E-02\n",
            "  Starting inner iteration 086. Current err =  4.00001E-02\n",
            "  Starting inner iteration 087. Current err =  4.00001E-02\n",
            "  Starting inner iteration 088. Current err =  4.00000E-02\n",
            "  Starting inner iteration 089. Current err =  4.00000E-02\n",
            "  Starting inner iteration 090. Current err =  4.00000E-02\n",
            "  Starting inner iteration 091. Current err =  4.00000E-02\n",
            "  Starting inner iteration 092. Current err =  4.00000E-02\n",
            "  Starting inner iteration 093. Current err =  4.00000E-02\n",
            "  Starting inner iteration 094. Current err =  4.00000E-02\n",
            "  Starting inner iteration 095. Current err =  4.00000E-02\n",
            "  Starting inner iteration 096. Current err =  4.00000E-02\n",
            "  Starting inner iteration 097. Current err =  4.00000E-02\n",
            "  Starting inner iteration 098. Current err =  4.00000E-02\n",
            "  Starting inner iteration 099. Current err =  4.00000E-02\n",
            "  Starting inner iteration 100. Current err =  4.00000E-02\n",
            "  Starting inner iteration 101. Current err =  4.00000E-02\n",
            "  Starting inner iteration 102. Current err =  4.00000E-02\n",
            "  Starting inner iteration 103. Current err =  4.00000E-02\n",
            "  Starting inner iteration 104. Current err =  4.00000E-02\n",
            "  Starting inner iteration 105. Current err =  4.00000E-02\n",
            "  Starting inner iteration 106. Current err =  4.00000E-02\n",
            "  Starting inner iteration 107. Current err =  4.00000E-02\n",
            "  Starting inner iteration 108. Current err =  4.00000E-02\n",
            "  Starting inner iteration 109. Current err =  4.00000E-02\n",
            "  Starting inner iteration 110. Current err =  4.00000E-02\n",
            "  Starting inner iteration 111. Current err =  4.00000E-02\n",
            "  Starting inner iteration 112. Current err =  4.00000E-02\n",
            "  Starting inner iteration 113. Current err =  4.00000E-02\n",
            "  Starting inner iteration 114. Current err =  4.00000E-02\n",
            "  Starting inner iteration 115. Current err =  4.00000E-02\n",
            "  Starting inner iteration 116. Current err =  4.00000E-02\n",
            "  Starting inner iteration 117. Current err =  4.00000E-02\n",
            "  Starting inner iteration 118. Current err =  4.00000E-02\n",
            "  Starting inner iteration 119. Current err =  4.00000E-02\n",
            "  Starting inner iteration 120. Current err =  4.00000E-02\n",
            "  Starting inner iteration 121. Current err =  4.00000E-02\n",
            "  Starting inner iteration 122. Current err =  4.00000E-02\n",
            "  Starting inner iteration 123. Current err =  4.00000E-02\n",
            "  Starting inner iteration 124. Current err =  4.00000E-02\n",
            "  Starting inner iteration 125. Current err =  4.00000E-02\n",
            "  Starting inner iteration 126. Current err =  4.00000E-02\n",
            "  Starting inner iteration 127. Current err =  4.00000E-02\n",
            "  Starting inner iteration 128. Current err =  4.00000E-02\n",
            "  Starting inner iteration 129. Current err =  4.00000E-02\n",
            "  Starting inner iteration 130. Current err =  4.00000E-02\n",
            "  Starting inner iteration 131. Current err =  4.00000E-02\n",
            "  Starting inner iteration 132. Current err =  4.00000E-02\n",
            "  Starting inner iteration 133. Current err =  4.00000E-02\n",
            "  Starting inner iteration 134. Current err =  4.00000E-02\n",
            "  Starting inner iteration 135. Current err =  4.00000E-02\n",
            "  Starting inner iteration 136. Current err =  4.00000E-02\n",
            "  Starting inner iteration 137. Current err =  4.00000E-02\n",
            "  Starting inner iteration 138. Current err =  4.00000E-02\n",
            "  Starting inner iteration 139. Current err =  4.00000E-02\n",
            "  Starting inner iteration 140. Current err =  4.00000E-02\n",
            "  Starting inner iteration 141. Current err =  4.00000E-02\n",
            "  Starting inner iteration 142. Current err =  4.00000E-02\n",
            "  Starting inner iteration 143. Current err =  4.00000E-02\n",
            "  Starting inner iteration 144. Current err =  4.00000E-02\n",
            "  Starting inner iteration 145. Current err =  4.00000E-02\n",
            "  Starting inner iteration 146. Current err =  4.00000E-02\n",
            "  Starting inner iteration 147. Current err =  4.00000E-02\n",
            "  Starting inner iteration 148. Current err =  4.00000E-02\n",
            "  Starting inner iteration 149. Current err =  4.00000E-02\n",
            "  Starting inner iteration 150. Current err =  4.00000E-02\n",
            "  Starting inner iteration 151. Current err =  4.00000E-02\n",
            "  Starting inner iteration 152. Current err =  4.00000E-02\n",
            "  Starting inner iteration 153. Current err =  4.00000E-02\n",
            "  Starting inner iteration 154. Current err =  4.00000E-02\n",
            "  Starting inner iteration 155. Current err =  4.00000E-02\n",
            "  Starting inner iteration 156. Current err =  4.00000E-02\n",
            "  Starting inner iteration 157. Current err =  4.00000E-02\n",
            "  Starting inner iteration 158. Current err =  4.00000E-02\n",
            "  Starting inner iteration 159. Current err =  4.00000E-02\n",
            "  Starting inner iteration 160. Current err =  4.00000E-02\n",
            "  Starting inner iteration 161. Current err =  4.00000E-02\n",
            "  Starting inner iteration 162. Current err =  4.00000E-02\n",
            "  Starting inner iteration 163. Current err =  4.00000E-02\n",
            "  Starting inner iteration 164. Current err =  4.00000E-02\n",
            "  Starting inner iteration 165. Current err =  4.00000E-02\n",
            "  Starting inner iteration 166. Current err =  4.00000E-02\n",
            "  Starting inner iteration 167. Current err =  4.00000E-02\n",
            "  Starting inner iteration 168. Current err =  4.00000E-02\n",
            "  Starting inner iteration 169. Current err =  4.00000E-02\n",
            "  Starting inner iteration 170. Current err =  4.00000E-02\n",
            "  Starting inner iteration 171. Current err =  4.00000E-02\n",
            "  Starting inner iteration 172. Current err =  4.00000E-02\n",
            "  Starting inner iteration 173. Current err =  4.00000E-02\n",
            "  Starting inner iteration 174. Current err =  4.00000E-02\n",
            "  Starting inner iteration 175. Current err =  4.00000E-02\n",
            "  Starting inner iteration 176. Current err =  4.00000E-02\n",
            "  Starting inner iteration 177. Current err =  4.00000E-02\n",
            "  Starting inner iteration 178. Current err =  4.00000E-02\n",
            "  Starting inner iteration 179. Current err =  4.00000E-02\n",
            "  Starting inner iteration 180. Current err =  4.00000E-02\n",
            "  Starting inner iteration 181. Current err =  4.00000E-02\n",
            "  Starting inner iteration 182. Current err =  4.00000E-02\n",
            "  Starting inner iteration 183. Current err =  4.00000E-02\n",
            "  Starting inner iteration 184. Current err =  4.00000E-02\n",
            "  Starting inner iteration 185. Current err =  4.00000E-02\n",
            "  Starting inner iteration 186. Current err =  4.00000E-02\n",
            "  Starting inner iteration 187. Current err =  4.00000E-02\n",
            "  Starting inner iteration 188. Current err =  4.00000E-02\n",
            "  Starting inner iteration 189. Current err =  4.00000E-02\n",
            "  Starting inner iteration 190. Current err =  4.00000E-02\n",
            "  Starting inner iteration 191. Current err =  4.00000E-02\n",
            "  Starting inner iteration 192. Current err =  4.00000E-02\n",
            "  Starting inner iteration 193. Current err =  4.00000E-02\n",
            "  Starting inner iteration 194. Current err =  4.00000E-02\n",
            "  Starting inner iteration 195. Current err =  4.00000E-02\n",
            "  Starting inner iteration 196. Current err =  4.00000E-02\n",
            "  Starting inner iteration 197. Current err =  4.00000E-02\n",
            "  Starting inner iteration 198. Current err =  4.00000E-02\n",
            "  Starting inner iteration 199. Current err =  4.00000E-02\n",
            "  Starting inner iteration 200. Current err =  4.00000E-02\n",
            "  Starting inner iteration 201. Current err =  4.00000E-02\n",
            "  Starting inner iteration 202. Current err =  4.00000E-02\n",
            "  Starting inner iteration 203. Current err =  4.00000E-02\n",
            "  Starting inner iteration 204. Current err =  4.00000E-02\n",
            "  Starting inner iteration 205. Current err =  4.00000E-02\n",
            "  Starting inner iteration 206. Current err =  4.00000E-02\n",
            "  Starting inner iteration 207. Current err =  4.00000E-02\n",
            "  Starting inner iteration 208. Current err =  4.00000E-02\n",
            "  Starting inner iteration 209. Current err =  4.00000E-02\n",
            "  Starting inner iteration 210. Current err =  4.00000E-02\n",
            "  Starting inner iteration 211. Current err =  4.00000E-02\n",
            "  Starting inner iteration 212. Current err =  4.00000E-02\n",
            "  Starting inner iteration 213. Current err =  4.00000E-02\n",
            "  Starting inner iteration 214. Current err =  4.00000E-02\n",
            "  Starting inner iteration 215. Current err =  4.00000E-02\n",
            "  Starting inner iteration 216. Current err =  4.00000E-02\n",
            "  Starting inner iteration 217. Current err =  4.00000E-02\n",
            "  Starting inner iteration 218. Current err =  4.00000E-02\n",
            "  Starting inner iteration 219. Current err =  4.00000E-02\n",
            "  Starting inner iteration 220. Current err =  4.00000E-02\n",
            "  Starting inner iteration 221. Current err =  4.00000E-02\n",
            "  Starting inner iteration 222. Current err =  4.00000E-02\n",
            "  Starting inner iteration 223. Current err =  4.00000E-02\n",
            "  Starting inner iteration 224. Current err =  4.00000E-02\n",
            "  Starting inner iteration 225. Current err =  4.00000E-02\n",
            "  Starting inner iteration 226. Current err =  4.00000E-02\n",
            "  Starting inner iteration 227. Current err =  4.00000E-02\n",
            "  Starting inner iteration 228. Current err =  4.00000E-02\n",
            "  Starting inner iteration 229. Current err =  4.00000E-02\n",
            "  Starting inner iteration 230. Current err =  4.00000E-02\n",
            "  Starting inner iteration 231. Current err =  4.00000E-02\n",
            "  Starting inner iteration 232. Current err =  4.00000E-02\n",
            "  Starting inner iteration 233. Current err =  4.00000E-02\n",
            "  Starting inner iteration 234. Current err =  4.00000E-02\n",
            "  Starting inner iteration 235. Current err =  4.00000E-02\n",
            "  Starting inner iteration 236. Current err =  4.00000E-02\n",
            "  Starting inner iteration 237. Current err =  4.00000E-02\n",
            "  Starting inner iteration 238. Current err =  4.00000E-02\n",
            "  Starting inner iteration 239. Current err =  4.00000E-02\n",
            "  Starting inner iteration 240. Current err =  4.00000E-02\n",
            "  Starting inner iteration 241. Current err =  4.00000E-02\n",
            "  Starting inner iteration 242. Current err =  4.00000E-02\n",
            "  Starting inner iteration 243. Current err =  4.00000E-02\n",
            "  Starting inner iteration 244. Current err =  4.00000E-02\n",
            "  Starting inner iteration 245. Current err =  4.00000E-02\n",
            "Ratio of e_loss/f_loss is 0.0 / 1.0\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X_unlabeled_Xb_Mc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_29812/527169389.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mutual_information\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"No_filter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/pc/f0ds212x5ql14xc9444c5gv00000gn/T/ipykernel_29812/1907173034.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(model, budget, step, criteria, sub_option)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ratio of e_loss/f_loss is\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maf_div\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0maf_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mur_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mur_fairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled_Xb_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled_Xb_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_option\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maf_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mrs_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_fairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled_Xb_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled_Xb_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled_Xy_Mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Filter_only\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maf_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnonal_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonal_fairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_active_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_unlabeled_Xb_Mc' is not defined"
          ]
        }
      ],
      "source": [
        "experiment(LogModel,200,10,\"mutual_information\",\"No_filter\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMfqalTRwiWuiIELJDbCQ7d",
      "mount_file_id": "1SZapm_bYNJDCJi8ECwmjrzaN8-1ruRgA",
      "name": "Logistic.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b6d1a97b03e52be1c553c50910cdb19760ee3e4d178c0a737039468e0b0f9e5f"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit ('thesis-project': venv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
