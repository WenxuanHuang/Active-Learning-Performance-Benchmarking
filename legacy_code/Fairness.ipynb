{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fairness.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "a2ef89d34cbfddaf50816f8d91581a3ca0913b9280767ed31a38c2db7dcc022c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.2 64-bit ('ML-for-COVID-19-dataset': venv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "metadata": {
      "interpreter": {
        "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WenxuanHuang/Active-Learning-Performance-Benchmarking/blob/main/Fairness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioi13nGDPDvQ",
        "outputId": "40a3a902-ba1f-4a51-a705-68e7cecb0990"
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "!pip install cvxpy\n",
        "!pip install aif360\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.7/dist-packages (1.0.31)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from cvxpy) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cvxpy) (1.4.1)\n",
            "Requirement already satisfied: scs>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from cvxpy) (2.1.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from cvxpy) (0.70.12.2)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy) (2.0.7.post1)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy) (0.6.2.post0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp>=0.4.1->cvxpy) (0.1.5.post0)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from multiprocess->cvxpy) (0.3.4)\n",
            "Collecting aif360\n",
            "  Downloading aif360-0.4.0-py3-none-any.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.1.5)\n",
            "Collecting tempeh\n",
            "  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (2.4.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (2.23.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (3.6.4)\n",
            "Collecting shap\n",
            "  Downloading shap-0.39.0.tar.gz (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 44.2 MB/s \n",
            "\u001b[?25hCollecting memory-profiler\n",
            "  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360) (5.4.8)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (8.8.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (21.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (57.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2021.5.30)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (4.62.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (1.3.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap->tempeh->aif360) (0.34.0)\n",
            "Building wheels for collected packages: memory-profiler, shap\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=0cd9d97ed933617fe970de160eb44c35847a2e52c183e18155631d1aebb31876\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491644 sha256=5bffc3f14672ae1c1fbbd8c5c83db3c8754dc65c1783a1f4ae14e557e758b309\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n",
            "Successfully built memory-profiler shap\n",
            "Installing collected packages: slicer, shap, memory-profiler, tempeh, aif360\n",
            "Successfully installed aif360-0.4.0 memory-profiler-0.58.0 shap-0.39.0 slicer-0.0.7 tempeh-0.1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glcQW20TLbnf"
      },
      "source": [
        "def retrieve_data_recid():\n",
        "    attributes = ['MarriageStatus','age','juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count', 'days_b_screening_arrest','c_days_from_compas','c_charge_degree']\n",
        "    bias = 'race'\n",
        "    target = 'two_year_recid'\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    data = pd.read_csv(\"https://raw.githubusercontent.com/WenxuanHuang/Active-Learning-Performance-Benchmarking/main/RecidivismData_Normalized.csv\", sep=',')\n",
        "    data_col = data.columns\n",
        "    df = data[(data[bias]==2)|(data[bias]==3)].copy().values\n",
        "\n",
        "    kf = KFold(n_splits=4) #differ from original method\n",
        "    for train_index, test_index in kf.split(df):\n",
        "        train, test = df[train_index], df[test_index]\n",
        "        # print(\"Size of X_train_full, X_test:\", train.shape, test.shape)\n",
        "\n",
        "    df_train = pd.DataFrame(data=train, columns=data_col)\n",
        "    df_test = pd.DataFrame(data=test, columns=data_col)\n",
        "\n",
        "    labeled = df_train.groupby(target, group_keys=False).apply(lambda x: x.sample(n=5, random_state = 42)) # ten sample in total labeled initially\n",
        "    df_X_labeled = labeled[attributes]\n",
        "    df_y_labeled = labeled[target]\n",
        "    X_labeled = df_X_labeled.values\n",
        "    y_labeled = df_y_labeled.values.astype('int64')\n",
        "    b_labeled = labeled[bias].values-2\n",
        "    (row_size, col_size) = X_labeled.shape\n",
        "\n",
        "    unlabeled = df_train.drop(df_X_labeled.index)\n",
        "    df_X_unlabeled = unlabeled[attributes]\n",
        "    df_y_unlabeled = unlabeled[target]\n",
        "    X_unlabeled = df_X_unlabeled.values\n",
        "    y_unlabeled = df_y_unlabeled.values.astype('int64')\n",
        "    b_unlabeled = unlabeled[bias].values-2\n",
        "\n",
        "    X_test = df_test[attributes].values\n",
        "    y_test = df_test[target].values\n",
        "    y_test=y_test.astype('int')\n",
        "    b_test = df_test[bias].values-2\n",
        "\n",
        "    X_fair_est = X_unlabeled\n",
        "    y_fair_est = y_unlabeled\n",
        "    b_fair_est = b_unlabeled\n",
        "    \n",
        "    return (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DIi-2mOLbnh"
      },
      "source": [
        "class BaseModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit_predict(self):\n",
        "        pass\n",
        "\n",
        "class LogModel(BaseModel):\n",
        "\n",
        "    def fit_predict(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        self.classifier = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "        self.classifier.fit(X_labeled, y_labeled)\n",
        "        # self.y_test_predicted = self.classifier.predict(X_test)\n",
        "        # self.y_unlabeled_predicted = self.classifier.predict(X_unlabeled)\n",
        "        self.y_test_score = self.classifier.score(X_test, y_test)\n",
        "        return (X_labeled, X_test, self.y_test_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5iL-thyLbni"
      },
      "source": [
        "class TrainModel:\n",
        "\n",
        "    def __init__(self, model_object):        \n",
        "        self.accuracies = []\n",
        "        self.model_object = model_object()        \n",
        "\n",
        "    def print_model_type(self):\n",
        "        print (self.model_object.model_type)\n",
        "\n",
        "    def train(self, X_labeled, y_labeled, X_test, y_test):\n",
        "        (X_labeled, X_test, self.y_test_score) = \\\n",
        "            self.model_object.fit_predict(X_labeled, y_labeled, X_test, y_test)\n",
        "        return (X_labeled, X_test)\n",
        "\n",
        "    def get_test_accuracy(self, i):\n",
        "        classif_rate = self.y_test_score * 100\n",
        "        self.accuracies.append(classif_rate)               \n",
        "        print('--------------------------------')\n",
        "        print('Iteration:',i)\n",
        "        print(\"Accuracy rate is %f \" % (classif_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "419XgwACLbni"
      },
      "source": [
        "class QueryFunction(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def pool_select(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomSelection(QueryFunction):\n",
        "\n",
        "    @staticmethod\n",
        "    def pool_select(probas_val, batch_size):\n",
        "        random_state = check_random_state(0)\n",
        "        # probas_val.shape[0] is the size of validation set\n",
        "        selection = np.random.choice(probas_val.shape[0], batch_size, replace=False)\n",
        "        # print('uniques chosen:',np.unique(selection).shape[0],'<= should be equal to:',batch_size)\n",
        "        return selection\n",
        "\n",
        "\n",
        "# class EntropySelection(QueryFunction):\n",
        "\n",
        "#     @staticmethod\n",
        "#     def pool_select(probas_val, batch_size):\n",
        "#         e = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "#         selection = (np.argsort(e)[::-1])[:batch_size]\n",
        "#         return selection\n",
        "\n",
        "# class MinStdSelection(QueryFunction):\n",
        "\n",
        "#     # select the samples where the std is smallest. There is uncertainty regarding the relevant class\n",
        "#     # and then train on these \"hard\" to classify samples.\n",
        "#     @staticmethod\n",
        "#     def pool_select(probas_val, batch_size):\n",
        "#         std = np.std(probas_val * 100, axis=1) \n",
        "#         selection = std.argsort()[:batch_size]\n",
        "#         selection = selection.astype('int64')\n",
        "#         print('std',std.shape,std)\n",
        "#         print('selection',selection, selection.shape, std[selection])\n",
        "#         return selection\n",
        "\n",
        "# class LeastConfidenceSelection(QueryFunction):\n",
        "\n",
        "#     @staticmethod\n",
        "#     def pool_select(probas_val, batch_size):\n",
        "#         sort_prob = -np.sort(-probas_val, axis=1)\n",
        "#         values = sort_prob[:, 0]\n",
        "#         selection = np.argsort(values)[:batch_size]\n",
        "#         return selection\n",
        "      \n",
        "      \n",
        "# class MarginSelection(QueryFunction):\n",
        "\n",
        "#     @staticmethod\n",
        "#     def pool_select(probas_val, batch_size):\n",
        "#         sort_prob = -np.sort(-probas_val, axis=1)\n",
        "#         values = sort_prob[:, 0] - sort_prob[:, 1]\n",
        "#         selection = np.argsort(values)[:batch_size]\n",
        "#         return selection\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_br27V3rLbnj"
      },
      "source": [
        "def normalizer(e_loss, f_loss):\n",
        "    e_loss = np.reshape(e_loss, (1,len(e_loss)))\n",
        "    f_loss = np.reshape(f_loss, (1,len(f_loss)))\n",
        "    e_scaled = preprocessing.normalize(e_loss)\n",
        "    # e_scaled=((e_loss-e_loss.min())/(e_loss.max()-e_loss.min()))\n",
        "    f_scaled = preprocessing.normalize(f_loss)\n",
        "    # f_scaled=((f_loss-f_loss.min())/(f_loss.max()-f_loss.min()))\n",
        "    return (e_scaled, f_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZksRq-DLbnj"
      },
      "source": [
        "def log_loss(probas_val):\n",
        "    \n",
        "    eps = np.finfo(probas_val.dtype).eps\n",
        "    probas_val = np.clip(probas_val, eps, 1 - eps)\n",
        "    e_loss = (-probas_val * np.log2(probas_val)).sum(axis=1)\n",
        "\n",
        "    return e_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-RWIEWKLbnk"
      },
      "source": [
        "# separation \\ Equal opportunity - Hardt, Price, Srebro (2016)\n",
        "\n",
        "# def metrics(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "#         fpr0 = metric.false_positive_rate(privileged=True)\n",
        "#         fpr1 = metric.false_positive_rate(privileged=False)\n",
        "#         fnr0 = metric.false_negative_rate(privileged=True)\n",
        "#         fnr1 = metric.false_negative_rate(privileged=False)\n",
        "#         tpr0 = metric.true_positive_rate(privileged=True)\n",
        "#         tpr1 = metric.true_positive_rate(privileged=False)\n",
        "#         tnr0 = metric.true_negative_rate(privileged=True)\n",
        "#         tnr1 = metric.true_negative_rate(privileged=False)\n",
        "\n",
        "def eqops(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_pred==1)&(y_fair_est==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    f_loss=abs((b0y1p1/b0y1)-(b1y1p1/b1y1))\n",
        "    # print(\"Debug fair_loss shape:\", b0p1, b0, b1p1, b1)\n",
        "    \n",
        "    return f_loss\n",
        "\n",
        "def eqods(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y0p1=X_fair_est[(b_fair_est==0)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0y0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)].shape[0]\n",
        "    b1y0p1=X_fair_est[(b_fair_est==1)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b1y0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)].shape[0]\n",
        "\n",
        "    b0y1p1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)&(y_fair_pred==1)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    fpr_loss=abs((b0y0p1/b0y0)-(b1y0p1/b1y0))\n",
        "    tpr_loss=abs((b0y1p1/b0y1)-(b1y1p1/b1y1))\n",
        "\n",
        "\n",
        "    f_loss = fpr_loss+tpr_loss # temporary solution\n",
        "    \n",
        "    return f_loss \n",
        "\n",
        "def disp_mist(X_fair_est, y_fair_est, b_fair_est, classifier):\n",
        "    \n",
        "    y_fair_pred = classifier.predict(X_fair_est)\n",
        "\n",
        "    b0y0p1=X_fair_est[(b_fair_est==0)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b0y0=X_fair_est[(b_fair_est==0)&(y_fair_est==0)].shape[0]\n",
        "    b1y0p1=X_fair_est[(b_fair_est==1)&(y_fair_est==0)&(y_fair_pred==1)].shape[0]\n",
        "    b1y0=X_fair_est[(b_fair_est==1)&(y_fair_est==0)].shape[0]\n",
        "\n",
        "    b0y1p0=X_fair_est[(b_fair_est==0)&(y_fair_est==1)&(y_fair_pred==0)].shape[0]\n",
        "    b0y1=X_fair_est[(b_fair_est==0)&(y_fair_est==1)].shape[0]\n",
        "    b1y1p0=X_fair_est[(b_fair_est==1)&(y_fair_est==1)&(y_fair_pred==0)].shape[0]\n",
        "    b1y1=X_fair_est[(b_fair_est==1)&(y_fair_est==1)].shape[0]\n",
        "\n",
        "    fpr_loss=abs((b0y0p1/b0y0)-(b1y0p1/b1y0))\n",
        "    fnr_loss=abs((b0y1p0/b0y1)-(b1y1p0/b1y1))\n",
        "\n",
        "\n",
        "    f_loss = fpr_loss+fnr_loss # temporary solution\n",
        "    \n",
        "    return f_loss \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9i6wv5JLbnl"
      },
      "source": [
        "# selecting fairness criteria\n",
        "\n",
        "def fair_measure(X_fair_est, y_fair_est, b_fair_est, classifier=None, criteria=0):\n",
        "\n",
        "    if criteria == 'equal_opportunity':\n",
        "        return eqops(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'disparate_mistreatment':\n",
        "        return disp_mist(X_fair_est, y_fair_est, b_fair_est, classifier)\n",
        "    elif criteria == 'equalized_odds':\n",
        "        return eqods(X_fair_est, y_fair_est, b_fair_est, classifier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNJNrm8CLbnl"
      },
      "source": [
        "def error_reduction_sampling(query_size, X_unlabeled, X_labeled, y_labeled, classifier, X_fair_est, y_fair_est, b_fair_est, probas_val, step, criteria):\n",
        "    # further to be defined, now assume only fairness loss\n",
        "    # query size used here\n",
        "    div = 0\n",
        "\n",
        "    unlabeled_size = len(X_unlabeled)\n",
        "    f_loss = np.zeros(unlabeled_size)\n",
        "    for i in range(unlabeled_size):\n",
        "        f_loss_temp = []\n",
        "        for j in range(2):\n",
        "            X_labeled_temp = np.append(X_labeled, [X_unlabeled[i]], axis = 0)\n",
        "            y_labeled_temp = np.append(y_labeled, [j], axis = 0)\n",
        "            classifier_temp = LogisticRegression(solver='liblinear').fit(X_labeled_temp, y_labeled_temp)\n",
        "            f_loss_temp = np.append(f_loss_temp, fair_measure(X_fair_est, y_fair_est, b_fair_est, classifier=classifier_temp, criteria=criteria))\n",
        "            f_loss_temp[np.isnan(f_loss_temp)] = 0\n",
        "            \n",
        "        proba_0 = classifier.predict_proba(X_unlabeled)[i][0]\n",
        "        proba_1 = 1 - proba_0\n",
        "        f_loss[i] = (f_loss_temp).dot([proba_0, proba_1])\n",
        "\n",
        "    e_loss = log_loss(probas_val)\n",
        "\n",
        "    e_scaled, f_scaled = normalizer(e_loss, f_loss)\n",
        "    f_scaled[np.isnan(f_scaled)] = 0\n",
        "\n",
        "    loss = div*(e_loss)+(1-div)*f_loss\n",
        "    \n",
        "    selection = np.argsort(loss)[::-1][:step]\n",
        "\n",
        "    return selection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6iOkrOHLbnm"
      },
      "source": [
        "class active_learning(object):\n",
        "\n",
        "    def __init__(self, step, budget, model_object, selection_function, criteria):\n",
        "        self.step = step\n",
        "        self.budget = budget\n",
        "        self.model_object = model_object\n",
        "        self.sample_selection_function = selection_function\n",
        "        self.criteria = criteria\n",
        "        \n",
        "    def run(self, X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, sub_option):\n",
        "  \n",
        "        self.clf_model = TrainModel(self.model_object)\n",
        "        (X_labeled, X_test) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "        active_iteration = 1\n",
        "        self.clf_model.get_test_accuracy(active_iteration)\n",
        "\n",
        "        self.query_size = len(X_labeled)\n",
        "        fairness = []\n",
        "        fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "\n",
        "        while self.query_size <= self.budget-self.step:\n",
        "\n",
        "            active_iteration += 1\n",
        "            self.query_size += self.step\n",
        "\n",
        "            probas_val = \\\n",
        "                self.clf_model.model_object.classifier.predict_proba(X_unlabeled)\n",
        "\n",
        "            # y_unlabeled_predicted = \\\n",
        "            #     self.clf_model.model_object.classifier.predict(X_unlabeled)\n",
        "            \n",
        "            # print(\"Debug predicted:\", y_unlabeled_predicted.shape)\n",
        "            # print(\"Debug probas_val:\", probas_val.shape)\n",
        "\n",
        "            if sub_option == \"Pre_filter\":\n",
        "                candidates = self.sample_selection_function.pool_select(probas_val, self.budget)\n",
        "                uncertain_samples = error_reduction_sampling(self.query_size, X_unlabeled[candidates], X_labeled, y_labeled, self.clf_model.model_object.classifier, X_fair_est[candidates], y_fair_est[candidates], b_fair_est[candidates], probas_val[candidates], self.step, self.criteria)\n",
        "            elif sub_option == \"No_filter\": \n",
        "                uncertain_samples = error_reduction_sampling(self.query_size, X_unlabeled, X_labeled, y_labeled, self.clf_model.model_object.classifier, X_fair_est, y_fair_est, b_fair_est, probas_val, self.step, self.criteria)\n",
        "            elif sub_option == \"Filter_only\":\n",
        "                uncertain_samples = self.sample_selection_function.pool_select(probas_val, self.step)\n",
        "\n",
        "            # print(\"Debug shape of X_unlabeled and loss:\", selection)\n",
        "\n",
        "            # uncertain_samples = self.sample_selection_function.pool_select(probas_val, self.step)\n",
        "\n",
        "            X_labeled = np.concatenate((X_labeled, X_unlabeled[uncertain_samples]))\n",
        "            y_labeled = np.concatenate((y_labeled, y_unlabeled[uncertain_samples]))\n",
        "            X_unlabeled = np.delete(X_unlabeled, uncertain_samples, axis=0)\n",
        "            y_unlabeled = np.delete(y_unlabeled, uncertain_samples, axis=0)\n",
        "\n",
        "            (X_labeled, X_test) = self.clf_model.train(X_labeled, y_labeled, X_test, y_test)\n",
        "            fairness = np.append(fairness, fair_measure(X_test, y_test, b_test, classifier=self.clf_model.model_object.classifier, criteria=self.criteria))\n",
        "            self.clf_model.get_test_accuracy(active_iteration)\n",
        "\n",
        "        return self.clf_model.accuracies, fairness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mRDsjGmLbnm"
      },
      "source": [
        "def non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled,y_labeled, X_test, y_test, b_test, budget, step, unfairness_criteria): \n",
        "\n",
        "    nonal_X_train = np.concatenate((X_unlabeled, X_labeled))\n",
        "    nonal_y_train = np.concatenate((y_unlabeled, y_labeled))\n",
        "    nonal_X_test = X_test\n",
        "    nonal_y_test = y_test\n",
        "    nonal_b_test = b_test\n",
        "    nonal_fairness = []\n",
        "\n",
        "    nonal_accuracies=[]\n",
        "\n",
        "    classifier_nonal = LogisticRegression(\n",
        "            solver='liblinear'\n",
        "            )\n",
        "\n",
        "    for i in np.arange(init_index,budget+1,step):\n",
        "        classifier_nonal.fit(nonal_X_train[:i], nonal_y_train[:i])\n",
        "        nonal_y_pred = classifier_nonal.predict(nonal_X_test)\n",
        "        nonal_fairness = np.append(nonal_fairness, fair_measure(nonal_X_test, nonal_y_test, nonal_b_test, classifier=classifier_nonal, criteria=unfairness_criteria))\n",
        "        nonal_accuracies.append(accuracy_score(nonal_y_test, nonal_y_pred)*100)\n",
        "\n",
        "    return nonal_accuracies, nonal_fairness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWADBKJuLbnn"
      },
      "source": [
        "def experiment(model,sampling_method,budget,step,criteria,sub_option=False):\n",
        "    \n",
        "    (X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est) = retrieve_data_recid()\n",
        "    init_index = len(X_labeled)\n",
        "        \n",
        "    act_alg = active_learning(step, budget, model, sampling_method, criteria)\n",
        "\n",
        "    accuracies, fairness = act_alg.run(X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, sub_option)\n",
        "\n",
        "    # print('Fairness:', fairness)\n",
        "\n",
        "    nonal_accuracies, nonal_fairness = non_active_learning(init_index, X_unlabeled, y_unlabeled, X_labeled, y_labeled, X_test, y_test, b_test, budget, step, criteria)\n",
        "\n",
        "    # print(\"active_accuracies\",accuracies)\n",
        "    # print(\"nonactive_accuracies\",nonal_accuracies)\n",
        "    \n",
        "    x_axis = np.arange(init_index,budget+1,step)\n",
        "    fig, (ax1, ax2) = plt.subplots(2)\n",
        "    fig.suptitle('Fairness and accuracy metrics')\n",
        "    ax1.plot(x_axis, fairness, color='r', label='active')\n",
        "    ax1.plot(x_axis, nonal_fairness, color='b', label='non-active')\n",
        "    ax1.legend()\n",
        "    ax2.plot(x_axis, accuracies, color='r', label='active')\n",
        "    ax2.plot(x_axis, nonal_accuracies, color='b', label='non-active')\n",
        "    ax2.legend()\n",
        "    ax1.set_xlabel('Sample size')\n",
        "    ax1.set_ylabel('Unfairness')\n",
        "    ax2.set_ylabel('Accuracies')\n",
        "\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIVN9P7JLbnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "df9b27bd-e6c6-44ff-f6f9-211d27a7ed27"
      },
      "source": [
        "experiment(LogModel,RandomSelection,1000,100,\"equalized_odds\",\"No_filter\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------\n",
            "Iteration: 1\n",
            "Accuracy rate is 57.629428 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-965458352739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRandomSelection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"equalized_odds\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"No_filter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-834bddfe0bf3>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(model, sampling_method, budget, step, criteria, sub_option)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mact_alg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfairness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_option\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# print('Fairness:', fairness)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-81e67412d376>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, X_labeled, y_labeled, b_labeled, row_size, col_size, X_unlabeled, y_unlabeled, b_unlabeled, X_test, y_test, b_test, X_fair_est, y_fair_est, b_fair_est, sub_option)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0muncertain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_reduction_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fair_est\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msub_option\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"No_filter\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0muncertain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_reduction_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msub_option\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Filter_only\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0muncertain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_selection_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobas_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-fbbdf2f83e05>\u001b[0m in \u001b[0;36merror_reduction_sampling\u001b[0;34m(query_size, X_unlabeled, X_labeled, y_labeled, classifier, X_fair_est, y_fair_est, b_fair_est, probas_val, step, criteria)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0my_labeled_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mclassifier_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mf_loss_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_loss_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfair_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mf_loss_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_loss_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d371f04ac663>\u001b[0m in \u001b[0;36mfair_measure\u001b[0;34m(X_fair_est, y_fair_est, b_fair_est, classifier, criteria)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdisp_mist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcriteria\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'equalized_odds'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meqods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-fca31a771cec>\u001b[0m in \u001b[0;36meqods\u001b[0;34m(X_fair_est, y_fair_est, b_fair_est, classifier)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meqods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_fair_est\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0my_fair_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mb0y0p1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_fair_est\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_fair_est\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fair_est\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fair_pred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 276\u001b[0;31m                                  dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}